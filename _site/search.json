[
  {
    "objectID": "posts/Python/4. Class/python 11_0511.html",
    "href": "posts/Python/4. Class/python 11_0511.html",
    "title": "파이썬 (0511) 11주차",
    "section": "",
    "text": "!pip install pillow\n\nCollecting pillow\n  Downloading Pillow-9.4.0-cp37-cp37m-manylinux_2_28_x86_64.whl (3.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 57.2 MB/s eta 0:00:00a 0:00:01\nInstalling collected packages: pillow\nSuccessfully installed pillow-9.4.0\n- 예제1\n- 예제2"
  },
  {
    "objectID": "posts/Python/4. Class/python 11_0511.html#클래스-사용법",
    "href": "posts/Python/4. Class/python 11_0511.html#클래스-사용법",
    "title": "파이썬 (0511) 11주차",
    "section": "클래스 사용법",
    "text": "클래스 사용법\n- 클래스를 선언\n\nclass STOOOP:\n    title=\"학교폭력\"\n    url= url1\n    end=\"멈춰~~~~\"\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end)\n\n\n규칙1: 메소드(=class안에 정의된 함수)의 첫번째 인자는 무조건 self\n규칙2: 메소드에서 class 안에 정의된 변수들(title, url, end)를 사용하려면 “self.변수이름”과 같은 형식으로 쓴다.\n\n즉 “self.title”, “self.url”, “self.end”와 같은 방식으로 써야한다.\n\n참고: 규칙2에서 가끔 self의 자리에 “STOOOP.title”, “STOOOP.url”, “STOOOP.end”와 같이 클래스의 이름으로 쓰기도 한다.\n\n- 클래스 사용에서\n- (예시1) STOOOP 클래스 -> school instance를 만드는 과정\n\nschool=STOOOP()\n\n\nschool.stop()\n\n학교폭력\n\n\n\n\n\n멈춰~~~~\n\n\n- (예시2) STOOOP 클래스 -> kospi 인스턴스를 만듬\n\nkospi=STOOOP()\n\n\nkospi.title=\"KOSPI 하락\"\n\n\nkospi.stop()\n\nKOSPI 하락\n\n\n\n\n\n멈춰~~~~\n\n\n\n클래스의 성능\n- 성능1: 클래스에서 인스터스를 생성\n\nschool = STOOOP()\nkospi = STOOOP()\n\n\n함수의 사용법과 비슷하다.\n클래스 이름을 쓰고 콘텐츠를 구체화 하는 과정에서 필요한 입력1, 입력2를 ()에 넣는다. 이때는 STOOOP(입력1, 입력2)와 같이 생성\n위의 예시는 따로 입력이 없으므로 비워둔 상태임. 즉 STOOOP()와 같은 식으로 생성.\n\n- 성능2: 클래스에서 만들어진 인스턴스는 그 내부에 변수를 따로 가지고 있는데, 그것을 독립적으로 출력 혹은 변경할 수 있다.\n\nschool.title #출ㄺ\n\n'학교폭력'\n\n\n\nkospi.title #출력\n\n'학교폭력'\n\n\n\nkospi.title = '코스피하락' #변경\n\n\nkospi.title\n\n'코스피하락'\n\n\n- 성능3: 클래스에서 만들어진 인스턴스는 그 내부에 자체적인 함수를 가지는데, 이것을 사용할 수 있다.\n\nschool.stop()\n\n학교폭력\n\n\n\n\n\n멈춰~~~~\n\n\n\nkospi.stop()\n\n코스피하락\n\n\n\n\n\n멈춰~~~~\n\n\n\n\n연습문제\n문제1 아래의 클래스를 구현하라.\n- 클래스내에는 변수 a가 있고, 변수 a의 초기값은 True이다.\n- 클래스에는 show()라는 메소드가 있는데, a의 값을 출력하는 기능을 한다.\n(풀이)\n\nclass Klass1:\n    a = True\n    def show(self):\n        print(self.a)\n\n\nex1=Klass1()\n\n\nex1.a\n\nTrue\n\n\n\nex1.show()\n\nTrue\n\n\n문제2 아래의 클래스를 구현하라.\n- 클래스내에는 변수 a가 있고, 변수 a의 초기값은 1이다.\n- 클래스에는 up()라는 메소드가 있는데, a의 값을 1증가시키는 기능을 한다.\n(풀이)\n\nclass Klass2:\n    a=1\n    def up(self):\n        self.a = self.a + 1\n\n\nex2=Klass2()\n\n\nex2.a\n\n1\n\n\n\nex2.up()\n\n\nex2.a\n\n2\n\n\n\nex2.up()\nex2.up()\nex2.up()\nex2.up()\nex2.a\n\n6\n\n\n문제3 아래의 클래스를 구현하라.\n- 클래스내에는 변수 a가 있고, 변수 a의 초기값은 0이다.\n- 클래스에는 up(),down(), show()라는 메소드가 있는데, 각각은 a의 값을 1증가시키고, 1감소시키고, a의 값을 출력하는 기능을 한다.\n(풀이)\n\nclass Klass3:\n    a=0\n    def up(self):\n        self.a=self.a+1\n        \n    def down(self):\n        self.a=self.a-1\n        \n    def show(self):\n        print(self.a)\n\n\nex3=Klass3()\n\n\nex3.show()\n\n0\n\n\n\nex3.up()\nex3.show()\n\n1\n\n\n\nex3.down()\nex3.show()\n\n0\n\n\n문제4 아래의 클래스를 구현하라.\n- 클래스내에는 변수 url이 있고, 초기값은 url1이다.\nurl1:’https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true’\n- 클래스에는 show()라는 메소드를 가지는데, 아래와 같은 기능을 한다. - 기능1: url의 그림을 출력 - 기능2: ‘당신은 이 그림을 \\(n\\)번 보았습니다.’ 출력.\\(n\\)은 그림을 본 횟수\n\nurl1\n\n'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\n\n\n(풀이)\n\nclass Klass4:\n    n = 1\n    url = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\n    def show(self):\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(\"당신은 이 이미지를 {}번 보았습니다\".format(self.n))\n        self.n = self.n+1 \n\n\na=Klass4()\n\n\na.show()\n\n\n\n\n당신은 이 이미지를 3번 보았습니다\n\n\n\nb=Klass4()\nb.url=url2\n\n\nb.show() # a와 독립적으로..\n\n\n\n\n당신은 이 이미지를 2번 보았습니다\n\n\n\n\n숙제\n\nimport numpy as np\n\n\nnp.random.choice([\"가위\",\"바위\",\"보\"])\n\n'보'\n\n\n\nclass homework:\n    def show(self):\n        self.work=np.random.choice([\"가위\",\"바위\",\"보\"])\n        print(self.work)\n\n\nwork = homework()\n\n\nwork.show()\n\n가위"
  },
  {
    "objectID": "posts/Python/4. Class/python 13_0530.html",
    "href": "posts/Python/4. Class/python 13_0530.html",
    "title": "파이썬 (0530) 13주차",
    "section": "",
    "text": "- 상속\n\n\n- 아래와 같은 클래스를 만들자. - 이름, 직급, 연봉에 대한 정보가 있다. - 연봉을 올려주는 메소드가 존재\n\nclass Employee:\n    def __init__(self, name, position=None, pay=0):\n        self.name = name\n        self.position = position\n        self.pay = pay\n    def _repr_html_(self):\n        html_str = \"\"\"\n        이름: {} <br/>\n        직급: {} <br/>\n        연봉: {} <br/>\n        \"\"\".format(self.name, self.position, self.pay)\n        return html_str\n    def giveraise(self,pct):\n        self.pay = self.pay * (1+pct)\n        \n\n- 확인\n\niu = Employee('iu', position='staff', pay=5000)\nhynn = Employee('hynn', position='staff', pay=4000)\nhd = Employee('hodonh', position='mgr', pay=8000)\n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5000 \n        \n\n\n\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4000 \n        \n\n\n\nhd\n\n\n        이름: hodonh \n        직급: mgr \n        연봉: 8000 \n        \n\n\n\niu.giveraise(0.1)\n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn.giveraise(0.2)\n\n\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4800.0 \n        \n\n\n- 회사의 모든 직원의 연봉을 10%씩 올려보자.\n\niu = Employee('iu', position='staff', pay=5000)\nhynn = Employee('hynn', position='staff', pay=4000)\nhd = Employee('hodonh', position='mgr', pay=8000)\n\n\nfor i in [iu, hynn, hd]:\n    print(i.name)\n\niu\nhynn\nhodonh\n\n\n\nfor i in [iu, hynn, hd]:\n    i.giveraise(0.1) # 일괄적으로 상승\n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4400.0 \n        \n\n\n\nhd\n\n\n        이름: hodonh \n        직급: mgr \n        연봉: 8800.0 \n        \n\n\n- 매니저 직급은 일반직원들의 상승분에서 5%의 보너스가 추가되어 상승한다고 가정\n- 모든회사 직원들의 연봉을 10% 상승\n(구현1) if문을 통한\n\niu = Employee('iu', position='staff', pay=5000)\nhynn = Employee('hynn', position='staff', pay=4000)\nhd = Employee('hodonh', position='mgr', pay=8000)\n\n\nfor i in [iu,hynn,hd]:\n    if i.position == 'mgr':\n        i.giveraise(0.1 + 0.05)\n    else:\n        i.giveraise(0.1)\n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4400.0 \n        \n\n\n\nhd\n\n\n        이름: hodonh \n        직급: mgr \n        연봉: 9200.0 \n        \n\n\n(구현2) 새로운 클래스를 만들자\n\nclass Manager:\n    def __init__(self, name, position=None, pay=0):\n        self.name = name\n        self.position = position\n        self.pay = pay\n    def _repr_html_(self):\n        html_str = \"\"\"\n        이름: {} <br/>\n        직급: {} <br/>\n        연봉: {} <br/>\n        \"\"\".format(self.name, self.position, self.pay)\n        return html_str\n    def giveraise(self,pct):\n        self.pay = self.pay * (1+pct+0.05)\n        \n\n\niu = Employee('iu', position='staff', pay=5000)\nhynn = Employee('hynn', position='staff', pay=4000)\nhd = Manager('hodonh', position='mgr', pay=8000)\n\n\nfor i in [iu,hynn,hd]:\n    i.giveraise(0.1)\n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4400.0 \n        \n\n\n\nhd\n\n\n        이름: hodonh \n        직급: mgr \n        연봉: 9200.000000000002 \n        \n\n\n- 구현3: 이미 만들어진 클래스에서\n\nclass Manager(Employee):\n    # 나머지 기타 함수내용은 Emplyee 클래스와 같음걸 표현하려면 위 가로에 Employee를 작성한다.\n    def giveraise(self,pct):\n        self.pay = self.pay * (1+pct+0.05)\n        \n\n\nhd=Manager('hodong',pay=8000)\nhd  # 명시하지 않았는데 상속됨\n\n\n        이름: hodong \n        직급: None \n        연봉: 8000 \n        \n\n\n\niu = Employee('iu', position='staff', pay=5000)\nhynn = Employee('hynn', position='staff', pay=4000)\nhd = Manager('hodonh', position='mgr', pay=8000)\n\n\nfor i in [iu,hynn,hd]:\n    i.giveraise(0.1)\n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4400.0 \n        \n\n\n\nhd\n\n\n        이름: hodonh \n        직급: mgr \n        연봉: 9200.000000000002 \n        \n\n\n- 요약: 이미 만들어진 클래스에서 대부분의 기능은 그대로 쓰지만 일부기능만 변경 혹은 추가하고 싶다면 클래스를 상속하면 된다!\n\n\n\n\n내가 만든 클래스를 계속 상속하는 경우\n\n- list 와 비슷한데 멤버들의 빈도가 계산되는 메소드를 포함하는 새로운 나만의 list를 만들자\n\nlst = ['a','b','a','c','b','a','d']\nlst\n\n['a', 'b', 'a', 'c', 'b', 'a', 'd']\n\n\n- 아래와 같은 딕셔너리를 만들고 싶다.\n\nfreq = {'a':3, 'b':2, 'c':1, 'd':1}  # 갯수\nfreq\n\n{'a': 3, 'b': 2, 'c': 1, 'd': 1}\n\n\n\nlst.frequency()를 입력하면 위의 기능이 수행되도록 변형된 list를 쓰고 싶다.\n\n- 구현\n(시도1) 절반 성공\n\nfreq = {'a':0, 'b':0, 'c':0, 'd':0}   # 일단 다 0이라 생각하고 코드 짜기 \nfreq\n\n{'a': 0, 'b': 0, 'c': 0, 'd': 0}\n\n\n\nfor item in lst:\n    print(item)\n\na\nb\na\nc\nb\na\nd\n\n\n\nfor item in lst:\n    print(freq[item])\n\n0\n0\n0\n0\n0\n0\n0\n\n\n\nfor item in lst:\n    freq[item] = freq[item] + 1\n\n\nfreq\n\n{'a': 3, 'b': 2, 'c': 1, 'd': 1}\n\n\n반쯤 성공.. 리스트가 a,b,c,d 라는걸 알고 있어야 함\n(시도2) 실패\n\nlst\n\n['a', 'b', 'a', 'c', 'b', 'a', 'd']\n\n\n\nfreq = dict()\nfreq\n\n{}\n\n\n\nfor item in lst:\n    freq[item] = freq[item] + 1\n\nKeyError: 'a'\n\n\n\nfreq['a']  # 매칭되는게 없다!\n\nKeyError: 'a'\n\n\n에러이유? freq['a'] 를 호출할 수 없다 -> freq.get('a',0)을 이용\n\nfreq.get('a')   # 4주차 3월 23일 복습  get메소드를 사용하면 없어도 에러를 표시하지 않음\n\n\nfreq.get?\n\n\nSignature: freq.get(key, default=None, /)\nDocstring: Return the value for key if key is in the dictionary, else default.\nType:      builtin_function_or_method\n\n\n\n\n\nkey에 대응하는 값이 있으면 그 값을 리턴하고 없으면 default를 리턴\n\n\nfreq.get('a',0)  # a값 없으면 0으로 리턴\n\n0\n\n\n(시도3)\n\nlst\n\n['a', 'b', 'a', 'c', 'b', 'a', 'd']\n\n\n\nfreq = dict()\nfreq\n\n{}\n\n\n\nfor item in lst:\n    freq[item] = freq.get(item,0) + 1\n\n\nfreq\n\n{'a': 3, 'b': 2, 'c': 1, 'd': 1}\n\n\n- 이것을 내가 정의하는 새로운 list의 메소드로 넣고 싶다.\n\nclass L(list):   # L 이라는 클래스에 list에 있는 모든걸 상속받겠다.\n    pass\n\n\na=[1,2,3]\na\n\n[1, 2, 3]\n\n\n\na?\n\n\n\nType:        list\nString form: [1, 2, 3]\nLength:      3\nDocstring:  \nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\n\n\n\n\n\nclass L(list):\n    def frequency(self):\n        freq = dict()\n        for item in self:\n            freq[item] = freq.get(item,0) + 1\n        return freq\n\n\nlst = L([1,1,1,2,2,3])\n\n\nlst?  \n\nSyntaxError: invalid syntax (<ipython-input-96-8d8983ef4faf>, line 1)\n\n\n\n # 리스트 같아 보이지만 타입이 L! 내가 설정한 클래스\n\n\nlst  #원래 list에 있는 repr 기능을 상속받아서 이루어지는 결과\n\n[1, 1, 1, 2, 2, 3]\n\n\n\n_lst = L([4,5,6])\n_lst + _lst   # L자로형끼리 덧셈\n\n[4, 5, 6, 4, 5, 6]\n\n\n\nlst + [4,5,6]   # l자료형과  list 자료형의 덧셈도 가능\n\n[1, 1, 1, 2, 2, 3, 10, 10, 4, 5, 6]\n\n\n\nL자료형의 덧셈은 list의 덧셈과 완전히 같음\n\n\nlst.append(10)\nlst   # 요론 기본적인 리스트 기능도 가능\n\n[1, 1, 1, 2, 2, 3, 10, 10]\n\n\n\nlst.frequency()  # 리스트에서 이것 기능만 추가된거랑 똑같고 나머지는 다 리스트랑 똑같다\n\n{1: 3, 2: 2, 3: 1}"
  },
  {
    "objectID": "posts/Python/4. Class/python 13_0530.html#appendix-사용자-정의-자료형의-유용함",
    "href": "posts/Python/4. Class/python 13_0530.html#appendix-사용자-정의-자료형의-유용함",
    "title": "파이썬 (0530) 13주차",
    "section": "Appendix: 사용자 정의 자료형의 유용함",
    "text": "Appendix: 사용자 정의 자료형의 유용함\n- 사용자정의 자료형이 어떤 경우에는 유용할 수 있다.\n\n!pip install matplotlib\n\nCollecting matplotlib\n  Downloading matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.2/11.2 MB 97.5 MB/s eta 0:00:00a 0:00:01\nCollecting fonttools>=4.22.0\n  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 965.4/965.4 kB 81.8 MB/s eta 0:00:00\nRequirement already satisfied: packaging>=20.0 in /home/koinup4/anaconda3/envs/py37/lib/python3.7/site-packages (from matplotlib) (23.0)\nRequirement already satisfied: python-dateutil>=2.7 in /home/koinup4/anaconda3/envs/py37/lib/python3.7/site-packages (from matplotlib) (2.8.2)\nCollecting pyparsing>=2.2.1\n  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.3/98.3 kB 18.6 MB/s eta 0:00:00\nCollecting kiwisolver>=1.0.1\n  Downloading kiwisolver-1.4.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 82.9 MB/s eta 0:00:00\nRequirement already satisfied: numpy>=1.17 in /home/koinup4/anaconda3/envs/py37/lib/python3.7/site-packages (from matplotlib) (1.21.6)\nCollecting cycler>=0.10\n  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\nRequirement already satisfied: pillow>=6.2.0 in /home/koinup4/anaconda3/envs/py37/lib/python3.7/site-packages (from matplotlib) (9.4.0)\nRequirement already satisfied: typing-extensions in /home/koinup4/anaconda3/envs/py37/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (4.4.0)\nRequirement already satisfied: six>=1.5 in /home/koinup4/anaconda3/envs/py37/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\nInstalling collected packages: pyparsing, kiwisolver, fonttools, cycler, matplotlib\nSuccessfully installed cycler-0.11.0 fonttools-4.38.0 kiwisolver-1.4.4 matplotlib-3.5.3 pyparsing-3.0.9\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n- 예제1\n\nyear = ['2016', '2017', '2017', '2017', 2017, 2018, 2018, 2019, 2019]\nvalue = np.random.randn(9)\n\n\ndf = pd.DataFrame({'year':year, 'value':value})\ndf\n\n\n\n\n\n  \n    \n      \n      year\n      value\n    \n  \n  \n    \n      0\n      2016\n      0.729447\n    \n    \n      1\n      2017\n      0.110630\n    \n    \n      2\n      2017\n      0.108119\n    \n    \n      3\n      2017\n      -0.095107\n    \n    \n      4\n      2017\n      -0.337716\n    \n    \n      5\n      2018\n      -0.134635\n    \n    \n      6\n      2018\n      -2.182677\n    \n    \n      7\n      2019\n      -0.150227\n    \n    \n      8\n      2019\n      0.849774\n    \n  \n\n\n\n\n\nplt.plot(df.year, df.value)\n\nTypeError: 'value' must be an instance of str or bytes, not a int\n\n\n\n\n\n\ndf.year\n\n0    2016\n1    2017\n2    2017\n3    2017\n4    2017\n5    2018\n6    2018\n7    2019\n8    2019\nName: year, dtype: object\n\n\n\ndtype이 object 로 되어있어서 그림이 그려지지 않는다.. float로 되어야 할텐데?\n에러의 이유: df.year에 str, int가 동시에 있음\n\n\nnp.array(df.year)\n\narray(['2016', '2017', '2017', '2017', 2017, 2018, 2018, 2019, 2019],\n      dtype=object)\n\n\n\n자료형의 형태를 바꿔주면 해결할 수 있다.\n\n\nnp.array(df.year, dtype=np.float64)\n\narray([2016., 2017., 2017., 2017., 2017., 2018., 2018., 2019., 2019.])\n\n\n\nnp.array(df.year).astype(np.float64) # 위와 같은 효과\n\narray([2016., 2017., 2017., 2017., 2017., 2018., 2018., 2019., 2019.])\n\n\n\ndf.year.astype(np.float64)  # 위와 같은 효과\n\n0    2016.0\n1    2017.0\n2    2017.0\n3    2017.0\n4    2017.0\n5    2018.0\n6    2018.0\n7    2019.0\n8    2019.0\nName: year, dtype: float64\n\n\n\nplt.plot(df.year.astype(np.float64), df.value,'.')\n\n\n\n\n- 예제2\n\nyear = ['2016', '2017', '2017', '2017년', 2017, 2018, 2018, 2019, 2019]\nvalue = np.random.randn(9)\n\n\ndf = pd.DataFrame({'year':year, 'value':value})\ndf\n\n\n\n\n\n  \n    \n      \n      year\n      value\n    \n  \n  \n    \n      0\n      2016\n      -0.254312\n    \n    \n      1\n      2017\n      0.839603\n    \n    \n      2\n      2017\n      -1.386845\n    \n    \n      3\n      2017년\n      0.010756\n    \n    \n      4\n      2017\n      0.949379\n    \n    \n      5\n      2018\n      0.280954\n    \n    \n      6\n      2018\n      -0.227516\n    \n    \n      7\n      2019\n      -1.100002\n    \n    \n      8\n      2019\n      0.152285\n    \n  \n\n\n\n\n\nnp.array(df.year, dtype=np.float64) # \"년\"이 써있어서 타입을 일괄적으로 바꾸기 어렵다.\n\nValueError: could not convert string to float: '2017년'\n\n\n\ndf.year   # 어떤 값이 있는지 확인\n\n0     2016\n1     2017\n2     2017\n3    2017년\n4     2017\n5     2018\n6     2018\n7     2019\n8     2019\nName: year, dtype: object\n\n\n\nnp.unique(df.year)   # 섞여있는 타입에서는 unique는 동작하지 않는다.\n\nTypeError: '<' not supported between instances of 'int' and 'str'\n\n\n\nL(df.year).frequency()\n\n{'2016': 1, '2017': 2, '2017년': 1, 2017: 1, 2018: 2, 2019: 2}\n\n\n\n’2016’과 같은 형태, ’2017년’과 같은 형태, 숫자형이 혼합 .. 이라는 파악 가능 -> 맞춤형 변환이 필요함\n\n\n'2017년'.replace(\"년\",\"\")\n\n'2017'\n\n\n\ndef f(a):    # 데이터의 구조를 모르면 이런 함수를 짤 수가 없다. -> 자료의 구조를 확인해준다는 의미에서 freq가 있다면 편리하다.\n    if type(a) is str:\n        if \"년\" in a:\n            return int(a.replace(\"년\",\"\"))\n        else:\n            return int(a)\n    else:\n        return a\n\n\n[f(a) for a in df.year]\n\n[2016, 2017, 2017, 2017, 2017, 2018, 2018, 2019, 2019]\n\n\n\ndf.year = [f(a) for a in df.year]\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      year\n      value\n    \n  \n  \n    \n      0\n      2016\n      -0.254312\n    \n    \n      1\n      2017\n      0.839603\n    \n    \n      2\n      2017\n      -1.386845\n    \n    \n      3\n      2017\n      0.010756\n    \n    \n      4\n      2017\n      0.949379\n    \n    \n      5\n      2018\n      0.280954\n    \n    \n      6\n      2018\n      -0.227516\n    \n    \n      7\n      2019\n      -1.100002\n    \n    \n      8\n      2019\n      0.152285\n    \n  \n\n\n\n\n\nplt.plot(df.year, df.value, '.')"
  },
  {
    "objectID": "posts/Python/4. Class/python 10_0509.html",
    "href": "posts/Python/4. Class/python 10_0509.html",
    "title": "파이썬 (0509) 10주차",
    "section": "",
    "text": "# jupyter notebook을 통한 ppt발표(슬라이드)가 가능. 관련 프로그램을 깔아야한다."
  },
  {
    "objectID": "posts/Python/4. Class/python 10_0509.html#밈meme과-클래스",
    "href": "posts/Python/4. Class/python 10_0509.html#밈meme과-클래스",
    "title": "파이썬 (0509) 10주차",
    "section": "밈(Meme)과 클래스",
    "text": "밈(Meme)과 클래스\n\n신혜선의 어쩔티비\n- 밈이란? 유전자처럼 복제가능한 something"
  },
  {
    "objectID": "posts/Python/4. Class/python 10_0509.html#클래스",
    "href": "posts/Python/4. Class/python 10_0509.html#클래스",
    "title": "파이썬 (0509) 10주차",
    "section": "클래스",
    "text": "클래스\n- 클래스에 대한 비유적 설명 (implicit definition)\n\n클래스는 과자틀과 비슷하다. 클래스란 똑같은 무엇인가를 계속 만들어 낼 수도 있는 설계도면이고 객체란 클래스로 만든 피조물을 뜻한다. (점프투파이썬)\nIn object-oriented programming, a class is an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions or methods). // 객체 지향 프로그래밍에서 클래스는 상태(멤버 변수) 및 동작 구현(멤버 함수 또는 메서드)에 대한 초기 값을 제공하는 객체 생성을 위한 확장 가능한 프로그램 코드 템플릿입니다.\nhttp://www.tcpschool.com/java/java_class_intro\nhttps://javacpro.tistory.com/29\nhttps://ko.wikipedia.org/wiki/%ED%81%B4%EB%9E%98%EC%8A%A4_(%EC%BB%B4%ED%93%A8%ED%84%B0_%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D)\n\n- 클래스에 대한 명시적 정의 (교수님 생각)\n\n클래스는 복제, 변형, 재생산을 용이하게 하기 위해 만들어진 확장가능한 프로그램의 코드의 단위(extensible program-code-template)이다. 즉 밈이다.\n\n- 클래스도 결국 밈이다. 생각해보면 클래스를 만들고 사용하는 과정이 인터넷에서 밈을 만들고 노는것과 닮아 있다.\n\n1단계: 개념의 인지 (이거 재미있겠다 밈으로 만들자 // 이 코드 쓸모있다, 이 코드를 쉽게 찍어내는 클래스로 만들어두자)\n2단계: 복사하고 싶은 속성을 추려 복사가능한 틀을 만듬 (밈 초기 컨텐츠 // 클래스의 선언)\n3단계: 밈에서 다양한 컨텐츠를 재생산, 때로는 변형하여 재생산, 때로는 그것을 응용한 다른밈을 만듬 (밈화 // 클래스의 인스턴스화, 상속, 메소드오버라이딩)"
  },
  {
    "objectID": "posts/Python/4. Class/python 10_0509.html#멈춰-밈을-컨텐츠화",
    "href": "posts/Python/4. Class/python 10_0509.html#멈춰-밈을-컨텐츠화",
    "title": "파이썬 (0509) 10주차",
    "section": "“멈춰” 밈을 컨텐츠화",
    "text": "“멈춰” 밈을 컨텐츠화\n- 멈춰 밈을 이용하여 코스피하락, 수강신청 매크로 등 다양한 예제를 만들자\n\nfrom IPython.core.display import HTML\n\n\n예비학습\n\n문자열포맷팅 (문자열끼워넣기)\n- 예제1\n\n'제 이름은 {}입니다.'.format('보람')\n\n'제 이름은 보람입니다.'\n\n\n-예제2\n\n'제 이름은 {}이고 사는 곳은 {}입니다.'.format('보람','전주')\n\n'제 이름은 보람이고 사는 곳은 전주입니다.'\n\n\n\n'제 이름은 {}이고 사는 곳은 {}입니다.'.format('전주','보람')\n\n'제 이름은 전주이고 사는 곳은 보람입니다.'\n\n\n-예제3\n\n'제 이름은 {name}이고 사는 곳은 {add}입니다.'.format(name='보람',add='전주')\n\n'제 이름은 보람이고 사는 곳은 전주입니다.'\n\n\n\n'제 이름은 {name}이고 사는 곳은 {add}입니다.'.format(add='전주',name='보람')\n\n'제 이름은 보람이고 사는 곳은 전주입니다.'\n\n\n\n\nHTML\n-예제1\n\nHTML(\"<p> 이름 </p>\")\n\n 이름 \n\n\n-예제2\n\nHTML(\"<img src='https://stat.jbnu.ac.kr/sites/stat/atchmnfl_mngr/imageSlide/469/temp_1573001043314100.jpg'>\")\n\n\n\n\n- 예제3\n\nHTML(\"<p> 전북대학교 </p><img src='https://stat.jbnu.ac.kr/sites/stat/atchmnfl_mngr/imageSlide/469/temp_1573001043314100.jpg'>\")\n\n 전북대학교 \n\n\n\n\nHTML을 이용한 밈생성\n- 밈을 위한 이미지 주소\n\nurl1='https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\nurl2='https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop2.png?raw=true'\n\n- 예제1: 원본\n\nhtmlstr.format(title='학교폭력',url=url1,end='멈춰~~~~')\n\n\"<p> 학교폭력 </p> <img src='https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'> <p> 멈춰~~~~ </p>\"\n\n\n\nhtmlstr = \"<p> {title} </p> <img src='{url}'> <p> {end} </p>\"\nHTML(htmlstr.format(title='학교폭력',url=url1,end='멈춰~~~~'))\n\n 학교폭력    멈춰~~~~ \n\n\n- 예제2: 코스피하락 멈춰어\n\nHTML(htmlstr.format(title='코스피하락',url=url1,end='멈춰~~~~'))\n\n 코스피하락    멈춰~~~~ \n\n\n- 예제3: 매크로 멈춰어\n\nHTML(htmlstr.format(title='수강신청매크로',url=url1,end='멈춰~~~~'))\n\n 수강신청매크로    멈춰~~~~ \n\n\n\n\n\n함수를 만들어서 코드를 관리\n- 함수의 선언\n\ndef stop():\n    htmlstr = \"<p> {title} </p> <img src='{url}'> <p> {end} </p>\"\n    display(HTML(htmlstr.format(title=ttl,url=url,end=end)))   #display로 받아주는게 좋다\n    \n\n- 사용\n\nttl = '돈쓰는거'\nurl = url1\nend = '멈춰 ㅠ'\nstop()\n\n 돈쓰는거    멈춰 ㅠ \n\n\n\nttl = '술담배'\nurl = url1\nend = '멈춰!'\nstop()\n\n 술담배    멈춰! \n\n\n\nttl = '코코 주워먹는거'\nurl = url2\nend = '멈춰!!!!!'\nstop()\n\n 코코 주워먹는거    멈춰!!!!! \n\n\n\n\n클래스를 만들어서 관리\n\nclass STOOOP: #STOOOP은 양식문서의 이름이라 생각하자.\n    title = \"학교폭력\"\n    url = url1\n    end = \"멈춰~~~~\"\n    def stop(self):  # 규칙1: class안에서 정의된 함수는 첫번째 입력으로 무조건 self를 받는다.\n        htmlstr = \"<p> {title} </p> <img src='{url}'> <p> {end} </p>\"\n        display(HTML(htmlstr.format(title=self.title,url=self.url,end=self.end))) \n        # 규칙2: class안에서 정의된 변수 (title, url, end)를 쓰려면 \"self.변수이름\"의 형태로 써야함\n\n\nt=1 학교폭력멈춰\n\nschool = STOOOP()   \n\n# STOOOP이라는 이름의 양식문서를 복사해 하나의 hwp 파일을 만들어 밈을 생성하고 그 파일이름을 school이라고 하자.\n# 그러니가 STOOP.hwp 와 school.hwp가 잇다..\n\n\nschool.title\n\n'학교폭력'\n\n\n\nschool.url\n\n'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\n\n\n\nschool.end\n\n'멈춰~~~~'\n\n\n\nschool.stop()\n\n 학교폭력    멈춰~~~~ \n\n\n\n\nt=2 코스피하락멈춰\n\nkospi = STOOOP() # 코스피하락 멈춰를 위해 STOOP.hwp양식문서에서 하나의 밈을 찍어낸다. (kospi.hwp)\nkospi.title = '코스피하락' #제목변경\n\n\nkospi.stop()\n\n 코스피하락    멈춰~~~~ \n\n\n\n\nt=3 수강신청매크로 멈춰\n\nmacro = STOOOP()\n\n\nmacro.title, macro.url, macro.end\n\n('학교폭력',\n 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true',\n '멈춰~~~~')\n\n\n\nmacro.title = \"수강신청매크로\"\n\n\nmacro.stop()\n\n 수강신청매크로    멈춰~~~~ \n\n\n\n\nt=4 수강신청 매크로 멈춰 끝 물결대신 느낌표\n\nmacro.end = \"멈춰!!!!!!!!\"\n\n\nmacro.title, macro.url, macro.end\n\n('수강신청매크로',\n 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true',\n '멈춰!!!!!!!!')\n\n\n\nmacro.stop()\n\n 수강신청매크로    멈춰!!!!!!!! \n\n\n\n\nt=5 코스피하락 다시 출력 (함수와 비교했을때 실수 발생x)\n\nkospi.stop()\n\n 코스피하락    멈춰~~~~ \n\n\n\n\nt=6 학교폭력 멈춰와 코스피하락 멈춰 동시에 출력\n\nschool.stop()\n\n 학교폭력    멈춰~~~~ \n\n\n\nkospi.stop()\n\n 코스피하락    멈춰~~~~ \n\n\n\n\nt=7 “학교폭력 멈춰”의 이미지를 신혜선으로 변경, “코스피하락 멈춰”의 title을 ’KOSPI하락’으로 변경\n\nschool.url = url2\nkospi.title = \"KOSPI하락\"\n\n\nschool.stop()\n\n 학교폭력    멈춰~~~~ \n\n\n\nkospi.stop()\n\n KOSPI하락    멈춰~~~~ \n\n\n\n\n\n숙제\n“수강신청 멈춰”의 이미지를 신혜선으로 변경하고 출력해볼 것\nmacro.url 변경 macro.stop() 을 사용\n\nmacro.url=url2\n\n\nmacro.stop()\n\n 수강신청매크로    멈춰!!!!!!!!"
  },
  {
    "objectID": "posts/Python/4. Class/python 12_0523.html",
    "href": "posts/Python/4. Class/python 12_0523.html",
    "title": "파이썬 (0523) 12주차",
    "section": "",
    "text": "import numpy as np"
  },
  {
    "objectID": "posts/Python/4. Class/python 12_0523.html#클래스공부-4단계",
    "href": "posts/Python/4. Class/python 12_0523.html#클래스공부-4단계",
    "title": "파이썬 (0523) 12주차",
    "section": "클래스공부 4단계",
    "text": "클래스공부 4단계\n\nMotivating Example\n- 가위바위보\n\n방법1\n\n\nclass RPC2:\n    def throw2(self):\n        print(np.random.choice(['가위', '바위','보']))\n\n\na=RPC2()\n\n\na.throw2()\n\n바위\n\n\n\n방법2\n\n\nclass RPC:\n    def thorw(self, candidate):\n        print(np.random.choice(candidate))\n\n\na=RPC()\n\n\na.thorw(['가위','바위','보'])\n\n바위\n\n\n\n방법3\n\n\nclass RPC3:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n    def throw3(self):\n        print(np.random.choice(self.candidate))\n\n\na=RPC3()   # __init__ 는 암묵적으로 실행\n\n\na.throw3()\n\n바위\n\n\n\n방법4\n\n\nclass RPC4:\n    pass\n\n\nb=RPC4()\n\n\ndef initt(b, candidate=['가위','바위','보']):\n    b.candidate = candidate\n\n\ninitt(b)\n\n\nb.candidate\n\n['가위', '바위', '보']\n\n\n\ndef throww(b):\n        print(np.random.choice(b.candidate))\n\n\nthroww(b)\n\n가위\n\n\n\n방법5\n\n\n# 위의 코드를 하나로 합치면..\n\n\nclass RPC4:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n    def throww(self):\n        print(np.random.choice(self.candidate))\n\n\na=RPC4()\n\n\na.throww()\n\n가위\n\n\n- 생각해보니까 throw는 choose + show 의 결합인 것 같다.\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n    def choose(self):\n        self.actions = np.random.choice(self.candidate)\n    def show(self):\n        print(self.actions)\n\n\na=RPC()\n\n\na.actions()   # 지금은 정의되지 않음\n\nAttributeError: 'RPC' object has no attribute 'actions'\n\n\n\na.choose()\n\n\na.actions  # 가위, 바위, 보 중 고른 결과가 나옴\n\n'가위'\n\n\n\na.show()\n\n가위\n\n\n보충학습 : 위와 같은 코드\n\nclass _RPS: ## 시점1\n    pass # <- 이렇게하면 아무기능이 없는 비어있는 클래스가 정의된다\n\n\n_a = _RPS() ## 시점2\ndef _init(_a,candidate=['가위','바위','보']):\n    _a.candidate = candidate \n_init(_a)\n\n\n_a.actions ## 시점3\n\nAttributeError: '_RPS' object has no attribute 'actions'\n\n\n\ndef _choose(_a): ## 시점4\n    _a.actions = np.random.choice(_a.candidate)\n_choose(_a)\n\n\n_a.actions ## 시점5\n\n'보'\n\n\n\ndef _show(_a): ## 시점6\n    print(_a.actions)\n_show(_a)\n\n보\n\n\n- 또 다른 인스턴스 b를 만들자. b는 가위만 낼 수 있다.\n\nRPC?\n\n\nInit signature: RPC(candidate=['가위', '바위', '보'])\nDocstring:      <no docstring>\nType:           type\nSubclasses:     \n\n\n\n\n\nb=RPC(['가위'])\n\n\nb.candidate\n\n['가위']\n\n\n\nb.choose()\nb.show()\n\n가위\n\n\n- a,b의 선택들을 모아서 기록을 하고 싶다.\n\nclass RPS:\n    def __init__(self,candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list() \n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])   # 지금 현재 내가 선택한 마지막만 보여줘!\n\n\na=RPS()\nb=RPS(['가위'])\n\n\nfor i in range(5):\n    a.choose()\n    a.show()\n\n바위\n가위\n바위\n바위\n보\n\n\n\na.actions   # 지금까지 뽑힌 히스토리들\n\n['바위', '가위', '바위', '바위', '보']\n\n\n\nfor i in range(5):\n    b.choose()\n    b.show()\n\n가위\n가위\n가위\n가위\n가위\n\n\n\nb.actions\n\n['가위', '가위', '가위', '가위', '가위']\n\n\n\na.candidate, a.actions\n\n(['가위', '바위', '보'], ['바위', '가위', '바위', '바위', '보'])\n\n\n\nb.candidate, b.actions\n\n(['가위'], ['가위', '가위', '가위', '가위', '가위'])\n\n\n- info라는 함수를 만들어서 a의 오브젝트가 가지고 있는 정보를 모두 보도록 하자.\n(예비학습) 문자열 \\n이 포함된다면?\n\n'asdf\\n1234'\n\n'asdf\\n1234'\n\n\n\nprint('asdf\\n1234')\n\nasdf\n1234\n\n\n예비학습 끝\n\nclass RPS: \n    def __init__(self,candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list() \n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def info(self):\n        print(\"낼 수 있는 패: {}\\n기록: {}\".format(self.candidate,self.actions))\n\n\na=RPS()\nb=RPS(['가위'])\n\n\nfor i in range(5):\n    a.choose()\n    a.show()    \n\n가위\n바위\n가위\n보\n보\n\n\n\nfor i in range(5):\n    b.choose()\n    b.show()\n\n가위\n가위\n가위\n가위\n가위\n\n\n\na.info()\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: ['가위', '바위', '가위', '보', '보']\n\n\n\nb.info()\n\n낼 수 있는 패: ['가위']\n기록: ['가위', '가위', '가위', '가위', '가위']\n\n\n- 만들고 보니까 info와 print의 기능이 거의 비슷함 \\(\\to\\) print(a)를 하면 a.info()와 동일한 효과를 내도록 만들 수 있을까?\n- 안될 거 같다. 왜?\n\n안될 것 같은 이유 1: print 는 파이썬 내장기능, 내장기능을 우리가 맘대로 커스터마이징해서 쓰기는 어려울 것 같다.\n안될 것 같은 이유 2: 이유1이 해결된다고 해도 문제다. 다 꼬아져버려… 그럼 지금까지 사용했던 print()의 결과는 어떻게 되는가?\n\n(예)\n\ntype(a)\n\n__main__.RPS\n\n\n\na?\n\n\nType:        RPS\nString form: <__main__.RPS object at 0x7f6565f29e10>\nDocstring:   <no docstring>\n\n\n\n\n- 그런데 a의 자료형(RPS자료형)에 해당하는 오브젝트들에 한정하여 print를 수정하는 방법이 가능하다면? (그럼 다른 오브젝트들은 수정된 print에 영향을 받지 않음)\n\n\n__str___\n- 관찰1: 현재 print(a)의 결과는 아래와 같다.\n\nprint(a)\n\n<__main__.RPS object at 0x7f6565f29e10>\n\n\n\nprint([1,2,3])\n\n[1, 2, 3]\n\n\n\na는 RPS클래스에서 만든 오브젝트이며 a가 저장된 메모리 주소는 0x7f6565f29e10라는 의미\n\n- 관찰2: a에는 __str__ 이 있다.\n\ndir(a)   # a + _ + tab을 누르면 숨겨진 메소드들이 나온다.\n\n['__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n 'actions',\n 'candidate',\n 'choose',\n 'info',\n 'show']\n\n\n\nset(dir(a)) & {'__str__'}\n\n{'__str__'}\n\n\n\na.__str__\n\n<method-wrapper '__str__' of RPS object at 0x7f6565f29e10>\n\n\n이것을 함수처럼 사용하니까 아래와 같이 된다.\n\na.__str__()\n\n'<__main__.RPS object at 0x7f6565f29e10>'\n\n\n?? print(a)를 해서 나오는 문자열이 리턴된다..\n\nprint(a.__str__()) # 이거 print(a)를 실행한 결과와 같다?\n\n<__main__.RPS object at 0x7f6565f29e10>\n\n\n- 생각: 만약 내가 a.__str__() 라는 함수를 재정의하여 리턴값을 boram hahaha로 바꾸게 되면 print(a)해서 나오는 결과는 어떻게 될까? (해커???)\n(예비학습)\n\ndef f():\n    print('adsf')\n\n\nf()\n\nadsf\n\n\n\ndef f():\n    print('boram hahaha')\n\n\nf()\n\nboram hahaha\n\n\n이런식으로 함수가 이미 정의되어 있더라도, 내가 나중에 덮어씌우면 그 함수의 기능을 다시 정의한다.\n(해킹시작)\n\nclass RPS: \n    def __init__(self,candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list() \n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __str__(self):\n        return 'boram hahaha'\n    def info(self):\n        print(\"낼 수 있는 패: {}\\n기록: {}\".format(self.candidate,self.actions))\n\n\na=RPS()\n\n\nprint(a)\n\nboram hahaha\n\n\n\nprint(a.__str__())\n\nboram hahaha\n\n\n\n# 다른건 다 변함이 없음\n\n\na.choose()\na.show()\n\n가위\n\n\n\na.actions\n\n['가위']\n\n\n\na.info()\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: ['가위']\n\n\n- __str__의 리턴값을 info에서 타이핑했던 문자열로 재정의 한다면?\n\nclass RPS: \n    def __init__(self,candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list() \n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __str__(self):\n        return \"낼 수 있는 패: {}\\n기록: {}\".format(self.candidate,self.actions)\n\n\na=RPS()\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\na.choose()\na.show()\n\n바위\n\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: ['보', '바위']\n\n\n\n\n파이썬의 비밀2\n- print(a)와 print(a.__str__()) 는 같은 문법이다.\n- 참고로 a.__Str__() 와 str(a) 도 같은 문법\n\na.__str__()\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: ['보', '바위']\"\n\n\n\nstr(a)\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: ['보', '바위']\"\n\n\n- 지금까지 썼던 기능들 확인!\n(예제1)\n\na=[1,2,3]\nprint(a)\n\n[1, 2, 3]\n\n\n\na.__str__()\n\n'[1, 2, 3]'\n\n\n\nstr(a)\n\n'[1, 2, 3]'\n\n\n(예제2)\n\na={1,2,3}\nprint(a)\n\n{1, 2, 3}\n\n\n\na.__str__()\n\n'{1, 2, 3}'\n\n\n\nstr(a)\n\n'{1, 2, 3}'\n\n\n(예제3)\n\na=np.array(1)\na.shape\n\n()\n\n\n\ntype(a.shape)\n\ntuple\n\n\n\nprint(a.shape)\n\n()\n\n\n\na.shape.__str__()\n\n'()'\n\n\n\nstr(a.shape)\n\n'()'\n\n\n(예제4)\n\na = range(10)\nprint(a)\n\nrange(0, 10)\n\n\n\na.__str__()\n\n'range(0, 10)'\n\n\n(예제5)\n\na = np.arange(100).reshape(10,10)\nprint(a)\n\n[[ 0  1  2  3  4  5  6  7  8  9]\n [10 11 12 13 14 15 16 17 18 19]\n [20 21 22 23 24 25 26 27 28 29]\n [30 31 32 33 34 35 36 37 38 39]\n [40 41 42 43 44 45 46 47 48 49]\n [50 51 52 53 54 55 56 57 58 59]\n [60 61 62 63 64 65 66 67 68 69]\n [70 71 72 73 74 75 76 77 78 79]\n [80 81 82 83 84 85 86 87 88 89]\n [90 91 92 93 94 95 96 97 98 99]]\n\n\n\na.__str__()\n\n'[[ 0  1  2  3  4  5  6  7  8  9]\\n [10 11 12 13 14 15 16 17 18 19]\\n [20 21 22 23 24 25 26 27 28 29]\\n [30 31 32 33 34 35 36 37 38 39]\\n [40 41 42 43 44 45 46 47 48 49]\\n [50 51 52 53 54 55 56 57 58 59]\\n [60 61 62 63 64 65 66 67 68 69]\\n [70 71 72 73 74 75 76 77 78 79]\\n [80 81 82 83 84 85 86 87 88 89]\\n [90 91 92 93 94 95 96 97 98 99]]'\n\n\n\nstr(a)\n\n'[[ 0  1  2  3  4  5  6  7  8  9]\\n [10 11 12 13 14 15 16 17 18 19]\\n [20 21 22 23 24 25 26 27 28 29]\\n [30 31 32 33 34 35 36 37 38 39]\\n [40 41 42 43 44 45 46 47 48 49]\\n [50 51 52 53 54 55 56 57 58 59]\\n [60 61 62 63 64 65 66 67 68 69]\\n [70 71 72 73 74 75 76 77 78 79]\\n [80 81 82 83 84 85 86 87 88 89]\\n [90 91 92 93 94 95 96 97 98 99]]'\n\n\n\n\n__repr__\n- 생각해보니까 print를 해서 원하는 정보를 확인하는 건 아니었음\n\na=[1,2,3]\n\n\na\n\n[1, 2, 3]\n\n\n\nprint(a)   # print(a.__str__()) + enter => a + enter 와 같은 효과?\n\n[1, 2, 3]\n\n\n- a + 엔터를 하면 print(a) + 엔터를 하는 것과 같은 효과인가?\n(반례)\n\na=np.array([1,2,3,4]).reshape(2,2)\n\n\na\n\narray([[1, 2],\n       [3, 4]])\n\n\n\nprint(a)\n\n[[1 2]\n [3 4]]\n\n\n- a + 엔터를 하면 print(a) + 엔터가 다른 경우도 있다. \\(\\to\\) 서로 다른 숨겨진 기능이 잇다! \\(\\to\\) 결론 : 그 기능은 __repr__에 저장되어 있다.\n\nclass RPS: \n    def __init__(self,candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list() \n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __repr__(self):\n        return \"낼 수 있는 패: {}\\n기록: {}\".format(self.candidate,self.actions)\n\n\na=RPS()\n\n\na  # print(a.__repr__())\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n- 그럼 지금까지 한것은?\n\na = np.array([1,2,3])\n\n\na\n\narray([1, 2, 3])\n\n\n\nprint(a)\n\n[1 2 3]\n\n\n\na.__repr__()\n\n'array([1, 2, 3])'\n\n\n\na.__str__()\n\n'[1 2 3]'\n\n\n\n\n파이썬의 비밀3\n- 대화형 콘솔에서 오브젝트 이름 + 엔터 를 쳐서 나오는 출력은 __repr__ 의 결과와 연관 있다.\n\na = np.array(range(10000)).reshape(100,100)\n\n\na\n\narray([[   0,    1,    2, ...,   97,   98,   99],\n       [ 100,  101,  102, ...,  197,  198,  199],\n       [ 200,  201,  202, ...,  297,  298,  299],\n       ...,\n       [9700, 9701, 9702, ..., 9797, 9798, 9799],\n       [9800, 9801, 9802, ..., 9897, 9898, 9899],\n       [9900, 9901, 9902, ..., 9997, 9998, 9999]])\n\n\n\na.__repr__()\n\n'array([[   0,    1,    2, ...,   97,   98,   99],\\n       [ 100,  101,  102, ...,  197,  198,  199],\\n       [ 200,  201,  202, ...,  297,  298,  299],\\n       ...,\\n       [9700, 9701, 9702, ..., 9797, 9798, 9799],\\n       [9800, 9801, 9802, ..., 9897, 9898, 9899],\\n       [9900, 9901, 9902, ..., 9997, 9998, 9999]])'\n\n\n- 참고로 a.__repr__() 은 representation의 약자인데, repr(a)와 같다.\n\n\n주피터 노브북의 비밀 (__repr__html__)\n- 요즘에는 IDE의 발전에 따라서 오브젝트이름+엔터 칠때 나오는 출력의 형태도 다양해지고 있다.\n\nimport pandas as pd\n\n\ndf = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      1\n      2\n    \n    \n      1\n      2\n      3\n    \n    \n      2\n      3\n      4\n    \n  \n\n\n\n\n\n예쁘게 나온다.\n\n- 그런데? print(df.__repr__())의 결과가 조금 다르게 나온당\n\nprint(df.__repr__())\n\n   a  b\n0  1  2\n1  2  3\n2  3  4\n\n\n- print(df.__repr__()) 는 예전 검은화면에서 코딩할 때가 나오는 출력임\nPython 3.10.2 | packaged by conda-forge | (main, Feb  1 2022, 19:28:35) [GCC 9.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n> >> import pandas as pd \n>>> df = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})>>> df\n   a  b\n0  1  2\n1  2  3\n2  3  4\n>>>\n- 주피터에서는? “오브젝트이름+엔터”치면 HTML(df._repr_html_())이 실행되고, _repr_html_()이 정의되어 있지 않으면 print(df.__repr__())이 실행된다.\n\ndf._repr_html_()\n\n'<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>a</th>\\n      <th>b</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>1</td>\\n      <td>2</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>2</td>\\n      <td>3</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>3</td>\\n      <td>4</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'\n\n\n\nhtml코드\n\n\nfrom IPython.core.display import HTML\n\n\nHTML('<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>a</th>\\n      <th>b</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>1</td>\\n      <td>2</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>2</td>\\n      <td>3</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>3</td>\\n      <td>4</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>')\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      1\n      2\n    \n    \n      1\n      2\n      3\n    \n    \n      2\n      3\n      4\n    \n  \n\n\n\n\n\nHTML(df._repr_html_())\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      1\n      2\n    \n    \n      1\n      2\n      3\n    \n    \n      2\n      3\n      4\n    \n  \n\n\n\n\n- 물론 df._repr_html_() 함수가 내부적으로 있어도 html이 지원되지 않는 환경이라면 print(__repr__())이 내부적으로 수행된다.\n\n\n__repr__와 __str__의 우선적용 순위\n(예제1)\n\nclass RPS: \n    def __init__(self,candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list() \n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __repr__(self):\n        return \"낼 수 있는 패: {}\\n기록: {}\".format(self.candidate,self.actions)\n\n\na=RPS()\na\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\na.__repr__()\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n\nrepr(a)\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n- 여기까지는 상식수준의 결과. 아래 관찰하자\n\nprint(a) # print(a.__str__())\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\na.__str__()\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n\nstr(a)\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n- __str__()은 건드린적이 없다…?\n\na.__repr__??\n\n\nSignature: a.__repr__()\nDocstring: Return repr(self).\nSource:   \n    def __repr__(self):\n        return \"낼 수 있는 패: {}\\n기록: {}\".format(self.candidate,self.actions)\nFile:      ~/Dropbox/coco/posts/python/<ipython-input-201-29baf6ff56bf>\nType:      method\n\n\n\n\n얘는 건드림\n\na.__str__??\n\n\nSignature:      a.__str__()\nCall signature: a.__str__(*args, **kwargs)\nType:           method-wrapper\nString form:    <method-wrapper '__str__' of RPS object at 0x7f6561614c10>\nDocstring:      Return str(self).\n\n\n\n\n얘는 안건드렸는디..\n(예제2)\n\nclass RPS: \n    def __init__(self,candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list() \n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __str__(self):\n        return \"낼 수 있는 패: {}\\n기록: {}\".format(self.candidate,self.actions)\n\n\na=RPS()\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\na\n\n<__main__.RPS at 0x7f6561429950>\n\n\n\na.__str__()\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n\na.__repr__()\n\n'<__main__.RPS object at 0x7f6561429950>'\n\n\n\na.__str__??\n\n\nSignature: a.__str__()\nDocstring: Return str(self).\nSource:   \n    def __str__(self):\n        return \"낼 수 있는 패: {}\\n기록: {}\".format(self.candidate,self.actions)\nFile:      ~/Dropbox/coco/posts/python/<ipython-input-214-cd2a21868510>\nType:      method\n\n\n\n\n\na.__repr_??\n\nObject `a.__repr_` not found.\n\n\n(예제3)\n\nclass RPS: \n    def __init__(self,candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list() \n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __repr__(self):\n        return \"haha\"\n    def __str__(self):\n        return \"낼 수 있는 패: {}\\n기록: {}\".format(self.candidate,self.actions)\n\n\na=RPS()\n\n\na\n\nhaha\n\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n- __str__ 와 __repr__ 을 건드리지 않고 출력결과를 바꾸고 싶다면?\n\nclass RPS: \n    def __init__(self,candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list() \n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} <br/> \n        기록: {}\n        \"\"\"\n        return html_str.format(self.candidate,self.actions)\n\n\na=RPS()\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보']  \n        기록: []\n        \n\n\n\nprint(a)\n\n<__main__.RPS object at 0x7f6561534410>\n\n\n\nstr(a)\n\n'<__main__.RPS object at 0x7f6561534410>'\n\n\n\nrepr(a)\n\n'<__main__.RPS object at 0x7f6561534410>'\n\n\n\nfor i in range(5):\n    a.choose()\n    a.show()\n\n바위\n바위\n바위\n가위\n보\n\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보']  \n        기록: ['바위', '바위', '바위', '가위', '보']"
  },
  {
    "objectID": "posts/Python/4. Class/python 13_0525.html",
    "href": "posts/Python/4. Class/python 13_0525.html",
    "title": "파이썬 (0525) 13주차",
    "section": "",
    "text": "import numpy as np"
  },
  {
    "objectID": "posts/Python/4. Class/python 13_0525.html#클래스-공부-5단계",
    "href": "posts/Python/4. Class/python 13_0525.html#클래스-공부-5단계",
    "title": "파이썬 (0525) 13주차",
    "section": "클래스 공부 5단계",
    "text": "클래스 공부 5단계\n- 지난시간까지 배운것: RPC자료형에 한정해서 print() 등의 기능을 조작할 수 있었다. (재정의 할 수 있었다.)\n- 이번시간에 배울것: 특정자료형에 한정하여 print 이외의 파이썬 내부기능을 조작하여 보자. (재정의하여 보자)\n\nmotive\n\na=1\nb=2\n\n\ntype(a)\n\nint\n\n\n\na+b\n\n3\n\n\n\na라는 인스턴스와 b라는 인스턴스를 + 라는 기호가 연결하고 있다.\n\n\na=[1,2]\nb=[3,4]\na+b\n\n[1, 2, 3, 4]\n\n\n\na라는 인스턴스와 b라는 인스턴스를 + 라는 기호가 연결하고 있다.\n\n- 동작이 다른 이유?\n\n클래스를 배우기 이전: int자료형의 + 는 “정수의 덧셈”을 의미하고 list 자료형의 +는 “자료의 추가”를 의미한다.\n클래스를 배운 이후: 아마 클래스는 + 라는 연산을 정의하는 숨겨진 메소드가 있을 것이다. (print가 그랬듯이) 그런데 int 클래스에서는 그 메소드를 “정수의 덧셈”이 되도록 정의하였고 list클래스에서는 그 메소드를 “자료의 추가”를 의미하도록 정의하였다.\n\n\na=1\nb=2\n\n\na.__add__\n\n<method-wrapper '__add__' of int object at 0x70c560>\n\n\n\ndir(a)\n\n['__abs__',\n '__add__',\n '__and__',\n '__bool__',\n '__ceil__',\n '__class__',\n '__delattr__',\n '__dir__',\n '__divmod__',\n '__doc__',\n '__eq__',\n '__float__',\n '__floor__',\n '__floordiv__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getnewargs__',\n '__gt__',\n '__hash__',\n '__index__',\n '__init__',\n '__init_subclass__',\n '__int__',\n '__invert__',\n '__le__',\n '__lshift__',\n '__lt__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__neg__',\n '__new__',\n '__or__',\n '__pos__',\n '__pow__',\n '__radd__',\n '__rand__',\n '__rdivmod__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__rfloordiv__',\n '__rlshift__',\n '__rmod__',\n '__rmul__',\n '__ror__',\n '__round__',\n '__rpow__',\n '__rrshift__',\n '__rshift__',\n '__rsub__',\n '__rtruediv__',\n '__rxor__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__sub__',\n '__subclasshook__',\n '__truediv__',\n '__trunc__',\n '__xor__',\n 'bit_length',\n 'conjugate',\n 'denominator',\n 'from_bytes',\n 'imag',\n 'numerator',\n 'real',\n 'to_bytes']\n\n\n\na.__add__(b)\n\n3\n\n\n\nb.__add__(a)\n\n3\n\n\n\na=[1,2]\nb=[3,4]\n\n\na.__add__(b)\n\n[1, 2, 3, 4]\n\n\n\nb.__add__(a)\n\n[3, 4, 1, 2]\n\n\n- a+b는 사실 내부적으로 a.__add__(b)의 축약구문이다. 따라서 먄악 a.__add__(b)의 기능을 바꾸면 (재정의하면) a+b의 기능도 바뀔 것이다.\n\n\n__add__\n- 예제\n\nclass Student:\n    def __init__(self, age=20.0, semester=1):\n        self.age = age\n        self.semester = semester\n        print(\"입학을 축하합니다. 나이는 {}이고 현재 {}학기 입니다.\".format(self.age, self.semester))\n    def __add__(self,val): \n        # val == 0: 휴학 \n        # val == 1: 등록 \n        if val==0: \n            self.age=self.age+0.5\n        elif val==1:\n            self.age=self.age+0.5 \n            self.semester= self.semester+1 \n    def _repr_html_(self):\n        html_str = \"\"\"\n        나이: {}<br/>\n        학기: {}<br/>\n        \"\"\"\n        return html_str.format(self.age, self.semester)\n\n\niu = Student()\n\n입학을 축하합니다. 나이는 20.0이고 현재 1학기 입니다.\n\n\n\niu.semester\n\n1\n\n\n\niu  # 클래스가 저장되어있는 주소를 _repr_html_ 통해서 바꿔즘\n\n\n        나이: 20.0\n        학기: 1\n        \n\n\n\niu + 1 #1학년 2학기 등록\niu\n\n\n        나이: 20.5\n        학기: 2\n        \n\n\n\niu + 0 # 휴학\niu\n\n\n        나이: 21.0\n        학기: 2\n        \n\n\n- 연산을 연속으로 하고 싶다.\n\niu + 1 + 0 + 0 + 0 + 0\n\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n\n\n- 에러의 이유?\n\n1+1+1 #이거는 되는데?\n\n3\n\n\n\n(1+1)+1\n\n3\n\n\n\n_a = (1+1)\ntype(_a)\n\nint\n\n\n\n_a + 1    # 이 연산은 int인스턴스 + int인스턴스 \n\n3\n\n\n(안되는거)\n\niu+1+1\n\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n\n\n\n_a=iu+1\n\n\ntype(_a)\n\nNoneType\n\n\n\n_a+1\n\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n\n\n- 에러를 해결하는 방법: iu+1의 결과로 Student 클래스의 인스턴스가 리턴되면 된다.\n\nclass Student:\n    def __init__(self, age=20.0, semester=1):\n        self.age = age\n        self.semester = semester\n        print(\"입학을 축하합니다. 나이는 {}이고 현재 {}학기 입니다.\".format(self.age, self.semester))\n    def __add__(self,val): \n        # val == 0: 휴학 \n        # val == 1: 등록 \n        if val==0: \n            self.age=self.age+0.5\n        elif val==1:\n            self.age=self.age+0.5 \n            self.semester= self.semester+1 \n        return self\n    def _repr_html_(self):\n        html_str = \"\"\"\n        나이: {}<br/>\n        학기: {}<br/>\n        \"\"\"\n        return html_str.format(self.age, self.semester)\n\n\niu = Student()\n\n입학을 축하합니다. 나이는 20.0이고 현재 1학기 입니다.\n\n\n\niu + 1   # __add__의 return에 Student클래스의 인스턴스가 리턴되면서 자동으로 __repr_html_()실행\n\n\n        나이: 23.0\n        학기: 4\n        \n\n\n\niu + 1 + 0 + 0 + 0 \n\n\n        나이: 25.0\n        학기: 5\n        \n\n\n\n\n__mul__\n\na=1\nb=1\na*b\n\n1\n\n\n\na.__mul__\n\n<method-wrapper '__mul__' of int object at 0x70c560>\n\n\n\nclass RPS: \n    def __init__(self,candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list() \n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} <br/> \n        기록: {}\n        \"\"\"\n        return html_str.format(self.candidate,self.actions)\n\n\na=RPS()\nb=RPS()\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보']  \n        기록: []\n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['가위', '바위', '보']  \n        기록: []\n        \n\n\n\na*b 해서 승패를 확인하기 위한 클래스를 만들자\n\n\nclass RPS: \n    def __init__(self,candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list() \n        self.results = list()\n    def __mul__(self,other):\n        self.choose()\n        other.choose()\n        if self.actions[-1]=='가위' and other.actions[-1]=='가위':\n            self.results.append(0)\n            other.results.append(0)\n        if self.actions[-1]=='가위' and other.actions[-1]=='바위':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1]=='가위' and other.actions[-1]=='보':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1]=='바위' and other.actions[-1]=='가위':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1]=='바위' and other.actions[-1]=='바위':\n            self.results.append(0)\n            other.results.append(0)\n        if self.actions[-1]=='바위' and other.actions[-1]=='보':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1]=='보' and other.actions[-1]=='가위':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1]=='보' and other.actions[-1]=='바위':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1]=='보' and other.actions[-1]=='보':\n            self.results.append(0)\n            other.results.append(0)\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} <br/> \n        액션: {} <br/>\n        승패: {}\n        \"\"\"\n        return html_str.format(self.candidate,self.actions,self.results)\n\n\na=RPS()\nb=RPS()\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보']  \n        액션: [] \n        승패: []\n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['가위', '바위', '보']  \n        액션: [] \n        승패: []\n        \n\n\n\nfor i in range(5):\n    a*b\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보']  \n        액션: ['보', '바위', '가위', '가위', '바위'] \n        승패: [1, 0, -1, 0, 1]\n        \n\n\n\n\nb\n\n\n        낼 수 있는 패: ['가위', '바위', '보']  \n        액션: ['바위', '바위', '바위', '가위', '가위'] \n        승패: [-1, 0, 1, 0, -1]"
  },
  {
    "objectID": "posts/Python/4. Class/python 13_0525.html#숙제",
    "href": "posts/Python/4. Class/python 13_0525.html#숙제",
    "title": "파이썬 (0525) 13주차",
    "section": "숙제",
    "text": "숙제\nRPS클래스에서 player a와 player b를 만들어라. Player a는 [‘가위’,‘보’] 중에 하나를 낼 수 있다. 그리고 Player b는 [‘가위’,‘바위’] 중에 하나를 낼 수 있다. 두 player는 가지고 있는 패를 (같은확률로) 랜덤으로 낸다. (즉 player a가 가위만 내거나 보만 내는 경우는 없다.)\n\n누가 더 유리한가? 이유를 스스로 생각해보라. (이유를 정리하여 숙제로 제출할 필요 없음)\n50000번의 시뮬레이션을 해보고 결과를 분석해보라.\n\n\nclass RPS: \n    def __init__(self,candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list() \n        self.results = list()\n    def __mul__(self,other):\n        self.choose()\n        other.choose()\n        if self.actions[-1]=='가위' and other.actions[-1]=='가위':\n            self.results.append(0)\n            other.results.append(0)\n        if self.actions[-1]=='가위' and other.actions[-1]=='바위':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1]=='보' and other.actions[-1]=='가위':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1]=='보' and other.actions[-1]=='바위':\n            self.results.append(1)\n            other.results.append(-1)\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} <br/> \n        액션: {} <br/>\n        승패: {}\n        \"\"\"\n        return html_str.format(self.candidate,self.actions,self.results)\n\n\na=RPS(['가위','보'])\nb=RPS(['가위','바위'])\n\n\nfor i in range(50000):\n    a*b\n\n\nsum(a.results)\n\n-12358\n\n\n\nsum(b.results)\n\n12358"
  },
  {
    "objectID": "posts/Python/4. Class/python 12_0518.html",
    "href": "posts/Python/4. Class/python 12_0518.html",
    "title": "파이썬 (0518) 12주차",
    "section": "",
    "text": "- 클래스 오브젝트에 소속된 변수와 인스턴스오브젝트에 소속된 변수를 설명한다.\n\n\n- 파이썬은 모든 것이 오브젝트로 이루어져 있다. <- 우선은 그냥 명언처럼 외우자\n- 오브젝트는 메모리 주소에 저장되는 모든 것을 의미한다.\n\na=1\nid(a)  # 메모리주소를 보는 명령어\n\n7390560\n\n\n\na='asdf'\nid(a)\n\n139914601692912\n\n\n\na=[1,2,3]\nid(a)\n\n139914601744176\n\n\n- 클래스와 인스턴스도 오브젝트이다.\n\nclass A:\n    x=0\n    def f(self):\n        print(self.x)\n\n\nA는 오브젝트\n\n\nid(A)\n\n37700928\n\n\n\na는 오브젝트\n\n\na=A()\nid(a)\n\n139914601844368\n\n\n\nb는 오브젝트\n\n\nb=A()\nid(b)\n\n139914601854160\n\n\n- 앞으로는 A를 클래스오브젝트, a,b를 인스턴스 오브젝트라고 부르자.\n\n\n\n- 시점0\n\nclass A:\n    x=0\n    y=0\n    def f(self):\n        self.x=self.x + 1\n        A.y = A.y + 1\n        # self.y = self.y + 1 이렇게 안쓰고 위에처럼 써보자!\n        print(\"현재 인스턴스에서 f가 {}번 실행\".format(self.x))\n        print(\"A클래스에서 만들어진 모든 인스턴스들에서 f가 총 {}번 실행\".format(self.y))\n\n\nid(A)\n\n38051088\n\n\n\nA.x, A.y\n\n(0, 0)\n\n\n- 시점1\n\na= A()\nb= A()\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 0], [0, 0], [0, 0])\n\n\n- 시점2\n\na.f()\n\n현재 인스턴스에서 f가 1번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 1번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 1], [1, 1], [0, 1])\n\n\n- 시점3\n\nb.f()\n\n현재 인스턴스에서 f가 1번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 2번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 2], [1, 2], [1, 2])\n\n\n- 시점4\n\nb.f()\n\n현재 인스턴스에서 f가 2번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 3번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 3], [1, 3], [2, 3])\n\n\n- 시점5\n\na.f()\n\n현재 인스턴스에서 f가 2번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 4번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 4], [2, 4], [2, 4])\n\n\n- 시점6\n\nc=A()\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y], [c.x, c.y]\n\n([0, 4], [2, 4], [2, 4], [0, 4])\n\n\n- 시점7\n\nc.f()\n\n현재 인스턴스에서 f가 1번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 5번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y], [c.x, c.y]\n\n([0, 5], [2, 5], [2, 5], [1, 5])\n\n\n- 신기한점: 각 인스턴스에서 인스턴스이름.f()를 실행한 횟수를 서로 공유하는 듯 하다. (A가 관리하는 것처럼 느껴진다.)\n- x와 y는 약간 느낌이 다르다. x는 지점소속, y는 본사소속의 느낌?\n\n이 예제에서 x는 인스턴스오브젝트에 소속된 변수, y는 클래스 오브젝트에 소속된 변수처럼 느껴짐\n\n(약속) 앞으로는 인스턴스 오브젝트에 소속된 변수를 인스턴스 변수라고 하고, 클래스 오브젝트에 소속된 변수를 클래스 변수라고 하자.\n- 인스턴스 변수와 클래스 변수를 구분하는 방법? 인스턴스이름.__dict__를 쓰면 인스턴스 변수만 출력된다.\n\n따라서 a. + tab을 눌러서 나오는 변수중 a.__dict__에 출력되지 않으면 클래스 변수이다.\n\n\na.__dict__\n\n{'x': 2}\n\n\n\nb.__dict__\n\n{'x': 2}\n\n\n\nc.__dict__\n\n{'x': 1}\n\n\n- 이 예제에서 아래는 모두 클래스 변수이다.\n\na.y, b.y, c.y\n\n(5, 5, 5)\n\n\n\n\n\n- 시점0\n\nclass A:\n    x=0\n    y=0\n    def f(self):\n        self.x=self.x + 1\n        A.y = A.y + 1\n        # self.y = self.y + 1 이렇게 안쓰고 위에처럼 써보자!\n        print(\"현재 인스턴스에서 f가 {}번 실행\".format(self.x))\n        print(\"A클래스에서 만들어진 모든 인스턴스들에서 f가 총 {}번 실행\".format(self.y))\n\n\na=A()\n\n\n[A.x, A.y], [a.x, a.y]\n\n([0, 0], [0, 0])\n\n\n- 시점1\n\na.f()\n\n현재 인스턴스에서 f가 1번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 1번 실행\n\n\n\na.f()\n\n현재 인스턴스에서 f가 2번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 2번 실행\n\n\n\na.f()\n\n현재 인스턴스에서 f가 3번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 3번 실행\n\n\n\n[A.x,A.y], [a.x, a.y]\n\n([0, 3], [3, 3])\n\n\n- 시점2\n\na.x = 0 # f의 실행기록을 초기화 하고 싶다.\n\n\na.f()\n\n현재 인스턴스에서 f가 1번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 4번 실행\n\n\n\n[A.x,A.y], [a.x, a.y]\n\n([0, 4], [1, 4])\n\n\n\n\n\n\nclass A:\n    x=0\n    y=0\n    def f(self):\n        self.x=self.x + 1\n        A.y = A.y + 1\n        # self.y = self.y + 1 이렇게 안쓰고 위에처럼 써보자!\n        print(\"현재 인스턴스에서 f가 {}번 실행\".format(self.x))\n        print(\"A클래스에서 만들어진 모든 인스턴스들에서 f가 총 {}번 실행\".format(self.y))\n\n\na=A()\n\n\nb=A()\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 0], [0, 0], [0, 0])\n\n\n- 시점1\n\na.f()\n\n현재 인스턴스에서 f가 1번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 1번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 1], [1, 1], [0, 1])\n\n\n- 시점2\n\nA.y = 100\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 100], [1, 100], [0, 100])\n\n\n\na.f()\n\n현재 인스턴스에서 f가 2번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 101번 실행\n\n\n\n\n\n\nclass A:\n    x=0\n    y=0\n    def f(self):\n        self.x=self.x + 1\n        A.y = A.y + 1\n        # self.y = self.y + 1 이렇게 안쓰고 위에처럼 써보자!\n        print(\"현재 인스턴스에서 f가 {}번 실행\".format(self.x))\n        print(\"A클래스에서 만들어진 모든 인스턴스들에서 f가 총 {}번 실행\".format(self.y))\n\n\na=A()\n\n\n[A.x, A.y], [a.x, a.y]\n\n([0, 0], [0, 0])\n\n\n- 시점1\n\na.f()\n\n현재 인스턴스에서 f가 1번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 1번 실행\n\n\n\n[A.x, A.y], [a.x, a.y]\n\n([0, 1], [1, 1])\n\n\n- 시점2\n\nA.x = 100   # 이렇게 되면 앞으로 만들어진 인스턴스튼 기본적으로 현재 인스턴스에서| 100번 f를 실행하였다는 정보를 가지고 태어나게 된다.\n\n\n[A.x, A.y], [a.x, a.y]\n\n([100, 1], [1, 1])\n\n\n- 시점3\n\nb=A()\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([100, 1], [1, 1], [100, 1])\n\n\n- 시점4\n\nb.f()\n\n현재 인스턴스에서 f가 101번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 2번 실행\n\n\n- 시점5\n\na.f()\n\n현재 인스턴스에서 f가 2번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 3번 실행\n\n\n\na.f()\n\n현재 인스턴스에서 f가 3번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 4번 실행\n\n\n\nb.f()\n\n현재 인스턴스에서 f가 102번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 5번 실행\n\n\n\n\n\n\nclass B:\n    x=100   # 초기자본금\n    y=0\n    def f(self):   # f를 실행할때마다 돈을 쓴다.\n        self.x=self.x - 1\n        B.y = B.y + 1\n        # self.y = self.y + 1 이렇게 안쓰고 위에처럼 써보자!\n        print(\"현재 인스턴스에서 {}원 잔액남음\".format(self.x))\n        print(\"A클래스에서 만들어진 모든 인스턴스들에서 총 {}원 사용\".format(self.y))\n\n\na=B()\n\n\nb=B()\n\n\n[B.x, B.y], [a.x, a.y], [b.x, b.y]\n\n([100, 0], [100, 0], [100, 0])\n\n\n- 시점1\n\na.f() # 돈을 쓴다\n\n현재 인스턴스에서 99원 잔액남음\nA클래스에서 만들어진 모든 인스턴스들에서 총 1원 사용\n\n\n\na.f()\n\n현재 인스턴스에서 98원 잔액남음\nA클래스에서 만들어진 모든 인스턴스들에서 총 2원 사용\n\n\n\nb.f()\n\n현재 인스턴스에서 99원 잔액남음\nA클래스에서 만들어진 모든 인스턴스들에서 총 3원 사용\n\n\n- 시점2\n\n[B.x, B.y], [a.x, a.y], [b.x, b.y]\n\n([100, 3], [98, 3], [99, 3])\n\n\n\nB.x=999\n\n\n[B.x, B.y], [a.x, a.y], [b.x, b.y]\n\n([999, 3], [98, 3], [99, 3])\n\n\n- 시점3\n\nc=B()\n\n\nc.f()\n\n현재 인스턴스에서 998원 잔액남음\nA클래스에서 만들어진 모든 인스턴스들에서 총 4원 사용\n\n\n- 시점4\n\na.f()\n\n현재 인스턴스에서 97원 잔액남음\nA클래스에서 만들어진 모든 인스턴스들에서 총 5원 사용\n\n\n\nb.f()\n\n현재 인스턴스에서 98원 잔액남음\nA클래스에서 만들어진 모든 인스턴스들에서 총 6원 사용\n\n\n\nc.f()\n\n현재 인스턴스에서 997원 잔액남음\nA클래스에서 만들어진 모든 인스턴스들에서 총 7원 사용\n\n\n\nc.f()\n\n현재 인스턴스에서 996원 잔액남음\nA클래스에서 만들어진 모든 인스턴스들에서 총 8원 사용\n\n\n\n\n\n\nclass A:\n    x=0\n    y=0\n    def f(self):\n        self.x=self.x + 1\n        A.y = A.y + 1\n        # self.y = self.y + 1 이렇게 안쓰고 위에처럼 써보자!\n        print(\"현재 인스턴스에서 f가 {}번 실행\".format(self.x))\n        print(\"A클래스에서 만들어진 모든 인스턴스들에서 f가 총 {}번 실행\".format(self.y))\n\n\n[A.x, A.y]\n\n[0, 0]\n\n\n\na=A()\nb=A()\n\n\n[A.x, A.y], [a.x, a.y], [b.x,b.y]\n\n([0, 0], [0, 0], [0, 0])\n\n\n- 시점1\n\na.f()\n\n현재 인스턴스에서 f가 1번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 1번 실행\n\n\n\nb.f()\n\n현재 인스턴스에서 f가 1번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 2번 실행\n\n\n\na.f()\n\n현재 인스턴스에서 f가 2번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 3번 실행\n\n\n- 시점2\n\na.__dict__\n\n{'x': 2}\n\n\n\na.y  # 인스턴스 a에 소속되어 있지만 클래스 변수 \n\n3\n\n\n\na.y = 999 # A.y 였으면 다 바꼈을 테지만 a.y 였다면??\n# 내가 하드코딩으로 a.y에 999 입력 -> 이것이 A.y나 b.y에도 반영될까? (x)\n\n\n[A.x, A.y], [a.x, a.y], [b.x,b.y]\n\n([0, 3], [2, 999], [1, 3])\n\n\n\na.__dict__\n\n{'x': 4, 'y': 999}\n\n\n- 시점3\n\nb.f()\n\n현재 인스턴스에서 f가 2번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 4번 실행\n\n\n\na.f()\n\n현재 인스턴스에서 f가 3번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 999번 실행\n\n\n\nb.f()\n\n현재 인스턴스에서 f가 3번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 6번 실행\n\n\n\nb.f()\n\n현재 인스턴스에서 f가 4번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 7번 실행\n\n\n\na.f()\n\n현재 인스턴스에서 f가 4번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 999번 실행\n\n\n- 요약 - 인스턴스에서 클래스 변수의 값을 변경하면? -> 클래스변수의 값이 변경되는 것이 아니라 인스턴스 변수가 새롭게 만들어져서 할당 된다. - 이 예제에서 a.y는 이제 클래스변수에서 인스턴스 변수로 재탄생 되었다. 즉, 999오브젝트가 새롭게 만들어져서 a.x라는 이름을 얻은것이다. - 기존의 A.y나 b.y에는 아무런 변화가 없다.\n\nid(999) #999도 오브젝트임\n\n139914476320048\n\n\n\na.y = 999 는 새로운 인스턴스 변수 y를 할당하는 역할을 한다. 클래스변수의 값을 변경하는 것이 아니다. (왜냐하면 애초에 a.y는 없는 값이었고, A.y를 빌리고 있었던 것임)\n\n\na.__dict__\n\n{'x': 4, 'y': 999}\n\n\n\nb.__dict__\n\n{'x': 4}\n\n\n\n\n\n\nclass A:\n    x=0\n    y=0\n    def f(self):\n        self.x=self.x + 1\n        A.y = A.y + 1\n        # self.y = self.y + 1 이렇게 안쓰고 위에처럼 써보자!\n        print(\"현재 인스턴스에서 f가 {}번 실행 (인스턴스레벨)\".format(self.x))\n        print(\"A클래스에서 만들어진 모든 인스턴스들에서 f가 총 {}번 실행 (클레스레벨)\".format(A.y))\n        print(\"A클래스에서 만들어진 모든 인스턴스들에서 f가 총 {}번 실행 (인스턴스레벨)\".format(self.y))\n\n\na=A()\n\n\na.f()\n\n현재 인스턴스에서 f가 1번 실행 (인스턴스레벨)\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 1번 실행 (클레스레벨)\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 1번 실행 (인스턴스레벨)\n\n\n\nb=A()\n\n\nb.f()\n\n현재 인스턴스에서 f가 1번 실행 (인스턴스레벨)\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 2번 실행 (클레스레벨)\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 2번 실행 (인스턴스레벨)\n\n\n- 시점1\n\na.y = 999\n\n\na.f()\n\n현재 인스턴스에서 f가 2번 실행 (인스턴스레벨)\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 3번 실행 (클레스레벨)\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 999번 실행 (인스턴스레벨)\n\n\n\na.f()\n\n현재 인스턴스에서 f가 3번 실행 (인스턴스레벨)\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 4번 실행 (클레스레벨)\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 999번 실행 (인스턴스레벨)\n\n\n\n\n\n- 의문: 아래의 코드에서 x는 클래스 변수라고 봐야할까? 인스턴스 변수라고 봐야할까? —> 클래스 변수!\nclass SoWhaTV: \n    x=0   # 이 시점에서 x는 클래스변수인가? 아니면 인스턴스 변수인가?\n    def f(self):\n        print(self.x)\n- 시점0\n\nclass A:\n    x=0\n    y=0\n    def f(self):\n        self.x=self.x + 1\n        A.y = A.y + 1\n        # self.y = self.y + 1 이렇게 안쓰고 위에처럼 써보자!\n        print(\"현재 인스턴스에서 f가 {}번 실행 (인스턴스레벨)\".format(self.x))\n        print(\"A클래스에서 만들어진 모든 인스턴스들에서 f가 총 {}번 실행 (클레스레벨)\".format(A.y))\n        print(\"A클래스에서 만들어진 모든 인스턴스들에서 f가 총 {}번 실행 (인스턴스레벨)\".format(self.y))\n\n\na=A()\nb=A()\n\n\na.x, a.y, b.x, b.y\n\n(0, 0, 0, 0)\n\n\n\na.__dict__, b.__dict__\n\n({}, {})\n\n\n\n지금 시점에서 a.x, a.y, b.x, b.y는 모두 클래스 변수임\n\n- 시점1\n\na.f()\n\n현재 인스턴스에서 f가 1번 실행 (인스턴스레벨)\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 1번 실행 (클레스레벨)\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 1번 실행 (인스턴스레벨)\n\n\n\na.__dict__, b.__dict__\n\n({'x': 1}, {})\n\n\n\n이 순간 a.x가 클래스변수에서 인스턴스 변수로 변경되었다. (예제5와 같이..) 왜? f가 실행되면서 self.x = self.x + 1이 실행되었으므로!\n\n- 시점2\n\nb.f()\n\n현재 인스턴스에서 f가 1번 실행 (인스턴스레벨)\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 2번 실행 (클레스레벨)\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 2번 실행 (인스턴스레벨)\n\n\n\na.__dict__, b.__dict__\n\n({'x': 1}, {'x': 1})\n\n\n\n\n\n- 아래처럼 코드를 바꾸면 어떻게 되는가?\n\nclass A:\n    def __init__(self):\n        self.x=0 # 인스턴스 변수로 나중에 쓸꺼니까 명시함\n        A.y=0  # 클래스변수로 나중에 쓸꺼니까 명시함\n    def f(self):\n        self.x=self.x + 1\n        A.y = A.y + 1\n        # self.y = self.y + 1 이렇게 안쓰고 위에처럼 써보자!\n        print(\"현재 인스턴스에서 f가 {}번 실행 (인스턴스레벨)\".format(self.x))\n        print(\"A클래스에서 만들어진 모든 인스턴스들에서 f가 총 {}번 실행 (클레스레벨)\".format(A.y))\n        #print(\"A클래스에서 만들어진 모든 인스턴스들에서 f가 총 {}번 실행 (인스턴스레벨)\".format(self.y))\n\n- 사용\n\na=A()\nb=A()\n\n\na.f()\n\n현재 인스턴스에서 f가 1번 실행 (인스턴스레벨)\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 1번 실행 (클레스레벨)\n\n\n\nb.f()\n\n현재 인스턴스에서 f가 1번 실행 (인스턴스레벨)\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 2번 실행 (클레스레벨)\n\n\n\nb.f()\n\n현재 인스턴스에서 f가 2번 실행 (인스턴스레벨)\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 3번 실행 (클레스레벨)\n\n\n\nb.f()\n\n현재 인스턴스에서 f가 3번 실행 (인스턴스레벨)\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 4번 실행 (클레스레벨)\n\n\n\na.f()\n\n현재 인스턴스에서 f가 2번 실행 (인스턴스레벨)\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 5번 실행 (클레스레벨)\n\n\n- 잘 되는 것 같다?\n- 조금만 생각해보면 엉터리라는 것을 알 수 있다. 아래를 관찰하자.\n\nc=A()   # 이 시점에서 __init__()이 실행된다\n\n\na.f()\n\n현재 인스턴스에서 f가 3번 실행 (인스턴스레벨)\nA클래스에서 만들어진 모든 인스턴스들에서 f가 총 1번 실행 (클레스레벨)\n\n\n\n클래스 레벨의 변수가 왜 초기화가 되었지?\n\n- 오류의 이유? c=A()가 실행되는 시점에 __init__()이 실행되면서 A.y=0이 실행된다. 따라서 강제 초기화가 진행되었다.\n\nㅇ"
  },
  {
    "objectID": "posts/Python/4. Class/python 11_0516.html",
    "href": "posts/Python/4. Class/python 11_0516.html",
    "title": "파이썬 (0516) 11주차",
    "section": "",
    "text": "from PIL import Image\nimport requests"
  },
  {
    "objectID": "posts/Python/4. Class/python 11_0516.html#클래스-공부-2단계",
    "href": "posts/Python/4. Class/python 11_0516.html#클래스-공부-2단계",
    "title": "파이썬 (0516) 11주차",
    "section": "클래스 공부 2단계",
    "text": "클래스 공부 2단계\n\ninit()\n- STOOOP를 다시 복습\n\nurl1 = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\nurl2 = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop2.png?raw=true'\n\n\nclass STOOOP: \n    title = '학교폭력!' \n    url = url1\n    end = '멈춰~~~~'\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end) \n\n\ns1=STOOOP() # STOOOP라는 클래스에서 s1이라는 인스턴스를 만드는 과정\n\n\ns1.title, s1.url, s1.end\n\n('학교폭력!',\n 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true',\n '멈춰~~~~')\n\n\n\ns1.stop()\n\n학교폭력!\n\n\n\n\n\n멈춰~~~~\n\n\n- 왜 s1의 default title이 항상 “학교폭력”이어야 하는가? => __init__의 개발\n- 성능4: __init__() 함수를 이용하여 “클래스->인스턴스”의 시점에서 수행하는 일련의 동작들을 묶어서 수행할 수 있음\n\nclass STOOOP: \n    #title = '학교폭력!' \n    url = url1\n    end = '멈춰~~~~'\n    def __init__(self,title):\n        self.title = title\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end) \n\n- 잘못된사용\n\ns1=STOOOP()  # 이 시점에서 _init_ 이 수행된다!\n\nTypeError: __init__() missing 1 required positional argument: 'title'\n\n\n- 올바른사용\n\ns1=STOOOP(\"수강신청\")  # 이 시점에서 _init_ 이 수행된다!\n\n\ns1.title, s1.url, s1.end\n\n('수강신청',\n 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true',\n '멈춰~~~~')\n\n\n\ns1.stop()\n\n수강신청\n\n\n\n\n\n멈춰~~~~\n\n\n- 잘못된 사용에서 에러가 발생한 이유?\nTypeError: __init__() missing 1 required positional argument: 'title'\n\ns1 = STOOOP() 가 실행되는 순간 __init__() 이 내부적으로 실행된다.\n그런데 __init__()의 첫번째 입력은 self 는 입력안해도 무방했음. (현재는 메소드와 함수를 구분하는 문법으로 self를 이해하면 된다.) 그런데 두번째 입력은 title은 입력을 해야했음.\n그런데 title을 입력하지 않아서 발생하는 에러\n\n- __init__(self, arg1, arg2, ...) 함수에 대하여 - 엄청나게 특별해 보이지만 사실 몇가지 특별한 점을 제외하고는 어떠한 마법도 없는 함수이다. - 특별한점1: 첫번째 입력으로 반드시 self를 넣어야함. (이거 ㄴ사실 클래스 내의 메소드 거의 다 그러하다.) - 특별한점2: 클래스에서 인스턴스를 만드는 시점에 자동으로 생성됨 - 특별한점3: __init__(self, arg1, arg2, ...) 의 입력중 self 이외의 입력들은 “클래스->인스턴스”의 시점에서 “인스턴스이름 = 클래스이름(arg1,arg2,…)”와 같이 사용한다. (이 예제의 경우 STOOOP(title)와 같이 사용해야함)\n- title 이 디폴트로 들어가는 상황도 불편했지만, title을 명시적으로 넣지 않으면 에러가 발생하는 것도 불편하다?\n\nclass STOOOP: \n    #title = '학교폭력!' \n    url = url1\n    end = '멈춰~~~~'\n    def __init__(self,title=None):\n        self.title = title\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end) \n\n\ns2=STOOOP()\ns3=STOOOP('KOSPI하락')\n\n\ns2.stop()\n\nNone\n\n\n\n\n\n멈춰~~~~\n\n\n\n제목이 없으면 없는데로\n\n\ns3.stop()\n\nKOSPI하락\n\n\n\n\n\n멈춰~~~~\n\n\n\n\nself의 의미\n- 이전 예제를 다시 복습\n\nclass Klass4:\n    n = 1\n    url = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\n    def show(self):\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(\"당신은 이 이미지를 {}번 보았습니다\".format(self.n))\n        self.n = self.n+1 \n\n\nk4=Klass4()\n\n\nk4.show()\n\n\n\n\n당신은 이 이미지를 5번 보았습니다\n\n\n- 위의 예제는 아래와 같이 구현할 수도 있다.\n\nclass Klass4:\n    n = 1\n    url = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\n    def show(self):\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(\"당신은 이 이미지를 {}번 보았습니다\".format(self.n))\n       # self.n = self.n+1 \n\n\nk4=Klass4()\n\n\nk4.n\n\n1\n\n\n\nk4.show()\n\n\n\n\n당신은 이 이미지를 1번 보았습니다\n\n\n\nk4.n = k4.n +1\n\n\nk4.show()\n\n\n\n\n당신은 이 이미지를 2번 보았습니다\n\n\n\nk4.n = k4.n + 1\n\n\nk4.show()\n\n\n\n\n당신은 이 이미지를 3번 보았습니다\n\n\n- 결국에는 k4.n = k4.n +1 의 기능을 구현하여 넣은 것이 self.n = slef.n + 1 이다.\n- 따라서 self는 k4에 대응한다. 즉 self는 인스턴스의 이름에 대응한다. 우리가 하고 싶은 것은 클래스를 선언하는 시점에 인스턴스가 생성된 이후의 시점에 대한 어떠한 동작들을 정의하고 싶음. 그런데 클래스를 설계하는 시점에서는 인스턴스의 이름이 정해지지 않았으므로 (아직 인스턴스가 태어나지도 않음) 이러한 동작들을 정의하기 불편하다. 그래서 클래스를 설계하는 시점에서 그 클래스로부터 만들어지는 인스턴스는 그냥 self라는 가칭으로 부른다. (굳이 비유하면 self는 인스턴스의 태명같은것..)\n\n# 여기서 말하는 인스턴스는 정확하게 무엇인가? 그냥.. self, k4 와 같이 이름을 말하는건가?\n\n- self의 의미는 (이후에 만들어질) 인스턴스의 이름이다. (즉 self는 인스턴스의 태명같은 것임)\n\n\n파이썬의 비밀1\n탐구 : 인스턴스의 자료형이 뭔지 탐구하자!\n- 아래의 두 클래스를 비교해보자\n\nclass STOOOP: \n    #title = '학교폭력!' \n    url = url1\n    end = '멈춰~~~~'\n    def __init__(self,title=None): \n        self.title = title\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end) \n\n\nclass Klass4:\n    n = 1\n    url = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\n    def show(self):\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(\"당신은 이 이미지를 {}번 보았습니다\".format(self.n))\n        #self.n = self.n+1     \n\n- 인스턴스를 생성해보자.\n\nk4=Klass4()\ns1=STOOOP()\n\n- 타입을 알아보자.\n\nk4?\n\n\nType:        Klass4\nString form: <__main__.Klass4 object at 0x7fc527259890>\nDocstring:   <no docstring>\n\n\n\n\n\ns1?\n\n\nType:        STOOOP\nString form: <__main__.STOOOP object at 0x7fc527259bd0>\nDocstring:   <no docstring>\n\n\n\n\n-??? 타입은 자료형, 즉 int, float, list 이런 것 아니었나?\n\na=[1,2,3]\na?\n\n\n\nType:        list\nString form: [1, 2, 3]\nLength:      3\nDocstring:  \nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\n\n\n\n\n\na=3.14\na?\n\n\nType:        float\nString form: 3.14\nDocstring:   Convert a string or number to a floating point number, if possible.\n\n\n\n\n- 그런데 지금 k4, s1의 타입은 Klass4, STOOOP이다.\n\n가설1: 사실 파이썬 내부에 Klass4, STOOOP 이라는 자료형이 있었다. 그런데 내가 만든 k4, s1이 우연히 그 자료형을 따르는 것 (억지스러움)\n\n\nclass SoWhatTV:\n    title='어쩔티비'\n\n\na=SoWhatTV()\na?\n\n\nType:        SoWhatTV\nString form: <__main__.SoWhatTV object at 0x7fc5259e6810>\nDocstring:   <no docstring>\n\n\n\n\n\n# 잉.. 우연히 자료형이 sowhattv가 잇을거 같진 않은데. .가설1이 맞진 않는거 같다\n\n\n가설2: type이 list인것은 사실 list라는 클래스에서 생긴 인스턴스이다. -> 리스트자료형을 찍어낼 수 있는 어떠한 클래스가 내부적으로 존재할 것이다.\n\n깨달음1\n- 가설2가 맞다면 아래는 모두 어딘가에서 찍혀진 인스턴스이다.\n\na=[1,2,3]\na?\n\n\n\nType:        list\nString form: [1, 2, 3]\nLength:      3\nDocstring:  \nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\n\n\n\n\n\na='1'\na?\n\n\nType:        str\nString form: 1\nLength:      1\nDocstring:  \nstr(object='') -> str\nstr(bytes_or_buffer[, encoding[, errors]]) -> str\nCreate a new string object from the given object. If encoding or\nerrors is specified, then the object must expose a data buffer\nthat will be decoded using the given encoding and error handler.\nOtherwise, returns the result of object.__str__() (if defined)\nor repr(object).\nencoding defaults to sys.getdefaultencoding().\nerrors defaults to 'strict'.\n\n\n\n\n- 그리고 위의 a=[1,2,3]과 같은 것들은 모두 “클래스->인스턴스”에 해당하는 과정이었음\n깨달음2\n- 생각해보니까 아래와 같이 list를 선언하는 방식도 있었음\n\na=list()\na\n\n[]\n\n\n\nlist라는 이름의 클래스에서 a라는 인스턴스를 찍어내는 문법이다.\n\n- 아래도 가능\n\na=list((1,2,3))\na?\n\n\n\nType:        list\nString form: [1, 2, 3]\nLength:      3\nDocstring:  \nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\n\n\n\n\n- list라는 이름의 클래스에서 a라는 인스턴스를 찍어내는 문법. 여기에서 (1,2,3)은 __init__()의 입력이다.\n깨달음3\n- 각 자료형마다 특수한 기능들이 있다.\n\na=[1,2,3]\na\n\n[1, 2, 3]\n\n\n- a. + tab 을 하면 append, clear 등등이 나온다.\n- 이러한 기능은 지금까지 우리가 “list자료형 특수기능들” 이라고 부르면서 사용했다. 그런데 a가 list클래스에서 생성된 인스턴스라는 관점에서 보면 이러한 기능들은 list클래스에서 정의된 메소드라고 볼 수 있다.\n깨달음4\n- a.f()는 f(a)로 해석가능하다고 했다. 이 해석에 따르면 메소드의 첫번째 입력은 메소드가 소속된 인스턴스라고 해석할 수 있다.\n- 동일한 논리로 아래의 코드는 stop()의 입력에서 s1을 넣는다는 의미이다.\n\ns1.stop()   #s1자체가 입력이 되는것\n\nNone\n\n\n\n\n\n멈춰~~~~"
  },
  {
    "objectID": "posts/Python/4. Class/python 11_0516.html#숙제",
    "href": "posts/Python/4. Class/python 11_0516.html#숙제",
    "title": "파이썬 (0516) 11주차",
    "section": "숙제",
    "text": "숙제\n아래의 조건에 맞는 클래스를 생성하라.\n\n[‘가위’,‘바위’]와 같은 리스트를 입력으로 받아 인스턴스를 생성한다.\n위의 리스트에서 하나의 값을 뽑는 메소드 f를 가지고 있다.\n\n\n# # 너무 헷 갈ㄹ  ㅕ .. \n\n# # 사용예시\n\n\n# a = Klass(['가위','바위'])\n# a.f() # 가위가 1/2 바위가 1/2의 확률로 출력 \n# b = Klass(['가위','바위','보'])\n# b.f() # 가위, 바위, 보가 1/3의 확률로 출력"
  },
  {
    "objectID": "posts/Python/4. Class/python 14_0606.html",
    "href": "posts/Python/4. Class/python 14_0606.html",
    "title": "파이썬 (0606) 14주차",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd"
  },
  {
    "objectID": "posts/Python/4. Class/python 14_0606.html#클래스공부-7단계",
    "href": "posts/Python/4. Class/python 14_0606.html#클래스공부-7단계",
    "title": "파이썬 (0606) 14주차",
    "section": "클래스공부 7단계",
    "text": "클래스공부 7단계\n\n함수공부\n- 다시 함수를 공부해보자\n\ndef f(x):\n    return x+1\n\n\nf(3)\n\n4\n\n\n- 함수의 사용방법? - 입력으로 변수 x를 받음 = 입력으로 인스턴스 x를 받음 - 출력으로 변수 x+1을 리턴 = 출력으로 인스턴스 x+1을 리턴\n- 사실1: 파이썬에서 함수는 인스턴스를 입력으로 받고 인스턴스를 출력한다.\n- 함수의 자료형?\n\nf\n\n<function __main__.f(x)>\n\n\n\n?f\n\n\nSignature: f(x)\nDocstring: <no docstring>\nFile:      ~/Dropbox/coco/posts/python/<ipython-input-2-9897bae5f29b>\nType:      function\n\n\n\n\n\ntype이 function 이다.\nf는 function 의 class의 instance이다.\n결국 f도 하나의 오브젝트에 불과하다.\n\n- 사실2: 함수도 결국 인스턴스이다. -> 함수의 입력으로 함수를 쓸 수도 있고 함수의 출력으로 함수가 나올 수도 있다.\n\n\n함수형 프로그래밍\n(예제1) 숫자입력, 함수출력\n\ndef f(a):\n    def _f(x):\n        return (x-a)**2\n    return _f\n\n\ng=f(10)    # g(x) = (x-10)**2\n\n\ng(2)    # (8)**2\n\n64\n\n\n\n해석: f(a)는 a를 입력으로 받고 g(x) = (x-a)^2 함수를 리턴해주는 함수\n\n(예제1)의 다른표현: 익명함수 lambda\n표현1\n\ndef f(a):\n    _f = lambda x: (x-a)**2   # lambda x : (x-a)**2 가 실행되는 순간 함수오브젝트가 만들어지고 그것이 _f로 저장됨\n    return _f\n\n\ng=f(10)    # g(x) = (x-10)**2\n\n\ng(2)\n\n64\n\n\n표현2\n\ndef f(a):\n    return lambda x: (x-a)**2\n\n\ng=f(10)    # g(x) = (x-10)**2\n\n\ng(2)\n\n64\n\n\n\nlambda x: (x-a)**2 는 \\(\\text{lambda}(x) = (x-a)^2\\) 의 느낌으로 기억하면 쉽다.\nlambda x: (x-a)**2 는 “아직 이름이 없는 함수 오브젝트를 (가칭 lmabda라고 하자) 만들고 기능은 x를 입력으로 하고 (x-a)**2를 출력하도록 하자” 라는 뜻이로 해석\n\n\n(lambda x,y : x<y)(2,3)\n\nTrue\n\n\n\nf=lambda x,y : x<y   # 위와 같은 코드\nf(2,3)\n\nTrue\n\n\n(예제2) 함수입력, 숫자출력\n\ndef f(x):\n    return x**2\n\n\nf(3)\n\n9\n\n\n\ndef d(f,x):    # 함수를 입력을 받는 함수를 정의\n    h=0.0000000000001\n    return (f(x+h)-f(x)) / h\n\n\\[f'(x)\\approx \\frac{f(x+h)-f(x)}{h}\\]\n\n\\(h\\) 의 값이 점점 0에 가까울수록 등호에 가까워짐\n\n\nd(f,4)   # f'(4) = 2*4 = 8\n\n8.029132914089132\n\n\n(예제3) 함수입력, 함수출력\n\ndef f(x):\n    return x**2\n\n\ndef derivate(f):\n    def df(x):\n        h=0.0000000000001\n        return (f(x+h)-f(x)) / h\n    return df\n\n\nff = derivate(f)  # f미분\n\n\nff(10)  #f의 도함수\n\n19.895196601282805\n\n\n원래함수 시각화\n\nx=np.linspace(-1,1,100)\nplt.plot(x,f(x))\n\n\n\n\n도함수 시각화\n\nx=np.linspace(-1,1,100)\nplt.plot(x,ff(x))\n\n\n\n\n(예제3)의 다른표현\n\ndef f(x):\n    return x**2\n\n\ndef derivate(f):\n    h=0.0000000000001\n    return lambda x:(f(x+h)-f(x)) / h\n\n\nff=derivate(f)\n\n\nff(10)\n\n19.895196601282805\n\n\n(예제4) 함수들의 리스트\n\n# # 리스트의 컴마 컴마 안에 들어갈 수 있는것은 \n# [인스턴스, 인스턴스, 인스턴스]\n# [오브젝트, 오브젝트, 오브젝트]\n# [함수오브젝트, 함수오브젝트, 함수오브젝트]\n# # ....\n\n\nflst = [lambda x:x, lambda x:x**2, lambda x:x**3]\nflst\n\n[<function __main__.<lambda>(x)>,\n <function __main__.<lambda>(x)>,\n <function __main__.<lambda>(x)>]\n\n\n\nfor f in flst:\n    print(f(2))\n\n2\n4\n8\n\n\n\nfor f in flst:\n    plt.plot(x,f(x),'--')\n\n\n\n\n위 아래 동일\n\nplt.plot(x, (lambda x:x)(x),'--')\nplt.plot(x, (lambda x:x**2)(x),'--')\nplt.plot(x, (lambda x:x**3)(x),'--')\n\n\n\n\n\n\n정리\n- 지금까지 개념 - 함수: 변수를 입력으로 받아서 변수를 출력하는 개념 - 변수: 어떠한 값을 저장하는 용도로 쓰이거나 함수의 입력 혹은 출력으로 사용함\n- 파이썬의 함수형프로그래밍을 잘하려면? - 변수든 함수든 둘다 인스턴스임 - 변수를 함수처럼: 메소드 - 함수를 변수처럼(\\(\\star\\)) : 함수자체를 함수의 입력으로 혹은 출력으로 쓸 수도 있음. 함수를 특정 값처럼 생각해서 함수들의 list를 만들 수도 있다.\n\n\ncollable object\n- 함수 오브젝트의 비밀?\n\nf = lambda x: x+1\n\n\nf(4)\n\n5\n\n\n\n?f\n\n\nSignature: f(x)\nDocstring: <no docstring>\nFile:      ~/Dropbox/coco/posts/python/<ipython-input-66-1a55436594c9>\nType:      function\n\n\n\n\n\ndir(f)\n\n['__annotations__',\n '__call__',\n '__class__',\n '__closure__',\n '__code__',\n '__defaults__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__get__',\n '__getattribute__',\n '__globals__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__kwdefaults__',\n '__le__',\n '__lt__',\n '__module__',\n '__name__',\n '__ne__',\n '__new__',\n '__qualname__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__']\n\n\n\nset(dir(f)) & {'__call__'}\n\n{'__call__'}\n\n\n\n함수 오브젝트에는 숨겨징 기능 __call__이 있다.\n\n\nf.__call__(3)    # f(3)\n\n4\n\n\n\nf.__call__(4)   # f(4)\n\n5\n\n\n\n여기에 우리가 정의한 내용이 있따.\n\ncall 만 정의를 해주면 함수처럼 쓸 수 있다 ?! -> list의 dir 확인해보면 call 없음\n- 함수처럼 쓸 수 없는 인스턴스는 단지 call이 없는 것일 뿐이다.\n\nclass Klass:\n    def __init__(self):\n        self.name = 'boram'\n\n\na=Klass()\n\n\na()\n\nTypeError: 'Klass' object is not callable\n\n\n\nTypeError: ‘Klass’ object is not callable\n\n\nclass Klass2(Klass):  # 상속\n    def __call__(self):\n        print(self.name)\n\n\nb=Klass2()\n\n\nb()\n\nboram\n\n\n\nb는 collable obeject 라는 의미. 즉 숨겨진 메서드로 __call__를 가진 오브젝트!\nKlass는 collable object를 만들지 못하지만 Klass2는 collable object를 만든다.\n\n- 클래스로 함수를 만들기\n\nclass AddConstant:\n    def __init__(self,c):\n        self.c = c\n    def __call__(self,a):\n        return a + self.c\n\n\nf = AddConstant(3)   # collabe object 생성, f.c에는 3이 저장되어 있음.\n\n\nf(7)   # f.c와 7을 더하는 기능을 수행, # f(x) = x+3 을 수행함\n\n10\n\n\n\nf(10)\n\n13\n\n\n- 클래스도 일종의 오브젝트이고 함수처럼 Klass()와 같이 사용하여 인스턴스를 만들었음. -> Klass.__call__()는 Klass()와 같은 역할을 할 것이다.\n\nclass Klass:\n    def __init__(self):\n        self.name='coco'\n    \n\n\na=Klass.__call__()   # 이것이 a=Klass()와 같은 효과\n\n\na.name\n\n'coco'\n\n\n\n\n파이썬의 비밀 1~4\n\n파이썬의 비밀1: 자료형은 클래스의 비밀이다.(11주차)\n파이썬의 비밀2: 클래스에는 __str__처럼 숨겨진 메서드가 존재한다. 이를 이용하여 파이썬 내부의 기능을 가로챌 수 있다.(12주차0523)\n파이썬의 비밀3: 주피터노트북에서는 “오브젝트이름+엔터” 를 쳐서 나오는 출력은 __repr__로 가로챌 수 잇다.(주피터의 비밀)\n파이썬의 비밀4: 함수와 클래스는 숨겨진 메서드에 __call__을 가진 오브젝트일 뿐이다."
  },
  {
    "objectID": "posts/Python/4. Class/python 14_0606.html#클래스공부-8단계",
    "href": "posts/Python/4. Class/python 14_0606.html#클래스공부-8단계",
    "title": "파이썬 (0606) 14주차",
    "section": "클래스공부 8단계",
    "text": "클래스공부 8단계\n\nfor문의 복습\n- 아래와 같은 예제들을 관찰하여 for문을 복습하자.\n(예제1)\n\nfor i in [1,2,3,4]:\n    print(i)\n\n1\n2\n3\n4\n\n\n(예제2)\n\nfor i in (1,2,3,4):\n    print(i)\n\n1\n2\n3\n4\n\n\n(예제3)\n\nfor i in'1234':\n    print(i)\n\n1\n2\n3\n4\n\n\n(예제4)\n\na=5\nfor i in a:\n    print(i)\n\nTypeError: 'int' object is not iterable\n\n\n\n5라고 출력되어야 하지 않나?\n\n- 의문1:\nfor i in ???:\n    print(i)\n에서 ??? 자리에 올수 있는 것이 무엇일까?\n(예제5)\n상황1\n\nlst = [[1,2,3,4],[3,4,5,6]]\nfor l in lst:\n    print(l)\n\n[1, 2, 3, 4]\n[3, 4, 5, 6]\n\n\n상황2\n\ndf=pd.DataFrame(lst)\ndf\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n    \n  \n  \n    \n      0\n      1\n      2\n      3\n      4\n    \n    \n      1\n      3\n      4\n      5\n      6\n    \n  \n\n\n\n\n\nfor i in df:\n    print(i)\n\n0\n1\n2\n3\n\n\n칼럼이름들이 나오는 것 같음 -> 확인해보자\n\ndf.columns = pd.Index(['X'+str(i) for i in range(1,5)])\ndf\n\n\n\n\n\n  \n    \n      \n      X1\n      X2\n      X3\n      X4\n    \n  \n  \n    \n      0\n      1\n      2\n      3\n      4\n    \n    \n      1\n      3\n      4\n      5\n      6\n    \n  \n\n\n\n\n\nfor i in df:\n    print(i)\n\nX1\nX2\nX3\nX4\n\n\n- 의문2: for의 출력결과는 어떻게 예측할 수 있을까?\n\n\nfor문의 동작원리\n- 의문1의 해결: 아래의 ???자리에 올 수 있는 것은 dir()하여 __iter__가 있는 object이다.\nfor i in ???:\n    print(i)\n이러한 오브젝트를 iterable object라고 한다.\n- 확인\n\na=[1,2,3]\nset(dir(a)) & {'__iter__'}\n\n{'__iter__'}\n\n\n\na='123'\nset(dir(a)) & {'__iter__'}\n\n{'__iter__'}\n\n\n\na=3\nset(dir(a)) & {'__iter__'}\n\nset()\n\n\n\n예상대로 예제 1~4에서는 int클래스의 instance만 __iter__ 가 없다.\n\n- __iter__ 의 역할: iterable object를 iterator로 만들 수 있다!\n\nlst = [1,2,3]\nlst\n\n[1, 2, 3]\n\n\n\nlst[1] # 충실한 리스트\n\n2\n\n\n\nltor = iter(lst)\n#ltor = lst.__iter__()\nltor\n\n<list_iterator at 0x7f4b7b39efd0>\n\n\n\nltor[1]   # 더이상 리스트가 아니다.\n\nTypeError: 'list_iterator' object is not subscriptable\n\n\n\nltor?\n\n\nType:        list_iterator\nString form: <list_iterator object at 0x7f4b7b39efd0>\nDocstring:   <no docstring>\n\n\n\n\n- iterator가 되면 무엇이 좋은가? -> 숨겨진 기능 __next__ 가 열린다.\n\nset(dir(lst)) & {'__next__'}\n\nset()\n\n\n\nset(dir(ltor)) & {'__next__'}\n\n{'__next__'}\n\n\n\nlst에는 __next__가 없지만 ltor에는 있다!\n\n- 그래서 __next__의 기능은? -> 원소를 차례대로 꺼내준다. + 더 이상 꺼낼 원소가 없으면 StopIteration Error를 발생시킨다.\n\nlst\n\n[1, 2, 3]\n\n\n\nltor.__next__()\n\n1\n\n\n\nltor.__next__()\n\n2\n\n\n\nltor.__next__()\n\n3\n\n\n\nltor.__next__()\n\nStopIteration: \n\n\n- for 문의 동작원리\nfor i in lst\n    print(i)\n\nlst.__iter__() 혹은 iter(lst) 를 이용하여 lst를 iterator로 만든다. (iterable object를 iterator object로 만든다.)\niterator에서 .__next__() 함수를 호출하고 결과를 i에 저장한 뒤에 for문 블락안에 있는 내용 (들여쓰기 된 내용)을 실행한다. -> 반복\nStopIteration 에러가 발생하면 for무늘 멈춘다.\n\n- 아래의 ??? 자리에 올 수 있는 것이 iterable object가 아니라 iterator 자체여도 for 문이 돌아갈까? -> (당연히 돌아가야 할 것 같음)\nfor i in ???\n    print(i)\n\nfor i in iter([1,2,3]):\n    print(i)\n\n1\n2\n3\n\n\n\n당연히 가능!\n\n- a가 iterator일때 iter(a) 의 출력결과가 a와 같도록 조정한다면 for문의 동작원리 (1) ~ (3) 을 수정하지 않아도 좋다. -> 실제로 이렇게 동작한다.\n\nltor?\n\n\nType:        list_iterator\nString form: <list_iterator object at 0x7f4b7b39efd0>\nDocstring:   <no docstring>\n\n\n\n\n\ndir(ltor)\n\n['__class__',\n '__delattr__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__length_hint__',\n '__lt__',\n '__ne__',\n '__new__',\n '__next__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__setstate__',\n '__sizeof__',\n '__str__',\n '__subclasshook__']\n\n\n- 요약 - iterable object는 숨겨진 기능으로 __iter__를 가진다. - iterator object는 숨겨진 기능으로 __iter__와 __next__를 가진다. (즉 iterator는 그 자체로 iterable object가 된다!)\n\nlst = [1,2,3]\nltor = iter(lst)\n\n\nset(dir(lst)) & {'__iter__', '__next__'}\n\n{'__iter__'}\n\n\n\nset(dir(ltor)) & {'__iter__', '__next__'}\n\n{'__iter__', '__next__'}\n\n\n- 의문2의 해결: for문의 출력결과는 어떻게 예측할 수 있을까? iterator를 만들어서 __next__()의 출력값을 확인하면 알 수 있다.\n\nfor i in df:\n    print(i)\n\nX1\nX2\nX3\nX4\n\n\n\ndftor = iter(df)\ndftor?\n\n\nType:        map\nString form: <map object at 0x7f4b7c0d1350>\nDocstring:  \nmap(func, *iterables) --> map object\nMake an iterator that computes the function using arguments from\neach of the iterables.  Stops when the shortest iterable is exhausted.\n\n\n\n\n\ndftor.__next__()\n\n'X1'\n\n\n\ndftor.__next__()\n\n'X2'\n\n\n\ndftor.__next__()\n\n'X3'\n\n\n\ndftor.__next__()\n\n'X4'\n\n\n\ndftor.__next__()\n\nStopIteration: \n\n\n\n\nrange()\n- 파이썬에서 for문을 처음 배울 때: range(5)를 써라!\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\n\nrange(5) 가 도대체 무엇인가?\n\n\nset(dir(range(5))) & {'__iter__', '__next__'}\n\n{'__iter__'}\n\n\n- range(5)의 정체는 그냥 iterable object이다.\n- 그래서 언제든지 iterator로 바꿀 수 있다.\n\nrtor = iter(range(5))\nrtor\n\n<range_iterator at 0x7f4b7af5d8a0>\n\n\n\nset(dir(rtor)) & {'__iter__', '__next__'}\n\n{'__iter__', '__next__'}\n\n\n- for문에서 range(5)가 행동하는 방법?\n\nrtor = iter(range(5))\n\n\nrtor.__next__()\n\n0\n\n\n\nrtor.__next__()\n\n1\n\n\n\nrtor.__next__()\n\n2\n\n\n\nrtor.__next__()\n\n3\n\n\n\nrtor.__next__()\n\n4\n\n\n\nrtor.__next__()\n\nStopIteration: \n\n\n\n\nzip\n- 이터레이터의 개념을 알면 for문에 대한 이해도가 대폭 상승한다.\n\nfor i in zip([1,2,3],'abc'):\n    print(i)\n\n(1, 'a')\n(2, 'b')\n(3, 'c')\n\n\n\nzip은 뭐지?\n\n\nzip([1,2,3],'abc')\n\n<zip at 0x7f4b7c63a690>\n\n\n- 어차피 for i in ????: 의 ???? 자리는 iterable object의 자리이다.\n\nset(dir(zip([1,2,3],'abc'))) & {'__iter__', '__next__'}\n\n{'__iter__', '__next__'}\n\n\n\n__next__() 함수가 있음 \\(\\to\\) zip([1,2,3],'abc')은 그 자체로 iterator 이다!\n\n\nz = zip([1,2,3],'abc')\n\n\nz.__next__()\n\n(1, 'a')\n\n\n\nz.__next__()\n\n(2, 'b')\n\n\n\nz.__next__()\n\n(3, 'c')\n\n\n\nz.__next__()\n\nStopIteration: \n\n\n\n\n사용자정의 이터레이터\n- 내가 이터레이터를 만들어보자.\n\nclass Klass: # 찌를 내는 순간 for문이 멈추도록 하는 이터레이터를 만들자.\n    def __init__(self):\n        self.candidate = [\"묵\", \"찌\", \"빠\"]\n    def __iter__(self):\n        return self\n    def __next__(self):\n        action = np.random.choice(self.candidate)\n        if action == \"찌\":\n            print(\"찌가 나와서 for문을 멈춥니다.\")\n            raise StopIteration\n        else:\n            return action\n\n\na=Klass()\n\n\na?\n\n\nType:        Klass\nString form: <__main__.Klass object at 0x7f4b7aabf0d0>\nDocstring:   <no docstring>\n\n\n\n\n\nset(dir(a)) & {'__iter__', '__next__'}  # a는 이터레이터\n\n{'__iter__', '__next__'}\n\n\n\na.__next__()\n\n'묵'\n\n\n\na.__next__()\n\n'빠'\n\n\n\na.__next__()\n\n'묵'\n\n\n\na.__next__()\n\n'빠'\n\n\n\na.__next__()\n\n'묵'\n\n\n\na.__next__()\n\n'묵'\n\n\n\na.__next__()\n\n찌가 나와서 for문을 멈춥니다.\n\n\nStopIteration: \n\n\n\nfor i in a:\n    print(i)\n\n묵\n묵\n묵\n찌가 나와서 for문을 멈춥니다.\n\n\n\n\n파이썬의 비밀 1~5\n\n파이썬의 비밀1: 자료형은 클래스의 비밀이다.(11주차)\n파이썬의 비밀2: 클래스에는 __str__처럼 숨겨진 메서드가 존재한다. 이를 이용하여 파이썬 내부의 기능을 가로챌 수 있다.(12주차0523)\n파이썬의 비밀3: 주피터노트북에서는 “오브젝트이름+엔터” 를 쳐서 나오는 출력은 __repr__로 가로챌 수 잇다.(주피터의 비밀)\n파이썬의 비밀4: 함수와 클래스는 숨겨진 메서드에 __call__을 가진 오브젝트일 뿐이다.\n파이썬의 비밀5: for문의 비밀 (iterable object, iterator, StopIteration Error)"
  },
  {
    "objectID": "posts/Python/4. Class/python 14_0606.html#클래스공부-9단계",
    "href": "posts/Python/4. Class/python 14_0606.html#클래스공부-9단계",
    "title": "파이썬 (0606) 14주차",
    "section": "클래스공부 9단계",
    "text": "클래스공부 9단계\n\n예비학습 (변수의 범위)\n커널을 재시작하고 아래를 관찰하자\n예제1\n- 관찰1: 함수내의 변수 출력\n\ndef f():\n    x=10\n    print(x)\n\n\nf()\n\n10\n\n\n- 관찰2: 함수내의 변수가 없을 경우 출력이 되지 않음\n\ndef g():\n    print(x)\n\n\ng()\n\nNameError: name 'x' is not defined\n\n\n- 관찰3: 동일한 이름의 변수가 global에 있다면 함수내에 (local에) 그 이름의 변수가 선언되지 않아도 global의 변수를 빌려서 사용함\n\nx=20        # global\ndef g():    # local\n    print(x)\n\n\ng()\n\n20\n\n\n- 관찰4: f()가 실행되면서 x=10이 함수내에 (=local에) 실행되지만 이 결과가 외부의 x=20에 (=global에) 영향을 미치지는 못함\n\nf()\n\n10\n\n\n\nx\n\n20\n\n\n예제2\n(코드1)\n\nx= 38\ndef nextyear():\n    y=x+1\n    print(x,y)\nnextyear()\n\n38 39\n\n\n(코드2)\n\nx= 38\ndef nextyear():\n    y=x+1\n    print(x,y)\n    x=0\nnextyear()\n\nUnboundLocalError: local variable 'x' referenced before assignment\n\n\n- 해석: - 잘못된 해석: 코드1은 실행되었고 코드 2에서 에러가 남. 코드1과 2의 차이점은 x=0 이라는 코드가 코드2에 추가로 포함되어 있다는 것이다. 따라서 x=0이 잘못된 코드이고 이걸 실행하는 과정에서 에러가 발생했다. - 올바른 해석: 코드1에서는 x가 global variable 이고 코드2에서는 x가 local bariable이어서 생기는 문제\n- 코드2의 올바른 수정\n\nx= 38\ndef nextyear():\n    x=0\n    y=x+1\n    print(x,y)\nnextyear()\n\n0 1\n\n\n\n\n인스턴스 변수, 클래스 변수 (12주차) 0518\n- 예비학습이 주는 교훈\n(원칙1) global 에서 정의된 이름은 local에서 정의된 이름이 없을 경우 그를 대신할 수 있다. (local은 경우에 따라서 global에 있는 변수를 빌려 쓸 수 있다.)\n(원칙2) local과 global 에서 같은 이름이 ’x’가 각각 정의되어 있는 경우? global의 변수와 local의 변수는 각각 따로 행동하며 서로 영향을 주지 않는다. (독립적이다)\n\n만약에 local이 global의 변수를 같이 쓰고 있었다고 할지라도, 추후 새롭게 local에 이름이 새롭게 같은 이름의 변수가 정의된다면 그 순간 local과 global의 변수를 각자 따로 행동하며 서로 영향을 주지 않는다. \\(\\to\\) 아래 예제 확인\n\n\nx=10\ndef f():\n    print(x)\n\n\nf() # x를 빌려쓰는 신세\n\n10\n\n\n\ndef f():\n    x=20   # 이제 새롭게 x를 정의했으니까\n    print(x)\n\n\nf()    # 다른길을 간다\n\n20\n\n\n- 이전에 공부하였던 인스턴스변수와 클래스변수 역시 비슷한 행동을 보인다.\n\nclass Moo:\n    x=0  # 클래스변수\n    \n    \n    ## 인스턴스변수는 self.x 또는 __init__ 이렇게\n\n\nmoo=Moo()\n\n(관찰1)\n\nMoo.x, moo.x\n\n(0, 0)\n\n\n\nmoo.x는 사실 정의한적 없지만 Moo.x 를 빌려쓰고 있다 (원칙1)\n\n(관찰2)\n\nMoo.x=100\n\n\nMoo.x, moo.x\n\n(100, 100)\n\n\n\nMoo.x를 변화시키면 moo.x도 변화한다. (빌려쓰고 있는 것이므로, 원칙1 재확인)\n\n(관찰3)\n\nmoo.x = 200\n\n\nMoo.x, moo.x\n\n(100, 200)\n\n\n\nmoo.x=200 을 하는 순간 새롭게 인스턴스 변수를 선언한 셈이 된다. 따라서 원칙2가 적용되어 이제부터 Moo.x와 moo.x는 서로 독립적으로 행동한다.\n\n(관찰4)\n\nMoo.x= -99\n\n\nMoo.x, moo.x\n\n(-99, 200)\n\n\n\nmoo.x=99\n\n\nMoo.x, moo.x\n\n(-99, 99)\n\n\n\nMoo.x를 바꾼다고 해서 moo.x가 영향받지 않고 moo.x를 바꿔도 Moo.x가 영향 받지 않음 (완전히 독립, 원칙2의 재확인)\n\n- 포인트: (1) 클래스변수와 인스턴스 변수의 구분 (2) 인스턴스 변수가 정의되지 않으면 클래스변수를 빌려쓸 수 있음 (3) 인스턴스변수와 클래스변수가 같은 이름으로 저장되어 있으면 각각 독립적으로 행동\n\n\n인스턴스 메서드\n- self의 비밀: 사실 클래스에서 정의된 함수의 첫번째 인자의 이름이 꼭 self일 필요는 없다. (무엇으로 전달하든 클래스안에서 정의된 메소드의 첫번째 인자는 기본적으로 instance의 태명 역할을 한다.)\n\nclass Moo:\n    def __init__(abab):\n        abab.name = 'boram'\n    def f(self):\n        print(self.name)\n\n\nmoo=Moo()\n\n\nmoo.name\n\n'boram'\n\n\n\nmoo.f()\n\nboram\n\n\n\n# self대신에 ababab 이런거 써도 되긴 함\n\n- 인스턴스 메서드: 위의 __init__ 와 f 와 같이 첫번째 인자를 인스턴스의 태명으로 받는 함수를 인스턴스 메서드(간단히 메서드) 라고 한다. - 인스턴스 메소드는 self.f() 와 같이 사용한다. 의미는 f(self) 이다.\n\nmoo.name = 'hynn'\n\n\nmoo.__init__()   # 인스턴스메서드의 사용예시: self.__init__()의 꼴로 사용\n\n\nmoo.name\n\n'boram'\n\n\n\nmoo.f() # 인스턴스메서드의 사용예시: self.__init__()의 꼴로 사용\n\nboram\n\n\n\nMoo.__init__()  # 사용안됨\n\nTypeError: __init__() missing 1 required positional argument: 'abab'\n\n\n\nMoo.f()  # 사용안됨\n\nTypeError: f() missing 1 required positional argument: 'self'\n\n\n\n\n클래스 메서드\n- 클래스 메서드: 함수의 첫 인자로 클래스오브젝트를 받는 메서드를 클래스 메서드라고 한다.\n- 목표: Moo.f()와 같은 형태로 사용할 수 있는 함수를 만들어 보자 -> 클래스 메서드를 만들어보자!\n\nclass Moo:\n    def f(self):\n        print(\"인스턴스 메서드\")\n\n\nmoo=Moo()\n\n\nmoo.f()\n\n인스턴스 메서드\n\n\n\nMoo.f()\n\nTypeError: f() missing 1 required positional argument: 'self'\n\n\n\nclass Moo:\n    @classmethod\n    def f(cls):  # 함수의 첫 인자로 클래스오브젝트를 받는다. cls는 클래스 Moo의 가칭이라고 생각하면 된다.\n        print(\"클래스 메서드\")\n\n\nmoo=Moo()\n\n\nMoo.f()\n\n클래스 메서드\n\n\n\nmoo.f()  # 상위에서 정의한걸 빌려옴.. \n#인스턴스 메서드를 따로 정의한적은 없지만 같은 이름의 클래스 메서드가 있으므로 빌려서 씀\n\n클래스 메서드\n\n\n- 예제\n\nclass Moo:\n    @classmethod\n    def set_class_x(cls,value): # 클래스메서드\n        cls.x=value   # 클래스변수 선언, note: Moo.x = value와 같은 코드 \n    def set_instance_x(self, value): # 인스턴스메서드\n        self.x = value  # 인스턴스 변수선언\n\n\nmoo=Moo()\n\n\nMoo.set_class_x(10)   # 클래스 메서드로 클래스 변수에 10을 설정\n\n\nMoo.set_instance_x(10)   # 클래스에서 인스턴스 메서드를 사용 -> 사용 불가\n\nTypeError: set_instance_x() missing 1 required positional argument: 'value'\n\n\n\nMoo.x, moo.x   # 인스턴스 변수는 따로 설정하지 않았지만 클래스 변수값을 빌려쓰고 있음\n\n(10, 10)\n\n\n\nmoo.set_class_x(20) # 인스턴스에서는 원래 set_class_x 라는 메서드는 없지만 클래스에는 있어서 빌려씀\n\n\nMoo.x, moo.x  # 현재 moo.x는 클래스 변수를 빌려쓰고 있는 상황이므로 같이 바뀜\n\n(20, 20)\n\n\n\nmoo.set_instance_x(-20) # 인스턴스에서 인스턴스 메서드를 사용하여 인스턴스 변수값을 -20으로 설정\n#-> 이때부터 인스턴스변수와 클래스 변수는 서로 독립적인 노선을 간다.\n\n\nMoo.x, moo.x \n\n(20, -20)\n\n\n\nMoo.set_class_x(30)   # 독립적인 노선을 가기로 했으므로 클래스변수만 30으로 바뀜\n\n\nMoo.x, moo.x \n\n(30, -20)\n\n\n\nmoo.set_class_x(-40)   # 여전히 인스턴스에서 set_class_x라는 함수는 없으므로 클래스메서드를 빌려쓰고 있음\nMoo.x, moo.x\n\n(-40, -20)\n\n\n\n\n스태틱 메서드\n- 스태틱 메서드: 첫 인자로 인스턴스와 클래스 모두 받지 않음. (클래스 안에 정의되어 있지만 그냥 함수와 같음)\n\nclass Cals:\n    @staticmethod\n    def add(a,b):\n        return a+b\n    @staticmethod\n    def sub(a,b):\n        return a-b\n\n\nfs = Cals()\n\n\nfs.add(1,2)\n\n3\n\n\n\nfs.sub(1,2)\n\n-1\n\n\n\nfs는 그냥 함수들을 묶어놓은 느낌? 정리하기 편하게"
  },
  {
    "objectID": "posts/Python/4. Class/python 14_0606.html#클래스공부-10단계",
    "href": "posts/Python/4. Class/python 14_0606.html#클래스공부-10단계",
    "title": "파이썬 (0606) 14주차",
    "section": "클래스공부 10단계",
    "text": "클래스공부 10단계\n\n문자열 join\n- 예제\n\nlst = list('abcd')\n\n\nlist('abcd')\n\n['a', 'b', 'c', 'd']\n\n\n\n'abcd' #위의 리스트를 이렇게 모여서 쓰여지게 하고 싶다\n\n'abcd'\n\n\n\n''.join(lst)\n\n'abcd'\n\n\n- 해설: ’’는 string object이고 .join는 string object에 소속된 메서드이다.\n\na=''\n\n\na?\n\n\nType:        str\nString form: \nLength:      0\nDocstring:  \nstr(object='') -> str\nstr(bytes_or_buffer[, encoding[, errors]]) -> str\nCreate a new string object from the given object. If encoding or\nerrors is specified, then the object must expose a data buffer\nthat will be decoded using the given encoding and error handler.\nOtherwise, returns the result of object.__str__() (if defined)\nor repr(object).\nencoding defaults to sys.getdefaultencoding().\nerrors defaults to 'strict'.\n\n\n\n\n\na.join?\n\n\nSignature: a.join(iterable, /)\nDocstring:\nConcatenate any number of strings.\nThe string whose method is called is inserted in between each given string.\nThe result is returned as a new string.\nExample: '.'.join(['ab', 'pq', 'rs']) -> 'ab.pq.rs'\nType:      builtin_function_or_method\n\n\n\n\n\na.join(lst)  # join(a,lst)와 같은효과\n\n'abcd'\n\n\n- join의 간단한 사용방법\n\n'-'.join(lst)  # '' 안에 - 넣어서 \n\n'a-b-c-d'\n\n\n\n\nmatplotlib\n- 파이썬의 모든 것은 객체이다: - matplotlib의 다른사용 (객체지향적 언어로 그림 그리기!)\n- 그림 오브젝트 생성\n\nfig = plt.figure() # 그림 오브젝트가 생성되고 fig라는 이름 \n\n<Figure size 432x288 with 0 Axes>\n\n\n\nfig\n\n<Figure size 432x288 with 0 Axes>\n\n\n- 그림 오브젝트의 액시즈를 확인 -> 아무것도 없음..\n\nfig.axes\n\n[]\n\n\n- (0,0) 자리에 (가로=1, 세로=1) 크기의 그림틀(액시즈)을 넣어보자.\n\nfig.add_axes([0,0,1,1])\n\n<Axes:>\n\n\n\nfig.axes\n\n[<Axes:>]\n\n\n\nfig\n\n\n\n\n- 액시즈추가\n\nfig.add_axes([0,1.2, 1,1])  #   (0,1.2) 위치에  가로길이가 1, 세로길이가 1인 그림\n\n<Axes:>\n\n\n\nfig\n\n\n\n\n- (0.5,0.5) 위치에 (가로=1, 세로=1 ) 크기의 그림 추가\n\nfig.add_axes([0.5,0.5,1,1])\n\n<Axes:>\n\n\n\nfig\n\n\n\n\n- fig 의 세번째 액시즈에 접근\n\na3=fig.axes[2]   # id 찍어보면 어딘가게 엊장되어 있음. 오브젝트임\na3\n\n<Axes:>\n\n\n- 액시즈의 메소드중에 plot가 있음 -> 이것을 그림으로 그려보자\n\na3.plot([1,2,3],[4,5,3],'--r')   # --r : 점선으로 빨간색으로 \n\n\nfig\n\n\n\n\n- 다시 세번째 축에 접근하여 다른그림을 그려보자.\n\nfig.axes[-1].plot([1,2,3],[5,4,3],':o')\nfig\n\n\n\n\n- 이제 첫번째 축에 접근하여 다른그림을 그려보자.\n\nfig.axes[0].plot([1,2,3],[4,1,4],'--b')\nfig\n\n\n\n\n- 클래스에 대한 이해가 없다면 위와 같은 그림을 그리기도 힘들고 코드를 해석하기도 힘듬\n\n\nshallow copy\n- 아래의 코드를 관찰하자.\n\na=[1,2,3]\nb=a\na=a+[4]\n\n현재 a,b의 출력 결과는?\n\na, b\n\n([1, 2, 3, 4], [1, 2, 3])\n\n\n- 이제 다시 아래의 코드를 관찰하자.\n\na=[1,2,3]\nb=a\na.append(4)\n\n현재 a,b의 출력 결과는?\n\na, b\n\n([1, 2, 3, 4], [1, 2, 3, 4])"
  },
  {
    "objectID": "posts/Python/1. Basic/python 3_0321.html",
    "href": "posts/Python/1. Basic/python 3_0321.html",
    "title": "파이썬 (0321) 3주차",
    "section": "",
    "text": "(리스트가 아니고) 튜플을 쓰는 이유\n- 책의 설명 (파이썬에 한정되는 것은 아니고 모든 언어에 존재하는 불변형 객체에 적용가능한 설명) - 실수방지 - 빠르다, 다중작업에 유리하다, 여러사람과 작업하기에 유리하다, 깊은복사/얕은복사시 원하지 않는 오류(side effect라고 함)를 방지할 수 있다, 메모리관리에도 유리함 등등 - 느낌: 불변형은 기능제한이 있는데 가볍고 빠른, 가변형은 기능은 풍부하지만 약간 느리고 무거운 느낌임 (불변형: 라면사리, 가변형:라면)\n- 교수님 설명 (파이썬 한정 불변객체, 즉 튜플에 대한 설명) - 튜플의 장점은 소괄호의 생략에 있음 (파이썬과 줄리아만 가능) - 이것이 언패킹구문과 결합하여 어마무시한 가독성을 제공한다.\n\ndef mycal(a,b):\n    return a+b, a-b, a*b, a/b # 여러개의 값을 리턴하는 듯 보인다. ->  사실은 길이가 4인 튜플 1개를 리턴\n\n\nmycal(2,3)\n\n(5, -1, 6, 0.6666666666666666)\n\n\n\n_, _, mulrslt, _ = mycal(2,3) # 병렬할당\n\n\nmulrslt\n\n6\n\n\n- 의문: 왜 튜플만 괄호를 생략할 수 있지?\n- 교수님 생각 - 튜플을 먼저 만들고, 괄호를 생략하는 문법을 추가한것은 아닐것임 - 원래 괄호없이 컴마만 대충찍어서 선언가능한 간단한 타입의 벡터형을 만들고 싶었을 것임 - 왜? 괄호없는 벡터를 만들고 + 연패킹을 사용하면 여러가지 구문들이 엄청나게 간단해짐 - 컴마컴마로 선언하는 벡터는 한 두번 쓰고 버리는 경우가 많으며 대부분 이름도 필요 없음 -> 원소에 접근해서 sorting하여 순서를 바꾸고 싶다던가 원소를 추가할 이유가 없음 -> 비싼 가변형으로 만들 이유가 없다. - 필요한것: 데이터가 벡터의 형태로 모여있기만 하면 된다.\n- 다른사람들 의견 (컴공과) - 튜플 + 언패킹에 충격 \\(\\to\\) 파이썬 편하다..\n\n\n인덱싱고급(스트라이딩)\n- 스트라이딩 [start:stop:step]\n\nlst = list('abcdefgh')\nlst\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n\n\n\nlst[0:8:2]\n\n['a', 'c', 'e', 'g']\n\n\n- 생략\n\nlst[:]\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n\n\n\nlst[::2]\n\n['a', 'c', 'e', 'g']\n\n\n\nlst[:8:2]\n\n['a', 'c', 'e', 'g']\n\n\n- 예제: 짝수/홀수 원소 추출\n\nlst\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n\n\n\nlst[::2]  # 1,3,5,7, ... 홀수 원소\n\n['a', 'c', 'e', 'g']\n\n\n\nlst[1::2] # 2,4,6,8,... 짝수 원소\n\n['b', 'd', 'f', 'h']\n\n\n- step = -1이면?\n\nlst[::-1]    # 뒤에서부터\n\n['h', 'g', 'f', 'e', 'd', 'c', 'b', 'a']\n\n\n\nreverse와 같은 기능\n\n(reverse)와 비교\n관찰1: reverse 메소드는 리소드 자체를 변화시킴\n\nlst = list('abcdefgh')\nlst\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n\n\n\nlst.reverse()  # 리버스는 자체가 변화한다.\nlst  \n\n['h', 'g', 'f', 'e', 'd', 'c', 'b', 'a']\n\n\n관찰2: [::-1] 는 리스트는 변화시키지 않음\n\nlst = list('abcdefgh')\nlst\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n\n\n\nlst[::-1]\n\n['h', 'g', 'f', 'e', 'd', 'c', 'b', 'a']\n\n\n\nlst\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n\n\n- 사실 -step은 쓰는 것이 조금 까다롭다.\n(예제) 처음과 끝을 생략하지 않고 아래와 동일한 효과를 주는 코드를 만들어 보자.\n\nlst = list('abcdefgh')\nlst[::-1]\n\n['h', 'g', 'f', 'e', 'd', 'c', 'b', 'a']\n\n\n결국 lst[?:?:-1]의 꼴에서 적당히 ?의 값을 채우면 된다.\n\nlst[-1::-1] # 일단 첫 시작ㄷ은 제일 마지막 원소\n\n['h', 'g', 'f', 'e', 'd', 'c', 'b', 'a']\n\n\n\nlst[-1:0:-1] # 마지막 인덱스는 포함x\n\n['h', 'g', 'f', 'e', 'd', 'c', 'b']\n\n\n\nlst[-1:-1:-1] \n\n[]\n\n\n잠깐 인덱스를 생각해보자.\n\n\n\na\nb\nc\nd\ne\nf\ng\nh\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n\n\n\n\nlst[-1:-8:-1] \n\n['h', 'g', 'f', 'e', 'd', 'c', 'b']\n\n\n\nlst[-1:-9:-1] \n\n['h', 'g', 'f', 'e', 'd', 'c', 'b', 'a']\n\n\n(예제)\n\nlst[2::2]\n\n['c', 'e', 'g']\n\n\n\nlst[-2::-2]\n\n['g', 'e', 'c', 'a']\n\n\n\nlst[-2:2:2]\n\n[]\n\n\n\nlst[2:3:2]\n\n['c']\n\n\n\nlst[2:2:2]\n\n[]\n\n\n\nlst[2:2:-2]\n\n[]\n\n\n결론: -step을 자주 쓰진 말자\n\n\n컴프리헨션 고급 (if문이 포함된 컴프리헨션)\n- 예제: 제곱수준에서 12로 나누어 떨어지는 수만 원소로 가지는 리스트를 만들고 싶다. - 제곱수: 1,4, 9, 16, 25, 36, … - 12로 나누어 떨어지는 수: 36…\n(예비학습)\n\n12 % 5 # 나머지 리턴\n\n2\n\n\n(풀이)\n\nlst=[]\nfor i in range(1,101):\n    if (i**2 % 12 == 0 ):\n        lst.append(i**2)\n\n\nlst\n\n[36,\n 144,\n 324,\n 576,\n 900,\n 1296,\n 1764,\n 2304,\n 2916,\n 3600,\n 4356,\n 5184,\n 6084,\n 7056,\n 8100,\n 9216]\n\n\n(풀이2)\n\n[i**2 for i in range(1,101) if (i**2 % 12==0)]\n\n[36,\n 144,\n 324,\n 576,\n 900,\n 1296,\n 1764,\n 2304,\n 2916,\n 3600,\n 4356,\n 5184,\n 6084,\n 7056,\n 8100,\n 9216]\n\n\n\n\n함수고급 (조건부리턴)\n- 홀수 짝수를 판별하는 함수 만들기1\n\ndef test(a):\n    if a% 2 ==0:\n        return 'even'\n    else:\n        return 'odd'\n\n\ntest(0)\n\n'even'\n\n\n\ntest(3)\n\n'odd'\n\n\n\n[test(a) for a in range(1,11)]\n\n['odd', 'even', 'odd', 'even', 'odd', 'even', 'odd', 'even', 'odd', 'even']\n\n\n- 홀수 짝수를 판별하는 함수 만들기2\n\ndef test(a):\n    return 'even' if a%2==0 else 'odd'\n\n\ntest(2)\n\n'even'\n\n\n\n[test(a) for a in range(1,11)]\n\n['odd', 'even', 'odd', 'even', 'odd', 'even', 'odd', 'even', 'odd', 'even']\n\n\n\n\nlen함수\n- 0차원 자료형은 len함수가 동작하지 않음\n\na=1\nlen(a)\n\nTypeError: object of type 'int' has no len()\n\n\n\na=True\nlen(a)\n\nTypeError: object of type 'bool' has no len()\n\n\n\na=3.14\nlen(a)\n\nTypeError: object of type 'float' has no len()\n\n\n\nnote: 이것이 어떠한 수학적인 의미를 가지거나 0차원의 본질적인 진리를 뜻하는 것은 아니다. R에서는 1, 3.14, True의 길이가 1로 존재함\n\n- 1차원 자료형은 len함수가 동작\n\na='boram'\nlen(a)\n\n5\n\n\n\na=[1,2,3,4,5,6,7]\nlen(a)\n\n7\n\n\n\na=1,2,3,4\nlen(a)\n\n4\n\n\n\na=range(10)\nlen(a)\n\n10\n\n\n- 길이가 1인 1차원 자료형과 0차원 자료형은 다른것임\n\na='g'\nlen(a)\n\n1\n\n\n\na=[1]   \nlen(a)\n\n1\n\n\n\na=(1,)\nlen(a)\n\n1\n\n\n\na=range(1)\nlen(a)\n\n1\n\n\n- 길이가 0인 1차원 자료형도 존재함\n\na=''\nlen(a)\n\n0\n\n\n\na=[]\nlen(a)\n\n0\n\n\n\na=()\nlen(a)\n\n0\n\n\n\na=range(0)\nlen(a)\n\n0\n\n\n\n\nsummary : str, list, tuple\n- str,list, tuple은 모두 시퀀스형이라는 공통점이 있다. \\(\\to\\) 원소의 위치번호로 인덱싱이 가능\n\nlst=[1,2,3,4]\n\n\nlst[0]  #위치번호=0\n\n1\n\n\n\nlst[-1]  #위치번호=1\n\n4\n\n\n- str, list, tuple은 차이점도 존재함. 잠깐 정리해보자.\n*** 시퀀스형의 카테고리***\n\n컨테이너형: list, tuple\n균일형: str\n가변형: list\n불변형: tuple, str\n\n*****표로 정리하면*****\n\n\n\n\n컨테이너형\n균일형\n\n\n\n\n가변형\nlist\n-\n\n\n불변형\ntuple\nstr\n\n\n\n- 시퀀스형이 아닌 1차원 자료형도 있을까? 원소의 위치번호로 인덱싱이 불가능한 자료형\n- 왜 이런게 필요할까? - 벡터에서 원소를 뽑는것은 정보의 모임에서 정보를 검색하는 것과 같다. - 정보를 순서대로 나열한 뒤에 그 순서를 이용하여 검색하는 방법은 유용하다. - 하지만 경우에 따라서는 키워드 를 기억해서 그 키워드를 바탕으로 정보에 접근하는 방법이 유용할 수 있다.\n****카카오톡 대화내용 검색****\n(상황1) 오늘 아침에 와이프가 뭔가를 카톡으로 부탁함. 그런데 그 뭔가가 기억안남.\n(상황2) 개강전에 동료교수와 함께 저녁약속을 카톡으로 잡았었음. 그런데 그게 언제인지 기억안남.\n(상황3) 오늘아침 동료교수와 함께 점심약속을 카톡으로 잡았었음. 그런데 그 장소가 기억나지 않음\n- 순서대로 정리된 자료를 검색할때는 시퀀스형이 유리하다. 그런데 키워드로 검색하고 싶을 경우는 딕셔너리 타입이 유리하다.\n\n\ndict\n선언\n- 방법1\n\nscore={'boram':49, 'iu':80}\nscore\n\n{'boram': 49, 'iu': 80}\n\n\n\ntype(score)\n\ndict\n\n\n- 방법2\n\nscore=dict(boram=49, iu=80)\nscore\n\n{'boram': 49, 'iu': 80}\n\n\n\ntype(score)\n\ndict\n\n\n- 방법3\n\n_lst= [['boram',40],['iu',80]] \n_lst\n\n[['boram', 40], ['iu', 80]]\n\n\n\ndict(_lst)\n\n{'boram': 40, 'iu': 80}\n\n\n- 방법4\n\n_tpl = ('boram',49),('iu',80)\n_tpl\n\n(('boram', 49), ('iu', 80))\n\n\n\ndict(_tpl)\n\n{'boram': 49, 'iu': 80}\n\n\n원소추출\n\nscore={'boram':49, 'iu':80}\nscore\n\n{'boram': 49, 'iu': 80}\n\n\nboram의 점수를 추출하고 싶다면?\n\nscore[0]   # 이렇게 ㄴㄴ \n\nKeyError: 0\n\n\n\nscore['boram']   # 위치번호가 아닌 key를 넣어야 한다.\n\n49\n\n\n- 리스트로 저장했다면?\n\nscore=[['boram',49],['iu',80]]\nscore\n\n[['boram', 49], ['iu', 80]]\n\n\n(방법1)\n\nscore[0][1]  # boram의 점수를 출력하란 의미 , 가독성이 떨어짐,\n\n49\n\n\n(방법2)\n\n_keys = [score[i][0] for i in range(len(score))]   #리스트컴프리헨션\n_keys\n\n['boram', 'iu']\n\n\n\n_values = [score[i][1] for i in range(len(score)) if score[i][0]=='boram']  \n_values\n\n[49]\n\n\n원소추가, 변경, 삭제\n\nscore={'boram':49, 'iu':80}\nscore\n\n{'boram': 49, 'iu': 80}\n\n\n- 추가\n\nscore['hynn']=99 # 추가\n\n\nscore\n\n{'boram': 49, 'iu': 80, 'hynn': 99}\n\n\n\nscore['boram']\n\n49\n\n\n- 변경\n\nscore['iu']=99   # 변경 ( 가변형)\nscore\n\n{'boram': 49, 'iu': 99, 'hynn': 99}\n\n\n\nscore\n\n{'boram': 49, 'iu': 99, 'hynn': 99}\n\n\n-삭제1\n\nscore={'boram':49, 'iu':80, 'hynn':99}\ndel score['boram']   # 삭제 방법 1\nscore\n\n{'iu': 80, 'hynn': 99}\n\n\n-삭제2\n\nscore={'boram':49, 'iu':80, 'hynn':99}\nscore.pop('boram')   # 삭제 방법 2\nscore\n\n{'iu': 80, 'hynn': 99}\n\n\n- 참고로 리스트였다면 이러한 삭제작업역시 비효율적이였을 것임\n\nscore = [['guebin',49],['iu',80],['hynn',99]] \nscore\n\n[['guebin', 49], ['iu', 80], ['hynn', 99]]\n\n\n\nscore = [[key,val] for key,val in score if key != 'guebin'] \nscore\n\n[['iu', 80], ['hynn', 99]]\n\n\n(숙제) 길이가 4인 dictionary를 생성 - len 함수를 이용하여 길이를 측정 - key를 이용하여 각 원소에 접근하여 보기\n\n_number = {'sung':2195, 'park':2836, 'choi':4236, 'kim':4738}\n_number\n\n{'sung': 2195, 'park': 2836, 'choi': 4236, 'kim': 4738}\n\n\n\nlen(_number)\n\n4\n\n\n\n_number['park']\n\n2836\n\n\n\n_number['choi']\n\n4236\n\n\n\n_number['jung']=4280\n\n\n_number\n\n{'sung': 2195, 'park': 2836, 'choi': 4236, 'kim': 4738, 'jung': 4280}\n\n\n\n_number.pop('kim')\n\n4738\n\n\n\n_number\n\n{'sung': 2195, 'park': 2836, 'choi': 4236, 'jung': 4280}"
  },
  {
    "objectID": "posts/Python/1. Basic/python 1_0307.html",
    "href": "posts/Python/1. Basic/python 1_0307.html",
    "title": "파이썬 (0307) 1주차",
    "section": "",
    "text": "- 파이썬의 기본자료형은 int, float, bool, str, list, tuple, dict, set 등이 있다.\n\n0차원 자료형: int, float, bool\n1차원 자료형: str, list, tuple, dict, set\n\n\n\n- int형\n\na=100\n\n\ntype(a)\n\nint\n\n\n- float형\n\na?\n\n\na=1.2*3\n\n\ntype(a)\n\nfloat\n\n\n- bool형\n\na=True   # 숫자1\nb=False  # 숫자0\n\n\ntype(a)\n\nbool\n\n\n\n# bool형의 연산\na+b\n\n1\n\n\n- complex형\n\na=1+2j\nb=2-2j\n\n\na\n\n(1+2j)\n\n\n\ntype(a)\n\ncomplex\n\n\n\na+b\n\n(3+0j)\n\n\n\ntype(a+b)\n\ncomplex\n\n\n- 형태변환: float -> int\n\na=3.0\ntype(a)\n\nfloat\n\n\n\na=int(a)\n\n\na?\n\n\na=3.14\nint(a)\n\n# 0.14날라가고 3만나옴. 정보의 손실이 있다.\n\n3\n\n\n- 형태변환: int $$ float\n\na=3\ntype(a)\n\nint\n\n\n\na=float(a)\ntype(a)\n\nfloat\n\n\n- 형태변환: bool $$ int/float\n(예시1)\n\na=True\ntype(a)\n\nbool\n\n\n\nint(a)\n\n1\n\n\n\nfloat(a)\n\n1.0\n\n\n(예시2)\n\na=1\nbool(a)\n\nTrue\n\n\n(예시3)\n\na=1.0\nbool(a)\n\nTrue\n\n\n\na=0.0\nbool(a)\n\nFalse\n\n\n- 이상한 형태변환도 가능하다\n\nbool(-3.14)\n\nTrue\n\n\n\nbool(3.14)\n\nTrue\n\n\n\nbool(0)\n\nFalse\n\n\n\nbool(3.24342)\n\nTrue\n\n\n\n# 위와 같은 코드를 의도적으로 사용하진 않는다. \n\n- 형태변환이 항상 가능한 것도 아님\n\nfloat(3+0j) # 사실상 3+0j= 3이므로 float으로 형변환하면 3.0이 되어야 할 것 같은데 오류가 남\n\nTypeError: can't convert complex to float\n\n\n- 암묵적형변환 (implicit)\n(예비학습) implicit의 의미 - 추운날씨 -> 보일러좀 틀자! 명시적(explicit) / 오늘 날씨가 좀 춥지 않아? (implicit) - 짜장면 먹을래? -> 싫어 (explicit) / 난 어제 짜장면 먹었는데… (implicit)\n\nint(True) #명시적\n\n1\n\n\n\nTrue * 1 # 암묵적형\n\n1\n\n\n\n1 * 1.0\n\n1.0\n\n\n\nTrue+True\n\n2\n\n\n\n\n\n- str\n\n# 선언\na='br'\n\n\na\n\n'br'\n\n\n\n# 연산\n# 더하기 연산\na='x'\nb='2'\n\n\na+b\n\n'x2'\n\n\n\n# 빼기 연산 없다. \na-b\n\nTypeError: unsupported operand type(s) for -: 'str' and 'str'\n\n\n\n# 곱하기 연산\na*2\n\n'xx'\n\n\n\n2*a\n\n'xx'\n\n\n\n# 의미상 맞지 않는 것은 수행되지 않는다.\na='x'\nb='y'\na+b\n\n'xy'\n\n\n\na*b\n\nTypeError: can't multiply sequence by non-int of type 'str'\n\n\n\n# 나눗셈연산은 없다. \na='xx'\na/2\n\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\n\n\n\n\n\n- str은 하나의 벡터 문자가 여러개 있는 형태라고 생각하기\n\na='boram'\na\n\n'boram'\n\n\n\n5개의 칸에 글씨가 하나씩 들어가 있음\n\n\na[0]  # 0이 첫번쨰 원소\n\n'b'\n\n\n\na[1] # 두번째 원소\n\n'o'\n\n\n\n# 마지막 원소를 호출하려면 -1로 호출할 수도 있다.\na[-1]\n\n'm'\n\n\n\na[4]\n\n'm'\n\n\n\na[-2]\n# 마지막에서 2번째 원소는 -2로 호출 가능\n\n'a'\n\n\n\n어려개의 원소는 :을 이용하여 호출할 수 있음\n\n\na[0:3] # a[0], a[1], a[2]까지만 뽑힌다. a[3]은 호출되지 않는다.\n\n'bor'\n\n\n\na[1:3]\n\n'or'\n\n\n\nindex=1부터 시작해서 마지막 원소까지 호출하려면?\n\n\na[5]\n\nIndexError: string index out of range\n\n\n\na[1:5]  # a[5]는 없는데,, 이렇게 쓰니까 헷갈릴 수 있다.\n\n'oram'\n\n\n\na[1:-1]   # 이것은 a[1:4] 와 같음\n\n'ora'\n\n\n\n# 해결책! 생략한다.\na[1:]\n\n'oram'\n\n\n- 생략의 응용1\n\na='k-pop'\na\n\n'k-pop'\n\n\n\na[2:]\n\n'pop'\n\n\n\na[2:5]\n\n'pop'\n\n\n- 생략의 응용2\n\na='k-pop'\na\n\n'k-pop'\n\n\n\na[0:2]\n\n'k-'\n\n\n\na[:2]   # 앞을 생략하면 첫 원소부터 나온다.\n\n'k-'\n\n\n- 생략의 응용3\n\na='k-pop'\na\n\n'k-pop'\n\n\n\na[:]\n\n'k-pop'\n\n\n\na[0:5]\n\n'k-pop'\n\n\n\n\n- 파이썬의 변수는 단순히 정보를 담는 그릇이 아니다. 유용한 기능을 제공하는 경우가 있다.\n\na='ABCD'  #a라는 변수는 'ABCD'라는 정보를 담는 그릇의 역할만 하지 않고, 특화된 어떠한 기능도 제공한다.\na\n\n'ABCD'\n\n\n\na.lower() #소문자변환\n\n'abcd'\n\n\n\n# lower()는 문자열에 특화된 기능이며 아래 내용은 안됨\na=3.14\na.lower()\n\nAttributeError: 'float' object has no attribute 'lower'\n\n\n- 자료형에 특화된 기능(=함수)을 확인하는 방법 a. + tab 으로 목록 확인 가능\n\na='boram'\n\n\na.lower?\n\n\na.upper()   # 대문자 변환\n# upper(a)\n\n'BORAM'\n\n\n\na.capitalize()\n\n'Boram'\n\n\n- 마음의눈: a.f() 형태를 읽는 팁 - a.f()는 f(a)로 생각하면 편리함 - a.f(2)는 f(a,2)로 생각하면 편리함 - 이런점에서 R %>% 연산자와 비슷하다고 생각할 수 있다. (약간 다름)\n- 사실 .은 좀 더 다양한 상황에서 쓰일 수 있다. 변수이름.함수이름() 의 형태가 아니라 - 패키지이름.함수이름() - 패키지이름.변수이름 - 패키지이름.패키지이름.함수이름()\n… 와 같이 다양한 형태가 가능하다. 근본적인 고통점은 .을 기준으로 상위개념.하위개념으로 이해하는 것이 좋다.\n\n\n\n\n- len 함수 : 원소의 갯수를 알려주는 함수\n(0차원) len함수가 동작하지 않는다.\n\na=3.14\nlen(a)\n\nTypeError: object of type 'float' has no len()\n\n\n\nb=True\nlen(b)\n\nTypeError: object of type 'bool' has no len()\n\n\n(1차원) len함수가 잘 동작함\n\na='3.14'\nlen(a)\n\n4\n\n\n\nb=[1,2,3]\nlen(b)\n\n3\n\n\n숙제\n\na='BoramKim'\na\n\n'BoramKim'\n\n\n\na[:5]\n\n'Boram'\n\n\n\na[5:]\n\n'Kim'"
  },
  {
    "objectID": "posts/Python/1. Basic/python 4_0323.html",
    "href": "posts/Python/1. Basic/python 4_0323.html",
    "title": "파이썬 (0323) 4주차",
    "section": "",
    "text": "연산\n- 하나있다.`\n\nscore={'boram':49, 'iu':80}\nscore\n\n{'boram': 49, 'iu': 80}\n\n\n\n'boram' in score\n\nTrue\n\n\n\n\n'iu' in score\n\nTrue\n\n\n\n'hynn' in score\n\nFalse\n\n\n- in은 사실 다른 자료형도 가능하다`\n(관찰1)\n\n'a' in 'boram'\n\nTrue\n\n\n\n'c' in 'boram'\n\nFalse\n\n\n(관찰2)\n\ntpl = 1,2,3\ntpl\n\n(1, 2, 3)\n\n\n\n1 in tpl\n\nTrue\n\n\n\n4 in tpl\n\nFalse\n\n\n(관찰3)\n\nscore=[['boram',49], ['iu',80]]\nscore\n\n[['boram', 49], ['iu', 80]]\n\n\n\n['boram', 49] in score\n\nTrue\n\n\n- in 연산자가 dict형에 사용되면 key를 기준으로 True, False을 판단한다.\n\n\n메소드\n(get)\n\nscore={'boram':49, 'iu':80}\nscore\n\n{'boram': 49, 'iu': 80}\n\n\n\nscore.get('boram')\n\n49\n\n\n아래와 같은 기능\n\nscore['boram']\n\n49\n\n\n미묘한 차이점이 존재함\n\nscore['hynn']  # hynn이 없어서 키에러 출력, 그런 key는 없다..\n\nKeyError: 'hynn'\n\n\n\nscore.get('hynn')  #hynn이 없으면 아무것도 출력안함\n\n(kyes, values, items)\n-.keys()는 딕셔너리의 키를 리턴한다.\n\nscore={'boram':49, 'iu':80}\nscore\n\n{'boram': 49, 'iu': 80}\n\n\n\n?score.keys\n\n\n_keys=score.keys()\n_keys\n\ndict_keys(['boram', 'iu'])\n\n\n\ntype(_keys)   # 모르는 자료형이지만, list나 tuple과 같이 자료형을 바꿀수 있다.\n\ndict_keys\n\n\n\nlist(_keys)  # 아무튼 그 이상한 자료형도 리스트화가 가능\n\n['boram', 'iu']\n\n\n-.values()는 딕셔너리의 키를 리턴한다.\n\n_values=score.values()\n_values\n\ndict_values([49, 80])\n\n\n\ntype(_values)\n\ndict_values\n\n\n\nlist(_values)\n\n[49, 80]\n\n\n-.items()는 딕셔너리의 키를 리턴한다.\n\n_items=score.items()\n_items\n\ndict_items([('boram', 49), ('iu', 80)])\n\n\n\ntype(_items)\n\ndict_items\n\n\n\nlist(_items)\n\n[('boram', 49), ('iu', 80)]\n\n\n- for문에서의 dict\n(예시1)\n\nfor i in score.keys():\n    print(i)\n\nboram\niu\n\n\n\nfor i in score:\n    print(i)\n\nboram\niu\n\n\n\n딕셔너리 그자체도 for문에 넣을 수 있다.\ni에는 value가 삭제되어 들어간다. (즉 key만)\n결과를 보면 score대신에 score.keys()와 list(score)를 넣었을때와 결과가 같다.\n\n\nNote: list(score)하면 key만 리턴된다.\n\n(예시2)\n\nfor i in score.values():\n    print(i)\n\n49\n80\n\n\n(예시3)\n\nfor k in score.items():\n    print(k)\n\n('boram', 49)\n('iu', 80)\n\n\n(예시4)\n\nfor i,j in score.items():\n    print(i,j)\n\nboram 49\niu 80\n\n\n(예시5)\n\nfor i,j in score.items():\n    print(i + '의 중간고사 점수는 %s점입니다' %j)\n\nboram의 중간고사 점수는 49점입니다\niu의 중간고사 점수는 80점입니다\n\n\n[보충학습] 문자열 새치기\n\n'제 이름은 %s입니다.'  % '김보람' \n\n'제 이름은 김보람입니다.'\n\n\n\n'제 이름은 %s입니다.'  % [1,2]\n\n'제 이름은 [1, 2]입니다.'\n\n\n\n1+1\n\n2\n\n\n\n[1,2]+[3,4]\n\n[1, 2, 3, 4]\n\n\n\n%는 새치기연산자임. %s는 새치기하는 자리라고 생각\n\n보충학습끝\n\n\n딕셔너리 고급\n키는 문자열만 가능한 것이 아니다.\n- 정수키\n\nscore = {0:49, 1:80, 1:99} # key를 0,1,2로\nscore\n\n{0: 49, 1: 99}\n\n\n- 인덱싱은?\n\nscore[0] # 키로 인덱싱을 하고 있는데 마치 원소의 위치로 인덱싱을 하는 기분\n\n49\n\n\n- 그럼 혹시 이것도?\n\nscore[:2]\n\nTypeError: unhashable type: 'slice'\n\n\n\nscore[-1]   # 될리가 없지..\n\nKeyError: -1\n\n\n- key로 가능한 것이 문자열만 가능한 것이 아니라 다른 것도 가능하다. (숫자,튜플,,)\n(예시)\n\nscore={(0,'boram'):49, (1, 'iu'):80, (2, 'hynn'):99}\nscore\n\n{(0, 'boram'): 49, (1, 'iu'): 80, (2, 'hynn'): 99}\n\n\n\nscore[(0,'boram')]\n\n49\n\n\n\nscore[0,'boram'] #tuple이니까 가로 생략 가능\n\n49\n\n\n(예시)\n\nscore={('boram',0):10, ('boram',1):20, ('boram',2):30} #0은 출석점수, 1은 레포트 점수, 2는 중간고사 점수\nscore\n\n{('boram', 0): 10, ('boram', 1): 20, ('boram', 2): 30}\n\n\n\nscore[('boram',0)]\n\n10\n\n\n\nscore['boram',0]\n\n10\n\n\n\nscore[('broam,3')] = 99  # 보람의 기말고사 점수를 추가\n\n\nscore\n\n{('boram', 0): 10, ('boram', 1): 20, ('boram', 2): 30, 'broam,3': 99}\n\n\n- 문자열, 숫자값, 튜플의 공통점? 불변객체\n\na=11\n\n\na=22  # 22로 수정된 것이 아니고 재할당된것임..\n\n\na\n\n22\n\n\n\na='boram'\n\n\na='Broam'\n\n\na  # 이것도 재할당..\n\n'Broam'\n\n\n\na[0]\n\n'B'\n\n\n\na[0]='b'  # 문자열 불변\n\nTypeError: 'str' object does not support item assignment\n\n\n\n# 수정이랑 재할당을 구분하는 방법 -> 메모리 주소 값을 찍어보면 된다.\n\n[참고로만]\n(인트형은 불변)\n\na=1\na, id(a)\n\n(1, 2254873389360)\n\n\n\na=2\na, id(a)\n\n(2, 2254873389392)\n\n\n(문자열도 불변)\n\na='boram'\na, id(a)\n\n('boram', 2254959000432)\n\n\n\na='Boram'\na, id(a)\n\n('Boram', 2254988509296)\n\n\n(리스트는 가변)\n\na=list('boram')\na, id(a)\n\n(['b', 'o', 'r', 'a', 'm'], 2254989018304)\n\n\n\na[0]='B'\n\n\na,id(a)   #id가 같다. 편집!\n\n(['B', 'o', 'r', 'a', 'm'], 2254989018304)\n\n\n\n\n집합\n\n선언\n\na={'notebook', 'desktop'}\n\n\n\n원소추출\n- 일단 인덱스로는 못한다.\n\na={'notebook', 'desktop'}\na[0]\n\nTypeError: 'set' object is not subscriptable\n\n\n- 딱히 하는 방법이 없다. 그리고 이걸 하는 의미가 없다. 원소에 접근해서 뭐하려고…!!\n원소추가\n- 이건 의미가 있다.\n\na={'notebook', 'desktop'}\n\n\na.add('ipad')\na\n\n{'desktop', 'ipad', 'notebook'}\n\n\n\na.add('notebook') # 이미 원소로 있는 건 추가 되지 않음\na\n\n{'desktop', 'ipad', 'notebook'}\n\n\n원소삭제\n\na.remove('notebook')\na\n\n{'desktop', 'ipad'}\n\n\n연산\n- in 연산자\n\n1 in [1,2,3,4]\n\nTrue\n\n\n\n5 in [1,2,3,4]\n\nFalse\n\n\n\na=('desktop','ipad','notebook')\na\n\n('desktop', 'ipad', 'notebook')\n\n\n\n'notebook' in a\n\nTrue\n\n\n- 참고로 in 연산자는 집합에서만 쓰는 것은 아님\n- 합집합, 교집합, 차집합\n\nday1 = {'notebook', 'desktop'}\nday2 = {'notebook', 'ipad'}\n\n\nday1 | day2   # 합집합\n\n{'desktop', 'ipad', 'notebook'}\n\n\n\nday1 & day2 # 교집합\n\n{'notebook'}\n\n\n\nday1 - day2  # 차집합\n\n{'desktop'}\n\n\n\nday2 - day1\n\n{'ipad'}\n\n\n- 부분집합\n\nday1={'notebook','desktop'}\nday2= day1 | {'ipad'}\n\n\nday1\n\n{'desktop', 'notebook'}\n\n\n\nday2\n\n{'desktop', 'ipad', 'notebook'}\n\n\n\nday1<day2   # day1는 day2의 부분집합인가?\n\nTrue\n\n\n\nday2<day1\n\nFalse\n\n\n메소드\n- 합집합\n\nday1= {'notebook','desktop'}\nday2 = {'notebook','ipad'}\n\n\nday1.union(day2)\n\n{'desktop', 'ipad', 'notebook'}\n\n\n\n# 나머지 메소드는 스스로 찾아보세용\n\nfor문\n\nday1= {'notebook','desktop'}\nday2 = {'notebook','ipad'}\n\n\nfor i in day1|day2:\n    print(i)\n\ndesktop\nipad\nnotebook\n\n\n(숙제) 길이가 4인 집합을 두개만들고 공통원소를 2개로 설정한 뒤 합집합을 구하는 코드를 작성하라.\n\nboram={'father','mother','me','coco'}\n\n\nlen(boram)\n\n4\n\n\n\nsu={'father','mother','su','name'}\n\n\nlen(su)\n\n4\n\n\n\nboram | su\n\n{'coco', 'father', 'me', 'mother', 'name', 'su'}"
  },
  {
    "objectID": "posts/Python/1. Basic/python 3_0316.html",
    "href": "posts/Python/1. Basic/python 3_0316.html",
    "title": "파이썬 (0316) 3주차",
    "section": "",
    "text": "- 컨테이너형타입이라는 점, 그리고 연산 및 인덱싱을 하는 방법은 리스트와 같음\n\n차이점1: [] 대신에 ()를 사용\n차이점2: 불변형이다. (원소의 값을 바꿀 수 없음)\n차이점3: 하나의 원소를 선언할 때는 (1,)와 같이 해야 한다.\n차이점4: 의미가 명확할때는 튜플의 ()를 생략가능하다.\n\n컨테이너형이라는 것이 무슨의미?\n\na=(4,6,'pencil',3.2+4.6j,[3,4])\n\n\ntype(a[3])\n\ncomplex\n\n\n\ntype(a[2])\n\nstr\n\n\n- 불변형이라는 것은 무슨 의미?\n\na[2] = 'Pencil'\n\nTypeError: 'tuple' object does not support item assignment\n\n\n참고로 a를 튜플이 아니라 리스트로 선언하면 값이 잘 바뀐다.\n- 하나의 원소로 이루어진 튜플을 만들때는 쉼표를 붙여야함\n\n[1]+[2,3,4]\n\n[1, 2, 3, 4]\n\n\n\n(1)+(2,3,4)   # int형+tuple형 이므로 계산 불가 \n\nTypeError: unsupported operand type(s) for +: 'int' and 'tuple'\n\n\n\ntype(1)\n\nint\n\n\n\n(1,)+(2,3,4)\n\n(1, 2, 3, 4)\n\n\n- 마지막차이점! 의미가 명확할때 튜플의 괄호는 생략가능하다. (중요)\n\na=(1,2)\na\n\n(1, 2)\n\n\n의미가 명확할때 생략해야함\n\n1,2 + 3,4,5\n\n(1, 5, 4, 5)\n\n\n\n(1,2)+(3,4,5)\n\n(1, 2, 3, 4, 5)\n\n\n\n\n\n- 소괄호를 이용\n\na=(1,2,3)\na\n\n(1, 2, 3)\n\n\n\ntype(a)\n\ntuple\n\n\n- 생략가능하다는 점이 포인트\n\na=1,2,3\na\n\n(1, 2, 3)\n\n\n\ntype(a)\n\ntuple\n\n\n- 원소가 하나인 튜플을 만들고 싶다면?\n\na=(1,)\na\n\n(1,)\n\n\n\n\n\n- 리스트와 동일\n\n(1,2)+(3,4,5)\n\n(1, 2, 3, 4, 5)\n\n\n\n(1,2)*2\n\n(1, 2, 1, 2)\n\n\n\n\n\n- 리스트와 동일\n\na=(1,2,3,-4,-5)\na\n\n(1, 2, 3, -4, -5)\n\n\n\na[-1]\n\n-5\n\n\n\na[-3:]\n\n(3, -4, -5)\n\n\n\n\n\n\n\n\n책의설명: 실수로 값이 변경되는 것을 방지할 수 있다.\nshaaly copy / deep copy 를 막을 수 있는 무기\n\n\n\n\n- 예제: 여러변수를 동시에 출력하고 싶을 경우 (다중출력?)\n변수를 아래와 같이 선언하였다고 하자.\n\na=1\nb=2\nc=3\n\n선언된 값을 확인하려면?\n\na\n\n1\n\n\n\nb\n\n2\n\n\n\nc\n\n3\n\n\n튜플을 이용하면?\n\na,b,c #괄호하나 생략하는 것이 편함\n\n(1, 2, 3)\n\n\n- 예제: 다중할당1 (여러개의 변수를 동시에 선언하고 싶을 경우)\n\nname, age, sex, height, weight = 'Tom', 20, 'M', 180, 70\n\n\nname, age, sex, height, weight\n\n('Tom', 20, 'M', 180, 70)\n\n\n\nheight\n\n180\n\n\n- 예제: 다중할당2, 위도와 경도\n\ncoor = (37,127) #서울\ncoor\n\n(37, 127)\n\n\n\nlat, long = coor   # (왼쪽) 가로가 생략된 튜플 \n\n\nlat\n\n37\n\n\n\nlong\n\n127\n\n\n- 잠깐만: 다중할당은 꼭 튜플에서만 가능한가?\n\n[x,y,z] = [1,2,3]\nx,y,z # 다중출력\n\n(1, 2, 3)\n\n\n\n[x,y] = 'hi'\nx,y\n\n('h', 'i')\n\n\n튜플과 같이 사용하면 가독성이 극대화 (그래서 다중할당은 거의 튜플과 세트로 사용함)\n\nx,y,z=1,2,3\nx,y,z\n\n(1, 2, 3)\n\n\n- 예제: 임시변수 사용없이 두 변수의 값을 교환\n\na=10\nb=20\n\n\na,b= b,a\n\n\na\n\n20\n\n\n\nb\n\n10\n\n\n- 예제: for문과 튜플\n\nlst = [['boram', 202212345, 'F'],\n      ['iu',202212365,'F'],\n      ['hodong',202215323,'M']]\nlst\n\n[['boram', 202212345, 'F'], ['iu', 202212365, 'F'], ['hodong', 202215323, 'M']]\n\n\n\nfor i in lst:\n    print(i)\n\n['boram', 202212345, 'F']\n['iu', 202212365, 'F']\n['hodong', 202215323, 'M']\n\n\n\nfor name, studentid, sex in lst:\n    print(name)\n\nboram\niu\nhodong\n\n\n\nfor name, studentid, sex in lst:\n    print(name, sex)\n\nboram F\niu F\nhodong M\n\n\n- 예제: for문과 튜플, dummy variable _\n\nfor name, studentid, sex in lst:\n    print(name)\n\nboram\niu\nhodong\n\n\n\nfor name, _, _ in lst:\n    print(name)   #name만 관심있으므로 그 외는 언더바를 통해 작성하는 편리함\n\nboram\niu\nhodong\n\n\n\nfor name, _ in lst:\n    print(name)\n\nValueError: too many values to unpack (expected 2)\n\n\n\nfor name, *args in lst:    #  *args 를 통해 위 오류 해결\n    print(name)\n\nboram\niu\nhodong\n\n\n- 예제: 튜플과 언패킹연산자 *\n\nhead, body, *tail = range(1,11)\nhead, body, tail\n\n(1, 2, [3, 4, 5, 6, 7, 8, 9, 10])\n\n\n\nhead1, head2, *body, tail1, tail2, tail3 = range(1,11)\nhead1, head2, body, tail1, tail2, tail3\n\n(1, 2, [3, 4, 5, 6, 7], 8, 9, 10)\n\n\n\nhead1, *body, tail1, *tail2, *tail3 = range(1,11) #명확하지 않아서 오류남\n\nSyntaxError: multiple starred expressions in assignment (2478039376.py, line 1)\n\n\n\n*head, body, tail1, tail2, tail3 = range(1,11)\nhead, body, tail1\n\n([1, 2, 3, 4, 5, 6], 7, 8)\n\n\n(관찰)\nhead1, head2, body, tail1, tail2, tail3 = (1, 2, [3, 4, 5, 6, 7], 8, 9, 10)\nhead1, head3, *body, tail1, tail2, tail3 = (1,2, 3,4,5,6,7, 8, 9, 10) \n* 를 붙이면 1차원 자료구조가 풀린다!\n\n*[1,2,3]\n\nSyntaxError: can't use starred expression here (386627056.py, line 1)\n\n\n\nprint([1,2,3])\n\n[1, 2, 3]\n\n\n\nprint(*[1,2,3])   # 풀린다!!!\n\n1 2 3\n\n\n- 예제: 함수의 입력으로 *args 를 넣을때\n[예비학습] 함수 벼락치기\n\ndef myadd(a,b):\n    return a+b\n\n\nmyadd(3,4)\n\n7\n\n\n\nmyadd(3,-3)\n\n0\n\n\n예제: 두 점 사이의 거리를 구하는 함수를 만들어 보자.\n점 \\(p=(p_x,p_y)\\) 와 \\(q=(q_x,q_y)\\)의 거리는 \\(\\sqrt{(p_x-q_x)^2, (p_y-q_y)^2}\\)이다. 이것을 계산하는 프로그램을 만들자\n\nimport numpy as np\ndef dist(px,py,qx,qy):\n    return np.sqrt((px-qx)**2 + (py-qy)**2)\n\n\n\ndist(0,3,4,0) # 헷갈려\n\n5.0\n\n\n\np=(0,3)\nq=(4,0)\ndist(p,q)\n\nTypeError: dist() missing 2 required positional arguments: 'qx' and 'qy'\n\n\n(방법1)\n\npx, py = p #또는(0,3)\nqx, qy = (4,0)\ndist(px,py,qx,qy)\n\n5.0\n\n\n(방법2)\n\ndef dist2(p, q):\n    px, py = p\n    qx, qy = q\n    return np.sqrt((px-qx)**2 + (py-qy)**2)\n\n\n#def dist2(p, q):\n#    px=p[0]\n#    py=p[1]\n#    qx=q[0]\n#    qy=q[1]\n#    return np.sqrt((px-qx)**2 + (py-qy)**2)\n\n\np=(0,3)\nq=(4,0)\ndist2(p,q)\n\n5.0\n\n\n(방법3)\n\ndist(*p, *q)    # 입력을 *(px,py), *(qx, qy) 형태로 넣기도 하고\n\n5.0\n\n\n\ndist(px, py, qx, qy)  # 입력을 px,py,qx,qy 형태로 넣기도 하고\n\n5.0\n\n\n(숙제) 원소로 자기학번을 포함하는 튜플을 만들기 (길이가 1인 튜플)\n\ntype((202250926,))\n\ntuple\n\n\n\nlen((202250926,))\n\n1\n\n\n\nㅁ"
  },
  {
    "objectID": "posts/Python/1. Basic/python 4_0328.html",
    "href": "posts/Python/1. Basic/python 4_0328.html",
    "title": "파이썬 (0328) 4주차",
    "section": "",
    "text": "강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-zcnjAged1xIatgznRTy93c\n\n- (1/8) 파이썬이 어려웠던 이유\n- (2/8) 1세대 프로그래머\n- (3/8) 1세대 프로그래머의 삶 with python\n- (4/8) 1세대 프로그래머의 삶 with ipython\n- (5/8) 2세대 프로그래머, 3세대 프로그래머 (1)\n- (6/8) 3세대 프로그래머(2), 4세대 프로그래머\n- (7/8) 5세대 프로그래머\n- (8/8) 다양한 개발환경 구축방법 다시 리뷰, 숙제설명\n\n\n파이썬이 어려웠던 이유\n- 파이썬 배우는 초보자에게 가장 어려운것!\n\n선생님마다 설치하는 방법이 모두 다름\n\n- 왜 저렇게 설치방법이 다른가? 왜 다른 방법으로 각각 파이썬을 실행하는가? 이런것이 너무 어려움\n\n방법1: 파이썬프로그램 다운로드 -> 시작버튼 눌러서 설치\n방법2: 아나콘다 설치 (그럼 자동으로 파이썬이 설치됨)\n방법3: 아나콘다 설치 + 가상환경\n…\n\n- 심지어 실행하는것도 다름\n\n방법1: 파이썬 프롬프트\n방법2: .py를 이용하여 실행?\n방법3: IDLE\n방법4: 파이참\n방법5: 스파이더\n방법6: Visual Studio Code\n방법7: 주피터노트북, 주피터랩\n\n가상환경을 만들어서 해라..\n아나콘다 네비게이터에 주피터가 있다..\n\n…\n\n- 머리아프니까 collab을 쓰라는 사람도 있음. 아니면 도커이미지를 줄테니까 그걸 쓰라는 사람도 있음. AWS를 쓰라는 사람도 있음.. \\(\\to\\) 이게 더 머리아픔\n- 핵심: 그냥 (1) 컴퓨터에 (2) 파이썬을 깔아서 (3) 실행하는 것임\n- 의문: 그런데 방법이 왜이렇게 많은가? 엑셀처럼 프로그램 설치하고 아이콘 더블클릭하면 끝나는 식으로 만들어야 하는것 아닌가?\n\n개발환경 구축방법이 많은 이유?\n- 파이썬 개발환경 구축은 수많은 방법이 있다.\n- 이는 마치 라면의 레시피를 검색하면 수많은 방법이 나오는것과 유사함.\n\n방법1: 스프를 먼저 넣고 끓인다음 라면을 넣어야 합니다.\n방법2: 양은냄비에 물넣고 물이 끊으면 라면과 스프를 같이 넣고 마지막에 계란을 넣는다.\n방법3: 먹다남은 삼겹살을 후라이팬에 볶은다음에 물을 붓고 라면을 넣는다.\n방법4: 용기에 라면+스프+뜨거운물 랩을 씌운뒤에 젓가락으로 구멍을 뚫고 전자렌지에 돌린다.\n…\n\n- 우리는 모든 방법을 나열할 순 없지만 모든 방법을 이해할 수 있다. 왜냐하면 라면을 끓이는 공통적인 맥락을 우리는 알고 있으니까\n- 파이썬을 설치하는 다양한 방법 역시 공통맥락을 파악하면 이해하기 쉽다.\n- 제목적: 파이썬을 설치하고 실행하는 공통맥락을 설명하고 싶음\n- 설치하는 방법이 다양한 이유? 파이썬이 인기있음 + 다양한 방법을 설치를 하면 각자의 장점이 뚜렷해서\n\n\n\n1세대 프로그래머\n\npython\n- 윈도우에서 anaconda prompt 실행 -> python\n(base) C:\\Users\\python>python\nPython 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> [1,2,3]+[4]\n[1, 2, 3, 4]\n>>> a=[1,2,3]+[4]\n>>> a\n[1, 2, 3, 4]\n- 2개를 실행할 수도 있음. (두 환경은 각각 서로 독립적인 파이썬, 변수가 공유되지 않음) \\(\\star\\)\n- 아쉬운점: `?list’와 같이 도움말 기능이 동작하지 않음\n>>> ?list\n  File \"<stdin>\", line 1\n    ?list\n    ^\nSyntaxError: invalid syntax\n>>> \n\n\nipython\n- 윈도우에서 anaconda prompt 실행 -> ipython\n(base) C:\\Users\\python>ipython\nPython 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\nType 'copyright', 'credits' or 'license' for more information\nIPython 7.29.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: a=[1,2,3]\n\nIn [2]: a\nOut[2]: [1, 2, 3]\n\nIn [3]: a+[4]\nOut[3]: [1, 2, 3, 4]\n- ?list가 가능\nIn [4]: ?list\nInit signature: list(iterable=(), /)\nDocstring:\nBuilt-in mutable sequence.\n\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\nType:           type\nSubclasses:     _HashedSeq, StackSummary, DeferredConfigList, SList, _ImmutableLineList, FormattedText, NodeList, _ExplodedList, Stack, _Accumulator, ...\n\n- 색깔이 알록달록해서 문법을 보기 편하다. (구문강조)\n\n\n1세대 프로그래머의 삶 with python\n- 1부터 10까지 합을 구하는 프로그램을 만들고 싶음\n- 시도1: python을 키고 아래와 같이 실행\n(base) C:\\Users\\python>python\nPython 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> total = 0\n>>> for i in range(10):\n...     total=total+i\n...\n>>> total\n45\n>>>\n- 반성: 정답은 55인데 45가 출력되었다! \\(\\to\\) range(10)을 range(1,11)으로 바꿔야겠다!\n- 시도2: range(1,11)을 바꿔야겠다고 생각하고 다시 입력하다가 오타가 발생\n>>> total =0\n>>> for i in range(1,11):\n...     total = totla +i\n...\n\n앗 totla이라고 잘못쳤다.\n\n- 반성: 다음에는 정신을 똑바로 차려야겠다.\n- 불편한점: … 다..\n\n\n1세대 프로그래머의 삶 with ipython\n- ipython을 사용한 프로그래머는 좀더 상황이 낫다\n(base) C:\\Users\\python>ipython\nPython 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\nType 'copyright', 'credits' or 'license' for more information\nIPython 7.29.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: total = 0\n\nIn [2]: for i in range(1,11):\n   ...:     total = total + i\n   ...:\n\nIn [3]: total\nOut[3]: 55\n\n편한점1: 자동으로 들여쓰기가 되어서 편함\n편한점2: 화살표를 이용해서 for문을 쓰는 도중에 위아래로 이동가능\n불편한점1: 화살표로 이동할수는 있는데 마우스로는 이동할 수 없다.\n불편한점2: 내가 작성한 코드를 관리하기 어렵다.\n\n\n\n\n2세대 프로그래머: 메모장 + anconda prompt를 이용 (.py를 이용한 python활용)\n- 메모장을 키고 아래의 내용을 적는다.\ntotal = 0 \nfor i in range(1,11): \n    total = total + i\nprint(total)\n- 파일이름을 mysum.py로 저장한다.\n- anaconda prompt에서 mysum.py파일이 저장된 폴더로 이동 -> 실행\n(base) C:\\Users\\python>cd Desktop\n\n(base) C:\\Users\\python\\Desktop>dir\n C 드라이브의 볼륨에는 이름이 없습니다.\n 볼륨 일련 번호: 9AFD-A05F\n\n C:\\Users\\python\\Desktop 디렉터리\n\n2022-03-27  오전 11:32    <DIR>          .\n2022-03-27  오전 11:32    <DIR>          ..\n2022-03-27  오전 12:01             2,306 Chrome.lnk\n2022-03-26  오후 08:32             2,332 Microsoft Edge.lnk\n2022-03-27  오전 11:33                71 mysum.py\n               3개 파일               4,709 바이트\n               2개 디렉터리  743,643,467,776 바이트 남음\n\n(base) C:\\Users\\python\\Desktop>python mysum.py\n55\n\n(base) C:\\Users\\python\\Desktop>\n- 소감 - 편한점1: 마우스를 이용하여 이동가능 - 편한점2: 내가 작업한 내용은 바탕화면의 메모장에 저장이 되어있음 - 아쉬운점: ipython의 장점은 활용못함 (구문강조, 도움말기능)\n\n\n3세대 프로그래머: 메모장 + ipython\n- 전체적인 개발방식 - 메모장: 코드를 편집, 저장 - ipython: anaconda prompt처럼 메모장의 코드를 실행하고 결과를 확인 + 구문강조, 도움말확인기능 등을 이용하여 짧은 코드를 빠르게 작성\n- 기능 - ipython에서 !python mysum.py를 입력하면 anaconda prompt에서 python mysum.py를 입력한 것과 같은 효과 - ipython에서 %run mysum을 입력하면 메모장에서 mysum.py에 입력된 내용을 복사해서 ipython에 붙여넣어 실행한것과 같은 효과\n\n\n4세대 프로그래머: IDE(통합개발환경)를 사용\n- 메모장과 ipython을 하나로 통합한 프로그램이 등장! - jupyter notebook, jupyter lab - spyder - idle - VScode - …\n- 주피터의 트릭 (실제로 주피터는 ipython에 기생할 뿐 아무런 역할도 안해요)\n\n주피터를 실행\n새노트북을 생성 (파이썬으로 선택)\n\n\n컴퓨터는 내부적으로 ipython을 실행하고 그 ipython이랑 여러분이 방금만든 그 노트북과 연결\n\n\n처음보이는 cell에 1+1을 입력 -> 쉬프트엔터 -> 결과2가 출력\n\n\n처음보이는 cell하나 = 자동으로 열린 하나의 메모장\ncell 1+1을 입력 = 메모장에 1+1을 적음\n쉬프트+엔터후 결과2를 출력 = cell의 내용을 복사 -> ipython에 붙여넣음 -> ipython 계산된 결과를 복사 -> cell로 돌아와 붙여넣기\n\n\n새로운 cell을 추가하고 2+2을 입력 -> 쉬프트엔터 -> 결과4가 출력\n\n\n새로운 cell을 추가 = 새로운 메모장 추가\ncell 2+2을 입력 = 새로운 메모장에 2+2를 적음\n쉬프트+엔터후 결과4를 출력 = cell의 내용을 복사 -> ipython에 붙여넣음 -> ipython 계산된 결과를 복사 -> cell로 돌아와 붙여넣기\n\n- 중요한 사실들 - IDE는 내부적으로 연산을 수행하는 능력이 없다. (생각해볼것: 왜 R을 꼭 설치하고 Rstudio를 설치해야 했을까?)\n\n주피터에서 커널을 재시작한다는 의미는 메모장이 열린채로 ipython을 껐다가 다시 실행한다는 의미\n주피터는 단순히 ’메모장의 내용을 복사하여 붙여넣는 기계’라고 볼 수 있다. 이렇게 생각하면 주피터는 꼭 ipython에 연결할 이유는 없다. 실제로 주피터에 R을 연결해서 쓸 수 있다. 즉 하나의 IDE가 여러개의 언어와 연결될 수 있다.\nJupyterlab이라는 프로그램은 크롬에 있는 내용과 ipython간의 통신을 제어하는 프로그램일 뿐이다.\n\n\n\n5세대 프로그래머: 가상컴퓨터(anaconda), 원격컴퓨터(server), 클라우드컴퓨터(colab)의 개념 등장\n- 지금까지는 ipython이 실행되는 컴퓨터와 크롬이 실행되는 컴퓨터가 동일하다는 전제였음.\n- 생각해보니까 어차피 ipython이 실행된 컴퓨터에서 내가 크롬에 입력한 명령 “전달”되기만 하면 되므로 꼭 같은 컴퓨터일 필요는 없다.\n\n모델1: 원격컴퓨터\n- 준비상태 - 전북대컴퓨터: ipython을 실행 + 이 컴퓨터는 인터넷 연결이 되어있어야함 - 우리집노트북: 크롬실행 + 이 컴퓨터도 인터넷이 연결되어 있어야함\n- 명령입력 - 우리집노트북 크롬에서 1+1을 입력하고 쉬프트 엔터를 누름\n- 우리집노트북 -> 전북대컴퓨터 - 우리집 노트북의 내부의 어떤프로그램은 1+1이라는 명령을 복사하여 카카오톡으로 전북대 컴퓨터에 전달 - 전북대 컴퓨터의 내부의 어떤프로그램은 1+1이라는 명령을 카톡으로 받아서 그것을 ipython에게 전달\n- 전북대컴퓨터 -> 우리집노트북 - 전북대컴퓨터 내부의 ipython은 2라는 출력결과를 계산함 - 전북대컴퓨터 내부의 어떤프로그램은 계산결과를 카톡으로 우리집 노트북에 알려줌 - 나는 우리집 노트북에서 계산결과를 받아볼 수 있다.\n\n\n모델2: 원격컴퓨터 + 가상컴퓨터\n- 준비상태 - 성능좋은 전북대 컴퓨터 1개 - 내 노트북 1개 (그냥 싸고 가벼운거) - 대학원생 아이패드 1개 (그냥 싸고 가벼운거)\n- 아이디어\n\n성능좋은 전북대 컴퓨터를 논리적으로 3개로 분리 \\(\\to\\) 이를 각각 (base) (py39jl17) (py38r40) 컴퓨터라고 하자.\n나는 (py39jl17)에 접속하여 파이썬 3.9와 줄리아 1.7을 설치한뒤 실습한다.\n대학원생은 (py38r40)에 접속하여 파이썬 3.8과 R 4.0을 설치하고 실습한다.\n(base)는 예비용으로 아무것도 설치안한 깨끗한 상태 유지\n내가 뭘 실수해서 (py39jl17)컴퓨터가 망가졌으나 (py38r40)은 아무 타격없다.\n나는 (py39jl17)를 삭제하고 (base)로 부터 다시 새로운 컴퓨터를 복사하여 (py39jl17)을 다시 만든다.\n\n\n\n모델3: 가상컴퓨터\n- 여러분들 사례\n\n여러분들의 컴퓨터는 (base), (py39) 2개의 컴퓨터로 나누어져 있음\n여러분들이 (py39)에만 주피터랩을 설치\n(py39)에 있는 ipython과 여러분의 크롬창이 서로 통신하면서 실습\n장점: 서로 다른 환경에 서로다른 파이썬과 R등을 설치할 수 있다. \\(\\to\\) 패키지간의 충돌이 최소화 (파이썬 입문 수업을 듣고, 이후에 파이썬을 이용하는 어떤수업을 들음)\n\n\n\n모델4: 클라우드\n- 사례1 - 성능이 그저그런 컴퓨터 27개 - 대학원생을 포함하여 쓸 사람은 5명 - 한사람당 27/5(=5.4)대의 컴퓨터식 할당\n- 사례2: 구글코랩 - 구글에 여러가지 성능을 가진 컴퓨터가 \\(n\\)대 있음 - \\(m\\)명의 사람이 \\(n\\)대의 컴퓨터에 접속 - 적당히 컴퓨터 자언을 분배하여 사용\n\n\n\n요약 및 정리\n- 결국 (1) 컴퓨터에 (2) 파이썬을 설치하고 (3) 실행하는 과정은 생각보다 다양한 선택의 조합이 가능하다.\n\n그냥 내 노트북에 파이썬을 설치할지? 내 노트북안에 가상컴퓨터를 만들고 거기에 파이썬을 설치할지? 학교의 데스크탑에 파이썬을 설치하고 쓸지? 설치를 안하고 구글컴퓨터에 설치된 파이썬을 난 쓰기만 할지?\npython설치할지? ipython를 설치할지? 어차피 가상환경을 쓸꺼니가 anaconda를 설치할지? 아니면 코랩쓸꺼니까 설치안할지?\n어떤 IDE를 쓸지? IDE를 쓰지 않을지? 내가 IDE를 직접구성해서 만들지?\n\n하지만 공통적으로 관통하는 원리가 있다\n\n\n숙제\n- 주피터랩에서 ’myprod.py’파일을 만들고 1부터 5까지의 곱을 계산하는 코드를 작성후 %run myprod를 실행하여 출력결과를 확인\n\n%run myprod\n\n120"
  },
  {
    "objectID": "posts/Python/1. Basic/python 2_0314.html",
    "href": "posts/Python/1. Basic/python 2_0314.html",
    "title": "파이썬 (0314) 2주차",
    "section": "",
    "text": "- 리스트의 선언\n\na=[11,22]\na\n\n[11, 22]\n\n\n\ntype(a)\n\nlist\n\n\n- 비어있는 리스트의 선언\n\na=[] # 방법1\na\n\n[]\n\n\n\na=list() # 방법2\na\n\n[]\n\n\n\n\n\n- 더하기연산\n\na=[11,22]\nb=[12,13]\n\n\na\n\n[11, 22]\n\n\n\nb\n\n[12, 13]\n\n\n\na+b\n\n[11, 22, 12, 13]\n\n\n\n우리의 예상과 다른 결과가 나옴 \\(\\to\\) 파이썬은 R처럼 자체적으로 좋은 계산기능을 내장하고 있찌 않음\n\n- 브로드캐스팅과 같이 R에서는 당연히 가능했던 기능을 사용할 수 없음\n\na=[1,2,3]\nb=1\na+b\n\nTypeError: can only concatenate list (not \"int\") to list\n\n\n- 뺄셈은 정의되지 않음\n\na=[1,2]\nb=[1,2]\na-b\n\nTypeError: unsupported operand type(s) for -: 'list' and 'list'\n\n\n- 곱하기는 정의 가능\n\na=[1,2]\n\n\n2*a  #a+a\n\n[1, 2, 1, 2]\n\n\n- 나눗셈은 정의되지 않음\n\na=[1,2,1,2]\na/2\n\nTypeError: unsupported operand type(s) for /: 'list' and 'int'\n\n\n- 더하기와 곱하기는 원소의 추가와 반복추가를 의미하지만 그렇다고 해서 뺄샘과 나눗셈이 원소의 삭제를 의미하는 것은 아님\n\na=[1,2,3]\na-[3]   #이런건없다\n\nTypeError: unsupported operand type(s) for -: 'list' and 'list'\n\n\n\na=[1,2,1,2,1,2,]\na/3    # 이런건없다\n\nTypeError: unsupported operand type(s) for /: 'list' and 'int'\n\n\n- 더하기와 곱하기가 원소의 추가와 반복추가를 의미하여 편리할때도 있긴하지만, 우리는 산술적인 +, *를 원하는 경우도 있다. 이럴 경우는 어떻게 할 수 있을까?\n(예제)\n\na=[1,2]\nb=[3,4]\n\na+b=[4,6]이 되도록 하려면?\n(풀이1)\n\ntype(a)\n\nlist\n\n\n\ntype(a[0])\n\nint\n\n\n\na[0]+b[0]  #a의 첫번째 원소 추출, b의 첫번째 원소 추출, 둘을 더함\n\n4\n\n\n\na[1]+b[1]\n\n6\n\n\n\n[a[0]+b[0], a[1]+b[1]]\n\n[4, 6]\n\n\n풀이가 가능한 이유? a,b는 리스트이지만 a[0], a[1], b[0], b[1]은 각각 인트형임. 인트형은 +연산이 가능함\n(풀이2)\nnumpy패키지 (파이썬의 여러 수치연산들을 담당하는 라이브러리) - 이러한 벡터연산은 누구나 필요로 하는 연산 - 내가 아니더라도 누군가가 프로그램화 해놓았을 것임 - 그 누군가가 자신이 만든 코드를 잘 정리하여 무료로 배포했을 수도 있음 (패키지를 배포한다고 표현) - 그 패키지를 가져와서 설치한 뒤 사용하기만 하면 된다.\n패키지를 설치하는 방법 - !pip install numpy # 최신버전을 설치함 - !conda install -c conda-forge numpy -y # 안전한 버전을 설치함\n설치된 패키지를 사용하는 방법 - import numpy 한뒤에 numpy.??로 기능을 사용 - import numpy as np 한뒤에 np.??로 기능을 사용\n파이썬의 기본 패키지 numpy pandas matplotlib\n\n!pip install numpy\n\nRequirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (1.21.5)\n\n\n\nimport numpy # 설치한 패키지를 쓰겠다고 선언함 library(tidyverse)와 비슷.\n\n\na=[1,2]\nb=[3,4]\n\n\na+b\n\n[1, 2, 3, 4]\n\n\n\ntype(numpy.array(a))\n\nnumpy.ndarray\n\n\n\naa=numpy.array(a)   #aa는 리스트가 아니라 넘파이 어레이, numpy.array()는 numpy패키지에서 제공하는 array함수를 쓰겠다는 의미\nbb=numpy.array(b)   \n\n\naa+bb\n\narray([4, 6])\n\n\n\na+b\n\n[1, 2, 3, 4]\n\n\n이런것도 가능\n\n2*aa+1\n\narray([3, 5])\n\n\n\n2*aa+1+bb\n\narray([6, 9])\n\n\n(풀이3)\n\nimport numpy as np # 설치한 numpy라는 패키지를 쓰겠음. 그런데 numpy 말고 np라는 이름으로 쓰겠음\n\n\nnp.array(a)+np.array(b)\n\narray([4, 6])\n\n\n\n\n\n- str형과 동일한 방식\n\na=[11,22,33,44,55]\n\n\na[0:3]\n\n[11, 22, 33]"
  },
  {
    "objectID": "posts/Python/1. Basic/python 2_0314.html#콘테이너형-객체-가변객체",
    "href": "posts/Python/1. Basic/python 2_0314.html#콘테이너형-객체-가변객체",
    "title": "파이썬 (0314) 2주차",
    "section": "콘테이너형 객체, 가변객체",
    "text": "콘테이너형 객체, 가변객체\n- 객체 - Object - Something\n- 리스트의 원소는 int, float따위만 가능한 것이 아니다. (리스트는 콘테이너형 객체이므로)\n\nlst = [1,3.14,True, 'a', [1,2],\n      (1,2), {'name':'iu','age':30},{1,2,3}]\n\n\nlst\n\n[1, 3.14, True, 'a', [1, 2], (1, 2), {'name': 'iu', 'age': 30}, {1, 2, 3}]\n\n\n각 원소의 타입을 알아보자\n\ntype(lst[0])\n\nint\n\n\n\ntype(lst[1])\n\nfloat\n\n\n\ntype(lst[2])\n\nbool\n\n\n\ntype(lst[3])\n\nstr\n\n\n\ntype(lst[4])\n\nlist\n\n\n\ntype(lst[5])\n\ntuple\n\n\n\ntype(lst[6])   # dictionary\n\ndict\n\n\n\ntype(lst[7]) #집합\n\nset\n\n\n- str은 컨테이너형이 아니다\n\n# 컨테이너형이 아닌것\n'abcd'[0]\n\n'a'\n\n\n\nstr의 모든 원소는 문자임\n\n- 리스트의 원소를 수정할 수 있다. (리스트는 가변객체이므로)\n\na=[11,22,33]\n\n\na[0]\n\n11\n\n\n\na[0]=111\n\n\na\n\n[111, 22, 33]\n\n\n- 원소수정은 당연한 기능 같은데 이것이 불가능한 경우도 있다.\n(가능한경우)\n\n'boram'[1]\n\n'o'\n\n\n\na=['b','o','r','a','m']\n\n\na[0]\n\n'b'\n\n\n\na[0]='B'\n\n\na\n\n['B', 'o', 'r', 'a', 'm']\n\n\n(불가능한경우)\n\na='boram'\n\n\na\n\n'boram'\n\n\n\na[0]\n\n'b'\n\n\n\na[0]='B'\n\nTypeError: 'str' object does not support item assignment\n\n\n- 리스트 원소 삭제\n(예제)\n아래와 같이 문자로 된 리스트를 선언하자.\n\na=['b','o','r','a','m']\na\n\n['b', 'o', 'r', 'a', 'm']\n\n\n사실 더 쉽게 선언할 수 있음\n\na='boram'   #string으로 a를 선언\n\n\ntype(a)\n\nstr\n\n\n\nlist(a)\n\n['b', 'o', 'r', 'a', 'm']\n\n\n\na=list(a)  #list(a)를 통하여 str을 list로 변환 \n\n\na  # 그 결과를 a에 다시 저장\n\n['b', 'o', 'r', 'a', 'm']\n\n\n첫 번째 원소를 삭제하고 싶다면?\n\n\ndel a[0]\na\n\n['o', 'r', 'a', 'm']\n\n\n- 리스트의 원소 추가\n(예제) 비어있는 리스틀를 만들고 원소 0,1,2 를 차례로 추가하여 보자.\n(풀이1)\n\na=[]\na\n\n[]\n\n\n\na= a+[0]\na\n\n[0]\n\n\n\na=a+[1]\na\n\n[0, 1]\n\n\n\na= a+[2]\na\n\n[0, 1, 2]\n\n\n(풀이2)\n\na=[]\na\n\n[]\n\n\n\na+=[0]\na\n\n[0]\n\n\n\na+=[1]\na\n\n[0, 1]\n\n\n\na+=[2]\na\n\n[0, 1, 2]\n\n\n\n암기법: 중복되는 변수를 지우고 연산자의 순서를 바꾼다.\n\n(풀이3) 리스트 특화기능(=메소드)를 이용\n\na=[]\na\n\n[]\n\n\n\na.append?\n\n\na.append(0)\na\n\n[0]\n\n\n\na.append(1)\na\n\n[0, 1]\n\n\n\na.append(2)\na\n\n[0, 1, 2]\n\n\n- a+[4]와 a.append(4)의 차이점은?\n(관찰1)\n\na=[1,2,3]\na+[4]  ## 리스트 a와 리스트 [4]의 연산결과\n\n[1, 2, 3, 4]\n\n\n\na  # a는 그대로임. 변화없음\n\n[1, 2, 3]\n\n\n(관찰2)\n\na=[1,2,3]\na.append(4)\n\n\na    # a자체가 변화함\n\n[1, 2, 3, 4]\n\n\n비슷해보이지만 굉장히 미묘한 차이가 있음\na.append(4) : a에 4를 append하라 \\(\\to\\) a가 변함\na+[4] : a와 4를 연산하고 수행결과를 보여달라"
  },
  {
    "objectID": "posts/Python/1. Basic/python 2_0314.html#메소드리스트자료형에-특화된-특수한-함수들",
    "href": "posts/Python/1. Basic/python 2_0314.html#메소드리스트자료형에-특화된-특수한-함수들",
    "title": "파이썬 (0314) 2주차",
    "section": "메소드(리스트자료형에 특화된 특수한 함수들)",
    "text": "메소드(리스트자료형에 특화된 특수한 함수들)\n(append)\n\na=[1,2,3,4]\na.append?\n\n\na.append(5)\na\n\n[1, 2, 3, 4, 5]\n\n\n(clear)\n\na=[1,2,3,4]\na.clear?\n\n\na.clear()\n\n\na\n\n[]\n\n\n(copy)\n\na=[1,2,3,4]\na.copy?\n\n\nb=a.copy()\nb\n\n[1, 2, 3, 4]\n\n\n(count)\n\na=[1,1,2,3,3,4,4,4,]\na.count(1)\n\n2\n\n\n\na.count(2) #특정 원소가 몇개 포함되어있는지 숫자 세줌\n\n1\n\n\n(extend)\n\na=[1,2,3,4]\nb=[-1,-2,-3,-4]\n\n\na.extend(b)\na\n\n[1, 2, 3, 4, -1, -2, -3, -4]\n\n\n\na.append(b)   \na\n\n[1, 2, 3, 4, -1, -2, -3, -4, [-1, -2, -3, -4]]\n\n\n(index)\n\na=[11,22,'a',True,22]\na.index(True)\n\n3\n\n\n\na.index('a')\n\n2\n\n\n\na.index(22)\n\n1\n\n\n(insert)\n\na=[1,2,3]\n\n\na.insert(1,88)\na\n\n[1, 88, 2, 3]\n\n\n(pop)\n\na=['a',1,2,'d']\na.pop()   # index= -1이므로 마지막원소가 나타남\n\n'd'\n\n\n\na   # a는 마지막 원소가 사라진 상태\n\n['a', 1, 2]\n\n\n\na.pop(0)   # index=0 이므로 첫번째 원소가 나타남\n\n'a'\n\n\n\na    # a에서는 첫번쨰 원소가 사라진 상태\n\n[1, 2]\n\n\n(remove)\n\na=['a',2,3,'d']\na.remove('d')\n\n\na\n\n['a', 2, 3]\n\n\n\na.remove('a')\na\n\n[2, 3]\n\n\n(reverse)\n\na=[1,2,3,4]\na.reverse()\na\n\n[4, 3, 2, 1]\n\n\n(sort)\n\na=[1,3,2,4]\na.sort()\na\n\n[1, 2, 3, 4]\n\n\n(다른예제들)\n\na=list('boram')\na\n\n['b', 'o', 'r', 'a', 'm']\n\n\n\na.sort()\na\n\n['a', 'b', 'm', 'o', 'r']\n\n\n\na.sort(reverse=True)\na\n\n['r', 'o', 'm', 'b', 'a']"
  },
  {
    "objectID": "posts/Python/1. Basic/python 2_0314.html#중첩리스트",
    "href": "posts/Python/1. Basic/python 2_0314.html#중첩리스트",
    "title": "파이썬 (0314) 2주차",
    "section": "중첩리스트",
    "text": "중첩리스트\n\nA=[[1,2,3],[4,5,6],[7,8,9]]\nA\n\n[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n\n- A는 아래와 같은 매트릭스로 이해할 수 있다.\n$\n\\[\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix}\\]\n$\n- A에서 (2,1)의 원소를 뽑고 싶다. = 1을 뽑고싶다.\n\nA[1,0]\n\nTypeError: list indices must be integers or slices, not tuple\n\n\n\n실패\n\n\nA[1][0]\n\n4\n\n\n\n성공\n\n성고의 이유를 분석해 보자.\n\nA[0]\n\n[1, 2, 3]\n\n\n\nA[0][0]\n\n1\n\n\n- 매트릭스는 아니지만 매트릭스 같음! - 1차원 배열을 다차원 배열로 확장할 수 있는 기본 아이디어를 제공함"
  },
  {
    "objectID": "posts/Python/1. Basic/python 2_0314.html#리스트컴프리헨션",
    "href": "posts/Python/1. Basic/python 2_0314.html#리스트컴프리헨션",
    "title": "파이썬 (0314) 2주차",
    "section": "리스트컴프리헨션 (★★★)",
    "text": "리스트컴프리헨션 (★★★)\n- 리스트 컴프리헨션을 이해하기 전에 for문에 대하여 알아보자.\n[예비학습] for문 벼락치기\n프로그램안에서 반복해서 무언가를 하고싶다 \\(\\to\\) for\n\nfor i in [0,1,2,3]:   # 반복실행계획\n    print(i)          # 반복실행내용, 탭을 이용하여 들여쓰기 해야한다. \n\n0\n1\n2\n3\n\n\n(예제) 1,2,3,4의 합을 for문을 이용하여 구해보자.\n\n_sum = 0\nfor i in [1,2,3,4]:\n    _sum = _sum + i\n\n\n_sum\n\n10\n\n\n- 예제: $ 2^0, 2^1, 2^2, 2^3$ 를 원소로 가지는 리스트를 생성\n(풀이1)\n\nx=[2**0, 2**1, 2**2, 2**3]    ## 2의 0승\nx\n\n[1, 2, 4, 8]\n\n\n(풀이2) for문을 이용\n\nx=[]\nfor i in [0,1,2,3]:\n    x.append(2**i)  \n\n\nx\n\n[1, 2, 4, 8]\n\n\n(풀이3) for문을 이용\n\nx=[]\nfor i in [0,1,2,3]:\n    x= x+[2**i]\nx\n\n[1, 2, 4, 8]\n\n\n(풀이4) for문을 이용\n\nx=[]\nfor i in [0,1,2,3]:\n    x+= [2**i]\nx(풀이2) for문을 이용\n\n[1, 2, 4, 8]\n\n\n(풀이5) 리스트컴프리헨션을 이용한 풀이\n\nx= [2**i for i in [0,1,2,3]]\nx\n\n[1, 2, 4, 8]\n\n\n- 리스트컴프리헨션의 문법 암기방법 - 집합에서 조건제시법을 연상 - 원소나열법, 조건제시법 - \\(\\{2^0, 2^1, 2^2, 2^3\\} = \\{2^i: \\text{for} i \\in \\{0,1,2,3\\}\\)\n- 리스트컴프리헨션이란? - 리스트를 매우 효율적으로 만드는 테크닉 - for문에 비하여 가지고 있는 장점 : 1. 코드가 간결하다. 2, 빠르다\n- 예제: 리스트 컴프리헨션을 이용하여 아래와 같은 리스트를 만들자.\n\n['SSSS','PPPP','AAAA','MMMM']\n\n['SSSS', 'PPPP', 'AAAA', 'MMMM']\n\n\n(풀이)\n\n[i*4 for i in 'SPAM']\n\n['SSSS', 'PPPP', 'AAAA', 'MMMM']\n\n\n- 예제: 리스트 컴프리헨션을 이용하여 아래와 같은 리스트를 만들자.\n- 예제: 리스트 컴프리헨션을 이용하여 아래와 같은 리스트를 만들자.\n\n['X1','X2','X3','Y1','Y2','Y3']\n\n['X1', 'X2', 'X3', 'Y1', 'Y2', 'Y3']\n\n\n(풀이)\n\nfor i in 'XY':\n    for j in '123':\n        print(i+j)\n\nX1\nX2\nX3\nY1\nY2\nY3\n\n\n\n[i+j for i in 'XY' for j in '123']\n\n['X1', 'X2', 'X3', 'Y1', 'Y2', 'Y3']\n\n\n- 예제: 리스트 컴프리헨션을 이용하여 통계1,,..,통계5,수학1,…,수학5를 만들어라\n(풀이)\n\n[i+j for i in ['stat', 'math'] for j in '12345']\n\n['stat1',\n 'stat2',\n 'stat3',\n 'stat4',\n 'stat5',\n 'math1',\n 'math2',\n 'math3',\n 'math4',\n 'math5']\n\n\n(다른풀이) 참고로 for문을 쓰면 좀 복잡해진다.\n\n_lst=[]\nfor x in ['stat', 'math']:\n    for y in '12345':\n        _lst = _lst + [x+y]\n\n\n_lst\n\n['stat1',\n 'stat2',\n 'stat3',\n 'stat4',\n 'stat5',\n 'math1',\n 'math2',\n 'math3',\n 'math4',\n 'math5']\n\n\n- 예제: ’jbnu’를 이용하여 아래와 같은 리스트르 만들어라.\n\n['j','b','n','u']\n\n['j', 'b', 'n', 'u']\n\n\n\nlist('jbnu')\n\n['j', 'b', 'n', 'u']\n\n\n(풀이)\n\n[x for x in 'jbnu']\n\n['j', 'b', 'n', 'u']\n\n\n-예제: x에는 무엇이 있을까?\n(경우1)\n\nx=1\nlst=[]\nfor x in 'jbnu':\n    lst = lst + [x]\n\n\nlst\n\n['j', 'b', 'n', 'u']\n\n\n\nx\n\n'u'\n\n\n(경우2)\n\nx=1\nlst = [x for x in 'jbnu']\nlst\n\n['j', 'b', 'n', 'u']\n\n\n\nx\n\n1\n\n\n- 예제: [X1,X2,X3,…,X100]과 같은 리스트를 만들어보자.\n(풀이)\n\n['X'+str(i) for i in [1,2,3,4]]\n\n['X1', 'X2', 'X3', 'X4']\n\n\n\n['X'+str(i) for i in [1:100]]   #오류!!\n\nSyntaxError: invalid syntax (1716365648.py, line 1)\n\n\n[예비학습]\n\nrange(0,10)\n\nrange(0, 10)\n\n\n\n_tmp = range(0,10)\n\n\ntype(_tmp)\n\nrange\n\n\n\nlist(_tmp)\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\nlist(range(0,10)) #0을 포함, 10을 미포함\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\n이게 중요한 것. range(0,10)을 리스트화시키면 [0,1,2,…,9]와 같은기능을 얻을 수 있다.\n\n\nlist(range(10))  # 0은 생략가능\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\nlist(range(2,10))  # 2는 포함, 10은 미포함\n\n[2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\nlist(range(1,10,2))  # 2칸씩! \n\n[1, 3, 5, 7, 9]\n\n\n예비학습 끝\n\n['X'+str(i) for i in list(range(1,101))]  # 세로로 넘 길엉 \n\n['X1',\n 'X2',\n 'X3',\n 'X4',\n 'X5',\n 'X6',\n 'X7',\n 'X8',\n 'X9',\n 'X10',\n 'X11',\n 'X12',\n 'X13',\n 'X14',\n 'X15',\n 'X16',\n 'X17',\n 'X18',\n 'X19',\n 'X20',\n 'X21',\n 'X22',\n 'X23',\n 'X24',\n 'X25',\n 'X26',\n 'X27',\n 'X28',\n 'X29',\n 'X30',\n 'X31',\n 'X32',\n 'X33',\n 'X34',\n 'X35',\n 'X36',\n 'X37',\n 'X38',\n 'X39',\n 'X40',\n 'X41',\n 'X42',\n 'X43',\n 'X44',\n 'X45',\n 'X46',\n 'X47',\n 'X48',\n 'X49',\n 'X50',\n 'X51',\n 'X52',\n 'X53',\n 'X54',\n 'X55',\n 'X56',\n 'X57',\n 'X58',\n 'X59',\n 'X60',\n 'X61',\n 'X62',\n 'X63',\n 'X64',\n 'X65',\n 'X66',\n 'X67',\n 'X68',\n 'X69',\n 'X70',\n 'X71',\n 'X72',\n 'X73',\n 'X74',\n 'X75',\n 'X76',\n 'X77',\n 'X78',\n 'X79',\n 'X80',\n 'X81',\n 'X82',\n 'X83',\n 'X84',\n 'X85',\n 'X86',\n 'X87',\n 'X88',\n 'X89',\n 'X90',\n 'X91',\n 'X92',\n 'X93',\n 'X94',\n 'X95',\n 'X96',\n 'X97',\n 'X98',\n 'X99',\n 'X100']\n\n\n(아래와 같은 풀이도 가능)\n\n['X'+str(i) for i in range(1,13)]  # 리스트화해주지 않아도 가능 for i in 뒤에 list뿐 아니라.. str도 되고... \n\n['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12']\n\n\n(딴생각) for문 뒤에 올 수 있는 것이 무엇인지 생각해보자.\n\nfor i in '1234':\n    print(i)\n\n1\n2\n3\n4\n\n\n\nfor i in [1,2,3,4]:\n    print(i)\n\n1\n2\n3\n4\n\n\n\nfor i in (1,2,3,4):   # 튜플\n    print(i)\n\n1\n2\n3\n4\n\n\n\nfor i in {1,2,3,4}: # set\n    print(i)\n\n1\n2\n3\n4\n\n\n\nfor i in {'name':'iu','age':31}:   # 딕셔너리\n    print(i)\n\nname\nage\n\n\n\nfor i in range(1,5):\n    print(i)\n\n1\n2\n3\n4\n\n\n(숙제)\n리스트컴프리헨션을 이용하여 아래와 같은 리스트를 만들어라\n[‘X1’,‘X2X2’,‘X3X3X3’,‘X4X4X4X4’,‘X5X5X5X5X5’]\n\n[('X'+str(i))*i for i in range(1,6)]\n\n['X1', 'X2X2', 'X3X3X3', 'X4X4X4X4', 'X5X5X5X5X5']"
  },
  {
    "objectID": "posts/Python/3. Pandas/python 10_0506 .html",
    "href": "posts/Python/3. Pandas/python 10_0506 .html",
    "title": "파이썬 (0506) 10주차",
    "section": "",
    "text": "!pip install numpy\n!pip install pandas\n\nRequirement already satisfied: numpy in /home/koinup4/anaconda3/envs/py37/lib/python3.7/site-packages (1.21.6)\nCollecting pandas\n  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/11.3 MB 89.6 MB/s eta 0:00:00a 0:00:01\nRequirement already satisfied: pytz>=2017.3 in /home/koinup4/anaconda3/envs/py37/lib/python3.7/site-packages (from pandas) (2022.7.1)\nRequirement already satisfied: numpy>=1.17.3 in /home/koinup4/anaconda3/envs/py37/lib/python3.7/site-packages (from pandas) (1.21.6)\nRequirement already satisfied: python-dateutil>=2.7.3 in /home/koinup4/anaconda3/envs/py37/lib/python3.7/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: six>=1.5 in /home/koinup4/anaconda3/envs/py37/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\nInstalling collected packages: pandas\nSuccessfully installed pandas-1.3.5\n\n\n\nimport numpy as np\nimport pandas as pd\n\n\n부분 데이터 꺼내기: 판다스를 왜 써야할까?\n\n기본 인덱싱\n-예제1: 기본인덱싱\n\na='asdf'\na[2]\n\n'd'\n\n\n\na[-1]\n\n'f'\n\n\n- 예제2: 슬라이싱\n\na='asdf'\na[1:3]\n\n'sd'\n\n\n\na[-2:]\n\n'df'\n\n\n- 예제3: 스트라이딩\n\na='afsdf'\na[::2]\n\n'asf'\n\n\n- 예제4: 불가능한것\n\na='afsd'\na[[1,2]] # 리스트로 전달해서 뽑는것은 불가능 -> 정수인덱스 리스트화시켜서 인덱싱하는것\n\nTypeError: string indices must be integers\n\n\n\na='afsd'\na[[True,True,False,True]] # 리스트로 전달해서 뽑는것은 불가능 -> 정수인덱스 리스트화시켜서 인덱싱하는것\n\nTypeError: string indices must be integers\n\n\n\n\n팬시인덱싱\n- 예제1: 인덱스의 리스트(혹은 ndarray)를 전달\n\na=np.arange(5)\na[0]\n\n0\n\n\n\na[[0,1]]\n\narray([0, 1])\n\n\n\na[[0,1,-2]]\n\narray([0, 1, 3])\n\n\n- 예제2: bool로 이루어진 리스트 (혹은 ndarray)를 전달\n\na=np.arange(55,61)\na\n\narray([55, 56, 57, 58, 59, 60])\n\n\n\na[[True,True,False,True,True,False]]\n\narray([55, 56, 58, 59])\n\n\n\na<58\n\narray([ True,  True,  True, False, False, False])\n\n\n\na[a<58]\n\narray([55, 56, 57])\n\n\n\n\n2차원 자료형의 인덱싱\n- 예제1\n\na=np.arange(4*3).reshape(4,3)\na\n\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11]])\n\n\n\na[0:2,1]\n\narray([1, 4])\n\n\n- 예제2 : 차원을 유지하면서 인덱싱을 하고 싶으면?\n\na[0:2,[1]]\n\narray([[1],\n       [4]])\n\n\n\n\nHASH\n- 예제1 : (key, value)\n\nd={'att':67, 'rep':45, 'mid':30, 'fin':100}\nd\n\n{'att': 67, 'rep': 45, 'mid': 30, 'fin': 100}\n\n\n\nd['att'] # key를 넣으면 value가 리턴\n\n67\n\n\n- 예제2: numpy비교\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,200)\nrep = np.random.choice(np.arange(5,21)*5,200)\nmid = np.random.choice(np.arange(0,21)*5,200)\nfin = np.random.choice(np.arange(0,21)*5,200)\nkey = ['202212'+str(s) for s in np.random.choice(np.arange(300,501),200,replace=False)]\ntest_dic = {key[i] : {'att':att[i], 'rep':rep[i], 'mid':mid[i], 'fin':fin[i]} for i in range(200)}\ntest_ndarray = np.array([key,att,rep,mid,fin],dtype=np.int64).T\n\n\ntest_dic\n\n{'202212377': {'att': 65, 'rep': 45, 'mid': 0, 'fin': 10},\n '202212473': {'att': 95, 'rep': 30, 'mid': 60, 'fin': 10},\n '202212310': {'att': 65, 'rep': 85, 'mid': 15, 'fin': 20},\n '202212460': {'att': 55, 'rep': 35, 'mid': 35, 'fin': 5},\n '202212320': {'att': 80, 'rep': 60, 'mid': 55, 'fin': 70},\n '202212329': {'att': 75, 'rep': 40, 'mid': 75, 'fin': 85},\n '202212408': {'att': 65, 'rep': 70, 'mid': 60, 'fin': 75},\n '202212319': {'att': 60, 'rep': 25, 'mid': 20, 'fin': 35},\n '202212348': {'att': 95, 'rep': 55, 'mid': 65, 'fin': 90},\n '202212306': {'att': 90, 'rep': 25, 'mid': 95, 'fin': 50},\n '202212308': {'att': 55, 'rep': 45, 'mid': 75, 'fin': 30},\n '202212366': {'att': 95, 'rep': 60, 'mid': 25, 'fin': 55},\n '202212367': {'att': 95, 'rep': 35, 'mid': 0, 'fin': 25},\n '202212461': {'att': 50, 'rep': 55, 'mid': 90, 'fin': 45},\n '202212354': {'att': 50, 'rep': 65, 'mid': 50, 'fin': 70},\n '202212361': {'att': 95, 'rep': 100, 'mid': 25, 'fin': 40},\n '202212400': {'att': 50, 'rep': 65, 'mid': 35, 'fin': 85},\n '202212490': {'att': 65, 'rep': 85, 'mid': 10, 'fin': 5},\n '202212404': {'att': 70, 'rep': 65, 'mid': 65, 'fin': 80},\n '202212326': {'att': 90, 'rep': 70, 'mid': 100, 'fin': 30},\n '202212452': {'att': 80, 'rep': 45, 'mid': 80, 'fin': 85},\n '202212362': {'att': 55, 'rep': 45, 'mid': 85, 'fin': 70},\n '202212396': {'att': 65, 'rep': 35, 'mid': 45, 'fin': 20},\n '202212356': {'att': 70, 'rep': 25, 'mid': 50, 'fin': 70},\n '202212305': {'att': 85, 'rep': 55, 'mid': 30, 'fin': 80},\n '202212398': {'att': 90, 'rep': 30, 'mid': 30, 'fin': 0},\n '202212410': {'att': 100, 'rep': 65, 'mid': 50, 'fin': 70},\n '202212385': {'att': 80, 'rep': 70, 'mid': 50, 'fin': 100},\n '202212430': {'att': 80, 'rep': 35, 'mid': 25, 'fin': 65},\n '202212498': {'att': 55, 'rep': 75, 'mid': 20, 'fin': 25},\n '202212423': {'att': 75, 'rep': 75, 'mid': 85, 'fin': 95},\n '202212327': {'att': 80, 'rep': 95, 'mid': 5, 'fin': 5},\n '202212347': {'att': 95, 'rep': 60, 'mid': 65, 'fin': 10},\n '202212483': {'att': 95, 'rep': 60, 'mid': 90, 'fin': 75},\n '202212447': {'att': 100, 'rep': 75, 'mid': 70, 'fin': 25},\n '202212496': {'att': 100, 'rep': 55, 'mid': 35, 'fin': 85},\n '202212358': {'att': 80, 'rep': 60, 'mid': 65, 'fin': 55},\n '202212399': {'att': 70, 'rep': 80, 'mid': 0, 'fin': 10},\n '202212459': {'att': 85, 'rep': 65, 'mid': 60, 'fin': 60},\n '202212313': {'att': 100, 'rep': 95, 'mid': 0, 'fin': 25},\n '202212304': {'att': 95, 'rep': 60, 'mid': 15, 'fin': 45},\n '202212431': {'att': 75, 'rep': 40, 'mid': 30, 'fin': 10},\n '202212325': {'att': 70, 'rep': 80, 'mid': 50, 'fin': 25},\n '202212471': {'att': 50, 'rep': 45, 'mid': 10, 'fin': 10},\n '202212463': {'att': 100, 'rep': 100, 'mid': 100, 'fin': 50},\n '202212441': {'att': 75, 'rep': 50, 'mid': 60, 'fin': 5},\n '202212445': {'att': 85, 'rep': 50, 'mid': 35, 'fin': 100},\n '202212323': {'att': 80, 'rep': 35, 'mid': 75, 'fin': 80},\n '202212442': {'att': 95, 'rep': 45, 'mid': 35, 'fin': 80},\n '202212346': {'att': 65, 'rep': 85, 'mid': 85, 'fin': 15},\n '202212411': {'att': 90, 'rep': 30, 'mid': 25, 'fin': 5},\n '202212468': {'att': 65, 'rep': 65, 'mid': 35, 'fin': 70},\n '202212331': {'att': 80, 'rep': 65, 'mid': 30, 'fin': 90},\n '202212345': {'att': 95, 'rep': 80, 'mid': 45, 'fin': 35},\n '202212339': {'att': 65, 'rep': 75, 'mid': 50, 'fin': 35},\n '202212383': {'att': 90, 'rep': 55, 'mid': 100, 'fin': 30},\n '202212462': {'att': 95, 'rep': 25, 'mid': 95, 'fin': 90},\n '202212344': {'att': 100, 'rep': 50, 'mid': 80, 'fin': 10},\n '202212472': {'att': 50, 'rep': 55, 'mid': 35, 'fin': 60},\n '202212437': {'att': 90, 'rep': 70, 'mid': 35, 'fin': 25},\n '202212336': {'att': 50, 'rep': 55, 'mid': 15, 'fin': 75},\n '202212438': {'att': 80, 'rep': 50, 'mid': 55, 'fin': 90},\n '202212454': {'att': 50, 'rep': 75, 'mid': 65, 'fin': 90},\n '202212384': {'att': 70, 'rep': 40, 'mid': 90, 'fin': 5},\n '202212402': {'att': 65, 'rep': 85, 'mid': 20, 'fin': 90},\n '202212397': {'att': 60, 'rep': 30, 'mid': 0, 'fin': 50},\n '202212318': {'att': 50, 'rep': 65, 'mid': 15, 'fin': 0},\n '202212371': {'att': 60, 'rep': 95, 'mid': 30, 'fin': 70},\n '202212469': {'att': 70, 'rep': 70, 'mid': 5, 'fin': 0},\n '202212379': {'att': 75, 'rep': 45, 'mid': 15, 'fin': 75},\n '202212364': {'att': 50, 'rep': 60, 'mid': 15, 'fin': 50},\n '202212450': {'att': 85, 'rep': 90, 'mid': 90, 'fin': 90},\n '202212337': {'att': 80, 'rep': 25, 'mid': 85, 'fin': 20},\n '202212458': {'att': 55, 'rep': 75, 'mid': 95, 'fin': 90},\n '202212494': {'att': 85, 'rep': 30, 'mid': 45, 'fin': 15},\n '202212478': {'att': 65, 'rep': 30, 'mid': 45, 'fin': 15},\n '202212373': {'att': 85, 'rep': 95, 'mid': 35, 'fin': 25},\n '202212474': {'att': 60, 'rep': 25, 'mid': 10, 'fin': 50},\n '202212455': {'att': 95, 'rep': 45, 'mid': 90, 'fin': 35},\n '202212317': {'att': 85, 'rep': 50, 'mid': 60, 'fin': 45},\n '202212341': {'att': 60, 'rep': 50, 'mid': 100, 'fin': 70},\n '202212386': {'att': 100, 'rep': 75, 'mid': 60, 'fin': 0},\n '202212328': {'att': 100, 'rep': 90, 'mid': 85, 'fin': 75},\n '202212417': {'att': 55, 'rep': 100, 'mid': 100, 'fin': 60},\n '202212370': {'att': 70, 'rep': 60, 'mid': 30, 'fin': 40},\n '202212486': {'att': 70, 'rep': 90, 'mid': 95, 'fin': 40},\n '202212333': {'att': 55, 'rep': 50, 'mid': 0, 'fin': 5},\n '202212360': {'att': 100, 'rep': 100, 'mid': 45, 'fin': 90},\n '202212350': {'att': 85, 'rep': 70, 'mid': 90, 'fin': 80},\n '202212382': {'att': 100, 'rep': 85, 'mid': 65, 'fin': 85},\n '202212392': {'att': 60, 'rep': 65, 'mid': 35, 'fin': 15},\n '202212449': {'att': 65, 'rep': 75, 'mid': 75, 'fin': 85},\n '202212394': {'att': 65, 'rep': 25, 'mid': 40, 'fin': 0},\n '202212444': {'att': 75, 'rep': 75, 'mid': 50, 'fin': 40},\n '202212487': {'att': 50, 'rep': 55, 'mid': 80, 'fin': 55},\n '202212425': {'att': 75, 'rep': 30, 'mid': 20, 'fin': 50},\n '202212312': {'att': 100, 'rep': 50, 'mid': 25, 'fin': 65},\n '202212448': {'att': 90, 'rep': 30, 'mid': 95, 'fin': 35},\n '202212434': {'att': 55, 'rep': 100, 'mid': 80, 'fin': 0},\n '202212451': {'att': 75, 'rep': 60, 'mid': 15, 'fin': 40},\n '202212433': {'att': 60, 'rep': 25, 'mid': 25, 'fin': 50},\n '202212424': {'att': 85, 'rep': 35, 'mid': 10, 'fin': 60},\n '202212351': {'att': 60, 'rep': 100, 'mid': 55, 'fin': 40},\n '202212324': {'att': 70, 'rep': 55, 'mid': 50, 'fin': 75},\n '202212314': {'att': 80, 'rep': 65, 'mid': 95, 'fin': 85},\n '202212446': {'att': 65, 'rep': 35, 'mid': 15, 'fin': 65},\n '202212401': {'att': 85, 'rep': 70, 'mid': 100, 'fin': 0},\n '202212307': {'att': 100, 'rep': 30, 'mid': 60, 'fin': 65},\n '202212300': {'att': 65, 'rep': 70, 'mid': 55, 'fin': 70},\n '202212342': {'att': 85, 'rep': 55, 'mid': 85, 'fin': 90},\n '202212479': {'att': 85, 'rep': 95, 'mid': 80, 'fin': 10},\n '202212443': {'att': 85, 'rep': 70, 'mid': 75, 'fin': 5},\n '202212387': {'att': 100, 'rep': 35, 'mid': 70, 'fin': 0},\n '202212372': {'att': 95, 'rep': 45, 'mid': 55, 'fin': 65},\n '202212376': {'att': 95, 'rep': 85, 'mid': 40, 'fin': 65},\n '202212466': {'att': 55, 'rep': 50, 'mid': 30, 'fin': 85},\n '202212391': {'att': 85, 'rep': 50, 'mid': 5, 'fin': 65},\n '202212368': {'att': 75, 'rep': 90, 'mid': 85, 'fin': 85},\n '202212427': {'att': 95, 'rep': 70, 'mid': 10, 'fin': 5},\n '202212414': {'att': 85, 'rep': 35, 'mid': 80, 'fin': 95},\n '202212426': {'att': 95, 'rep': 50, 'mid': 80, 'fin': 90},\n '202212316': {'att': 100, 'rep': 65, 'mid': 75, 'fin': 40},\n '202212355': {'att': 95, 'rep': 70, 'mid': 70, 'fin': 0},\n '202212477': {'att': 95, 'rep': 70, 'mid': 20, 'fin': 25},\n '202212484': {'att': 100, 'rep': 60, 'mid': 10, 'fin': 5},\n '202212456': {'att': 55, 'rep': 35, 'mid': 25, 'fin': 10},\n '202212500': {'att': 60, 'rep': 90, 'mid': 40, 'fin': 5},\n '202212381': {'att': 85, 'rep': 90, 'mid': 85, 'fin': 75},\n '202212335': {'att': 75, 'rep': 85, 'mid': 25, 'fin': 35},\n '202212475': {'att': 55, 'rep': 30, 'mid': 50, 'fin': 45},\n '202212343': {'att': 70, 'rep': 60, 'mid': 75, 'fin': 75},\n '202212412': {'att': 80, 'rep': 30, 'mid': 95, 'fin': 5},\n '202212428': {'att': 90, 'rep': 85, 'mid': 80, 'fin': 15},\n '202212330': {'att': 90, 'rep': 25, 'mid': 95, 'fin': 5},\n '202212375': {'att': 60, 'rep': 85, 'mid': 50, 'fin': 20},\n '202212413': {'att': 90, 'rep': 50, 'mid': 95, 'fin': 95},\n '202212303': {'att': 75, 'rep': 95, 'mid': 65, 'fin': 40},\n '202212374': {'att': 60, 'rep': 40, 'mid': 35, 'fin': 0},\n '202212409': {'att': 55, 'rep': 100, 'mid': 15, 'fin': 80},\n '202212440': {'att': 70, 'rep': 75, 'mid': 80, 'fin': 0},\n '202212393': {'att': 75, 'rep': 65, 'mid': 25, 'fin': 20},\n '202212492': {'att': 90, 'rep': 75, 'mid': 80, 'fin': 25},\n '202212357': {'att': 50, 'rep': 75, 'mid': 75, 'fin': 20},\n '202212465': {'att': 55, 'rep': 45, 'mid': 35, 'fin': 45},\n '202212415': {'att': 90, 'rep': 70, 'mid': 90, 'fin': 0},\n '202212405': {'att': 75, 'rep': 30, 'mid': 100, 'fin': 60},\n '202212435': {'att': 90, 'rep': 85, 'mid': 0, 'fin': 40},\n '202212380': {'att': 85, 'rep': 70, 'mid': 35, 'fin': 0},\n '202212369': {'att': 100, 'rep': 75, 'mid': 100, 'fin': 85},\n '202212467': {'att': 55, 'rep': 35, 'mid': 20, 'fin': 10},\n '202212429': {'att': 70, 'rep': 75, 'mid': 90, 'fin': 90},\n '202212495': {'att': 90, 'rep': 90, 'mid': 55, 'fin': 55},\n '202212420': {'att': 55, 'rep': 60, 'mid': 40, 'fin': 0},\n '202212302': {'att': 100, 'rep': 90, 'mid': 5, 'fin': 30},\n '202212481': {'att': 50, 'rep': 55, 'mid': 25, 'fin': 80},\n '202212422': {'att': 100, 'rep': 100, 'mid': 90, 'fin': 55},\n '202212388': {'att': 70, 'rep': 45, 'mid': 70, 'fin': 75},\n '202212480': {'att': 85, 'rep': 95, 'mid': 85, 'fin': 90},\n '202212378': {'att': 55, 'rep': 25, 'mid': 95, 'fin': 45},\n '202212457': {'att': 75, 'rep': 30, 'mid': 10, 'fin': 95},\n '202212419': {'att': 65, 'rep': 85, 'mid': 15, 'fin': 60},\n '202212432': {'att': 70, 'rep': 90, 'mid': 70, 'fin': 0},\n '202212395': {'att': 60, 'rep': 85, 'mid': 70, 'fin': 85},\n '202212464': {'att': 100, 'rep': 25, 'mid': 10, 'fin': 20},\n '202212476': {'att': 75, 'rep': 25, 'mid': 80, 'fin': 25},\n '202212332': {'att': 90, 'rep': 95, 'mid': 40, 'fin': 80},\n '202212301': {'att': 95, 'rep': 90, 'mid': 50, 'fin': 50},\n '202212497': {'att': 90, 'rep': 90, 'mid': 65, 'fin': 85},\n '202212309': {'att': 95, 'rep': 75, 'mid': 50, 'fin': 40},\n '202212493': {'att': 55, 'rep': 60, 'mid': 70, 'fin': 5},\n '202212311': {'att': 95, 'rep': 85, 'mid': 0, 'fin': 15},\n '202212416': {'att': 65, 'rep': 60, 'mid': 35, 'fin': 20},\n '202212489': {'att': 65, 'rep': 50, 'mid': 5, 'fin': 5},\n '202212359': {'att': 90, 'rep': 25, 'mid': 60, 'fin': 25},\n '202212349': {'att': 100, 'rep': 40, 'mid': 40, 'fin': 15},\n '202212403': {'att': 70, 'rep': 25, 'mid': 100, 'fin': 75},\n '202212418': {'att': 100, 'rep': 30, 'mid': 70, 'fin': 70},\n '202212406': {'att': 50, 'rep': 55, 'mid': 55, 'fin': 5},\n '202212485': {'att': 70, 'rep': 35, 'mid': 70, 'fin': 100},\n '202212390': {'att': 70, 'rep': 60, 'mid': 60, 'fin': 80},\n '202212365': {'att': 55, 'rep': 45, 'mid': 90, 'fin': 5},\n '202212338': {'att': 55, 'rep': 55, 'mid': 10, 'fin': 95},\n '202212363': {'att': 65, 'rep': 80, 'mid': 10, 'fin': 30},\n '202212321': {'att': 90, 'rep': 25, 'mid': 35, 'fin': 55},\n '202212499': {'att': 100, 'rep': 30, 'mid': 30, 'fin': 85},\n '202212340': {'att': 70, 'rep': 85, 'mid': 70, 'fin': 65},\n '202212421': {'att': 60, 'rep': 100, 'mid': 45, 'fin': 100},\n '202212407': {'att': 70, 'rep': 25, 'mid': 100, 'fin': 15},\n '202212439': {'att': 70, 'rep': 35, 'mid': 80, 'fin': 25},\n '202212488': {'att': 65, 'rep': 60, 'mid': 30, 'fin': 35},\n '202212453': {'att': 95, 'rep': 35, 'mid': 40, 'fin': 95},\n '202212482': {'att': 50, 'rep': 80, 'mid': 65, 'fin': 90},\n '202212334': {'att': 100, 'rep': 40, 'mid': 80, 'fin': 80},\n '202212322': {'att': 55, 'rep': 30, 'mid': 95, 'fin': 100},\n '202212353': {'att': 65, 'rep': 40, 'mid': 65, 'fin': 70},\n '202212491': {'att': 55, 'rep': 70, 'mid': 40, 'fin': 95},\n '202212352': {'att': 65, 'rep': 85, 'mid': 25, 'fin': 85},\n '202212315': {'att': 85, 'rep': 85, 'mid': 100, 'fin': 10},\n '202212470': {'att': 80, 'rep': 65, 'mid': 35, 'fin': 60},\n '202212436': {'att': 50, 'rep': 95, 'mid': 45, 'fin': 85}}\n\n\n학번 ’202212460’에 해당하는 학생의 출석점수를 알고 싶다면?\n(풀이1)\n\ntest_dic['202212460']['att']\n\n55\n\n\n(풀이2)\n\ntest_ndarray\n\narray([['202212377', '202212473', '202212310', '202212460', '202212320',\n        '202212329', '202212408', '202212319', '202212348', '202212306',\n        '202212308', '202212366', '202212367', '202212461', '202212354',\n        '202212361', '202212400', '202212490', '202212404', '202212326',\n        '202212452', '202212362', '202212396', '202212356', '202212305',\n        '202212398', '202212410', '202212385', '202212430', '202212498',\n        '202212423', '202212327', '202212347', '202212483', '202212447',\n        '202212496', '202212358', '202212399', '202212459', '202212313',\n        '202212304', '202212431', '202212325', '202212471', '202212463',\n        '202212441', '202212445', '202212323', '202212442', '202212346',\n        '202212411', '202212468', '202212331', '202212345', '202212339',\n        '202212383', '202212462', '202212344', '202212472', '202212437',\n        '202212336', '202212438', '202212454', '202212384', '202212402',\n        '202212397', '202212318', '202212371', '202212469', '202212379',\n        '202212364', '202212450', '202212337', '202212458', '202212494',\n        '202212478', '202212373', '202212474', '202212455', '202212317',\n        '202212341', '202212386', '202212328', '202212417', '202212370',\n        '202212486', '202212333', '202212360', '202212350', '202212382',\n        '202212392', '202212449', '202212394', '202212444', '202212487',\n        '202212425', '202212312', '202212448', '202212434', '202212451',\n        '202212433', '202212424', '202212351', '202212324', '202212314',\n        '202212446', '202212401', '202212307', '202212300', '202212342',\n        '202212479', '202212443', '202212387', '202212372', '202212376',\n        '202212466', '202212391', '202212368', '202212427', '202212414',\n        '202212426', '202212316', '202212355', '202212477', '202212484',\n        '202212456', '202212500', '202212381', '202212335', '202212475',\n        '202212343', '202212412', '202212428', '202212330', '202212375',\n        '202212413', '202212303', '202212374', '202212409', '202212440',\n        '202212393', '202212492', '202212357', '202212465', '202212415',\n        '202212405', '202212435', '202212380', '202212369', '202212467',\n        '202212429', '202212495', '202212420', '202212302', '202212481',\n        '202212422', '202212388', '202212480', '202212378', '202212457',\n        '202212419', '202212432', '202212395', '202212464', '202212476',\n        '202212332', '202212301', '202212497', '202212309', '202212493',\n        '202212311', '202212416', '202212489', '202212359', '202212349',\n        '202212403', '202212418', '202212406', '202212485', '202212390',\n        '202212365', '202212338', '202212363', '202212321', '202212499',\n        '202212340', '202212421', '202212407', '202212439', '202212488',\n        '202212453', '202212482', '202212334', '202212322', '202212353',\n        '202212491', '202212352', '202212315', '202212470', '202212436'],\n       ['65', '95', '65', '55', '80', '75', '65', '60', '95', '90', '55',\n        '95', '95', '50', '50', '95', '50', '65', '70', '90', '80', '55',\n        '65', '70', '85', '90', '100', '80', '80', '55', '75', '80',\n        '95', '95', '100', '100', '80', '70', '85', '100', '95', '75',\n        '70', '50', '100', '75', '85', '80', '95', '65', '90', '65',\n        '80', '95', '65', '90', '95', '100', '50', '90', '50', '80',\n        '50', '70', '65', '60', '50', '60', '70', '75', '50', '85', '80',\n        '55', '85', '65', '85', '60', '95', '85', '60', '100', '100',\n        '55', '70', '70', '55', '100', '85', '100', '60', '65', '65',\n        '75', '50', '75', '100', '90', '55', '75', '60', '85', '60',\n        '70', '80', '65', '85', '100', '65', '85', '85', '85', '100',\n        '95', '95', '55', '85', '75', '95', '85', '95', '100', '95',\n        '95', '100', '55', '60', '85', '75', '55', '70', '80', '90',\n        '90', '60', '90', '75', '60', '55', '70', '75', '90', '50', '55',\n        '90', '75', '90', '85', '100', '55', '70', '90', '55', '100',\n        '50', '100', '70', '85', '55', '75', '65', '70', '60', '100',\n        '75', '90', '95', '90', '95', '55', '95', '65', '65', '90',\n        '100', '70', '100', '50', '70', '70', '55', '55', '65', '90',\n        '100', '70', '60', '70', '70', '65', '95', '50', '100', '55',\n        '65', '55', '65', '85', '80', '50'],\n       ['45', '30', '85', '35', '60', '40', '70', '25', '55', '25', '45',\n        '60', '35', '55', '65', '100', '65', '85', '65', '70', '45',\n        '45', '35', '25', '55', '30', '65', '70', '35', '75', '75', '95',\n        '60', '60', '75', '55', '60', '80', '65', '95', '60', '40', '80',\n        '45', '100', '50', '50', '35', '45', '85', '30', '65', '65',\n        '80', '75', '55', '25', '50', '55', '70', '55', '50', '75', '40',\n        '85', '30', '65', '95', '70', '45', '60', '90', '25', '75', '30',\n        '30', '95', '25', '45', '50', '50', '75', '90', '100', '60',\n        '90', '50', '100', '70', '85', '65', '75', '25', '75', '55',\n        '30', '50', '30', '100', '60', '25', '35', '100', '55', '65',\n        '35', '70', '30', '70', '55', '95', '70', '35', '45', '85', '50',\n        '50', '90', '70', '35', '50', '65', '70', '70', '60', '35', '90',\n        '90', '85', '30', '60', '30', '85', '25', '85', '50', '95', '40',\n        '100', '75', '65', '75', '75', '45', '70', '30', '85', '70',\n        '75', '35', '75', '90', '60', '90', '55', '100', '45', '95',\n        '25', '30', '85', '90', '85', '25', '25', '95', '90', '90', '75',\n        '60', '85', '60', '50', '25', '40', '25', '30', '55', '35', '60',\n        '45', '55', '80', '25', '30', '85', '100', '25', '35', '60',\n        '35', '80', '40', '30', '40', '70', '85', '85', '65', '95'],\n       ['0', '60', '15', '35', '55', '75', '60', '20', '65', '95', '75',\n        '25', '0', '90', '50', '25', '35', '10', '65', '100', '80', '85',\n        '45', '50', '30', '30', '50', '50', '25', '20', '85', '5', '65',\n        '90', '70', '35', '65', '0', '60', '0', '15', '30', '50', '10',\n        '100', '60', '35', '75', '35', '85', '25', '35', '30', '45',\n        '50', '100', '95', '80', '35', '35', '15', '55', '65', '90',\n        '20', '0', '15', '30', '5', '15', '15', '90', '85', '95', '45',\n        '45', '35', '10', '90', '60', '100', '60', '85', '100', '30',\n        '95', '0', '45', '90', '65', '35', '75', '40', '50', '80', '20',\n        '25', '95', '80', '15', '25', '10', '55', '50', '95', '15',\n        '100', '60', '55', '85', '80', '75', '70', '55', '40', '30', '5',\n        '85', '10', '80', '80', '75', '70', '20', '10', '25', '40', '85',\n        '25', '50', '75', '95', '80', '95', '50', '95', '65', '35', '15',\n        '80', '25', '80', '75', '35', '90', '100', '0', '35', '100',\n        '20', '90', '55', '40', '5', '25', '90', '70', '85', '95', '10',\n        '15', '70', '70', '10', '80', '40', '50', '65', '50', '70', '0',\n        '35', '5', '60', '40', '100', '70', '55', '70', '60', '90', '10',\n        '10', '35', '30', '70', '45', '100', '80', '30', '40', '65',\n        '80', '95', '65', '40', '25', '100', '35', '45'],\n       ['10', '10', '20', '5', '70', '85', '75', '35', '90', '50', '30',\n        '55', '25', '45', '70', '40', '85', '5', '80', '30', '85', '70',\n        '20', '70', '80', '0', '70', '100', '65', '25', '95', '5', '10',\n        '75', '25', '85', '55', '10', '60', '25', '45', '10', '25', '10',\n        '50', '5', '100', '80', '80', '15', '5', '70', '90', '35', '35',\n        '30', '90', '10', '60', '25', '75', '90', '90', '5', '90', '50',\n        '0', '70', '0', '75', '50', '90', '20', '90', '15', '15', '25',\n        '50', '35', '45', '70', '0', '75', '60', '40', '40', '5', '90',\n        '80', '85', '15', '85', '0', '40', '55', '50', '65', '35', '0',\n        '40', '50', '60', '40', '75', '85', '65', '0', '65', '70', '90',\n        '10', '5', '0', '65', '65', '85', '65', '85', '5', '95', '90',\n        '40', '0', '25', '5', '10', '5', '75', '35', '45', '75', '5',\n        '15', '5', '20', '95', '40', '0', '80', '0', '20', '25', '20',\n        '45', '0', '60', '40', '0', '85', '10', '90', '55', '0', '30',\n        '80', '55', '75', '90', '45', '95', '60', '0', '85', '20', '25',\n        '80', '50', '85', '40', '5', '15', '20', '5', '25', '15', '75',\n        '70', '5', '100', '80', '5', '95', '30', '55', '85', '65', '100',\n        '15', '25', '35', '95', '90', '80', '100', '70', '95', '85',\n        '10', '60', '85']], dtype='<U21')\n\n\n\ntest_ndarray.T #학번이 string으로 들어가있어서 모든 자료가 string으로 되어있음.. \n\narray([['202212377', '65', '45', '0', '10'],\n       ['202212473', '95', '30', '60', '10'],\n       ['202212310', '65', '85', '15', '20'],\n       ['202212460', '55', '35', '35', '5'],\n       ['202212320', '80', '60', '55', '70'],\n       ['202212329', '75', '40', '75', '85'],\n       ['202212408', '65', '70', '60', '75'],\n       ['202212319', '60', '25', '20', '35'],\n       ['202212348', '95', '55', '65', '90'],\n       ['202212306', '90', '25', '95', '50'],\n       ['202212308', '55', '45', '75', '30'],\n       ['202212366', '95', '60', '25', '55'],\n       ['202212367', '95', '35', '0', '25'],\n       ['202212461', '50', '55', '90', '45'],\n       ['202212354', '50', '65', '50', '70'],\n       ['202212361', '95', '100', '25', '40'],\n       ['202212400', '50', '65', '35', '85'],\n       ['202212490', '65', '85', '10', '5'],\n       ['202212404', '70', '65', '65', '80'],\n       ['202212326', '90', '70', '100', '30'],\n       ['202212452', '80', '45', '80', '85'],\n       ['202212362', '55', '45', '85', '70'],\n       ['202212396', '65', '35', '45', '20'],\n       ['202212356', '70', '25', '50', '70'],\n       ['202212305', '85', '55', '30', '80'],\n       ['202212398', '90', '30', '30', '0'],\n       ['202212410', '100', '65', '50', '70'],\n       ['202212385', '80', '70', '50', '100'],\n       ['202212430', '80', '35', '25', '65'],\n       ['202212498', '55', '75', '20', '25'],\n       ['202212423', '75', '75', '85', '95'],\n       ['202212327', '80', '95', '5', '5'],\n       ['202212347', '95', '60', '65', '10'],\n       ['202212483', '95', '60', '90', '75'],\n       ['202212447', '100', '75', '70', '25'],\n       ['202212496', '100', '55', '35', '85'],\n       ['202212358', '80', '60', '65', '55'],\n       ['202212399', '70', '80', '0', '10'],\n       ['202212459', '85', '65', '60', '60'],\n       ['202212313', '100', '95', '0', '25'],\n       ['202212304', '95', '60', '15', '45'],\n       ['202212431', '75', '40', '30', '10'],\n       ['202212325', '70', '80', '50', '25'],\n       ['202212471', '50', '45', '10', '10'],\n       ['202212463', '100', '100', '100', '50'],\n       ['202212441', '75', '50', '60', '5'],\n       ['202212445', '85', '50', '35', '100'],\n       ['202212323', '80', '35', '75', '80'],\n       ['202212442', '95', '45', '35', '80'],\n       ['202212346', '65', '85', '85', '15'],\n       ['202212411', '90', '30', '25', '5'],\n       ['202212468', '65', '65', '35', '70'],\n       ['202212331', '80', '65', '30', '90'],\n       ['202212345', '95', '80', '45', '35'],\n       ['202212339', '65', '75', '50', '35'],\n       ['202212383', '90', '55', '100', '30'],\n       ['202212462', '95', '25', '95', '90'],\n       ['202212344', '100', '50', '80', '10'],\n       ['202212472', '50', '55', '35', '60'],\n       ['202212437', '90', '70', '35', '25'],\n       ['202212336', '50', '55', '15', '75'],\n       ['202212438', '80', '50', '55', '90'],\n       ['202212454', '50', '75', '65', '90'],\n       ['202212384', '70', '40', '90', '5'],\n       ['202212402', '65', '85', '20', '90'],\n       ['202212397', '60', '30', '0', '50'],\n       ['202212318', '50', '65', '15', '0'],\n       ['202212371', '60', '95', '30', '70'],\n       ['202212469', '70', '70', '5', '0'],\n       ['202212379', '75', '45', '15', '75'],\n       ['202212364', '50', '60', '15', '50'],\n       ['202212450', '85', '90', '90', '90'],\n       ['202212337', '80', '25', '85', '20'],\n       ['202212458', '55', '75', '95', '90'],\n       ['202212494', '85', '30', '45', '15'],\n       ['202212478', '65', '30', '45', '15'],\n       ['202212373', '85', '95', '35', '25'],\n       ['202212474', '60', '25', '10', '50'],\n       ['202212455', '95', '45', '90', '35'],\n       ['202212317', '85', '50', '60', '45'],\n       ['202212341', '60', '50', '100', '70'],\n       ['202212386', '100', '75', '60', '0'],\n       ['202212328', '100', '90', '85', '75'],\n       ['202212417', '55', '100', '100', '60'],\n       ['202212370', '70', '60', '30', '40'],\n       ['202212486', '70', '90', '95', '40'],\n       ['202212333', '55', '50', '0', '5'],\n       ['202212360', '100', '100', '45', '90'],\n       ['202212350', '85', '70', '90', '80'],\n       ['202212382', '100', '85', '65', '85'],\n       ['202212392', '60', '65', '35', '15'],\n       ['202212449', '65', '75', '75', '85'],\n       ['202212394', '65', '25', '40', '0'],\n       ['202212444', '75', '75', '50', '40'],\n       ['202212487', '50', '55', '80', '55'],\n       ['202212425', '75', '30', '20', '50'],\n       ['202212312', '100', '50', '25', '65'],\n       ['202212448', '90', '30', '95', '35'],\n       ['202212434', '55', '100', '80', '0'],\n       ['202212451', '75', '60', '15', '40'],\n       ['202212433', '60', '25', '25', '50'],\n       ['202212424', '85', '35', '10', '60'],\n       ['202212351', '60', '100', '55', '40'],\n       ['202212324', '70', '55', '50', '75'],\n       ['202212314', '80', '65', '95', '85'],\n       ['202212446', '65', '35', '15', '65'],\n       ['202212401', '85', '70', '100', '0'],\n       ['202212307', '100', '30', '60', '65'],\n       ['202212300', '65', '70', '55', '70'],\n       ['202212342', '85', '55', '85', '90'],\n       ['202212479', '85', '95', '80', '10'],\n       ['202212443', '85', '70', '75', '5'],\n       ['202212387', '100', '35', '70', '0'],\n       ['202212372', '95', '45', '55', '65'],\n       ['202212376', '95', '85', '40', '65'],\n       ['202212466', '55', '50', '30', '85'],\n       ['202212391', '85', '50', '5', '65'],\n       ['202212368', '75', '90', '85', '85'],\n       ['202212427', '95', '70', '10', '5'],\n       ['202212414', '85', '35', '80', '95'],\n       ['202212426', '95', '50', '80', '90'],\n       ['202212316', '100', '65', '75', '40'],\n       ['202212355', '95', '70', '70', '0'],\n       ['202212477', '95', '70', '20', '25'],\n       ['202212484', '100', '60', '10', '5'],\n       ['202212456', '55', '35', '25', '10'],\n       ['202212500', '60', '90', '40', '5'],\n       ['202212381', '85', '90', '85', '75'],\n       ['202212335', '75', '85', '25', '35'],\n       ['202212475', '55', '30', '50', '45'],\n       ['202212343', '70', '60', '75', '75'],\n       ['202212412', '80', '30', '95', '5'],\n       ['202212428', '90', '85', '80', '15'],\n       ['202212330', '90', '25', '95', '5'],\n       ['202212375', '60', '85', '50', '20'],\n       ['202212413', '90', '50', '95', '95'],\n       ['202212303', '75', '95', '65', '40'],\n       ['202212374', '60', '40', '35', '0'],\n       ['202212409', '55', '100', '15', '80'],\n       ['202212440', '70', '75', '80', '0'],\n       ['202212393', '75', '65', '25', '20'],\n       ['202212492', '90', '75', '80', '25'],\n       ['202212357', '50', '75', '75', '20'],\n       ['202212465', '55', '45', '35', '45'],\n       ['202212415', '90', '70', '90', '0'],\n       ['202212405', '75', '30', '100', '60'],\n       ['202212435', '90', '85', '0', '40'],\n       ['202212380', '85', '70', '35', '0'],\n       ['202212369', '100', '75', '100', '85'],\n       ['202212467', '55', '35', '20', '10'],\n       ['202212429', '70', '75', '90', '90'],\n       ['202212495', '90', '90', '55', '55'],\n       ['202212420', '55', '60', '40', '0'],\n       ['202212302', '100', '90', '5', '30'],\n       ['202212481', '50', '55', '25', '80'],\n       ['202212422', '100', '100', '90', '55'],\n       ['202212388', '70', '45', '70', '75'],\n       ['202212480', '85', '95', '85', '90'],\n       ['202212378', '55', '25', '95', '45'],\n       ['202212457', '75', '30', '10', '95'],\n       ['202212419', '65', '85', '15', '60'],\n       ['202212432', '70', '90', '70', '0'],\n       ['202212395', '60', '85', '70', '85'],\n       ['202212464', '100', '25', '10', '20'],\n       ['202212476', '75', '25', '80', '25'],\n       ['202212332', '90', '95', '40', '80'],\n       ['202212301', '95', '90', '50', '50'],\n       ['202212497', '90', '90', '65', '85'],\n       ['202212309', '95', '75', '50', '40'],\n       ['202212493', '55', '60', '70', '5'],\n       ['202212311', '95', '85', '0', '15'],\n       ['202212416', '65', '60', '35', '20'],\n       ['202212489', '65', '50', '5', '5'],\n       ['202212359', '90', '25', '60', '25'],\n       ['202212349', '100', '40', '40', '15'],\n       ['202212403', '70', '25', '100', '75'],\n       ['202212418', '100', '30', '70', '70'],\n       ['202212406', '50', '55', '55', '5'],\n       ['202212485', '70', '35', '70', '100'],\n       ['202212390', '70', '60', '60', '80'],\n       ['202212365', '55', '45', '90', '5'],\n       ['202212338', '55', '55', '10', '95'],\n       ['202212363', '65', '80', '10', '30'],\n       ['202212321', '90', '25', '35', '55'],\n       ['202212499', '100', '30', '30', '85'],\n       ['202212340', '70', '85', '70', '65'],\n       ['202212421', '60', '100', '45', '100'],\n       ['202212407', '70', '25', '100', '15'],\n       ['202212439', '70', '35', '80', '25'],\n       ['202212488', '65', '60', '30', '35'],\n       ['202212453', '95', '35', '40', '95'],\n       ['202212482', '50', '80', '65', '90'],\n       ['202212334', '100', '40', '80', '80'],\n       ['202212322', '55', '30', '95', '100'],\n       ['202212353', '65', '40', '65', '70'],\n       ['202212491', '55', '70', '40', '95'],\n       ['202212352', '65', '85', '25', '85'],\n       ['202212315', '85', '85', '100', '10'],\n       ['202212470', '80', '65', '35', '60'],\n       ['202212436', '50', '95', '45', '85']], dtype='<U21')\n\n\n\ntest_ndarray\n\narray([[202212377,        65,        45,         0,        10],\n       [202212473,        95,        30,        60,        10],\n       [202212310,        65,        85,        15,        20],\n       [202212460,        55,        35,        35,         5],\n       [202212320,        80,        60,        55,        70],\n       [202212329,        75,        40,        75,        85],\n       [202212408,        65,        70,        60,        75],\n       [202212319,        60,        25,        20,        35],\n       [202212348,        95,        55,        65,        90],\n       [202212306,        90,        25,        95,        50],\n       [202212308,        55,        45,        75,        30],\n       [202212366,        95,        60,        25,        55],\n       [202212367,        95,        35,         0,        25],\n       [202212461,        50,        55,        90,        45],\n       [202212354,        50,        65,        50,        70],\n       [202212361,        95,       100,        25,        40],\n       [202212400,        50,        65,        35,        85],\n       [202212490,        65,        85,        10,         5],\n       [202212404,        70,        65,        65,        80],\n       [202212326,        90,        70,       100,        30],\n       [202212452,        80,        45,        80,        85],\n       [202212362,        55,        45,        85,        70],\n       [202212396,        65,        35,        45,        20],\n       [202212356,        70,        25,        50,        70],\n       [202212305,        85,        55,        30,        80],\n       [202212398,        90,        30,        30,         0],\n       [202212410,       100,        65,        50,        70],\n       [202212385,        80,        70,        50,       100],\n       [202212430,        80,        35,        25,        65],\n       [202212498,        55,        75,        20,        25],\n       [202212423,        75,        75,        85,        95],\n       [202212327,        80,        95,         5,         5],\n       [202212347,        95,        60,        65,        10],\n       [202212483,        95,        60,        90,        75],\n       [202212447,       100,        75,        70,        25],\n       [202212496,       100,        55,        35,        85],\n       [202212358,        80,        60,        65,        55],\n       [202212399,        70,        80,         0,        10],\n       [202212459,        85,        65,        60,        60],\n       [202212313,       100,        95,         0,        25],\n       [202212304,        95,        60,        15,        45],\n       [202212431,        75,        40,        30,        10],\n       [202212325,        70,        80,        50,        25],\n       [202212471,        50,        45,        10,        10],\n       [202212463,       100,       100,       100,        50],\n       [202212441,        75,        50,        60,         5],\n       [202212445,        85,        50,        35,       100],\n       [202212323,        80,        35,        75,        80],\n       [202212442,        95,        45,        35,        80],\n       [202212346,        65,        85,        85,        15],\n       [202212411,        90,        30,        25,         5],\n       [202212468,        65,        65,        35,        70],\n       [202212331,        80,        65,        30,        90],\n       [202212345,        95,        80,        45,        35],\n       [202212339,        65,        75,        50,        35],\n       [202212383,        90,        55,       100,        30],\n       [202212462,        95,        25,        95,        90],\n       [202212344,       100,        50,        80,        10],\n       [202212472,        50,        55,        35,        60],\n       [202212437,        90,        70,        35,        25],\n       [202212336,        50,        55,        15,        75],\n       [202212438,        80,        50,        55,        90],\n       [202212454,        50,        75,        65,        90],\n       [202212384,        70,        40,        90,         5],\n       [202212402,        65,        85,        20,        90],\n       [202212397,        60,        30,         0,        50],\n       [202212318,        50,        65,        15,         0],\n       [202212371,        60,        95,        30,        70],\n       [202212469,        70,        70,         5,         0],\n       [202212379,        75,        45,        15,        75],\n       [202212364,        50,        60,        15,        50],\n       [202212450,        85,        90,        90,        90],\n       [202212337,        80,        25,        85,        20],\n       [202212458,        55,        75,        95,        90],\n       [202212494,        85,        30,        45,        15],\n       [202212478,        65,        30,        45,        15],\n       [202212373,        85,        95,        35,        25],\n       [202212474,        60,        25,        10,        50],\n       [202212455,        95,        45,        90,        35],\n       [202212317,        85,        50,        60,        45],\n       [202212341,        60,        50,       100,        70],\n       [202212386,       100,        75,        60,         0],\n       [202212328,       100,        90,        85,        75],\n       [202212417,        55,       100,       100,        60],\n       [202212370,        70,        60,        30,        40],\n       [202212486,        70,        90,        95,        40],\n       [202212333,        55,        50,         0,         5],\n       [202212360,       100,       100,        45,        90],\n       [202212350,        85,        70,        90,        80],\n       [202212382,       100,        85,        65,        85],\n       [202212392,        60,        65,        35,        15],\n       [202212449,        65,        75,        75,        85],\n       [202212394,        65,        25,        40,         0],\n       [202212444,        75,        75,        50,        40],\n       [202212487,        50,        55,        80,        55],\n       [202212425,        75,        30,        20,        50],\n       [202212312,       100,        50,        25,        65],\n       [202212448,        90,        30,        95,        35],\n       [202212434,        55,       100,        80,         0],\n       [202212451,        75,        60,        15,        40],\n       [202212433,        60,        25,        25,        50],\n       [202212424,        85,        35,        10,        60],\n       [202212351,        60,       100,        55,        40],\n       [202212324,        70,        55,        50,        75],\n       [202212314,        80,        65,        95,        85],\n       [202212446,        65,        35,        15,        65],\n       [202212401,        85,        70,       100,         0],\n       [202212307,       100,        30,        60,        65],\n       [202212300,        65,        70,        55,        70],\n       [202212342,        85,        55,        85,        90],\n       [202212479,        85,        95,        80,        10],\n       [202212443,        85,        70,        75,         5],\n       [202212387,       100,        35,        70,         0],\n       [202212372,        95,        45,        55,        65],\n       [202212376,        95,        85,        40,        65],\n       [202212466,        55,        50,        30,        85],\n       [202212391,        85,        50,         5,        65],\n       [202212368,        75,        90,        85,        85],\n       [202212427,        95,        70,        10,         5],\n       [202212414,        85,        35,        80,        95],\n       [202212426,        95,        50,        80,        90],\n       [202212316,       100,        65,        75,        40],\n       [202212355,        95,        70,        70,         0],\n       [202212477,        95,        70,        20,        25],\n       [202212484,       100,        60,        10,         5],\n       [202212456,        55,        35,        25,        10],\n       [202212500,        60,        90,        40,         5],\n       [202212381,        85,        90,        85,        75],\n       [202212335,        75,        85,        25,        35],\n       [202212475,        55,        30,        50,        45],\n       [202212343,        70,        60,        75,        75],\n       [202212412,        80,        30,        95,         5],\n       [202212428,        90,        85,        80,        15],\n       [202212330,        90,        25,        95,         5],\n       [202212375,        60,        85,        50,        20],\n       [202212413,        90,        50,        95,        95],\n       [202212303,        75,        95,        65,        40],\n       [202212374,        60,        40,        35,         0],\n       [202212409,        55,       100,        15,        80],\n       [202212440,        70,        75,        80,         0],\n       [202212393,        75,        65,        25,        20],\n       [202212492,        90,        75,        80,        25],\n       [202212357,        50,        75,        75,        20],\n       [202212465,        55,        45,        35,        45],\n       [202212415,        90,        70,        90,         0],\n       [202212405,        75,        30,       100,        60],\n       [202212435,        90,        85,         0,        40],\n       [202212380,        85,        70,        35,         0],\n       [202212369,       100,        75,       100,        85],\n       [202212467,        55,        35,        20,        10],\n       [202212429,        70,        75,        90,        90],\n       [202212495,        90,        90,        55,        55],\n       [202212420,        55,        60,        40,         0],\n       [202212302,       100,        90,         5,        30],\n       [202212481,        50,        55,        25,        80],\n       [202212422,       100,       100,        90,        55],\n       [202212388,        70,        45,        70,        75],\n       [202212480,        85,        95,        85,        90],\n       [202212378,        55,        25,        95,        45],\n       [202212457,        75,        30,        10,        95],\n       [202212419,        65,        85,        15,        60],\n       [202212432,        70,        90,        70,         0],\n       [202212395,        60,        85,        70,        85],\n       [202212464,       100,        25,        10,        20],\n       [202212476,        75,        25,        80,        25],\n       [202212332,        90,        95,        40,        80],\n       [202212301,        95,        90,        50,        50],\n       [202212497,        90,        90,        65,        85],\n       [202212309,        95,        75,        50,        40],\n       [202212493,        55,        60,        70,         5],\n       [202212311,        95,        85,         0,        15],\n       [202212416,        65,        60,        35,        20],\n       [202212489,        65,        50,         5,         5],\n       [202212359,        90,        25,        60,        25],\n       [202212349,       100,        40,        40,        15],\n       [202212403,        70,        25,       100,        75],\n       [202212418,       100,        30,        70,        70],\n       [202212406,        50,        55,        55,         5],\n       [202212485,        70,        35,        70,       100],\n       [202212390,        70,        60,        60,        80],\n       [202212365,        55,        45,        90,         5],\n       [202212338,        55,        55,        10,        95],\n       [202212363,        65,        80,        10,        30],\n       [202212321,        90,        25,        35,        55],\n       [202212499,       100,        30,        30,        85],\n       [202212340,        70,        85,        70,        65],\n       [202212421,        60,       100,        45,       100],\n       [202212407,        70,        25,       100,        15],\n       [202212439,        70,        35,        80,        25],\n       [202212488,        65,        60,        30,        35],\n       [202212453,        95,        35,        40,        95],\n       [202212482,        50,        80,        65,        90],\n       [202212334,       100,        40,        80,        80],\n       [202212322,        55,        30,        95,       100],\n       [202212353,        65,        40,        65,        70],\n       [202212491,        55,        70,        40,        95],\n       [202212352,        65,        85,        25,        85],\n       [202212315,        85,        85,       100,        10],\n       [202212470,        80,        65,        35,        60],\n       [202212436,        50,        95,        45,        85]])\n\n\n(풀이2)\n\ntest_ndarray[test_ndarray[:,0] == 202212460]\n\narray([[202212460,        55,        35,        35,         5]])\n\n\n\ntest_ndarray[test_ndarray[:,0] == 202212460,1]   # 이게뭐여? 가독성이 떨어짐\n\narray([55])\n\n\n(풀이2)가 (풀이1)에 비하여 불편한 점 - test_ndarray의 첫칼럼은 student id이고 두번째 칼럼은 att라는 사실을 암기하고 있어야 한다. - student id가 아니고 만약에 학생이름(문자형)을 써서 데이터를 정리한다면 모든 자료형은 문자형이 되어야 한다. - 작성한 코드 가독성이 없다. (위치로 접근하므로)\n- 요약: hash 스타일로 정보를 추출하는 것이 유용할 때가 있다. 그리고 보통 hash 스타일로 정보를 뽑는것이 유리함.\nnumpy는 정보추출을 위해 개발된 자료형이 아니라 행렬 및 벡터의 수학연산을 지원하기 위해 개발된 자료형이다.)\n- 소망: 정보를 추출할때는 hash 스타일도 유용하다는 것은 ㅇㅋ \\(\\to\\) 하지만 난 가끔 넘파이스타일로 정보를 뽑고 싶엉. 그리고 딕셔너리 형태가 아니고 엑셀처럼(행렬처럼) 데이터를 보고 ㅅ피다!!! \\(\\to\\) pandas 개발\n\n\n\npandas 개발동기\n\n엑셀처럼 데이터를 테이블 형태로 정리하고 싶다.\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,20)\nrep = np.random.choice(np.arange(5,21)*5,20)\nmid = np.random.choice(np.arange(0,21)*5,20)\nfin = np.random.choice(np.arange(0,21)*5,20)\nkey = ['202212'+str(s) for s in np.random.choice(np.arange(300,501),20,replace=False)]\ntest_dic = {key[i] : {'att':att[i], 'rep':rep[i], 'mid':mid[i], 'fin':fin[i]} for i in range(20)}\ntest_ndarray = np.array([key,att,rep,mid,fin],dtype=np.int64).T\n\n\ntest_dic\n\n{'202212380': {'att': 65, 'rep': 55, 'mid': 50, 'fin': 40},\n '202212370': {'att': 95, 'rep': 100, 'mid': 50, 'fin': 80},\n '202212363': {'att': 65, 'rep': 90, 'mid': 60, 'fin': 30},\n '202212488': {'att': 55, 'rep': 80, 'mid': 75, 'fin': 80},\n '202212312': {'att': 80, 'rep': 30, 'mid': 30, 'fin': 100},\n '202212377': {'att': 75, 'rep': 40, 'mid': 100, 'fin': 15},\n '202212463': {'att': 65, 'rep': 45, 'mid': 45, 'fin': 90},\n '202212471': {'att': 60, 'rep': 60, 'mid': 25, 'fin': 0},\n '202212400': {'att': 95, 'rep': 65, 'mid': 20, 'fin': 10},\n '202212469': {'att': 90, 'rep': 80, 'mid': 80, 'fin': 20},\n '202212318': {'att': 55, 'rep': 75, 'mid': 35, 'fin': 25},\n '202212432': {'att': 95, 'rep': 95, 'mid': 45, 'fin': 0},\n '202212443': {'att': 95, 'rep': 55, 'mid': 15, 'fin': 35},\n '202212367': {'att': 50, 'rep': 80, 'mid': 40, 'fin': 30},\n '202212458': {'att': 50, 'rep': 55, 'mid': 15, 'fin': 85},\n '202212396': {'att': 95, 'rep': 30, 'mid': 30, 'fin': 95},\n '202212482': {'att': 50, 'rep': 50, 'mid': 45, 'fin': 10},\n '202212452': {'att': 65, 'rep': 55, 'mid': 15, 'fin': 45},\n '202212387': {'att': 70, 'rep': 70, 'mid': 40, 'fin': 35},\n '202212354': {'att': 90, 'rep': 90, 'mid': 80, 'fin': 90}}\n\n\n\n테이블 형태로 보고 싶다.\n\n(방법1) - 행렬이기는 하지만 방법 2,3,4에 비하여 우리가 원하는 만큼 가독성을 주는 형태는 아님\n\ntest_ndarray = np.array([key,att,rep,mid,fin],dtype=np.int64).T\ntest_ndarray\n\narray([[202212380,        65,        55,        50,        40],\n       [202212370,        95,       100,        50,        80],\n       [202212363,        65,        90,        60,        30],\n       [202212488,        55,        80,        75,        80],\n       [202212312,        80,        30,        30,       100],\n       [202212377,        75,        40,       100,        15],\n       [202212463,        65,        45,        45,        90],\n       [202212471,        60,        60,        25,         0],\n       [202212400,        95,        65,        20,        10],\n       [202212469,        90,        80,        80,        20],\n       [202212318,        55,        75,        35,        25],\n       [202212432,        95,        95,        45,         0],\n       [202212443,        95,        55,        15,        35],\n       [202212367,        50,        80,        40,        30],\n       [202212458,        50,        55,        15,        85],\n       [202212396,        95,        30,        30,        95],\n       [202212482,        50,        50,        45,        10],\n       [202212452,        65,        55,        15,        45],\n       [202212387,        70,        70,        40,        35],\n       [202212354,        90,        90,        80,        90]])\n\n\n(방법2)\n\npd.DataFrame(test_dic).T\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n    \n      202212370\n      95\n      100\n      50\n      80\n    \n    \n      202212363\n      65\n      90\n      60\n      30\n    \n    \n      202212488\n      55\n      80\n      75\n      80\n    \n    \n      202212312\n      80\n      30\n      30\n      100\n    \n    \n      202212377\n      75\n      40\n      100\n      15\n    \n    \n      202212463\n      65\n      45\n      45\n      90\n    \n    \n      202212471\n      60\n      60\n      25\n      0\n    \n    \n      202212400\n      95\n      65\n      20\n      10\n    \n    \n      202212469\n      90\n      80\n      80\n      20\n    \n    \n      202212318\n      55\n      75\n      35\n      25\n    \n    \n      202212432\n      95\n      95\n      45\n      0\n    \n    \n      202212443\n      95\n      55\n      15\n      35\n    \n    \n      202212367\n      50\n      80\n      40\n      30\n    \n    \n      202212458\n      50\n      55\n      15\n      85\n    \n    \n      202212396\n      95\n      30\n      30\n      95\n    \n    \n      202212482\n      50\n      50\n      45\n      10\n    \n    \n      202212452\n      65\n      55\n      15\n      45\n    \n    \n      202212387\n      70\n      70\n      40\n      35\n    \n    \n      202212354\n      90\n      90\n      80\n      90\n    \n  \n\n\n\n\n(방법3)\n\ntest_dic2 = {'att':{key[i]:att[i] for i in range(20)},\n             'rep':{key[i]:rep[i] for i in range(20)},\n             'mid':{key[i]:mid[i] for i in range(20)},\n             'fin':{key[i]:fin[i] for i in range(20)}}\n\n\ntest_dic2\n\n{'att': {'202212380': 65,\n  '202212370': 95,\n  '202212363': 65,\n  '202212488': 55,\n  '202212312': 80,\n  '202212377': 75,\n  '202212463': 65,\n  '202212471': 60,\n  '202212400': 95,\n  '202212469': 90,\n  '202212318': 55,\n  '202212432': 95,\n  '202212443': 95,\n  '202212367': 50,\n  '202212458': 50,\n  '202212396': 95,\n  '202212482': 50,\n  '202212452': 65,\n  '202212387': 70,\n  '202212354': 90},\n 'rep': {'202212380': 55,\n  '202212370': 100,\n  '202212363': 90,\n  '202212488': 80,\n  '202212312': 30,\n  '202212377': 40,\n  '202212463': 45,\n  '202212471': 60,\n  '202212400': 65,\n  '202212469': 80,\n  '202212318': 75,\n  '202212432': 95,\n  '202212443': 55,\n  '202212367': 80,\n  '202212458': 55,\n  '202212396': 30,\n  '202212482': 50,\n  '202212452': 55,\n  '202212387': 70,\n  '202212354': 90},\n 'mid': {'202212380': 50,\n  '202212370': 50,\n  '202212363': 60,\n  '202212488': 75,\n  '202212312': 30,\n  '202212377': 100,\n  '202212463': 45,\n  '202212471': 25,\n  '202212400': 20,\n  '202212469': 80,\n  '202212318': 35,\n  '202212432': 45,\n  '202212443': 15,\n  '202212367': 40,\n  '202212458': 15,\n  '202212396': 30,\n  '202212482': 45,\n  '202212452': 15,\n  '202212387': 40,\n  '202212354': 80},\n 'fin': {'202212380': 40,\n  '202212370': 80,\n  '202212363': 30,\n  '202212488': 80,\n  '202212312': 100,\n  '202212377': 15,\n  '202212463': 90,\n  '202212471': 0,\n  '202212400': 10,\n  '202212469': 20,\n  '202212318': 25,\n  '202212432': 0,\n  '202212443': 35,\n  '202212367': 30,\n  '202212458': 85,\n  '202212396': 95,\n  '202212482': 10,\n  '202212452': 45,\n  '202212387': 35,\n  '202212354': 90}}\n\n\n\npd.DataFrame(test_dic2)\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n    \n      202212370\n      95\n      100\n      50\n      80\n    \n    \n      202212363\n      65\n      90\n      60\n      30\n    \n    \n      202212488\n      55\n      80\n      75\n      80\n    \n    \n      202212312\n      80\n      30\n      30\n      100\n    \n    \n      202212377\n      75\n      40\n      100\n      15\n    \n    \n      202212463\n      65\n      45\n      45\n      90\n    \n    \n      202212471\n      60\n      60\n      25\n      0\n    \n    \n      202212400\n      95\n      65\n      20\n      10\n    \n    \n      202212469\n      90\n      80\n      80\n      20\n    \n    \n      202212318\n      55\n      75\n      35\n      25\n    \n    \n      202212432\n      95\n      95\n      45\n      0\n    \n    \n      202212443\n      95\n      55\n      15\n      35\n    \n    \n      202212367\n      50\n      80\n      40\n      30\n    \n    \n      202212458\n      50\n      55\n      15\n      85\n    \n    \n      202212396\n      95\n      30\n      30\n      95\n    \n    \n      202212482\n      50\n      50\n      45\n      10\n    \n    \n      202212452\n      65\n      55\n      15\n      45\n    \n    \n      202212387\n      70\n      70\n      40\n      35\n    \n    \n      202212354\n      90\n      90\n      80\n      90\n    \n  \n\n\n\n\n(방법4)\n\ndf = pd.DataFrame({'att':att, 'rep':rep, 'mid':mid, 'fin':fin}, index=key)\ndf\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n    \n      202212370\n      95\n      100\n      50\n      80\n    \n    \n      202212363\n      65\n      90\n      60\n      30\n    \n    \n      202212488\n      55\n      80\n      75\n      80\n    \n    \n      202212312\n      80\n      30\n      30\n      100\n    \n    \n      202212377\n      75\n      40\n      100\n      15\n    \n    \n      202212463\n      65\n      45\n      45\n      90\n    \n    \n      202212471\n      60\n      60\n      25\n      0\n    \n    \n      202212400\n      95\n      65\n      20\n      10\n    \n    \n      202212469\n      90\n      80\n      80\n      20\n    \n    \n      202212318\n      55\n      75\n      35\n      25\n    \n    \n      202212432\n      95\n      95\n      45\n      0\n    \n    \n      202212443\n      95\n      55\n      15\n      35\n    \n    \n      202212367\n      50\n      80\n      40\n      30\n    \n    \n      202212458\n      50\n      55\n      15\n      85\n    \n    \n      202212396\n      95\n      30\n      30\n      95\n    \n    \n      202212482\n      50\n      50\n      45\n      10\n    \n    \n      202212452\n      65\n      55\n      15\n      45\n    \n    \n      202212387\n      70\n      70\n      40\n      35\n    \n    \n      202212354\n      90\n      90\n      80\n      90\n    \n  \n\n\n\n\n(방법5)\n\ndf = pd.DataFrame({'att':att, 'rep':rep, 'mid':mid, 'fin':fin})\ndf\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      0\n      65\n      55\n      50\n      40\n    \n    \n      1\n      95\n      100\n      50\n      80\n    \n    \n      2\n      65\n      90\n      60\n      30\n    \n    \n      3\n      55\n      80\n      75\n      80\n    \n    \n      4\n      80\n      30\n      30\n      100\n    \n    \n      5\n      75\n      40\n      100\n      15\n    \n    \n      6\n      65\n      45\n      45\n      90\n    \n    \n      7\n      60\n      60\n      25\n      0\n    \n    \n      8\n      95\n      65\n      20\n      10\n    \n    \n      9\n      90\n      80\n      80\n      20\n    \n    \n      10\n      55\n      75\n      35\n      25\n    \n    \n      11\n      95\n      95\n      45\n      0\n    \n    \n      12\n      95\n      55\n      15\n      35\n    \n    \n      13\n      50\n      80\n      40\n      30\n    \n    \n      14\n      50\n      55\n      15\n      85\n    \n    \n      15\n      95\n      30\n      30\n      95\n    \n    \n      16\n      50\n      50\n      45\n      10\n    \n    \n      17\n      65\n      55\n      15\n      45\n    \n    \n      18\n      70\n      70\n      40\n      35\n    \n    \n      19\n      90\n      90\n      80\n      90\n    \n  \n\n\n\n\n\ndf=df.set_index([key])   #인덱스를 set_index로 설정해줄 수 있음\ndf\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n    \n      202212370\n      95\n      100\n      50\n      80\n    \n    \n      202212363\n      65\n      90\n      60\n      30\n    \n    \n      202212488\n      55\n      80\n      75\n      80\n    \n    \n      202212312\n      80\n      30\n      30\n      100\n    \n    \n      202212377\n      75\n      40\n      100\n      15\n    \n    \n      202212463\n      65\n      45\n      45\n      90\n    \n    \n      202212471\n      60\n      60\n      25\n      0\n    \n    \n      202212400\n      95\n      65\n      20\n      10\n    \n    \n      202212469\n      90\n      80\n      80\n      20\n    \n    \n      202212318\n      55\n      75\n      35\n      25\n    \n    \n      202212432\n      95\n      95\n      45\n      0\n    \n    \n      202212443\n      95\n      55\n      15\n      35\n    \n    \n      202212367\n      50\n      80\n      40\n      30\n    \n    \n      202212458\n      50\n      55\n      15\n      85\n    \n    \n      202212396\n      95\n      30\n      30\n      95\n    \n    \n      202212482\n      50\n      50\n      45\n      10\n    \n    \n      202212452\n      65\n      55\n      15\n      45\n    \n    \n      202212387\n      70\n      70\n      40\n      35\n    \n    \n      202212354\n      90\n      90\n      80\n      90\n    \n  \n\n\n\n\n\n\n해싱으로 원하는 정보를 뽑으면 좋겠다. (마치 딕셔너리처럼)\n- 예제1: 출설점수를 출력\n\ntest_dic2['att']\n\n{'202212380': 65,\n '202212370': 95,\n '202212363': 65,\n '202212488': 55,\n '202212312': 80,\n '202212377': 75,\n '202212463': 65,\n '202212471': 60,\n '202212400': 95,\n '202212469': 90,\n '202212318': 55,\n '202212432': 95,\n '202212443': 95,\n '202212367': 50,\n '202212458': 50,\n '202212396': 95,\n '202212482': 50,\n '202212452': 65,\n '202212387': 70,\n '202212354': 90}\n\n\n\ndf['att']\n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n- 예제2 : 학번 202212380의 출석점수 출력\n\ntest_dic2['att']['202212380']\n\n65\n\n\n\ndf['att']['202212380']\n\n65\n\n\n\n\n인덱싱으로 정보를 뽑는 기능도 지원을 하면 좋겠따. (마치 리스트나 넘파이처럼)\n- 예제1: 첫번째 학생의 기말고사 성적을 출력하고 싶다.\n\ntest_ndarray[0,-1]\n\n40\n\n\n\ndf.iloc[0,-1]  \n\n40\n\n\n\n벼락치기: df에서 iloc라는 특수기능을 이용하면 넘파이 인덱싱처럼 원소출력이 가능하다.\n\n-예제2: 홀수번째 학생의 점수를 뽑고 싶다.\n\ntest_ndarray[::2]\n\narray([[202212380,        65,        55,        50,        40],\n       [202212363,        65,        90,        60,        30],\n       [202212312,        80,        30,        30,       100],\n       [202212463,        65,        45,        45,        90],\n       [202212400,        95,        65,        20,        10],\n       [202212318,        55,        75,        35,        25],\n       [202212443,        95,        55,        15,        35],\n       [202212458,        50,        55,        15,        85],\n       [202212482,        50,        50,        45,        10],\n       [202212387,        70,        70,        40,        35]])\n\n\n\ndf.iloc[0::2]\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n    \n      202212363\n      65\n      90\n      60\n      30\n    \n    \n      202212312\n      80\n      30\n      30\n      100\n    \n    \n      202212463\n      65\n      45\n      45\n      90\n    \n    \n      202212400\n      95\n      65\n      20\n      10\n    \n    \n      202212318\n      55\n      75\n      35\n      25\n    \n    \n      202212443\n      95\n      55\n      15\n      35\n    \n    \n      202212458\n      50\n      55\n      15\n      85\n    \n    \n      202212482\n      50\n      50\n      45\n      10\n    \n    \n      202212387\n      70\n      70\n      40\n      35\n    \n  \n\n\n\n\n- 예제3: 맨 끝에서 3명의 점수를 출력하고 싶다.\n\ntest_ndarray[-3:]\n\narray([[202212452,        65,        55,        15,        45],\n       [202212387,        70,        70,        40,        35],\n       [202212354,        90,        90,        80,        90]])\n\n\n\ndf.iloc[-3:]\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212452\n      65\n      55\n      15\n      45\n    \n    \n      202212387\n      70\n      70\n      40\n      35\n    \n    \n      202212354\n      90\n      90\n      80\n      90\n    \n  \n\n\n\n\n- 예제4: 맨 끝에서 3명의 점수를 마지막 2개의 칼럼만 출력하고 싶다.\n\ntest_ndarray[-3:,-2:]\n\narray([[15, 45],\n       [40, 35],\n       [80, 90]])\n\n\n\ndf.iloc[-3:,-2:]\n\n\n\n\n\n  \n    \n      \n      mid\n      fin\n    \n  \n  \n    \n      202212452\n      15\n      45\n    \n    \n      202212387\n      40\n      35\n    \n    \n      202212354\n      80\n      90\n    \n  \n\n\n\n\n\n\n궁극: 해싱과 인덱싱을 모두 지원하는 아주 우수한 자료형을 만들고 싶어!\n- 예제1: 중간고사 점수가 20점 이상이면서 동시에 출석점수가 60점미만인 학생들의 기말고사 점수를 출력\n(방법1) 데이터베이스 스타일\n\ndf.query(\"mid>=20 and att<60\")\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212488\n      55\n      80\n      75\n      80\n    \n    \n      202212318\n      55\n      75\n      35\n      25\n    \n    \n      202212367\n      50\n      80\n      40\n      30\n    \n    \n      202212482\n      50\n      50\n      45\n      10\n    \n  \n\n\n\n\n\ndf.query(\"mid>=20 and att<60\")['fin']\n\n202212488    80\n202212318    25\n202212367    30\n202212482    10\nName: fin, dtype: int64\n\n\n(방법2) 넘파이 스타일\n\ntest_ndarray\n\narray([[202212380,        65,        55,        50,        40],\n       [202212370,        95,       100,        50,        80],\n       [202212363,        65,        90,        60,        30],\n       [202212488,        55,        80,        75,        80],\n       [202212312,        80,        30,        30,       100],\n       [202212377,        75,        40,       100,        15],\n       [202212463,        65,        45,        45,        90],\n       [202212471,        60,        60,        25,         0],\n       [202212400,        95,        65,        20,        10],\n       [202212469,        90,        80,        80,        20],\n       [202212318,        55,        75,        35,        25],\n       [202212432,        95,        95,        45,         0],\n       [202212443,        95,        55,        15,        35],\n       [202212367,        50,        80,        40,        30],\n       [202212458,        50,        55,        15,        85],\n       [202212396,        95,        30,        30,        95],\n       [202212482,        50,        50,        45,        10],\n       [202212452,        65,        55,        15,        45],\n       [202212387,        70,        70,        40,        35],\n       [202212354,        90,        90,        80,        90]])\n\n\n\ntest_ndarray[:,3]>=20 # 중간고사 점수 20점 이상\n\narray([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True, False,  True, False,  True,  True, False,\n        True,  True])\n\n\n\ntest_ndarray[:,1] < 60 # 출석이 60미만 \n\narray([False, False, False,  True, False, False, False, False, False,\n       False,  True, False, False,  True,  True, False,  True, False,\n       False, False])\n\n\n\n(test_ndarray[:,3]>=20) & (test_ndarray[:,1] < 60)\n\narray([False, False, False,  True, False, False, False, False, False,\n       False,  True, False, False,  True, False, False,  True, False,\n       False, False])\n\n\n\nnote: test_ndarray[:,3]>=20 & test_ndarray[:,1] < 60 와 같이 하면 에러가 난다. 가로로 묶어줘야 함\n\n\ntest_ndarray[(test_ndarray[:,3]>=20) & (test_ndarray[:,1] < 60),-1] \n\narray([80, 25, 30, 10])\n\n\n\n구현난이도 어려움, 가독성 꽝\n\n- 예제2: ’중간고사점수<기말고사점수’인 학생들의 출석점수 평균을 구하자.\n\ndf.query('mid<fin')['att'].mean()\n\n76.66666666666667\n\n\n\n\n\npandas 사용법\n\npandas 공부 1단계\n\n데이터프레임 선언\n- 방법1: dictionary에서 만든다.\n\npd.DataFrame({'att':[30,40,50],'mid':[50,60,70]})  # 리스트\n\n\n\n\n\n  \n    \n      \n      att\n      mid\n    \n  \n  \n    \n      0\n      30\n      50\n    \n    \n      1\n      40\n      60\n    \n    \n      2\n      50\n      70\n    \n  \n\n\n\n\n\npd.DataFrame({'att':(30,40,50),'mid':(50,60,70)})  # 튜플\n\n\n\n\n\n  \n    \n      \n      att\n      mid\n    \n  \n  \n    \n      0\n      30\n      50\n    \n    \n      1\n      40\n      60\n    \n    \n      2\n      50\n      70\n    \n  \n\n\n\n\n\npd.DataFrame({'att':np.array([30,40,50]),'mid':np.array([50,60,70])})  # 넘파이어레이\n\n\n\n\n\n  \n    \n      \n      att\n      mid\n    \n  \n  \n    \n      0\n      30\n      50\n    \n    \n      1\n      40\n      60\n    \n    \n      2\n      50\n      70\n    \n  \n\n\n\n\n- 방법2: 2차원 ndarray에서 형태변환 통해 만든다.\n\nnp.arange(2*3).reshape(2,3)\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n\npd.DataFrame(np.arange(2*3).reshape(2,3))\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      0\n      1\n      2\n    \n    \n      1\n      3\n      4\n      5\n    \n  \n\n\n\n\n\n\n열의 이름 부여\n- 방법1: 딕셔너리를 통하여 만들면 딕셔너리의 key가 자동으로 열의 이름이 된다.\n\npd.DataFrame({'att':np.array([30,40,50]),'mid':np.array([50,60,70])})\n\n\n\n\n\n  \n    \n      \n      att\n      mid\n    \n  \n  \n    \n      0\n      30\n      50\n    \n    \n      1\n      40\n      60\n    \n    \n      2\n      50\n      70\n    \n  \n\n\n\n\n- 방법2: pd.DataFrame()의 옵션에 columns를 이용\n\npd.DataFrame(np.arange(2*3).reshape(2,3),columns=['X1','X2','X3'])\n\n\n\n\n\n  \n    \n      \n      X1\n      X2\n      X3\n    \n  \n  \n    \n      0\n      0\n      1\n      2\n    \n    \n      1\n      3\n      4\n      5\n    \n  \n\n\n\n\n- 방법3: df.columns에 원하는 열이름을 덮어씀(1)\n\ndf=pd.DataFrame(np.arange(2*3).reshape(2,3))\ndf\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      0\n      1\n      2\n    \n    \n      1\n      3\n      4\n      5\n    \n  \n\n\n\n\n\ndf.columns\n\nRangeIndex(start=0, stop=3, step=1)\n\n\n\ndf.columns=['X1','X2','X3']\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      X1\n      X2\n      X3\n    \n  \n  \n    \n      0\n      0\n      1\n      2\n    \n    \n      1\n      3\n      4\n      5\n    \n  \n\n\n\n\n- 방법4: df.columns에 원하는 열이름을 덮어씀(2)\n\ndf.columns=pd.Index(['X1','X2','X3'])  # 위와 같은 코드\ndf\n\n\n\n\n\n  \n    \n      \n      X1\n      X2\n      X3\n    \n  \n  \n    \n      0\n      0\n      1\n      2\n    \n    \n      1\n      3\n      4\n      5\n    \n  \n\n\n\n\n방법4가 방법3의 방식보다 컴퓨터가 이해하기 좋다. (=불필요한 에러를 방지할 수 있다.)\n\ndf.columns\n\nIndex(['X1', 'X2', 'X3'], dtype='object')\n\n\n\ntype(df.columns)\n\npandas.core.indexes.base.Index\n\n\n\n['X1','X2','X3'], type(['X1','X2','X3'])\n\n(['X1', 'X2', 'X3'], list)\n\n\n\npd.Index(['X1','X2','X3'])\n\nIndex(['X1', 'X2', 'X3'], dtype='object')\n\n\n\n\n행의 이름 부여\n- 방법1: 중첩 dict이면 nested dic의 key가 알아서 행의 이름으로 된다. (안쪽..)\n\n{'att': {'boram':30, 'iu':40, 'hynn':50}, 'mid':{'boram':5, 'iu':45, 'hynn':90}}\n\n{'att': {'boram': 30, 'iu': 40, 'hynn': 50},\n 'mid': {'boram': 5, 'iu': 45, 'hynn': 90}}\n\n\n\npd.DataFrame({'att': {'boram':30, 'iu':40, 'hynn':50}, 'mid':{'boram':5, 'iu':45, 'hynn':90}})\n\n\n\n\n\n  \n    \n      \n      att\n      mid\n    \n  \n  \n    \n      boram\n      30\n      5\n    \n    \n      iu\n      40\n      45\n    \n    \n      hynn\n      50\n      90\n    \n  \n\n\n\n\n- 방법2: index옵션 이용\n\npd.DataFrame({'att': [30, 40,50], 'mid':[5,45, 90]}, index=['boram','iu','hynn'])\n\n\n\n\n\n  \n    \n      \n      att\n      mid\n    \n  \n  \n    \n      boram\n      30\n      5\n    \n    \n      iu\n      40\n      45\n    \n    \n      hynn\n      50\n      90\n    \n  \n\n\n\n\n- 방법3: df.index에 덮어씌움\n\ndf=pd.DataFrame({'att': [30, 40,50], 'mid':[5,45, 90]})\ndf\n\n\n\n\n\n  \n    \n      \n      att\n      mid\n    \n  \n  \n    \n      0\n      30\n      5\n    \n    \n      1\n      40\n      45\n    \n    \n      2\n      50\n      90\n    \n  \n\n\n\n\n\ndf.index\n\nRangeIndex(start=0, stop=3, step=1)\n\n\n\ndf.index=['boram','iu','hynn']\ndf\n\n\n\n\n\n  \n    \n      \n      att\n      mid\n    \n  \n  \n    \n      boram\n      30\n      5\n    \n    \n      iu\n      40\n      45\n    \n    \n      hynn\n      50\n      90\n    \n  \n\n\n\n\n\ndf.index= pd.Index(['boram','iu','hynn'])  #이게 컴퓨터가 볼 때 더 안전한 코드!\ndf\n\n\n\n\n\n  \n    \n      \n      att\n      mid\n    \n  \n  \n    \n      boram\n      30\n      5\n    \n    \n      iu\n      40\n      45\n    \n    \n      hynn\n      50\n      90\n    \n  \n\n\n\n\n- 방법4: df.set_index() 를 이용하여 덮어 씌운다.\n\ndf=pd.DataFrame({'att': [30, 40,50], 'mid':[5,45, 90]})\ndf\n\n\n\n\n\n  \n    \n      \n      att\n      mid\n    \n  \n  \n    \n      0\n      30\n      5\n    \n    \n      1\n      40\n      45\n    \n    \n      2\n      50\n      90\n    \n  \n\n\n\n\n\ndf.set_index(pd.Index(['boram','iu','hynn']))\n\n\n\n\n\n  \n    \n      \n      att\n      mid\n    \n  \n  \n    \n      boram\n      30\n      5\n    \n    \n      iu\n      40\n      45\n    \n    \n      hynn\n      50\n      90\n    \n  \n\n\n\n\n(주의) 아래는 에러가 난다.\n\ndf.set_index(['boram','iu','hynn'])\n\nKeyError: \"None of ['boram', 'iu', 'hynn'] are in the columns\"\n\n\n\ndf.set_index([['boram','iu','hynn']]) # 꺽쇠를 한번 더 넣어주면 에러를 피할 수 있따.\n\n\n\n\n\n  \n    \n      \n      att\n      mid\n    \n  \n  \n    \n      boram\n      30\n      5\n    \n    \n      iu\n      40\n      45\n    \n    \n      hynn\n      50\n      90\n    \n  \n\n\n\n\n\n\n자료형, len, shape, for문의 반복변수\n\ndf= pd.DataFrame({'att':[30,40,50],'mid':[5,45,90]})\ndf\n\n\n\n\n\n  \n    \n      \n      att\n      mid\n    \n  \n  \n    \n      0\n      30\n      5\n    \n    \n      1\n      40\n      45\n    \n    \n      2\n      50\n      90\n    \n  \n\n\n\n\n- type\n\ntype(df)\n\npandas.core.frame.DataFrame\n\n\n- len\n\nlen(df) #row의 갯수\n\n3\n\n\n- shape\n\ndf.shape\n\n(3, 2)\n\n\n- for문의 반복변수\n\nfor k in df:\n    print(k) #딕셔너리와 같다.\n\natt\nmid\n\n\n\nfor k in {'att':[30,40,50],'mid':[5,45,90]}: #딕셔너리\n    print(k)\n\natt\nmid\n\n\n\n\npd.Series\n- 2차원 ndarray가 pd.DataFrame에 대응한다면 1차원 ndarray는 pd.Series에 대응한다.\n\na=pd.Series(np.random.randn(10))\na\n\n0    0.106173\n1    0.723759\n2    0.217990\n3    0.194022\n4   -0.688990\n5   -0.351670\n6    0.990933\n7    1.212147\n8   -0.608965\n9    0.032549\ndtype: float64\n\n\n\ntype(a)\n\npandas.core.series.Series\n\n\n\nlen(a)\n\n10\n\n\n\na.shape\n\n(10,)\n\n\n\nfor value in a:\n    print(value)   #값들이 반복 넘파이어레이처럼..\n\n0.10617283591748639\n0.7237590624253404\n0.21798967912700873\n0.1940223087322443\n-0.6889899757985083\n-0.3516696436204985\n0.9909329773184973\n1.2121468150185186\n-0.6089654373693767\n0.03254898346416765\n\n\n\n\n\npandas공부 2단계\n- 데이터\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,20)\nrep = np.random.choice(np.arange(5,21)*5,20)\nmid = np.random.choice(np.arange(0,21)*5,20)\nfin = np.random.choice(np.arange(0,21)*5,20)\nkey = ['202212'+str(s) for s in np.random.choice(np.arange(300,501),20,replace=False)]\n\n\ndf=pd.DataFrame({'att':att, 'rep':rep, 'mid':mid, 'fin':fin}, index=key)\ndf\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n    \n      202212370\n      95\n      100\n      50\n      80\n    \n    \n      202212363\n      65\n      90\n      60\n      30\n    \n    \n      202212488\n      55\n      80\n      75\n      80\n    \n    \n      202212312\n      80\n      30\n      30\n      100\n    \n    \n      202212377\n      75\n      40\n      100\n      15\n    \n    \n      202212463\n      65\n      45\n      45\n      90\n    \n    \n      202212471\n      60\n      60\n      25\n      0\n    \n    \n      202212400\n      95\n      65\n      20\n      10\n    \n    \n      202212469\n      90\n      80\n      80\n      20\n    \n    \n      202212318\n      55\n      75\n      35\n      25\n    \n    \n      202212432\n      95\n      95\n      45\n      0\n    \n    \n      202212443\n      95\n      55\n      15\n      35\n    \n    \n      202212367\n      50\n      80\n      40\n      30\n    \n    \n      202212458\n      50\n      55\n      15\n      85\n    \n    \n      202212396\n      95\n      30\n      30\n      95\n    \n    \n      202212482\n      50\n      50\n      45\n      10\n    \n    \n      202212452\n      65\n      55\n      15\n      45\n    \n    \n      202212387\n      70\n      70\n      40\n      35\n    \n    \n      202212354\n      90\n      90\n      80\n      90\n    \n  \n\n\n\n\n\n첫번째 칼럼을 선택\n- 방법1\n\ndf.att \n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n- 방법2 : dict스타일\n\ndf['att']\n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n\ntype(df['att'])\n\npandas.core.series.Series\n\n\n- 방법3 : dict스타일\n\ndf[['att']]\n\n\n\n\n\n  \n    \n      \n      att\n    \n  \n  \n    \n      202212380\n      65\n    \n    \n      202212370\n      95\n    \n    \n      202212363\n      65\n    \n    \n      202212488\n      55\n    \n    \n      202212312\n      80\n    \n    \n      202212377\n      75\n    \n    \n      202212463\n      65\n    \n    \n      202212471\n      60\n    \n    \n      202212400\n      95\n    \n    \n      202212469\n      90\n    \n    \n      202212318\n      55\n    \n    \n      202212432\n      95\n    \n    \n      202212443\n      95\n    \n    \n      202212367\n      50\n    \n    \n      202212458\n      50\n    \n    \n      202212396\n      95\n    \n    \n      202212482\n      50\n    \n    \n      202212452\n      65\n    \n    \n      202212387\n      70\n    \n    \n      202212354\n      90\n    \n  \n\n\n\n\n\ntype(df[['att']])\n\npandas.core.frame.DataFrame\n\n\n\ndf.att 나 df[‘att’]는 series를 리턴하고 df[[‘att’]]는 dataframe을 리턴한다.\n\n- 방법4 : ndarray 스타일\n\ndf.iloc[:,0] \n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n- 방법5: ndarray스타일\n\ndf.iloc[:,[0]]\n\n\n\n\n\n  \n    \n      \n      att\n    \n  \n  \n    \n      202212380\n      65\n    \n    \n      202212370\n      95\n    \n    \n      202212363\n      65\n    \n    \n      202212488\n      55\n    \n    \n      202212312\n      80\n    \n    \n      202212377\n      75\n    \n    \n      202212463\n      65\n    \n    \n      202212471\n      60\n    \n    \n      202212400\n      95\n    \n    \n      202212469\n      90\n    \n    \n      202212318\n      55\n    \n    \n      202212432\n      95\n    \n    \n      202212443\n      95\n    \n    \n      202212367\n      50\n    \n    \n      202212458\n      50\n    \n    \n      202212396\n      95\n    \n    \n      202212482\n      50\n    \n    \n      202212452\n      65\n    \n    \n      202212387\n      70\n    \n    \n      202212354\n      90\n    \n  \n\n\n\n\n\ndf.iloc[:,0]은 series를 리턴하고 df.iloc[:,[0]]은 dataframe을 리턴한다.\n\n- 방법6: ndarray 스타일과 dict 스타일의 혼합\n\ndf.loc[:,'att'] \n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n- 방법7: ndarray 스타일과 dict 스타일의 혼합\n\ndf.loc[:,['att']] \n\n\n\n\n\n  \n    \n      \n      att\n    \n  \n  \n    \n      202212380\n      65\n    \n    \n      202212370\n      95\n    \n    \n      202212363\n      65\n    \n    \n      202212488\n      55\n    \n    \n      202212312\n      80\n    \n    \n      202212377\n      75\n    \n    \n      202212463\n      65\n    \n    \n      202212471\n      60\n    \n    \n      202212400\n      95\n    \n    \n      202212469\n      90\n    \n    \n      202212318\n      55\n    \n    \n      202212432\n      95\n    \n    \n      202212443\n      95\n    \n    \n      202212367\n      50\n    \n    \n      202212458\n      50\n    \n    \n      202212396\n      95\n    \n    \n      202212482\n      50\n    \n    \n      202212452\n      65\n    \n    \n      202212387\n      70\n    \n    \n      202212354\n      90\n    \n  \n\n\n\n\n\ndf.loc[:,‘att’]은 series를 리턴하고 df.loc[:,[‘att’]]은 dataframe을 리턴한다.\n\n- 방법7: nparray 스타일 + bool 인덱싱\n\ndf.iloc[:,[True,False,False,False]]\n\n\n\n\n\n  \n    \n      \n      att\n    \n  \n  \n    \n      202212380\n      65\n    \n    \n      202212370\n      95\n    \n    \n      202212363\n      65\n    \n    \n      202212488\n      55\n    \n    \n      202212312\n      80\n    \n    \n      202212377\n      75\n    \n    \n      202212463\n      65\n    \n    \n      202212471\n      60\n    \n    \n      202212400\n      95\n    \n    \n      202212469\n      90\n    \n    \n      202212318\n      55\n    \n    \n      202212432\n      95\n    \n    \n      202212443\n      95\n    \n    \n      202212367\n      50\n    \n    \n      202212458\n      50\n    \n    \n      202212396\n      95\n    \n    \n      202212482\n      50\n    \n    \n      202212452\n      65\n    \n    \n      202212387\n      70\n    \n    \n      202212354\n      90\n    \n  \n\n\n\n\n- 방법8: ndarray와 dict의 홉합형 + bool 인덱싱\n\ndf.loc[:,[True,False,False,False]]\n\n\n\n\n\n  \n    \n      \n      att\n    \n  \n  \n    \n      202212380\n      65\n    \n    \n      202212370\n      95\n    \n    \n      202212363\n      65\n    \n    \n      202212488\n      55\n    \n    \n      202212312\n      80\n    \n    \n      202212377\n      75\n    \n    \n      202212463\n      65\n    \n    \n      202212471\n      60\n    \n    \n      202212400\n      95\n    \n    \n      202212469\n      90\n    \n    \n      202212318\n      55\n    \n    \n      202212432\n      95\n    \n    \n      202212443\n      95\n    \n    \n      202212367\n      50\n    \n    \n      202212458\n      50\n    \n    \n      202212396\n      95\n    \n    \n      202212482\n      50\n    \n    \n      202212452\n      65\n    \n    \n      202212387\n      70\n    \n    \n      202212354\n      90\n    \n  \n\n\n\n\n\n\n여러개의 칼럼을 선택\n- 방법1: dict스타일\n\ndf[['att','fin']]\n\n\n\n\n\n  \n    \n      \n      att\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      40\n    \n    \n      202212370\n      95\n      80\n    \n    \n      202212363\n      65\n      30\n    \n    \n      202212488\n      55\n      80\n    \n    \n      202212312\n      80\n      100\n    \n    \n      202212377\n      75\n      15\n    \n    \n      202212463\n      65\n      90\n    \n    \n      202212471\n      60\n      0\n    \n    \n      202212400\n      95\n      10\n    \n    \n      202212469\n      90\n      20\n    \n    \n      202212318\n      55\n      25\n    \n    \n      202212432\n      95\n      0\n    \n    \n      202212443\n      95\n      35\n    \n    \n      202212367\n      50\n      30\n    \n    \n      202212458\n      50\n      85\n    \n    \n      202212396\n      95\n      95\n    \n    \n      202212482\n      50\n      10\n    \n    \n      202212452\n      65\n      45\n    \n    \n      202212387\n      70\n      35\n    \n    \n      202212354\n      90\n      90\n    \n  \n\n\n\n\n- 방법2: ndarray 스타일 (정수리스트로 인덱싱, 슬라이싱, 스트라이딩)\n\ndf.iloc[:,[0,1]] #정수의 리스트를 전달하여 칼럼추출\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n    \n  \n  \n    \n      202212380\n      65\n      55\n    \n    \n      202212370\n      95\n      100\n    \n    \n      202212363\n      65\n      90\n    \n    \n      202212488\n      55\n      80\n    \n    \n      202212312\n      80\n      30\n    \n    \n      202212377\n      75\n      40\n    \n    \n      202212463\n      65\n      45\n    \n    \n      202212471\n      60\n      60\n    \n    \n      202212400\n      95\n      65\n    \n    \n      202212469\n      90\n      80\n    \n    \n      202212318\n      55\n      75\n    \n    \n      202212432\n      95\n      95\n    \n    \n      202212443\n      95\n      55\n    \n    \n      202212367\n      50\n      80\n    \n    \n      202212458\n      50\n      55\n    \n    \n      202212396\n      95\n      30\n    \n    \n      202212482\n      50\n      50\n    \n    \n      202212452\n      65\n      55\n    \n    \n      202212387\n      70\n      70\n    \n    \n      202212354\n      90\n      90\n    \n  \n\n\n\n\n\ndf.iloc[:,0:2]  #슬라이싱, 0,1,2에서 마지막 2는 제외되고 0,1에 해당하는 것만 추출\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n    \n  \n  \n    \n      202212380\n      65\n      55\n    \n    \n      202212370\n      95\n      100\n    \n    \n      202212363\n      65\n      90\n    \n    \n      202212488\n      55\n      80\n    \n    \n      202212312\n      80\n      30\n    \n    \n      202212377\n      75\n      40\n    \n    \n      202212463\n      65\n      45\n    \n    \n      202212471\n      60\n      60\n    \n    \n      202212400\n      95\n      65\n    \n    \n      202212469\n      90\n      80\n    \n    \n      202212318\n      55\n      75\n    \n    \n      202212432\n      95\n      95\n    \n    \n      202212443\n      95\n      55\n    \n    \n      202212367\n      50\n      80\n    \n    \n      202212458\n      50\n      55\n    \n    \n      202212396\n      95\n      30\n    \n    \n      202212482\n      50\n      50\n    \n    \n      202212452\n      65\n      55\n    \n    \n      202212387\n      70\n      70\n    \n    \n      202212354\n      90\n      90\n    \n  \n\n\n\n\n\ndf.iloc[:,2:]  #슬라이싱\n\n\n\n\n\n  \n    \n      \n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      50\n      40\n    \n    \n      202212370\n      50\n      80\n    \n    \n      202212363\n      60\n      30\n    \n    \n      202212488\n      75\n      80\n    \n    \n      202212312\n      30\n      100\n    \n    \n      202212377\n      100\n      15\n    \n    \n      202212463\n      45\n      90\n    \n    \n      202212471\n      25\n      0\n    \n    \n      202212400\n      20\n      10\n    \n    \n      202212469\n      80\n      20\n    \n    \n      202212318\n      35\n      25\n    \n    \n      202212432\n      45\n      0\n    \n    \n      202212443\n      15\n      35\n    \n    \n      202212367\n      40\n      30\n    \n    \n      202212458\n      15\n      85\n    \n    \n      202212396\n      30\n      95\n    \n    \n      202212482\n      45\n      10\n    \n    \n      202212452\n      15\n      45\n    \n    \n      202212387\n      40\n      35\n    \n    \n      202212354\n      80\n      90\n    \n  \n\n\n\n\n\ndf.iloc[:,::2]  #슬라이싱, 스트라이딩\n\n\n\n\n\n  \n    \n      \n      att\n      mid\n    \n  \n  \n    \n      202212380\n      65\n      50\n    \n    \n      202212370\n      95\n      50\n    \n    \n      202212363\n      65\n      60\n    \n    \n      202212488\n      55\n      75\n    \n    \n      202212312\n      80\n      30\n    \n    \n      202212377\n      75\n      100\n    \n    \n      202212463\n      65\n      45\n    \n    \n      202212471\n      60\n      25\n    \n    \n      202212400\n      95\n      20\n    \n    \n      202212469\n      90\n      80\n    \n    \n      202212318\n      55\n      35\n    \n    \n      202212432\n      95\n      45\n    \n    \n      202212443\n      95\n      15\n    \n    \n      202212367\n      50\n      40\n    \n    \n      202212458\n      50\n      15\n    \n    \n      202212396\n      95\n      30\n    \n    \n      202212482\n      50\n      45\n    \n    \n      202212452\n      65\n      15\n    \n    \n      202212387\n      70\n      40\n    \n    \n      202212354\n      90\n      80\n    \n  \n\n\n\n\n- 방법3: ndarray와 dict의 혼합형\n\ndf.loc[:,['att','mid']]\n\n\n\n\n\n  \n    \n      \n      att\n      mid\n    \n  \n  \n    \n      202212380\n      65\n      50\n    \n    \n      202212370\n      95\n      50\n    \n    \n      202212363\n      65\n      60\n    \n    \n      202212488\n      55\n      75\n    \n    \n      202212312\n      80\n      30\n    \n    \n      202212377\n      75\n      100\n    \n    \n      202212463\n      65\n      45\n    \n    \n      202212471\n      60\n      25\n    \n    \n      202212400\n      95\n      20\n    \n    \n      202212469\n      90\n      80\n    \n    \n      202212318\n      55\n      35\n    \n    \n      202212432\n      95\n      45\n    \n    \n      202212443\n      95\n      15\n    \n    \n      202212367\n      50\n      40\n    \n    \n      202212458\n      50\n      15\n    \n    \n      202212396\n      95\n      30\n    \n    \n      202212482\n      50\n      45\n    \n    \n      202212452\n      65\n      15\n    \n    \n      202212387\n      70\n      40\n    \n    \n      202212354\n      90\n      80\n    \n  \n\n\n\n\n\ndf.loc[:,'att':'rep']  # key로 하는 슬라이싱은 마지막 'rep'까지 표시되어 나온다\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n    \n  \n  \n    \n      202212380\n      65\n      55\n    \n    \n      202212370\n      95\n      100\n    \n    \n      202212363\n      65\n      90\n    \n    \n      202212488\n      55\n      80\n    \n    \n      202212312\n      80\n      30\n    \n    \n      202212377\n      75\n      40\n    \n    \n      202212463\n      65\n      45\n    \n    \n      202212471\n      60\n      60\n    \n    \n      202212400\n      95\n      65\n    \n    \n      202212469\n      90\n      80\n    \n    \n      202212318\n      55\n      75\n    \n    \n      202212432\n      95\n      95\n    \n    \n      202212443\n      95\n      55\n    \n    \n      202212367\n      50\n      80\n    \n    \n      202212458\n      50\n      55\n    \n    \n      202212396\n      95\n      30\n    \n    \n      202212482\n      50\n      50\n    \n    \n      202212452\n      65\n      55\n    \n    \n      202212387\n      70\n      70\n    \n    \n      202212354\n      90\n      90\n    \n  \n\n\n\n\n\ndf.loc[:,:'rep']\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n    \n  \n  \n    \n      202212380\n      65\n      55\n    \n    \n      202212370\n      95\n      100\n    \n    \n      202212363\n      65\n      90\n    \n    \n      202212488\n      55\n      80\n    \n    \n      202212312\n      80\n      30\n    \n    \n      202212377\n      75\n      40\n    \n    \n      202212463\n      65\n      45\n    \n    \n      202212471\n      60\n      60\n    \n    \n      202212400\n      95\n      65\n    \n    \n      202212469\n      90\n      80\n    \n    \n      202212318\n      55\n      75\n    \n    \n      202212432\n      95\n      95\n    \n    \n      202212443\n      95\n      55\n    \n    \n      202212367\n      50\n      80\n    \n    \n      202212458\n      50\n      55\n    \n    \n      202212396\n      95\n      30\n    \n    \n      202212482\n      50\n      50\n    \n    \n      202212452\n      65\n      55\n    \n    \n      202212387\n      70\n      70\n    \n    \n      202212354\n      90\n      90\n    \n  \n\n\n\n\n\ndf.loc[:,'rep':]\n\n\n\n\n\n  \n    \n      \n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      55\n      50\n      40\n    \n    \n      202212370\n      100\n      50\n      80\n    \n    \n      202212363\n      90\n      60\n      30\n    \n    \n      202212488\n      80\n      75\n      80\n    \n    \n      202212312\n      30\n      30\n      100\n    \n    \n      202212377\n      40\n      100\n      15\n    \n    \n      202212463\n      45\n      45\n      90\n    \n    \n      202212471\n      60\n      25\n      0\n    \n    \n      202212400\n      65\n      20\n      10\n    \n    \n      202212469\n      80\n      80\n      20\n    \n    \n      202212318\n      75\n      35\n      25\n    \n    \n      202212432\n      95\n      45\n      0\n    \n    \n      202212443\n      55\n      15\n      35\n    \n    \n      202212367\n      80\n      40\n      30\n    \n    \n      202212458\n      55\n      15\n      85\n    \n    \n      202212396\n      30\n      30\n      95\n    \n    \n      202212482\n      50\n      45\n      10\n    \n    \n      202212452\n      55\n      15\n      45\n    \n    \n      202212387\n      70\n      40\n      35\n    \n    \n      202212354\n      90\n      80\n      90\n    \n  \n\n\n\n\n- 방법4: bool을 이용한 인덱싱\n\ndf.iloc[:,[True,False,True,False]]\n\n\n\n\n\n  \n    \n      \n      att\n      mid\n    \n  \n  \n    \n      202212380\n      65\n      50\n    \n    \n      202212370\n      95\n      50\n    \n    \n      202212363\n      65\n      60\n    \n    \n      202212488\n      55\n      75\n    \n    \n      202212312\n      80\n      30\n    \n    \n      202212377\n      75\n      100\n    \n    \n      202212463\n      65\n      45\n    \n    \n      202212471\n      60\n      25\n    \n    \n      202212400\n      95\n      20\n    \n    \n      202212469\n      90\n      80\n    \n    \n      202212318\n      55\n      35\n    \n    \n      202212432\n      95\n      45\n    \n    \n      202212443\n      95\n      15\n    \n    \n      202212367\n      50\n      40\n    \n    \n      202212458\n      50\n      15\n    \n    \n      202212396\n      95\n      30\n    \n    \n      202212482\n      50\n      45\n    \n    \n      202212452\n      65\n      15\n    \n    \n      202212387\n      70\n      40\n    \n    \n      202212354\n      90\n      80\n    \n  \n\n\n\n\n\ndf.loc[:,[True,False,True,False]]\n\n\n\n\n\n  \n    \n      \n      att\n      mid\n    \n  \n  \n    \n      202212380\n      65\n      50\n    \n    \n      202212370\n      95\n      50\n    \n    \n      202212363\n      65\n      60\n    \n    \n      202212488\n      55\n      75\n    \n    \n      202212312\n      80\n      30\n    \n    \n      202212377\n      75\n      100\n    \n    \n      202212463\n      65\n      45\n    \n    \n      202212471\n      60\n      25\n    \n    \n      202212400\n      95\n      20\n    \n    \n      202212469\n      90\n      80\n    \n    \n      202212318\n      55\n      35\n    \n    \n      202212432\n      95\n      45\n    \n    \n      202212443\n      95\n      15\n    \n    \n      202212367\n      50\n      40\n    \n    \n      202212458\n      50\n      15\n    \n    \n      202212396\n      95\n      30\n    \n    \n      202212482\n      50\n      45\n    \n    \n      202212452\n      65\n      15\n    \n    \n      202212387\n      70\n      40\n    \n    \n      202212354\n      90\n      80\n    \n  \n\n\n\n\n\ntest_ndarray[:,range(2)]\n\narray([[202212380,        65],\n       [202212370,        95],\n       [202212363,        65],\n       [202212488,        55],\n       [202212312,        80],\n       [202212377,        75],\n       [202212463,        65],\n       [202212471,        60],\n       [202212400,        95],\n       [202212469,        90],\n       [202212318,        55],\n       [202212432,        95],\n       [202212443,        95],\n       [202212367,        50],\n       [202212458,        50],\n       [202212396,        95],\n       [202212482,        50],\n       [202212452,        65],\n       [202212387,        70],\n       [202212354,        90]])\n\n\n\ndf.iloc[:,range(2)]\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n    \n  \n  \n    \n      202212380\n      65\n      55\n    \n    \n      202212370\n      95\n      100\n    \n    \n      202212363\n      65\n      90\n    \n    \n      202212488\n      55\n      80\n    \n    \n      202212312\n      80\n      30\n    \n    \n      202212377\n      75\n      40\n    \n    \n      202212463\n      65\n      45\n    \n    \n      202212471\n      60\n      60\n    \n    \n      202212400\n      95\n      65\n    \n    \n      202212469\n      90\n      80\n    \n    \n      202212318\n      55\n      75\n    \n    \n      202212432\n      95\n      95\n    \n    \n      202212443\n      95\n      55\n    \n    \n      202212367\n      50\n      80\n    \n    \n      202212458\n      50\n      55\n    \n    \n      202212396\n      95\n      30\n    \n    \n      202212482\n      50\n      50\n    \n    \n      202212452\n      65\n      55\n    \n    \n      202212387\n      70\n      70\n    \n    \n      202212354\n      90\n      90\n    \n  \n\n\n\n\n\n\n첫번째 행을 선택\n- 방법1\n\ntest_ndarray[0,:]\n\narray([202212380,        65,        55,        50,        40])\n\n\n\ntest_ndarray[0]\n\narray([202212380,        65,        55,        50,        40])\n\n\n\ndf.iloc[0]\n\natt    65\nrep    55\nmid    50\nfin    40\nName: 202212380, dtype: int64\n\n\n- 방법2\n\ndf.iloc[[0]]  # 데이터프레임처럼\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n  \n\n\n\n\n- 방법3\n\ndf.iloc[0,:]\n\natt    65\nrep    55\nmid    50\nfin    40\nName: 202212380, dtype: int64\n\n\n- 방법4\n\ndf.iloc[[0],:]\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n  \n\n\n\n\n- 방법5\n\ndf.loc['202212380']\n\natt    65\nrep    55\nmid    50\nfin    40\nName: 202212380, dtype: int64\n\n\n- 방법6\n\ndf.loc[['202212380']]  # 데이터프레임처럼\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n  \n\n\n\n\n- 방법7\n\ndf.loc['202212380',:]\n\natt    65\nrep    55\nmid    50\nfin    40\nName: 202212380, dtype: int64\n\n\n- 방법8\n\ndf.loc[['202212380'],:]\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n  \n\n\n\n\n- 방법9\n\nlen(df)\n\n20\n\n\n\n[True]+[False]*19\n\n[True,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False]\n\n\n\n_lst = [True]+[False]*19\n\n\ndf.iloc[_lst]\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n  \n\n\n\n\n\ndf.iloc[_lst,:]\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n  \n\n\n\n\n\ndf.loc[_lst]\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n  \n\n\n\n\n\ndf.loc[_lst,:]\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n  \n\n\n\n\n\n\n여러개의 행을 선택\n- 방법1\n\ndf.iloc[[0,2]]\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n    \n      202212363\n      65\n      90\n      60\n      30\n    \n  \n\n\n\n\n\ndf.iloc[[0,2],:]\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n    \n      202212363\n      65\n      90\n      60\n      30\n    \n  \n\n\n\n\n- 방법2\n\ndf.loc[['202212380','202212363']]\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n    \n      202212363\n      65\n      90\n      60\n      30\n    \n  \n\n\n\n\n\ndf.loc[['202212380','202212363'],:]\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n    \n      202212363\n      65\n      90\n      60\n      30\n    \n  \n\n\n\n\n- 그 밖의 방법들\n\ndf.iloc[::4]  # 스트라이딩\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n    \n      202212312\n      80\n      30\n      30\n      100\n    \n    \n      202212400\n      95\n      65\n      20\n      10\n    \n    \n      202212443\n      95\n      55\n      15\n      35\n    \n    \n      202212482\n      50\n      50\n      45\n      10\n    \n  \n\n\n\n\n\ndf.iloc[:5]\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n    \n      202212370\n      95\n      100\n      50\n      80\n    \n    \n      202212363\n      65\n      90\n      60\n      30\n    \n    \n      202212488\n      55\n      80\n      75\n      80\n    \n    \n      202212312\n      80\n      30\n      30\n      100\n    \n  \n\n\n\n\n\ndf.loc[:'202212312']\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n    \n      202212370\n      95\n      100\n      50\n      80\n    \n    \n      202212363\n      65\n      90\n      60\n      30\n    \n    \n      202212488\n      55\n      80\n      75\n      80\n    \n    \n      202212312\n      80\n      30\n      30\n      100\n    \n  \n\n\n\n\n\ndf.att < 80\n\n202212380     True\n202212370    False\n202212363     True\n202212488     True\n202212312    False\n202212377     True\n202212463     True\n202212471     True\n202212400    False\n202212469    False\n202212318     True\n202212432    False\n202212443    False\n202212367     True\n202212458     True\n202212396    False\n202212482     True\n202212452     True\n202212387     True\n202212354    False\nName: att, dtype: bool\n\n\n\ndf.loc[df.att<80]\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n    \n      202212363\n      65\n      90\n      60\n      30\n    \n    \n      202212488\n      55\n      80\n      75\n      80\n    \n    \n      202212377\n      75\n      40\n      100\n      15\n    \n    \n      202212463\n      65\n      45\n      45\n      90\n    \n    \n      202212471\n      60\n      60\n      25\n      0\n    \n    \n      202212318\n      55\n      75\n      35\n      25\n    \n    \n      202212367\n      50\n      80\n      40\n      30\n    \n    \n      202212458\n      50\n      55\n      15\n      85\n    \n    \n      202212482\n      50\n      50\n      45\n      10\n    \n    \n      202212452\n      65\n      55\n      15\n      45\n    \n    \n      202212387\n      70\n      70\n      40\n      35\n    \n  \n\n\n\n\n\ndf.loc[list(df.att<80),'rep':]  # 리스트로 바꿔주는게 컴퓨터에게 좀 더 명확한 전달\n\n\n\n\n\n  \n    \n      \n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      55\n      50\n      40\n    \n    \n      202212363\n      90\n      60\n      30\n    \n    \n      202212488\n      80\n      75\n      80\n    \n    \n      202212377\n      40\n      100\n      15\n    \n    \n      202212463\n      45\n      45\n      90\n    \n    \n      202212471\n      60\n      25\n      0\n    \n    \n      202212318\n      75\n      35\n      25\n    \n    \n      202212367\n      80\n      40\n      30\n    \n    \n      202212458\n      55\n      15\n      85\n    \n    \n      202212482\n      50\n      45\n      10\n    \n    \n      202212452\n      55\n      15\n      45\n    \n    \n      202212387\n      70\n      40\n      35\n    \n  \n\n\n\n\n\ndf.loc[df.att<80,'rep':] # 하지만 리스트화 안해도 되긴 한다..\n\n\n\n\n\n  \n    \n      \n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      55\n      50\n      40\n    \n    \n      202212363\n      90\n      60\n      30\n    \n    \n      202212488\n      80\n      75\n      80\n    \n    \n      202212377\n      40\n      100\n      15\n    \n    \n      202212463\n      45\n      45\n      90\n    \n    \n      202212471\n      60\n      25\n      0\n    \n    \n      202212318\n      75\n      35\n      25\n    \n    \n      202212367\n      80\n      40\n      30\n    \n    \n      202212458\n      55\n      15\n      85\n    \n    \n      202212482\n      50\n      45\n      10\n    \n    \n      202212452\n      55\n      15\n      45\n    \n    \n      202212387\n      70\n      40\n      35\n    \n  \n\n\n\n\n\ndf.iloc[list(df.att<80),1:]\n\n\n\n\n\n  \n    \n      \n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212380\n      55\n      50\n      40\n    \n    \n      202212363\n      90\n      60\n      30\n    \n    \n      202212488\n      80\n      75\n      80\n    \n    \n      202212377\n      40\n      100\n      15\n    \n    \n      202212463\n      45\n      45\n      90\n    \n    \n      202212471\n      60\n      25\n      0\n    \n    \n      202212318\n      75\n      35\n      25\n    \n    \n      202212367\n      80\n      40\n      30\n    \n    \n      202212458\n      55\n      15\n      85\n    \n    \n      202212482\n      50\n      45\n      10\n    \n    \n      202212452\n      55\n      15\n      45\n    \n    \n      202212387\n      70\n      40\n      35\n    \n  \n\n\n\n\n- 아래는 에러가 난다.\n\ndf.iloc[df.att<80,1:]\n\nValueError: Location based indexing can only have [integer, integer slice (START point is INCLUDED, END point is EXCLUDED), listlike of integers, boolean array] types\n\n\n\n\nquery (중요!!!)\n- 예제1\n\ndf.query('att==90 and mid>30')\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212469\n      90\n      80\n      80\n      20\n    \n    \n      202212354\n      90\n      90\n      80\n      90\n    \n  \n\n\n\n\n- 예제2\n\ndf.query('att<rep and mid<fin')\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212370\n      95\n      100\n      50\n      80\n    \n    \n      202212488\n      55\n      80\n      75\n      80\n    \n    \n      202212458\n      50\n      55\n      15\n      85\n    \n  \n\n\n\n\n- 예제3\n\ndf.query('att<rep<80')\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212318\n      55\n      75\n      35\n      25\n    \n    \n      202212458\n      50\n      55\n      15\n      85\n    \n  \n\n\n\n\n- 예제4\n\ndf.query('50<att<=90 and mid<fin')\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212488\n      55\n      80\n      75\n      80\n    \n    \n      202212312\n      80\n      30\n      30\n      100\n    \n    \n      202212463\n      65\n      45\n      45\n      90\n    \n    \n      202212452\n      65\n      55\n      15\n      45\n    \n    \n      202212354\n      90\n      90\n      80\n      90\n    \n  \n\n\n\n\n- 예제5\n\ndf.query('(mid+fin)/2 >= 60')\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212370\n      95\n      100\n      50\n      80\n    \n    \n      202212488\n      55\n      80\n      75\n      80\n    \n    \n      202212312\n      80\n      30\n      30\n      100\n    \n    \n      202212463\n      65\n      45\n      45\n      90\n    \n    \n      202212396\n      95\n      30\n      30\n      95\n    \n    \n      202212354\n      90\n      90\n      80\n      90\n    \n  \n\n\n\n\n- 예제6\n\n_mean = df.att.mean()\n_mean\n\n73.0\n\n\n\ndf.query('att>=73')\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212370\n      95\n      100\n      50\n      80\n    \n    \n      202212312\n      80\n      30\n      30\n      100\n    \n    \n      202212377\n      75\n      40\n      100\n      15\n    \n    \n      202212400\n      95\n      65\n      20\n      10\n    \n    \n      202212469\n      90\n      80\n      80\n      20\n    \n    \n      202212432\n      95\n      95\n      45\n      0\n    \n    \n      202212443\n      95\n      55\n      15\n      35\n    \n    \n      202212396\n      95\n      30\n      30\n      95\n    \n    \n      202212354\n      90\n      90\n      80\n      90\n    \n  \n\n\n\n\n\ndf.query('att>=_mean')  # keyError 가 난다!\n\nUndefinedVariableError: name '_mean' is not defined\n\n\n\ndf.query('att>=@_mean')   # 앞에 @ 골뱅이를 붙여주면 에러 안난다.\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212370\n      95\n      100\n      50\n      80\n    \n    \n      202212312\n      80\n      30\n      30\n      100\n    \n    \n      202212377\n      75\n      40\n      100\n      15\n    \n    \n      202212400\n      95\n      65\n      20\n      10\n    \n    \n      202212469\n      90\n      80\n      80\n      20\n    \n    \n      202212432\n      95\n      95\n      45\n      0\n    \n    \n      202212443\n      95\n      55\n      15\n      35\n    \n    \n      202212396\n      95\n      30\n      30\n      95\n    \n    \n      202212354\n      90\n      90\n      80\n      90\n    \n  \n\n\n\n\n- 예제7\n\ndf.query('index <= \"202212354\"')\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212312\n      80\n      30\n      30\n      100\n    \n    \n      202212318\n      55\n      75\n      35\n      25\n    \n    \n      202212354\n      90\n      90\n      80\n      90\n    \n  \n\n\n\n\n\ndf.query('index <= \"202212354\" or index==\"202212387\"')  # 밖에를 큰따옴표 하고 안쪽을 작은따옴표 해도 된다.\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      202212312\n      80\n      30\n      30\n      100\n    \n    \n      202212318\n      55\n      75\n      35\n      25\n    \n    \n      202212387\n      70\n      70\n      40\n      35\n    \n    \n      202212354\n      90\n      90\n      80\n      90\n    \n  \n\n\n\n\n사실 이 기능은 시계열자료에서 꽃핀다.\n- 예제8\n\npd.date_range('20230101',periods=10)\n\nDatetimeIndex(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04',\n               '2023-01-05', '2023-01-06', '2023-01-07', '2023-01-08',\n               '2023-01-09', '2023-01-10'],\n              dtype='datetime64[ns]', freq='D')\n\n\n\n_df=pd.DataFrame(np.random.normal(size=(10,4)),columns=list('ABCD'), index=pd.date_range('20230101',periods=10))\n_df\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      2023-01-01\n      -0.259429\n      0.369731\n      -0.279944\n      0.099409\n    \n    \n      2023-01-02\n      -0.932515\n      -0.311629\n      0.828348\n      -0.225257\n    \n    \n      2023-01-03\n      -0.011607\n      0.927334\n      -0.753145\n      1.013249\n    \n    \n      2023-01-04\n      -1.050379\n      -0.323094\n      0.813898\n      1.035724\n    \n    \n      2023-01-05\n      -0.921175\n      0.513109\n      -0.905361\n      0.893707\n    \n    \n      2023-01-06\n      -1.521594\n      0.856883\n      -0.401441\n      -1.111551\n    \n    \n      2023-01-07\n      0.958028\n      -0.015302\n      0.891259\n      -0.826834\n    \n    \n      2023-01-08\n      1.822226\n      -1.258543\n      -0.705506\n      -0.519831\n    \n    \n      2023-01-09\n      -0.593394\n      -1.399224\n      -1.616172\n      -0.626952\n    \n    \n      2023-01-10\n      -0.083539\n      0.528519\n      0.051522\n      0.126757\n    \n  \n\n\n\n\n\n_df.query(\"'2023-01-02' < index <= '2023-01-09'\")\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      2023-01-03\n      -0.011607\n      0.927334\n      -0.753145\n      1.013249\n    \n    \n      2023-01-04\n      -1.050379\n      -0.323094\n      0.813898\n      1.035724\n    \n    \n      2023-01-05\n      -0.921175\n      0.513109\n      -0.905361\n      0.893707\n    \n    \n      2023-01-06\n      -1.521594\n      0.856883\n      -0.401441\n      -1.111551\n    \n    \n      2023-01-07\n      0.958028\n      -0.015302\n      0.891259\n      -0.826834\n    \n    \n      2023-01-08\n      1.822226\n      -1.258543\n      -0.705506\n      -0.519831\n    \n    \n      2023-01-09\n      -0.593394\n      -1.399224\n      -1.616172\n      -0.626952\n    \n  \n\n\n\n\n\n_df.query(\"'2023-01-02' < index <= '2023-01-09' and A+B<C\")\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      2023-01-04\n      -1.050379\n      -0.323094\n      0.813898\n      1.035724\n    \n    \n      2023-01-06\n      -1.521594\n      0.856883\n      -0.401441\n      -1.111551\n    \n    \n      2023-01-09\n      -0.593394\n      -1.399224\n      -1.616172\n      -0.626952\n    \n  \n\n\n\n\n- query가 만능은 아니다.\n\ndf.columns = pd.Index(['att score', 'rep score', 'mid score','fin score'])\n\n\ndf.query(\"att score < 90\")  # 변수이름에 띄어쓰기가 들어가면 에러가 난다.\n\nSyntaxError: invalid syntax (<unknown>, line 1)\n\n\n\ndf.att socre\n\nSyntaxError: invalid syntax (<ipython-input-285-4116dfe6888b>, line 1)\n\n\n\ndf.loc[df[\"att score\"] < 90, :] # 이렇게 하면 됨\n\n\n\n\n\n  \n    \n      \n      att score\n      rep score\n      mid score\n      fin score\n    \n  \n  \n    \n      202212380\n      65\n      55\n      50\n      40\n    \n    \n      202212363\n      65\n      90\n      60\n      30\n    \n    \n      202212488\n      55\n      80\n      75\n      80\n    \n    \n      202212312\n      80\n      30\n      30\n      100\n    \n    \n      202212377\n      75\n      40\n      100\n      15\n    \n    \n      202212463\n      65\n      45\n      45\n      90\n    \n    \n      202212471\n      60\n      60\n      25\n      0\n    \n    \n      202212318\n      55\n      75\n      35\n      25\n    \n    \n      202212367\n      50\n      80\n      40\n      30\n    \n    \n      202212458\n      50\n      55\n      15\n      85\n    \n    \n      202212482\n      50\n      50\n      45\n      10\n    \n    \n      202212452\n      65\n      55\n      15\n      45\n    \n    \n      202212387\n      70\n      70\n      40\n      35\n    \n  \n\n\n\n\n\n\n\npandas 공부 3단계\n\n전치\n\nndarray = np.arange(2*3).reshape(2,3)\ndf=pd.DataFrame(ndarray)\ndf\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      0\n      1\n      2\n    \n    \n      1\n      3\n      4\n      5\n    \n  \n\n\n\n\n\nndarray.T\n\narray([[0, 3],\n       [1, 4],\n       [2, 5]])\n\n\n\ndf.T\n\n\n\n\n\n  \n    \n      \n      0\n      1\n    \n  \n  \n    \n      0\n      0\n      3\n    \n    \n      1\n      1\n      4\n    \n    \n      2\n      2\n      5\n    \n  \n\n\n\n\n\n\n합\n\nndarray.sum(axis=0)\n\narray([3, 5, 7])\n\n\n\ndf.sum(axis=0)\n\n0    3\n1    5\n2    7\ndtype: int64\n\n\n\nndarray.sum(axis=1)\n\narray([ 3, 12])\n\n\n\ndf.sum(axis=1)\n\n0     3\n1    12\ndtype: int64\n\n\n\n\ncumsum\n\ndf\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      0\n      1\n      2\n    \n    \n      1\n      3\n      4\n      5\n    \n  \n\n\n\n\n\nndarray.cumsum(axis=0) #누적해서 더해짐\n\narray([[0, 1, 2],\n       [3, 5, 7]])\n\n\n\ndf.cumsum(axis=0)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      0\n      1\n      2\n    \n    \n      1\n      3\n      5\n      7\n    \n  \n\n\n\n\n\nndarray.cumsum(axis=1)\n\narray([[ 0,  1,  3],\n       [ 3,  7, 12]])\n\n\n\ndf.cumsum(axis=1)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      0\n      1\n      3\n    \n    \n      1\n      3\n      7\n      12\n    \n  \n\n\n\n\n\n\n형태변환\n\nndarray.tolist()\n\n[[0, 1, 2], [3, 4, 5]]\n\n\n\ndf.to_numpy()\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n\ndf.to_numpy().tolist()\n\n[[0, 1, 2], [3, 4, 5]]\n\n\n\ndf.to_dict()\n\n{0: {0: 0, 1: 3}, 1: {0: 1, 1: 4}, 2: {0: 2, 1: 5}}\n\n\n\n\n\npandas 공부 4단계 (생략)\n\n\n숙제\n- 아래의 DF에서 1,3번째 열을 추출하라.\n\ndf= pd.DataFrame({'att':[90,90,95],'rep':[80,90,90],'mid':[50,60,70], 'fin':[70,80,50]})\ndf\n\n\n\n\n\n  \n    \n      \n      att\n      rep\n      mid\n      fin\n    \n  \n  \n    \n      0\n      90\n      80\n      50\n      70\n    \n    \n      1\n      90\n      90\n      60\n      80\n    \n    \n      2\n      95\n      90\n      70\n      50\n    \n  \n\n\n\n\n\ndf[['att','mid']]\n\n\n\n\n\n  \n    \n      \n      att\n      mid\n    \n  \n  \n    \n      0\n      90\n      50\n    \n    \n      1\n      90\n      60\n    \n    \n      2\n      95\n      70\n    \n  \n\n\n\n\n\ndf.iloc[:,[0,2]]\n\n\n\n\n\n  \n    \n      \n      att\n      mid\n    \n  \n  \n    \n      0\n      90\n      50\n    \n    \n      1\n      90\n      60\n    \n    \n      2\n      95\n      70\n    \n  \n\n\n\n\n\ndf.iloc[:,::2]\n\n\n\n\n\n  \n    \n      \n      att\n      mid\n    \n  \n  \n    \n      0\n      90\n      50\n    \n    \n      1\n      90\n      60\n    \n    \n      2\n      95\n      70\n    \n  \n\n\n\n\n\ndf.loc[:,['att','mid']]\n\n\n\n\n\n  \n    \n      \n      att\n      mid\n    \n  \n  \n    \n      0\n      90\n      50\n    \n    \n      1\n      90\n      60\n    \n    \n      2\n      95\n      70"
  },
  {
    "objectID": "posts/Python/2. Numpy/python 7_0418.html",
    "href": "posts/Python/2. Numpy/python 7_0418.html",
    "title": "파이썬 (0418) 7주차",
    "section": "",
    "text": "import numpy as np"
  },
  {
    "objectID": "posts/Python/2. Numpy/python 7_0418.html#numpy공부-7단계",
    "href": "posts/Python/2. Numpy/python 7_0418.html#numpy공부-7단계",
    "title": "파이썬 (0418) 7주차",
    "section": "numpy공부 7단계",
    "text": "numpy공부 7단계\n\nnote 1: 메소드 도움말 확인하기\n- 파이썬에서 함수를 적용하는 2가지 방식 - np.sum(a) - a.sum()\n\na=np.array([1,2,3,4,5])\na\n\narray([1, 2, 3, 4, 5])\n\n\n\na.sum()\n\n15\n\n\n\nnp.sum(a)\n\n15\n\n\n- 넘파이에서 a.sum에 대한 도움말은 보통 np.sum()에 자세히 나와있음 \\(\\to\\) np.sum()의 도움말을 확인하고 np.sum(a)와 a.sum()이 동일함을 이용하여 a.sum()의 사용법을 미루어 유추해야함\n\na.sum?\n\n\nnp.sum?\n\n\nnp.sum([0.5, 1.5])\n\n2.0\n\n\n\n\nnote2: hstack, vstack\n- hstack, vstack를 쓰는 사람도 있다.\n\na=np.arange(6)\nb=-a\n\n\nnp.vstack([a,b])\n\narray([[ 0,  1,  2,  3,  4,  5],\n       [ 0, -1, -2, -3, -4, -5]])\n\n\n\nnp.stack([a,b],axis=0)\n\narray([[ 0,  1,  2,  3,  4,  5],\n       [ 0, -1, -2, -3, -4, -5]])\n\n\n\nnp.hstack([a,b])\n\narray([ 0,  1,  2,  3,  4,  5,  0, -1, -2, -3, -4, -5])\n\n\n\nnp.concatenate([a,b],axis=0)\n\narray([ 0,  1,  2,  3,  4,  5,  0, -1, -2, -3, -4, -5])\n\n\n\nnote3: append\n- 기능1:reshape(-1) + concat\n\na=np.arange(30).reshape(5,6)\nb= -np.arange(8).reshape(2,2,2)\n\n\na.shape, b.shape\n\n((5, 6), (2, 2, 2))\n\n\n\nnp.append(a,b)\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,  0, -1, -2, -3,\n       -4, -5, -6, -7])\n\n\n\nnp.concatenate([a.reshape(-1), b.reshape(-1)])\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,  0, -1, -2, -3,\n       -4, -5, -6, -7])\n\n\n- 기능2: concat\n\na=np.arange(2*3*4).reshape(2,3,4)\nb=-a\n\n\na.shape, b.shape, np.append(a,b, axis=0).shape   # 대괄호를 쓰지 않아도 됨\n\n((2, 3, 4), (2, 3, 4), (4, 3, 4))\n\n\n\na.shape, b.shape, np.append(a,b, axis=1).shape\n\n((2, 3, 4), (2, 3, 4), (2, 6, 4))\n\n\n\na.shape, b.shape, np.append(a,b, axis=2).shape\n\n((2, 3, 4), (2, 3, 4), (2, 3, 8))\n\n\n- concat과의 차이?\n\na=np.arange(2*3*4).reshape(2,3,4)\nb=-a\nc=2*a\n\n\nnp.concatenate([a,b,c],axis=0)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]],\n\n       [[  0,   2,   4,   6],\n        [  8,  10,  12,  14],\n        [ 16,  18,  20,  22]],\n\n       [[ 24,  26,  28,  30],\n        [ 32,  34,  36,  38],\n        [ 40,  42,  44,  46]]])\n\n\n\n\nnote4: revel, flatten\n\na=np.arange(2*3*4).reshape(2,3,4)\na\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n\n\n\na.reshape(-1) #디멘전 1차원으로\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23])\n\n\n\na.ravel()\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23])\n\n\n\na.flatten()\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23])\n\n\n\n\nnote 5: 기타 통계함수들\n- 평균, 중앙값, 표준편차, 분산\n\na=np.random.normal(loc=0, scale=2, size=(100,))\na\n\narray([-2.01759369e+00,  1.70831942e+00, -7.66284153e-01,  2.15177363e+00,\n        1.93917905e+00, -2.74073590e-01, -2.04642372e+00, -1.98463689e+00,\n        1.83815582e+00,  4.49207271e+00, -5.40520993e-03,  1.45933943e+00,\n       -1.88730370e+00,  2.53422937e+00, -1.43846951e+00, -2.69938884e-01,\n       -2.68912083e+00,  6.01230062e-01,  1.21155692e+00, -1.78259314e+00,\n        3.08941967e-01,  1.22338707e+00, -1.03232597e+00, -1.79667669e+00,\n        2.19458228e+00,  5.75514508e-01, -3.02570319e+00, -1.21868604e+00,\n       -9.60932070e-01,  1.11771254e+00, -5.34063250e-01, -2.68962004e+00,\n       -4.62864312e+00,  4.64113175e+00, -1.05051461e+00, -6.14152261e-01,\n       -1.56320062e+00,  1.18863285e-01,  1.71819177e+00,  5.04434396e-01,\n       -1.59021839e+00, -8.40274272e-01, -1.92903415e+00, -3.31025301e+00,\n       -5.44121948e+00,  1.71770231e+00,  1.78729433e+00,  1.04315736e+00,\n       -1.44847729e+00,  3.41070754e+00,  2.81655462e+00,  2.88886247e-01,\n        2.61248115e+00, -5.28811327e-01, -2.47391400e+00, -6.04240520e-02,\n       -2.86388739e+00,  2.50495252e+00,  5.34019240e+00,  8.27782165e-01,\n       -2.19088172e+00, -7.82626427e-01, -1.12548033e+00, -2.09109091e+00,\n       -2.06466297e+00, -5.36374068e-01, -3.65861892e+00, -1.42345921e+00,\n       -6.67080354e-01, -2.57114581e+00, -2.37356246e-01, -1.01485014e-02,\n       -3.65219208e+00,  1.30174327e+00,  9.43287089e-01, -5.41965726e-01,\n        1.89596089e+00, -3.26373304e+00, -1.66761926e+00, -1.14963754e+00,\n        4.34701574e-01, -4.87043020e-01, -5.10792557e-01, -9.05609502e-01,\n        3.51588424e-01, -9.72910253e-01, -1.11823422e+00, -8.02920775e-01,\n       -1.51091269e+00,  4.97543437e-01, -8.98957916e-03,  1.47902427e+00,\n       -8.44007525e-01, -5.03900902e-01,  1.26720080e+00, -5.25199252e+00,\n       -3.15857694e+00,  2.43006841e+00, -6.43759610e-01,  1.16296529e+00])\n\n\n\nnp.mean(a)\n\n-0.34664187661644286\n\n\n\nnp.median(a)\n\n-0.5352186588272133\n\n\n\nnp.std(a)\n\n2.0168674618593685\n\n\n\nnp.var(a)\n\n4.0677543587070515\n\n\n- corr matrix, cov matrix\n\nnp.random.seed(43052)\nx=np.random.randn(10000)\ny=np.random.randn(10000)*2\nz=np.random.randn(10000)*0.5\n\n\nnp.corrcoef([x,y,z]).round(2)\n\narray([[ 1.  , -0.01,  0.01],\n       [-0.01,  1.  ,  0.  ],\n       [ 0.01,  0.  ,  1.  ]])\n\n\n\nnp.cov([x,y,z]).round(2)\n\narray([[ 0.99, -0.02,  0.  ],\n       [-0.02,  4.06,  0.  ],\n       [ 0.  ,  0.  ,  0.25]])\n\n\n\n\nnote 6 : dtype\n- np.array는 항상 dtype이 있다.\n\na=np.array([1,2,3])\na\n\narray([1, 2, 3])\n\n\n\na.dtype\n\ndtype('int32')\n\n\n\na=np.array([1.0,2.0,3.0])\na\n\narray([1., 2., 3.])\n\n\n\na.dtype\n\ndtype('float64')\n\n\n\na=1\ntype(a)\n\nint\n\n\n\na=1.0\ntype(a)\n\nfloat\n\n\n- 같은 int라도 int16, int32, int64으로 나누어진다.\n\na= np.array([1,2,3], dtype=np.int64)\na\n\narray([1, 2, 3], dtype=int64)\n\n\n\na= np.array([1,2,3], dtype=np.int32)\na\n\narray([1, 2, 3])\n\n\n\na.dtype\n\ndtype('int32')\n\n\n- float도 float16, float32, float64가 있다.\n\na=np.array([1,2,3],dtype=np.float64) #64는 기본이라 표시가 안된당. \na\n\narray([1., 2., 3.])\n\n\n\na=np.array([1,2,3],dtype=np.float32)\na\n\narray([1., 2., 3.], dtype=float32)\n\n\n- 데이터타입은 아래와 같은 방법으로 변환시킬 수 있다.\n\na = np.array([1,2,3],dtype=np.int32)\na\n\narray([1, 2, 3])\n\n\n\na=a.astype(dtype=np.int64)\n\n\na.dtype\n\ndtype('int64')\n\n\n- 문자열의 경우\n\na= np.array(['a','b','c'])\na\n\narray(['a', 'b', 'c'], dtype='<U1')\n\n\n\na= np.array(['ab','b','c'])\na\n\narray(['ab', 'b', 'c'], dtype='<U2')\n\n\n\na= np.array(['absfd','b','c'])\na\n\narray(['absfd', 'b', 'c'], dtype='<U5')\n\n\n- 문자열+숫자혼합 => 문자열로 통일\n\na=np.array(['a',1])\na\n\narray(['a', '1'], dtype='<U11')\n\n\n\na=np.array(['a',1423])\na\n\narray(['a', '1423'], dtype='<U11')\n\n\n\na=np.array(['a',1.0])\na\n\narray(['a', '1.0'], dtype='<U32')\n\n\n- 숫자를 문자열로 전환:\n\na=np.array([1,2,3])\na\n\narray([1, 2, 3])\n\n\n\na.astype(np.str_)\n\n# 문자열 타입으로 바뀌는\n\narray(['1', '2', '3'], dtype='<U11')\n\n\n\n\nnote 7: 브로드캐스팅과 시간측정\n(예비학습)\n\nimport time\n\n\nt1=time.time()\n\n\nt2=time.time()\nt2-t1\n\n14.808058738708496\n\n\n예비학습끝\n(예제) x=[0,1,2,3,4]인 벡터가 있다고 하자. (i,j)의 원소는 (x[i]-x[j])**2를 의미하는 \\(5\\times5\\) 매트릭스를 구하라..\n(풀이)\n\nx=np.array(range(5))\nx\n\narray([0, 1, 2, 3, 4])\n\n\n\ndist= np.zeros([5,5])\ndist\n\narray([[0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.]])\n\n\n\nfor i in range(5):\n    for j in range(5):\n        dist[i,j] = (x[i]-x[j])**2\n\n\ndist\n\narray([[ 0.,  1.,  4.,  9., 16.],\n       [ 1.,  0.,  1.,  4.,  9.],\n       [ 4.,  1.,  0.,  1.,  4.],\n       [ 9.,  4.,  1.,  0.,  1.],\n       [16.,  9.,  4.,  1.,  0.]])\n\n\n(풀이2)\n\nx1=x.reshape(5,1).astype(dtype=np.float64)\nx2=x.reshape(1,5).astype(dtype=np.float64)\n\n\nx1\n\narray([[0.],\n       [1.],\n       [2.],\n       [3.],\n       [4.]])\n\n\n\nx2\n\narray([[0., 1., 2., 3., 4.]])\n\n\n\nx1-x2\n\narray([[ 0., -1., -2., -3., -4.],\n       [ 1.,  0., -1., -2., -3.],\n       [ 2.,  1.,  0., -1., -2.],\n       [ 3.,  2.,  1.,  0., -1.],\n       [ 4.,  3.,  2.,  1.,  0.]])\n\n\n\n(i,j)th element = x[i] - x[j]\n\n\n(x1-x2)**2\n\narray([[ 0,  1,  4,  9, 16],\n       [ 1,  0,  1,  4,  9],\n       [ 4,  1,  0,  1,  4],\n       [ 9,  4,  1,  0,  1],\n       [16,  9,  4,  1,  0]], dtype=int32)\n\n\n\n\ny=x=np.array(range(10000))\n\n\ndist= np.zeros([10000,10000])\ndist\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\nt1=time.time()\nfor i in range(10000):\n    for j in range(10000):\n        dist[i,j] = (y[i]-y[j])**2\nt2=time.time()\nt2-t1\n\n66.71002793312073\n\n\n\ny1=y.reshape(10000,1).astype(np.float64)\ny2=y.reshape(1,10000).astype(np.float64)\n\n\nt1=time.time()\ndist2=(y1-y2)**2\nt2=time.time()\nt2-t1\n\n0.426450252532959\n\n\n\ndist[:5,:5], dist2[:5,:5]\n\n(array([[ 0.,  1.,  4.,  9., 16.],\n        [ 1.,  0.,  1.,  4.,  9.],\n        [ 4.,  1.,  0.,  1.,  4.],\n        [ 9.,  4.,  1.,  0.,  1.],\n        [16.,  9.,  4.,  1.,  0.]]),\n array([[ 0.,  1.,  4.,  9., 16.],\n        [ 1.,  0.,  1.,  4.,  9.],\n        [ 4.,  1.,  0.,  1.,  4.],\n        [ 9.,  4.,  1.,  0.,  1.],\n        [16.,  9.,  4.,  1.,  0.]]))\n\n\n\n(dist-dist2).sum()\n\n0.0"
  },
  {
    "objectID": "posts/Python/2. Numpy/python 7_0418.html#matplotlib",
    "href": "posts/Python/2. Numpy/python 7_0418.html#matplotlib",
    "title": "파이썬 (0418) 7주차",
    "section": "matplotlib",
    "text": "matplotlib\n\nimport matplotlib.pyplot as plt\n\n\nplt.plot\n- 기본그림\n\nplt.plot([1,2,3],[3,4,5],'.')\n\n\n\n\n\nplt.plot(np.array([1,2,3]),np.array([3,4,5]),'.')\n\n\n\n\n- 예제들\n\nt=np.linspace(-6,6,100)\nt\n\narray([-6.        , -5.87878788, -5.75757576, -5.63636364, -5.51515152,\n       -5.39393939, -5.27272727, -5.15151515, -5.03030303, -4.90909091,\n       -4.78787879, -4.66666667, -4.54545455, -4.42424242, -4.3030303 ,\n       -4.18181818, -4.06060606, -3.93939394, -3.81818182, -3.6969697 ,\n       -3.57575758, -3.45454545, -3.33333333, -3.21212121, -3.09090909,\n       -2.96969697, -2.84848485, -2.72727273, -2.60606061, -2.48484848,\n       -2.36363636, -2.24242424, -2.12121212, -2.        , -1.87878788,\n       -1.75757576, -1.63636364, -1.51515152, -1.39393939, -1.27272727,\n       -1.15151515, -1.03030303, -0.90909091, -0.78787879, -0.66666667,\n       -0.54545455, -0.42424242, -0.3030303 , -0.18181818, -0.06060606,\n        0.06060606,  0.18181818,  0.3030303 ,  0.42424242,  0.54545455,\n        0.66666667,  0.78787879,  0.90909091,  1.03030303,  1.15151515,\n        1.27272727,  1.39393939,  1.51515152,  1.63636364,  1.75757576,\n        1.87878788,  2.        ,  2.12121212,  2.24242424,  2.36363636,\n        2.48484848,  2.60606061,  2.72727273,  2.84848485,  2.96969697,\n        3.09090909,  3.21212121,  3.33333333,  3.45454545,  3.57575758,\n        3.6969697 ,  3.81818182,  3.93939394,  4.06060606,  4.18181818,\n        4.3030303 ,  4.42424242,  4.54545455,  4.66666667,  4.78787879,\n        4.90909091,  5.03030303,  5.15151515,  5.27272727,  5.39393939,\n        5.51515152,  5.63636364,  5.75757576,  5.87878788,  6.        ])\n\n\n\nx=np.sin(t)\ny=np.cos(t)\n\n\nplt.plot(t,x)\n\n\n\n\n\nplt.plot(t,y)\n\n\n\n\n\nplt.plot(t,x)\nplt.plot(t,y)\n\n\n\n\n\nplt.plot(t,x)\nplt.plot(t,y,'.')\n\n\n\n\n\nplt.plot(t,x)\nplt.plot(t,y,'--')\n\n\n\n\n\n\nplt.hist\n\nX=np.random.randn(1000)\n\n\nplt.hist(X)\n\n(array([  3.,  14.,  66., 157., 232., 245., 155.,  92.,  28.,   8.]),\n array([-3.29472542, -2.65210581, -2.0094862 , -1.36686658, -0.72424697,\n        -0.08162736,  0.56099226,  1.20361187,  1.84623148,  2.4888511 ,\n         3.13147071]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n\nY=np.random.rand(1000)\nplt.hist(Y)\n\n(array([ 98., 127., 107.,  87.,  83.,  86.,  85., 118., 110.,  99.]),\n array([0.00162071, 0.10140453, 0.20118836, 0.30097218, 0.40075601,\n        0.50053983, 0.60032366, 0.70010748, 0.79989131, 0.89967513,\n        0.99945896]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n\nplt.hist(X)\nplt.hist(Y)\n\n(array([ 98., 127., 107.,  87.,  83.,  86.,  85., 118., 110.,  99.]),\n array([0.00162071, 0.10140453, 0.20118836, 0.30097218, 0.40075601,\n        0.50053983, 0.60032366, 0.70010748, 0.79989131, 0.89967513,\n        0.99945896]),\n <BarContainer object of 10 artists>)"
  },
  {
    "objectID": "posts/Python/2. Numpy/python 5_0406.html",
    "href": "posts/Python/2. Numpy/python 5_0406.html",
    "title": "파이썬 (0406) 5주차",
    "section": "",
    "text": "!pip install numpy \n\nCollecting numpy\n  Downloading numpy-1.24.1-cp39-cp39-win_amd64.whl (14.9 MB)\n     --------------------------------------- 14.9/14.9 MB 10.7 MB/s eta 0:00:00\nInstalling collected packages: numpy\nSuccessfully installed numpy-1.24.1\n\n\n\nimport numpy as np"
  },
  {
    "objectID": "posts/Python/2. Numpy/python 5_0406.html#넘파이-공부-1단계",
    "href": "posts/Python/2. Numpy/python 5_0406.html#넘파이-공부-1단계",
    "title": "파이썬 (0406) 5주차",
    "section": "넘파이 공부 1단계",
    "text": "넘파이 공부 1단계\n\n선언\n\nlist([1,2,3])\n\n[1, 2, 3]\n\n\n\n[1,2,3]\n\n[1, 2, 3]\n\n\n\na=np.array([1,2,3])  # list만들고 ndarray화 시킴\nl=[1,2,3]\n\n\n\n기본연산 브로드캐스팅\n\na\n\narray([1, 2, 3])\n\n\n\nl\n\n[1, 2, 3]\n\n\n\na+1 ## [1,2,3] + 1 = [2,3,4]\n\narray([2, 3, 4])\n\n\n\nl+1\n\nTypeError: can only concatenate list (not \"int\") to list\n\n\n\na+np.array([-1,-2,-3])\n\narray([0, 0, 0])\n\n\n\na-a\n\narray([0, 0, 0])\n\n\n\nl-l  # 리스트는 안됨\n\nTypeError: unsupported operand type(s) for -: 'list' and 'list'\n\n\n\na*2\n\narray([2, 4, 6])\n\n\n\nl*2\n\n[1, 2, 3, 1, 2, 3]\n\n\n\na/2\n\narray([0.5, 1. , 1.5])\n\n\n\nl/2\n\nTypeError: unsupported operand type(s) for /: 'list' and 'int'\n\n\n\na**2\n\narray([1, 4, 9])\n\n\n\nl**2\n\nTypeError: unsupported operand type(s) for ** or pow(): 'list' and 'int'\n\n\n\na%2   # %2 = 2로 나눈 나머지 리턴\n\narray([1, 0, 1], dtype=int32)\n\n\n\nl%2\n\nTypeError: unsupported operand type(s) for %: 'list' and 'int'\n\n\n\n\n기타 수학연산 지원\n\nnp.sqrt(2)\n\n1.4142135623730951\n\n\n\nnp.sqrt(a), np.sqrt(l)\n\n(array([1.        , 1.41421356, 1.73205081]),\n array([1.        , 1.41421356, 1.73205081]))\n\n\n\nnp.log(a), np.log(l)\n\n(array([0.        , 0.69314718, 1.09861229]),\n array([0.        , 0.69314718, 1.09861229]))\n\n\n\nnp.exp(a), np.exp(l)\n\n(array([ 2.71828183,  7.3890561 , 20.08553692]),\n array([ 2.71828183,  7.3890561 , 20.08553692]))\n\n\n\nnp.sin(a), np.sin(l)\n\n(array([0.84147098, 0.90929743, 0.14112001]),\n array([0.84147098, 0.90929743, 0.14112001]))"
  },
  {
    "objectID": "posts/Python/2. Numpy/python 5_0406.html#넘파이-공부-2단계",
    "href": "posts/Python/2. Numpy/python 5_0406.html#넘파이-공부-2단계",
    "title": "파이썬 (0406) 5주차",
    "section": "넘파이 공부 2단계",
    "text": "넘파이 공부 2단계\n\n인덱싱 1차원\n- 선언\n\nl=[11,22,33,44,55,66]\na=np.array(l)\n\n- 인덱스로 접근\n\nl[0], l[1], l[2], l[3], l[4], l[5]\n\n(11, 22, 33, 44, 55, 66)\n\n\n\na[0], a[1], a[2], a[3], a[4], a[5]\n\n(11, 22, 33, 44, 55, 66)\n\n\n- : 이용 (슬라이싱)\n\nl[2:4] # index 2에서 시작, index 4는 포함하지 않음\n\n[33, 44]\n\n\n\na[2:4]\n\narray([33, 44])\n\n\n- 점수배열에 의한 익덱싱\n\na\n\narray([11, 22, 33, 44, 55, 66])\n\n\n\n  a[[0,2,4]]  # index=0, index=2, index=4 에 해당하는 원소를 뽑고 싶다 -> 가능\n\narray([11, 33, 55])\n\n\n\n l[[0,2,4]]    # 리스트는 불가능\n\nTypeError: list indices must be integers or slices, not list\n\n\n- 부울값에 의한 인덱싱\n\na\n\narray([11, 22, 33, 44, 55, 66])\n\n\n\na[[True, True, False, True, False, False]]\n\narray([11, 22, 44])\n\n\n응용하면?\n\na<33\n\narray([ True,  True, False, False, False, False])\n\n\n\na[[ True,  True, False, False, False, False]]\n\narray([11, 22])\n\n\n\na[a<33]\n\narray([11, 22])\n\n\n리스트는 불가능\n\nl<33\n\nTypeError: '<' not supported between instances of 'list' and 'int'\n\n\n\nl[[True, True, False, True, False, False]]\n\nTypeError: list indices must be integers or slices, not list\n\n\n\n\n인덱싱 2차원\n- 중첩리스트와 2차원 np.array 선언\n\nA = [[1,2,3,4],[-1,-2,-3,-4],[5,6,7,8],[-5,-6,-7,-8]]\nA2 = np.array(A)\n\n\nA2\n\narray([[ 1,  2,  3,  4],\n       [-1, -2, -3, -4],\n       [ 5,  6,  7,  8],\n       [-5, -6, -7, -8]])\n\n\n\nA\n\n[[1, 2, 3, 4], [-1, -2, -3, -4], [5, 6, 7, 8], [-5, -6, -7, -8]]\n\n\n- A의 원소 인덱싱\n\nA[0][0] # A의 (1,1)의 원소\n\n1\n\n\n\nA[1][2] # A의 (2,3)의 원소\n\n-3\n\n\n\nA[-1][0] # A의 (4,1)의 원소\n\n-5\n\n\n- A2의 원소 인덱싱\n\nA2[0][0]\n\n1\n\n\n\nA2[1][2] # A2의 (2,3)의 원소\n\n-3\n\n\n\nA2[-1][0] # A2의 (4,1)의 원소\n\n-5\n\n\n- A2에서만 되는 기술 (넘파이에서 제시하는 신기술, R에서는 기본적으로 쓰던것, 이중list는 불가능)\n\nA2[0,0]\n\n1\n\n\n\nA2[1,2] # A2의 (2,3)의 원소\n\n-3\n\n\n\nA2[-1,0] # A2의 (4,1)의 원소\n\n-5\n\n\n- 정수배열에 의한 인덱싱 & 슬라이싱!\n\nA2\n\narray([[ 1,  2,  3,  4],\n       [-1, -2, -3, -4],\n       [ 5,  6,  7,  8],\n       [-5, -6, -7, -8]])\n\n\n\nA2[0,0:2]   # 1행 1열, 1행 2열\n\narray([1, 2])\n\n\n\nA2[0,:]  # 1행\n\narray([1, 2, 3, 4])\n\n\n\nA2[0]  # 1행\n\narray([1, 2, 3, 4])\n\n\n\nA2[[0,2],:]   # 1행, 3행\n\narray([[1, 2, 3, 4],\n       [5, 6, 7, 8]])\n\n\n\nA2[[0,2]]   # 1행, 3행\n\narray([[1, 2, 3, 4],\n       [5, 6, 7, 8]])\n\n\n\nA2[:,0] # 1열\n\narray([ 1, -1,  5, -5])\n\n\n\nA2[:,[0]] # 1열\n\narray([[ 1],\n       [-1],\n       [ 5],\n       [-5]])\n\n\n\nA2[:,[0,2]] # 1열, 3열\n\narray([[ 1,  3],\n       [-1, -3],\n       [ 5,  7],\n       [-5, -7]])\n\n\n\nA2[0:2,[0,2]]  # 1행-2행 / 1열-3열\n\narray([[ 1,  3],\n       [-1, -3]])\n\n\n\n\n1차원 배열의 선언\n- 리스트나 튜플을 선언하고 형변환\n\nnp.array((1,2,3)) # 튜플->넘파이어레이\n\narray([1, 2, 3])\n\n\n\nnp.array([1,2,3]) # 리스트->넘파이어레이\n\narray([1, 2, 3])\n\n\n- range()를 이용해서 선언하고 형변환\n\nnp.array(range(10))  # range(10)->넘파이어레이\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n- np.zeros, np.ones\n\nnp.zeros(3)\n\narray([0., 0., 0.])\n\n\n\nnp.ones(4)\n\narray([1., 1., 1., 1.])\n\n\n- np.linspace\n\nnp.linspace(0,1,12)   # 0부터 1까지 12개로 쪼개기   (양끝점 모두 포함)\n\narray([0.        , 0.09090909, 0.18181818, 0.27272727, 0.36363636,\n       0.45454545, 0.54545455, 0.63636364, 0.72727273, 0.81818182,\n       0.90909091, 1.        ])\n\n\n\nlen(np.linspace(0,1,12))\n\n12\n\n\n- np.arange\n\nnp.arange(5)  #np.array(range(5))\n\narray([0, 1, 2, 3, 4])\n\n\n\nnp.arange(1,6)   #np.array(range(1,6))\n\narray([1, 2, 3, 4, 5])\n\n\n\n\nreshape\n- reshape: ndarray의 특수한 기능\n\na=np.array([11,22,33,44,55,66])\na  #길이가 6인 벡터\n\narray([11, 22, 33, 44, 55, 66])\n\n\n\ntype(a)\n\nnumpy.ndarray\n\n\n\na.reshape\n\n<function ndarray.reshape>\n\n\n\na.reshape(2,3)  # (2,3) matrix라고 생각해도 무방\n\narray([[11, 22, 33],\n       [44, 55, 66]])\n\n\n\na.reshape(5,2)\n\nValueError: cannot reshape array of size 6 into shape (5,2)\n\n\nnote: reshape은 a자체를 변홧키는 것은 아님\n\na  # reshape은 a자체는 변화하지 않음\n\narray([11, 22, 33, 44, 55, 66])\n\n\n\nb=a.reshape(2,3)  # a를 reshape한 결과를 b에 저장\nb\n\narray([[11, 22, 33],\n       [44, 55, 66]])\n\n\n\n a  # a는 여전히 그대로 있음\n\narray([11, 22, 33, 44, 55, 66])\n\n\n- 다시 b를 a처럼 바꾸고 싶다.\n\nb\n\narray([[11, 22, 33],\n       [44, 55, 66]])\n\n\n\nb.reshape(6) # b는 (2,3) matrix, 그런데 이것을 길이가 6인 벡터로 만들고 싶다.\n\narray([11, 22, 33, 44, 55, 66])\n\n\n\na.shape   # 길이가 1인 튜플\n\n(6,)\n\n\n\nb.shape   # 길이가 2인 튜플이니까 2차원 \n\n(2, 3)\n\n\n- reshape with -1\n\na=np.arange(24)  #np.array(range(24))\na\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23])\n\n\n\na.reshape(2,?) # 에러..\n\nSyntaxError: invalid syntax (2529973538.py, line 1)\n\n\n\na.reshape(2,-1)\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])\n\n\n\na.reshape(3,-1)\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7],\n       [ 8,  9, 10, 11, 12, 13, 14, 15],\n       [16, 17, 18, 19, 20, 21, 22, 23]])\n\n\n\na.reshape(4,-1)\n\narray([[ 0,  1,  2,  3,  4,  5],\n       [ 6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17],\n       [18, 19, 20, 21, 22, 23]])\n\n\n\na.reshape(7,-1)  # 나눠떨어지지 않으니까.. \n\nValueError: cannot reshape array of size 24 into shape (7,newaxis)\n\n\n\nb=a.reshape(12,-1)\nb\n\narray([[ 0,  1],\n       [ 2,  3],\n       [ 4,  5],\n       [ 6,  7],\n       [ 8,  9],\n       [10, 11],\n       [12, 13],\n       [14, 15],\n       [16, 17],\n       [18, 19],\n       [20, 21],\n       [22, 23]])\n\n\n\nb.reshape(24) # b를 다시 길이가 24인 벡터로 만들고 싶다.\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23])\n\n\n\nb.reshape(-1) # b를 다시 길이가 24인 벡터로 만들고 싶다.\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23])\n\n\n\n\n2차원 배열의 선언\n\n\nnp.zeros((3,3))\n\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n\n\n\nnp.ones((3,3))\n\narray([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])\n\n\n\nnp.eye(3) \n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\n\nnp.diag([1,2,3,-1])\n\narray([[ 1,  0,  0,  0],\n       [ 0,  2,  0,  0],\n       [ 0,  0,  3,  0],\n       [ 0,  0,  0, -1]])\n\n\n\n\n랜덤으로 생성\n\nnp.random.randn(10)  # 표준정규분포에서 10개를 뽑음\n\narray([-1.62694735, -0.46057632,  0.9092888 , -0.52150285, -0.0409467 ,\n        0.98561001,  1.87613924, -2.08870029,  0.28577046, -0.15794105])\n\n\n\nnp.random.rand(10)  # 0~1 사이에서 10개를 뽑음\n\narray([0.7377278 , 0.61091057, 0.17571601, 0.29298532, 0.90149596,\n       0.84002052, 0.50700681, 0.40217981, 0.30557984, 0.34392417])\n\n\n\nnp.random.randn(4).reshape(2,2)  # 표준정규분포에서 4개를 뽑고 (2,2) nparray로 형태변환\n\narray([[0.50093512, 0.74336071],\n       [0.91296027, 0.04033486]])\n\n\n\nnp.random.rand(4).reshape(2,2)  # 0~1에서 4개를 뽑고 (2,2) nparray로 형태변환\n\narray([[0.30484011, 0.57731961],\n       [0.30645542, 0.2189475 ]])\n\n\n\n\n행렬\n\nA=np.array(range(4)).reshape(2,2)\nA\n\narray([[0, 1],\n       [2, 3]])\n\n\n\nA.T #전치행렬\n\narray([[0, 2],\n       [1, 3]])\n\n\n\nnp.linalg.inv(A)   # 역행렬\n\narray([[-1.5,  0.5],\n       [ 1. ,  0. ]])\n\n\n\nA @ np.linalg.inv(A)  # 단위행렬   # @는 행렬곱을 수행\n\narray([[1., 0.],\n       [0., 1.]])"
  },
  {
    "objectID": "posts/Python/2. Numpy/python 5_0406.html#숙제",
    "href": "posts/Python/2. Numpy/python 5_0406.html#숙제",
    "title": "파이썬 (0406) 5주차",
    "section": "숙제",
    "text": "숙제\n\nA=np.array(range(6))\nA # 길이가 6인 벡터\n\narray([0, 1, 2, 3, 4, 5])\n\n\n위와 같이 길이가 6인 벡터 A를 (2,3) ndarray로 변경\n\nA.reshape(2,3)\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n\nlen(A.reshape(2,3))\n\n2"
  },
  {
    "objectID": "posts/Python/2. Numpy/python 5_0404.html",
    "href": "posts/Python/2. Numpy/python 5_0404.html",
    "title": "파이썬 (0404) 5주차",
    "section": "",
    "text": "소스코드 관리(모듈, 패키지, 라이브러리)\nintro\n- 현재 파이썬은 길이가 2인 벡터의 덧셈을 지원하지 않음\n\na=[1,2]\nb=[3,4]\na+b\n\n[1, 2, 3, 4]\n\n\n- 아래와 같은 기능을 구현하는 함수를 만들고 싶음\n[1,2],[3,4] -> [4,6]\n- 구현\n\ndef vec2_add(a,b):\n    return [a[0]+b[0], a[1]+b[1]]\n\n- test\n\na=[1,2]\nb=[3,4]\n\n\nvec2_add(a,b)\n\n[4, 6]\n\n\nmake myfuns.py\n- 생각해보니까 vec2_add는 내가 앞으로 자주 쓸 기능임\n- 그런데 현재 사용방법으로는 내가 노트북파일을 새로 만들때마다 def vec2_add(a,b):와 같은 함수를 매번 정의해줘야 하는 불편함이 있다.\n해결1\n- 자주 사용하는 함수를 myfuns.py에 저장한다. (4주차 수업)\n# myfuns.py\ndef vec2_add(a,b):\n    return [a[0]+b[0], a[1]+b[1]]\n%run myfuns를 실행\n준비:“00” -> 커널재시작\n\n# \n\n\n%run myfuns\n\n\nvec2_add([1,2],[3,4])\n\n[4, 6]\n\n\n해결 2\n- 자주 사용하는 함수를 myfuns.py에 저장한다. (4주차 수업)\n# myfuns.py\ndef vec2_add(a,b):\n    return [a[0]+b[0], a[1]+b[1]]\n- import myfuns를 이용\n(준비) “00” -> 커널 재시작\n\nimport myfuns\n\n\na=[1,2]\nb=[3,4]\nmyfuns.vec2_add(a,b)\n\n[4, 6]\n\n\n\nimport 기본\n-사용방법1\n준비: “00” -> 커널재시작\n\nimport myfuns\n\n\nmyfuns.vec2_add([1,2],[3,4])\n\n[4, 6]\n\n\n\nmyfuns.vec2_add의 의미: myfuns.py라는 파일안에 vec2_add라는 함수가 있음. 그것을 실행하라.\n.의 의미: 상위, 하위의 개념!\n\n(주의) 아래와 같이 사용 불가능 하다.\n\nvec2_add([1,2],[3,4])  #myfuns가 import가 된거지 vec2Add가 import가 된 것이 아님.\n\nNameError: name 'vec2_add' is not defined\n\n\n- 사용방법2\n준비: “00” -> 커널재시작\n\nfrom myfuns import vec2_add\n\n\nvec2_add([1,2],[3,4])\n\n[4, 6]\n\n\n\nmyfuns.vec2_add([1,2],[3,4])  # myfuns안의 vec2_add만 임포트했지 myfuns자체를 임포트 한것은 아님..\n\nNameError: name 'myfuns' is not defined\n\n\n- 사용방법3\n준비: “00” -> 커널재시작\n\nimport myfuns\nfrom myfuns import vec2_add\n\n\nmyfuns.vec2_add([1,2],[3,4])\n\n[4, 6]\n\n\n\nvec2_add([1,2],[3,4])\n\n[4, 6]\n\n\n\nvec2_sub([1,2],[3,4])   # vec2_sub는 import하지 않았기 때문에 오류남.. \n\nNameError: name 'vec2_sub' is not defined\n\n\n\nmyfuns.vec2_sub([1,2],[3,4])\n\n[-2, -2]\n\n\n- 사용방법4\n준비: “00” -> 커널재시작\n\nfrom myfuns import vec2_add, vec2_sub\n\n\nvec2_add([1,2],[3,4]), vec2_sub([1,2],[3,4])\n\n([4, 6], [-2, -2])\n\n\n- 사용방법5\n준비: “00” -> 커널재시작\n\nfrom myfuns import *   # *는 all의 의미\n\n\nvec2_add([1,2],[3,4]), vec2_sub([1,2],[3,4])\n\n([4, 6], [-2, -2])\n\n\n- 사용방법6\n준비: “00” -> 커널재시작\n\nimport myfuns as mf \n\n\nmf.vec2_add([1,2],[3,4])\n\n[4, 6]\n\n\n\nmf.vec2_sub([1,2],[3,4])\n\n[-2, -2]\n\n\n(오히려 아래는 실행불가능)\n\nmyfuns.vec2_add([1,2],[3,4])\n\nNameError: name 'myfuns' is not defined\n\n\n- 잘못된 사용방법1\n준비: “00” -> 커널재시작\n\nimport myfuns as mf\nfrom mf import vec2_add\n\nModuleNotFoundError: No module named 'mf'\n\n\n- 사용방법 7\n준비: “00” -> 커널재시작\n\nimport myfuns as mf\nfrom myfuns import vec2_add as add\n\n\nmf.vec2_add([1,2],[3,4])\n\n[4, 6]\n\n\n\nvec2_add([1,2],[3,4])   # 위에서 vec2_add를 add로 부르기로 했음. 그래서 이건 안뎀 \n\nNameError: name 'vec2_add' is not defined\n\n\n\nadd([1,2],[3,4])\n\n[4, 6]\n\n\n\n\n도움말 작성기능\n- mf란 무엇인가?\n준비: “00” -> 커널재시작\n\nimport myfuns as mf\n\n\nmf\n\n<module 'myfuns' from 'C:\\\\Users\\\\koinu\\\\python2022\\\\myfuns.py'>\n\n\n\nmf?\n\n\nType:        module\nString form: <module 'myfuns' from 'C:\\\\Users\\\\koinu\\\\python2022\\\\myfuns.py'>\nFile:        c:\\users\\koinu\\python2022\\myfuns.py\nDocstring:   <no docstring>\n\n\n\n\ntype(mf)\n\nmodule\n\n\n\nmf의 타입은 모듈이라고 나옴, 현재 단계에서는 무엇인지 알기 어려움..\n\n- Docstring의 내용을 채울 수 있을까?\n준비1: myfuns.py의 파일을 수정한다. (큰따옴표 ““” 세개)\n준비2: “00”->커널재시작\n\nimport myfuns as mf\n\n\nmf?\n\n\nType:        module\nString form: <module 'myfuns' from 'C:\\\\Users\\\\koinu\\\\python2022\\\\myfuns.py'>\nFile:        c:\\users\\koinu\\python2022\\myfuns.py\nDocstring:   이것은 길이가 2인 벡터이 합 혹은 차를 구하는 모듈입니다.\n\n\n\n\n\n주의점\n- myfuns.py는 최초 한번만 import된다.\n준비: “00” -> 커널재시작\n\nimport myfuns\n\n\nmyfuns.vec2_add([1,2],[3,4])\n\n[4, 6]\n\n\nmyfuns.py파일을 열고 함수를 바꾸자.\n\"\"\"이것은 길이가 2인 벡터이 합 혹은 차를 구하는 모듈입니다.\"\"\"\n\ndef vec2_add(a,b):\n    print(\"이것은 myfuns.py에 정의된 함수입니다.\")\n    return [a[0]+b[0], a[1]+b[1]]\n\n\ndef vec2_sub(a,b):\n    return [a[0]-b[0], a[1]-b[1]]\n다시 myfuns를 로드하고 위를 실행하여 보자\n\nimport myfuns\n\n\nmyfuns.vec2_add([1,2],[3,4])\n\n[4, 6]\n\n\n바뀐내용이 적용되지 않는다.\n커널을 다시 시작하고 임포트해보자.\n“00” -> 커널재시작\n\nimport myfuns\n\n\nmyfuns.vec2_add([1,2],[3,4])\n\n이것은 myfuns.py에 정의된 함수입니다.\n\n\n[4, 6]\n\n\n- myfuns.py는 주피너노트북파일과 같은 폴더내에 존재해야 한다.\n준비1: “00”->커널재시작\n준비2: myfuns.py 을 복사하여 다른 폴더로 이동. 예를들면 IP0403폴더를 만들고 그 폴더안에 myfuns2.py파일을 만들자.\n\nimport myfuns  # 주피터노트북과 같은 폴더에 있는 myfuns는 잘 로드되지만\n\n\nimport myfuns2 # 주피터노트북과 다른 폴더에 있는 myfuns2는 그렇지 않다.\n\nModuleNotFoundError: No module named 'myfuns2'\n\n\n- IP0403 폴더에 있는 myfuns2.py를 실행하기 위해서는 아래와 같이 할 수 있다.\n준비: “00” -> 커널재시작\n\nfrom IP0403 import myfuns2\n\n\nmyfuns2.vec2_add([1,2],[3,4])\n\n이것은 myfuns2.py에 정의된 함수입니다.\n\n\n[4, 6]\n\n\n- 아래도 가능하다.\n준비: “00” -> 커널재시작\n\nfrom IP0403.myfuns2 import vec2_add as add\n\n\nadd([1,2],[3,4])\n\n이것은 myfuns2.py에 정의된 함수입니다.\n\n\n[4, 6]\n\n\n참고로 아래는 모두 정의되지 않음\n\nIP0403.myfuns2.vec2_add([1,2],[3,4])\n\nNameError: name 'IP0403' is not defined\n\n\n\nmyfuns2.vec2_add([1,2],[3,4])\n\nNameError: name 'myfuns2' is not defined\n\n\n\nvec2_add([1,2],[3,4])\n\nNameError: name 'vec2_add' is not defined\n\n\n\n\nimport 고급\n\n폴더와 함께 사용할시\n- 언뜻 생각하면 아래가 가능할 것 같다.\nimport IP0403\nIP0403.myfuns.vec2_add([1,2],[3,4])\n- 하지만 불가능\n준비: “00” -> 커널재시작\n\nimport IP0403\n\n여기까지는 됨..\n\nIP0403.myfuns2.add([1,2],[3,4])\n\nAttributeError: module 'IP0403' has no attribute 'myfuns2'\n\n\n\n여기서 불가능하다.\n\n- (암기) IP0403 폴더안에 __init__.py라는 파일을 만들고 내용에 아래와 같이 쓰면 가능하다.\n# ./IP0403/__init__.py\nform. import myfuns2\n준비1: 위의 지침을 따른다.\n준비2: “00” -> 커널재시작\n\nimport IP0403\n\n\nIP0403.myfuns2.vec2_add([1,2],[3,4])  \n\n이것은 myfuns2.py에 정의된 함수입니다.\n\n\n[4, 6]\n\n\n컴퓨터 상식\n\n. : 현재폴더를 의미\n.. : 상위폴더를 의미\n./myfuns.py : 현재폴더안에 있는 myfuns.py를 의미\n./IP0403/myfuns2.py : 현재폴더안에 IP0403폴더안의 myfuns2.py를 의미\n../myfuns.py : 현재폴더보다 한단계 상위폴더에 있는 myfuns.py를 의미\ncd ./IP0403 : 현재 폴더안에 있는 IP0403폴더로 이동해라. (cd IP0403으로 줄여쓸 수 있음)\ncd .. 현재 폴더보다 한단계 상위폴더로 이동해라.\n\n따라서 from . import myfuns2는 현재 폴더에서 myfuns2를 찾아서 임포트 하라는 의미로 해석가능\n- 의미상으로 보면 아래가 실해아능할 것 같은데 불가능하다.\n\n# import myfuns.py\nfrom . import myfuns\n\nImportError: attempted relative import with no known parent package\n\n\n\n\n\nslite-packages (실습금지)\nhttps://guebin.github.io/IP2022/2022/04/03/(5%EC%A3%BC%EC%B0%A8)-4%EC%9B%942%EC%9D%BC.html#site-packages-(%EC%8B%A4%EC%8A%B5%EA%B8%88%EC%A7%80)\n\n\n모듈, 패키지, 라이브러리?\n- 모듈의 개념은 아까 살펴본 것과 같다. (import를 하여 생기게 되는 오브젝트)\n- 교수님들: 모듈이 모이면 패키지라고 부른다. 그리고 라이브러리는 패키지보다 큰 개념이다.\n-그런데 구분이 모호하다.\n\nimport numpy as np   # 오잉 왜 안되지... \n\nModuleNotFoundError: No module named 'numpy'\n\n\n\ntype(np)\n\nNameError: name 'np' is not defined\n\n\n- python 에서 numpy의 type은 모듈\n- 그런데 numpy package라고 검색하면 검색이 된다.\n- 심지어 numpy library 라고 해도 검색가능\n- 교수님 생각: 넘파이모듈, 넘파이패키지, 넘파이라이브러리 다 맞는 말임\n(숙제)\n\nimport myfuns\n\n\nmyfuns.vec2_add([1,2],[5,6])\n\n이것은 myfuns.py에 정의된 함수입니다.\n\n\n[6, 8]\n\n\n\nmyfuns?\n\n\nType:        module\nString form: <module 'myfuns' from 'C:\\\\Users\\\\koinu\\\\python2022\\\\myfuns.py'>\nFile:        c:\\users\\koinu\\python2022\\myfuns.py\nDocstring:   이것은 길이가 2인 벡터이 합 혹은 차를 구하는 모듈입니다. 202250926"
  },
  {
    "objectID": "posts/Python/2. Numpy/python 7_0413.html",
    "href": "posts/Python/2. Numpy/python 7_0413.html",
    "title": "파이썬 (0413) 7주차",
    "section": "",
    "text": "import numpy as np"
  },
  {
    "objectID": "posts/Python/2. Numpy/python 7_0413.html#numpy공부-5단계-랜덤모듈",
    "href": "posts/Python/2. Numpy/python 7_0413.html#numpy공부-5단계-랜덤모듈",
    "title": "파이썬 (0413) 7주차",
    "section": "numpy공부 5단계 : 랜덤모듈",
    "text": "numpy공부 5단계 : 랜덤모듈\n\nnp.random.rand()\n- 0~1사이에서 10개의 난수 생성\n\nnp.random.rand(10)\n\narray([0.30133684, 0.33047977, 0.37682904, 0.34945581, 0.88634262,\n       0.272207  , 0.75103749, 0.55871507, 0.12304257, 0.88020941])\n\n\n- 0~2사이에서 10개의 난수 생성\n\nnp.random.rand(10)*2\n\narray([1.85950286, 0.90618509, 0.3153    , 0.47472741, 1.60545103,\n       1.07072774, 1.10650141, 0.77505785, 1.19933414, 1.76222208])\n\n\n- 1~2사이에서 10개의 난수 생성\n\nnp.random.rand(10)+1\n\narray([1.01747795, 1.52789889, 1.29223002, 1.53147587, 1.13455031,\n       1.51668185, 1.2430438 , 1.59676278, 1.8731811 , 1.36113831])\n\n\n- 1~3사이에서 10개의 난수 생성\n\nnp.random.rand(10)*2+1    # 1~3\n\narray([2.79324839, 2.37177079, 1.12638737, 1.71767497, 2.95057073,\n       1.23158048, 2.56688411, 2.94392262, 1.32675882, 2.29817471])\n\n\n\n\nnp.random.randn()\n- N(0,1) 에서 10개 추출\n\nnp.random.randn(10) # 표준정규분포에서 10개의 샘플 추출\n\narray([ 1.895967  , -0.26215342,  0.87906492,  0.45616171,  1.66244424,\n        0.72458419,  0.31057676, -0.55909889,  0.47656554,  0.35143513])\n\n\n- N(1,1)에서 10개 추출\n\nnp.random.randn(10)+1\n\narray([ 1.11007188, -0.44321876,  0.04904333, -0.10478302,  0.13301967,\n       -0.49468263,  1.7751611 , -0.84760291,  0.40840343,  0.638133  ])\n\n\n- N(0,4)에서 10개 추출 (평균이 0이고 분산이 4인 분포)\n\nnp.random.randn(10)*2\n\narray([-1.14364925,  2.60415043, -1.65488974, -0.59463897,  0.97607708,\n        2.33979589,  3.49290763, -1.50749403, -1.41447157,  0.45852112])\n\n\n- N(3,4)에서 10개 추출\n\nnp.random.randn(10)*2+3\n\narray([ 4.736406  ,  2.35419865,  2.8265146 ,  0.26470966, -0.4240817 ,\n        1.00836216,  6.23531314,  3.75134991,  0.60427655, -0.13645246])\n\n\n\n\nnp.random.randint()\n- [0,7)의 범위에서 하나의 정수를 랜덤으로 생성\n\nnp.random.randint(7)   #[0,7)의 범위에서 하나의 정수 생성\n\n3\n\n\n- [0,7)의 범위에서 20개의 정수를 랜덤으로 생성\n\nnp.random.randint(7,size=(20,))  # [0,7)의 범위에서 20개의 정수 생성\n\narray([2, 5, 5, 5, 2, 4, 3, 4, 1, 4, 1, 2, 3, 2, 4, 2, 2, 6, 1, 2])\n\n\n- [0,7)의 범우에서 (5,5) shape으로 정수를 랜덤으로 생성\n\nnp.random.randint(7,size=(5,5))  \n\narray([[1, 6, 2, 5, 0],\n       [3, 3, 3, 1, 0],\n       [4, 0, 2, 5, 6],\n       [1, 0, 1, 2, 0],\n       [6, 6, 5, 1, 3]])\n\n\n- 위와 같은 코드를 아래와 같이 구현가능\n\nnp.random.randint(low=7,size=(2,2))  # [0,7)의 범위에서 20개의 정수 생성\n\narray([[2, 4],\n       [6, 4]])\n\n\n- [10,20)의 범위에서 (5,5) shape 정수를 랜덤으로 생성\n\nnp.random.randint(low=10, high=20,size=(5,5))  \n\narray([[14, 19, 17, 14, 17],\n       [16, 11, 14, 17, 16],\n       [12, 11, 18, 17, 14],\n       [11, 15, 14, 18, 11],\n       [13, 19, 10, 17, 14]])\n\n\n- 의문: np.random.randint(low=7,size=(5,5)) 가 좀 이상하다. 사실 np.random.randint(high=7,size=(5,5))가 되어야 맞지 않는가?\n-> 저도 그렇게 생각하긴 하는데요, 구현이 이렇게 되어있습니다. 도움말 확인!\nReturn random integers from the \"discrete uniform\" distribution of the specified dtype in the \"half-open\" interval [`low`, `high`). If `high` is None (the default), then results are from [0, `low`).\n\n\nnp.random.choice()\n- ver1\n\nnp.random.choice(5,20)  # [0,5)에서 20개를 뽑음, 중복허용\n\narray([3, 2, 0, 3, 3, 3, 0, 0, 2, 2, 0, 0, 0, 1, 3, 1, 3, 2, 0, 0])\n\n\n\nnp.random.randint(5, size=(20,))\n\narray([3, 2, 2, 3, 4, 1, 0, 1, 4, 1, 3, 2, 2, 2, 4, 3, 2, 4, 2, 3])\n\n\n- ver2\n\nnp.random.choice([0,1,2,3],20) # [0,1,2,3] 에서 20개를 뽑음 , 중복허용\n\narray([2, 0, 1, 3, 1, 1, 2, 2, 1, 2, 0, 1, 1, 0, 1, 0, 2, 3, 3, 1])\n\n\n\nnp.random.choice([\"apple\",\"orange\",\"banana\"],20)\n\narray(['orange', 'banana', 'banana', 'orange', 'banana', 'orange',\n       'banana', 'orange', 'apple', 'orange', 'orange', 'apple', 'apple',\n       'orange', 'apple', 'apple', 'orange', 'orange', 'apple', 'apple'],\n      dtype='<U6')\n\n\n\nnp.random.choice([\"apple\",\"orange\",\"banana\"],2,replace=False) # 중복허용 X \n\narray(['apple', 'orange'], dtype='<U6')\n\n\n\n\n통계분포\n\nnp.random.binomial(n=10, p=0.1, size=(5,)) #X1, ..., X5 ~ B(10,0.2)\n\narray([1, 0, 2, 0, 2])\n\n\n\nnp.random.normal(loc=10,scale=2,size=(5,)) # X1, ..., X5 ~ N(10,4) \n\narray([8.5617943 , 8.9716337 , 7.90650741, 6.59782362, 7.90620931])\n\n\n\nnp.radom.randn(5)*2 + 10와 같은코드\n\n\nnp.random.uniform(low=2,high=4,size=(5,)) # X1, ..., X5 ~ U(2,4)  #균일분포\n\narray([2.49501161, 3.10469251, 3.89920656, 2.33160764, 2.28406983])\n\n\n\nnp.random.rand(5)*2+2와 같은 코드\n\n\nnp.random.poisson(lam=5,size=(5,)) # X1,...,X5 ~ Poi(5) \n\narray([5, 5, 7, 4, 6])"
  },
  {
    "objectID": "posts/Python/2. Numpy/python 7_0413.html#nupmy공부-6단계-기타-유용한-기본기능들",
    "href": "posts/Python/2. Numpy/python 7_0413.html#nupmy공부-6단계-기타-유용한-기본기능들",
    "title": "파이썬 (0413) 7주차",
    "section": "nupmy공부 6단계: 기타 유용한 기본기능들",
    "text": "nupmy공부 6단계: 기타 유용한 기본기능들\n\nnp.where, np.argwhere\n\na=np.array([0,0,0,1,0])\na\n\narray([0, 0, 0, 1, 0])\n\n\n\nnp.where(a==1) # 조건 a==1을 만족하는 인덱스를 출력하라\n\n(array([3], dtype=int64),)\n\n\n\nnp.argwhere(a==1)\n\narray([[3]], dtype=int64)\n\n\n\nnp.argwhere(a==0)\n\narray([[0],\n       [1],\n       [2],\n       [4]], dtype=int64)\n\n\n- 2차원\n\nnp.random.seed(43052)\na=np.random.randn(12).reshape(3,4)\na\n\narray([[ 0.38342049,  1.0841745 ,  1.14277825,  0.30789368],\n       [ 0.23778744,  0.35595116, -1.66307542, -1.38277318],\n       [-1.92684484, -1.4862163 ,  0.00692519, -0.03488725]])\n\n\n\nnp.where(a<0) # 조건을 만족하는 인덱스가 (1,2), (1,3), (2,0), (2,1), (2,3) 이라는 의미\n\n(array([1, 1, 2, 2, 2], dtype=int64), array([2, 3, 0, 1, 3], dtype=int64))\n\n\n\nnp.argwhere(a<0)  # 조건을 만족하는 인덱스가 (1,2), (1,3), (2,0), (2,1), (2,3) 이라는 의미\n\narray([[1, 2],\n       [1, 3],\n       [2, 0],\n       [2, 1],\n       [2, 3]], dtype=int64)\n\n\n\na[np.where(a<0)]  # 조건을 만족하는 인덱스가 모두 출력=> 1차원 array로 출력\n\narray([-1.66307542, -1.38277318, -1.92684484, -1.4862163 , -0.03488725])\n\n\n\na[np.argwhere(a<0)]  # 출력불가능\n\nIndexError: index 3 is out of bounds for axis 0 with size 3\n\n\n\na[np.argwhere(a<0)[0][0],np.argwhere(a<0)[0][1]] # 어거지로 출력할수는 있음 \n\n-1.6630754187023522\n\n\n- np.where의 특수기능\n\nnp.random.seed(43052)\na=np.random.randn(12).reshape(3,4)\na\n\narray([[ 0.38342049,  1.0841745 ,  1.14277825,  0.30789368],\n       [ 0.23778744,  0.35595116, -1.66307542, -1.38277318],\n       [-1.92684484, -1.4862163 ,  0.00692519, -0.03488725]])\n\n\n\nnp.where(a<0,0,a)   #a<0을 체크=> 조건에 맞으면 0, 조건에 안맞으면 a 출력\n\narray([[0.38342049, 1.0841745 , 1.14277825, 0.30789368],\n       [0.23778744, 0.35595116, 0.        , 0.        ],\n       [0.        , 0.        , 0.00692519, 0.        ]])\n\n\n\nnp.where(a<0,0,1) # #a<0을 체크=> 조건에 맞으면 0, 조건에 안맞으면 1 출력\n\narray([[1, 1, 1, 1],\n       [1, 1, 0, 0],\n       [0, 0, 1, 0]])\n\n\n- 요약 - np.where : 인덱스의 좌표를 읽는 가독성은 떨어짐. 그러나 조건에 맞는 원소를 출력하거나 조건에 맞는 특수기능을 처리하는 목적으로 좋은 함수 - np.argwhere : 인덱스의 좌표를 읽는 가독성은 좋은 편임. 그러나 조건에 맞는 원소를 출력하거나 처리하는 기능은 떨어짐\n\n\n인덱싱고급\n- 원래 a는 2d array\n\na=np.arange(12).reshape(3,4)\na\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n\n\n- 경우 1: 인덱싱 결과가 1d array로 나올 수 있음\n\na[0,:] # 인덱싱의 결과 축의 갯수가 바뀐다. 2d array -> 1d array\n\narray([0, 1, 2, 3])\n\n\n- 경우2: 인덱싱 결과가 2d array로 나올 수 있음\n\na[[0,1],:] # 이것은 축의 숫자가 유지됨 2d array-> 2d array\n\narray([[0, 1, 2, 3],\n       [4, 5, 6, 7]])\n\n\n- 경우1의 상황에서도 축의 갯수를 유지하면서 인덱싱하려면?\n\na[[0],:] # 인덱싱의 결과 축의 갯수가 유지된다. 2d array->2d array\n\narray([[0, 1, 2, 3]])\n\n\n- 미묘한 차이를 이해할 것\n\na[:,0], a[:,[0]]\n\n(array([0, 4, 8]),\n array([[0],\n        [4],\n        [8]]))\n\n\n\n\nnp.ix_\n- 아래의 인덱싱을 비교하자\n\na=np.arange(12).reshape(3,4)\na\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n\n\n\na[0:2,0:2]\n\narray([[0, 1],\n       [4, 5]])\n\n\n\na[[0,1],0:2]\n\narray([[0, 1],\n       [4, 5]])\n\n\n\na[0:2,[0,1]]\n\narray([[0, 1],\n       [4, 5]])\n\n\n- 언뜻 생각하면 위의 결과와 a[[0,1],[0,1]의 결과가 동일할 것 같다.\n\na[[0,1],[0,1]]\n\narray([0, 5])\n\n\n\n실제로는 [a[0,0],a[1,1]]이 array로 나옴\n\n- 사실 np.where에서 이미 관찰하였음\n\na\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n\n\n\nnp.where(a % 5 ==0)\n\n(array([0, 1, 2], dtype=int64), array([0, 1, 2], dtype=int64))\n\n\n\na[np.where(a % 5 ==0)]\n\narray([ 0,  5, 10])\n\n\n\na[[0, 1, 2],[0, 1, 2]]\n\narray([ 0,  5, 10])\n\n\n- a[[0,1],[0,1]]이 a[0:2,0:2]를 의미하게 하려면 아래와 같이 하면 된다.\n\na[np.ix_([0,1],[0,1])] # 유용해보이지만 생각보다 잘 쓰이는건 아님 \n\narray([[0, 1],\n       [4, 5]])\n\n\n(숙제)\n\nnp.random.uniform(low=1.3,high=1.7,size=(10,))\n\narray([1.65411132, 1.42531485, 1.54567744, 1.44735207, 1.33217747,\n       1.48856969, 1.47329978, 1.38976795, 1.30469965, 1.66634909])\n\n\n위와 같은코드를 np.random.rand()를 이용하여 구현하라."
  },
  {
    "objectID": "posts/Python/2. Numpy/python 6_0411.html",
    "href": "posts/Python/2. Numpy/python 6_0411.html",
    "title": "파이썬 (0411) 6주차",
    "section": "",
    "text": "imports\n\nimport numpy as np\n\n\n\nnumpy공부 3단계: 차원\n\n2차원 배열과 연립 1차 방정식\n- 아래의 연립방정식 고려\n\\(\\begin{cases} y+z+w = 3 \\\\ x+z+w = 3 \\\\ x+y+w = 3 \\\\ x+y+z = 3 \\end{cases}\\)\n- 행렬표현?\n\\(\\begin{bmatrix} 0 & 1 & 1 & 1 \\\\ 1 & 0 & 1 & 1 \\\\ 1 & 1 & 0 & 1 \\\\ 1 & 1 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ w \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 3 \\\\ 3 \\\\ 3 \\end{bmatrix}\\)\n- 풀이\n\nA = np.array([[0,1,1,1],[1,0,1,1],[1,1,0,1],[1,1,1,0]])\nA\n\narray([[0, 1, 1, 1],\n       [1, 0, 1, 1],\n       [1, 1, 0, 1],\n       [1, 1, 1, 0]])\n\n\n\nb= np.array([3,3,3,3]).reshape(4,1)\nb\n\narray([[3],\n       [3],\n       [3],\n       [3]])\n\n\n\nnp.linalg.inv(A) @ b \n\narray([[1.],\n       [1.],\n       [1.],\n       [1.]])\n\n\n- 다른풀이\nb를 아래와 같이 만들어도 된다.\n\nb=np.array([3,3,3,3])\nb\n\narray([3, 3, 3, 3])\n\n\n\nb.shape # b.shape은 길이가 1인 튜플로 나온다. \n\n(4,)\n\n\n\nnp.linalg.inv(A) @ b \n\narray([1., 1., 1., 1.])\n\n\n\n\n@의 유연성\n- 엄밀하게는 아래의 행렬곱이 가능하다. - (2,2) @ (2,1) => (2,1) - (1,2) @ (2,2) => (1,2)\n\nA = np.array([1,2,3,4]).reshape(2,2) \nb = np.array([1,2]).reshape(2,1) \nA@b\n\narray([[ 5],\n       [11]])\n\n\n\nA.shape, b.shape, (A@b).shape\n\n((2, 2), (2, 1), (2, 1))\n\n\n\nA = np.array([1,2,3,4]).reshape(2,2) \nb = np.array([1,2]).reshape(1,2) \nb@A \n\narray([[ 7, 10]])\n\n\n\nA.shape, b.shape, (b@A).shape\n\n((2, 2), (1, 2), (1, 2))\n\n\n- 당연히 아래는 성립안한다.\n\nA = np.array([1,2,3,4]).reshape(2,2) \nb = np.array([1,2]).reshape(2,1) \nb@A\n\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 1)\n\n\n\nA = np.array([1,2,3,4]).reshape(2,2) \nb = np.array([1,2]).reshape(1,2) \nA@b\n\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)\n\n\n- 아래는 어떨까? 계산가능할까? \\(\\to\\) 모두 계산가능! - (2,) @ (2,2) = (2,) - (2,2) @ (2,) = (2,)\n\nA = np.array([1,2,3,4]).reshape(2,2)\nb = np.array([1,2]) \nA@b\n\narray([ 5, 11])\n\n\n\nA.shape, b.shape, (A@b).shape \n\n((2, 2), (2,), (2,))\n\n\n\nb를 마치 (2,1)처럼 해석하여 행렬곱하고 결과는 다시 (2,) 로 만든것 같다.\n\n\nb@A\n\narray([ 7, 10])\n\n\n\nA.shape, b.shape, (b@A).shape \n\n((2, 2), (2,), (2,))\n\n\n\n이때는 \\(b\\)를 마치 (1,2)처럼 해석하여 행렬곱하고 결과는 다시 (2,)로 만든것 같다.\n\n- 아래는 어떠할까?\n\nb1 = np.array([1,2,3,4]) \nb2 = np.array([1,2,3,4]) \nb1@b2 \n\n30\n\n\n\nb1.shape, b2.shape, (b1@b2).shape \n\n((4,), (4,), ())\n\n\n\n(1,4) @ (4,1) = (1,1) 로 생각\n\n- 즉 위는 아래와 같이 해석하고 행렬곱한것과 결과가 같다.\n\nb1 = np.array([1,2,3,4]).reshape(1,4) \nb2 = np.array([1,2,3,4]).reshape(4,1) \nb1@b2 \n\narray([[30]])\n\n\n\nb1.shape, b2.shape, (b1@b2).shape \n\n((1, 4), (4, 1), (1, 1))\n\n\n- 때로는 (4,1) @ (1,4)와 같은 계산결과를 얻고 싶을 수 있는데 이때는 차원을 명시해야함\n\nb1 = np.array([1,2,3,4]).reshape(4,1) \nb2 = np.array([1,2,3,4]).reshape(1,4) \nb1@b2 \n\narray([[ 1,  2,  3,  4],\n       [ 2,  4,  6,  8],\n       [ 3,  6,  9, 12],\n       [ 4,  8, 12, 16]])\n\n\n\n\n차원\n- 넘파이배열의 차원은 .shape 으로 확인가능\n- 아래는 모두 미묘하게 다르다.\n\na=np.array(3.14) # 스칼라, 0d array \na, a.shape\n\n(array(3.14), ())\n\n\n\na=np.array([3.14]) # 벡터, 1d array \na, a.shape\n\n(array([3.14]), (1,))\n\n\n\na=np.array([[3.14]]) # 매트릭스, 2d array \na, a.shape\n\n(array([[3.14]]), (1, 1))\n\n\n\na=np.array([[[3.14]]]) # 텐서, 3d array \na, a.shape\n\n(array([[[3.14]]]), (1, 1, 1))\n\n\n\n\n\nnumpy공부 4단계: 축\n\nnp.concatenate\n- 기본예제\n\na=np.array([1,2]) \nb=-a\n\n\nnp.concatenate([a,b]) \n\narray([ 1,  2, -1, -2])\n\n\n- 응용\n\na=np.array([1,2])\nb=-a \nc=np.array([3,4,5])\n\n\nnp.concatenate([a,b,c])\n\narray([ 1,  2, -1, -2,  3,  4,  5])\n\n\n\n여기까진 딱히 칸캐터네이트의 메리트가 없어보임\n리스트였다면 a+b+c 하면 되는 기능이니까?\n\n- 2d array에 적용해보자.\n\na=np.arange(4).reshape(2,2) \nb=-a\n\n\nnp.concatenate([a,b]) \n\narray([[ 0,  1],\n       [ 2,  3],\n       [ 0, -1],\n       [-2, -3]])\n\n\n- 옆으로 붙일려면?\n\nnp.concatenate([a,b],axis=1)\n\narray([[ 0,  1,  0, -1],\n       [ 2,  3, -2, -3]])\n\n\n- 위의 코드에서 axis=1 이 뭐지? axis=0,2 등을 치면 결과가 어떻게 될까?\n\nnp.concatenate([a,b],axis=0)\n\narray([[ 0,  1],\n       [ 2,  3],\n       [ 0, -1],\n       [-2, -3]])\n\n\n\n이건 그냥 np.concatenate([a,b])와 같다.\nnp.concatenate([a,b])는 np.concatenate([a,b],axis=0)의 생략버전이군?\n\n\nnp.concatenate([a,b],axis=2)\n\nAxisError: axis 2 is out of bounds for array of dimension 2\n\n\n\n이런건 없다.\n\n- axis의 의미가 뭔지 궁금함. 좀 더 예제를 살펴보자.\n\na=np.array(range(2*3*4)).reshape(2,3,4)\na\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n\n\n\nb=-a\nb\n\narray([[[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\nnp.concatenate([a,b],axis=0) \n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\nnp.concatenate([a,b],axis=1) \n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11],\n        [  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23],\n        [-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\nnp.concatenate([a,b],axis=2) \n\narray([[[  0,   1,   2,   3,   0,  -1,  -2,  -3],\n        [  4,   5,   6,   7,  -4,  -5,  -6,  -7],\n        [  8,   9,  10,  11,  -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15, -12, -13, -14, -15],\n        [ 16,  17,  18,  19, -16, -17, -18, -19],\n        [ 20,  21,  22,  23, -20, -21, -22, -23]]])\n\n\n\n이번에는 axis=2까지 된다?\n\n\nnp.concatenate([a,b],axis=3) \n\nAxisError: axis 3 is out of bounds for array of dimension 3\n\n\n\naxis=3까지는 안된다?\n\n- 뭔가 나름의 방식으로 합쳐지는데 원리가 뭘까?\n(분석1) np.concatenate([a,b],axis=0)\n\na=np.array(range(2*3*4)).reshape(2,3,4) \nb=-a \n\n\na.shape, b.shape, np.concatenate([a,b],axis=0).shape\n\n((2, 3, 4), (2, 3, 4), (4, 3, 4))\n\n\n\n첫번째차원이 바뀌었다 => 첫번째 축이 바뀌었다 => axis=0 (파이썬은 0부터 시작하니까!)\n\n(분석2) np.concatenate([a,b],axis=1)\n\na=np.array(range(2*3*4)).reshape(2,3,4) \nb=-a \n\n\na.shape, b.shape, np.concatenate([a,b],axis=1).shape\n\n((2, 3, 4), (2, 3, 4), (2, 6, 4))\n\n\n\n두번째차원이 바뀌었다 => 두번째 축이 바뀌었다 => axis=1\n\n(분석3) np.concatenate([a,b],axis=2)\n\na=np.array(range(2*3*4)).reshape(2,3,4) \nb=-a \n\n\na.shape, b.shape, np.concatenate([a,b],axis=2).shape\n\n((2, 3, 4), (2, 3, 4), (2, 3, 8))\n\n\n\n세번째차원이 바뀌었다 => 세번째 축이 바뀌었다 => axis=2\n\n(분석4) np.concatenate([a,b],axis=3)\n\na=np.array(range(2*3*4)).reshape(2,3,4) \nb=-a \n\n\na.shape, b.shape, np.concatenate([a,b],axis=3).shape\n\nAxisError: axis 3 is out of bounds for array of dimension 3\n\n\n\n네번째차원이 없다 => 네번째 축이 없다 => axis=3으로 하면 에러가 난다.\n\n(보너스1)\n\na=np.array(range(2*3*4)).reshape(2,3,4) \nb=-a \n\n\nnp.concatenate([a,b],axis=-1)\n\narray([[[  0,   1,   2,   3,   0,  -1,  -2,  -3],\n        [  4,   5,   6,   7,  -4,  -5,  -6,  -7],\n        [  8,   9,  10,  11,  -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15, -12, -13, -14, -15],\n        [ 16,  17,  18,  19, -16, -17, -18, -19],\n        [ 20,  21,  22,  23, -20, -21, -22, -23]]])\n\n\n\na.shape, b.shape, np.concatenate([a,b],axis=-1).shape\n\n((2, 3, 4), (2, 3, 4), (2, 3, 8))\n\n\n\n마지막 차원이 바뀌었다 => 마지막 축이 바뀌었다 => axis = -1\n\n(보너스2)\n\na=np.array(range(2*3*4)).reshape(2,3,4) \nb=-a \n\n\nnp.concatenate([a,b],axis=-2)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11],\n        [  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23],\n        [-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\na.shape, b.shape, np.concatenate([a,b],axis=-2).shape\n\n((2, 3, 4), (2, 3, 4), (2, 6, 4))\n\n\n\n마지막에서 2번째 차원이 바뀌었다 => 마지막에서 2번째 축이 바뀌었다 => axis = -2\n\n(보너스3)\n\na=np.array(range(2*3*4)).reshape(2,3,4) \nb=-a \n\n\nnp.concatenate([a,b],axis=-3)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\na.shape, b.shape, np.concatenate([a,b],axis=-3).shape\n\n((2, 3, 4), (2, 3, 4), (4, 3, 4))\n\n\n\n마지막에서 3번째 차원이 바뀌었다 => 마지막에서 3번째 축이 바뀌었다 => axis = -3\n\n(보너스3)\n\na=np.array(range(2*3*4)).reshape(2,3,4) \nb=-a \n\n\nnp.concatenate([a,b],axis=-4)\n\nAxisError: axis -4 is out of bounds for array of dimension 3\n\n\n\n마지막에서 4번째 차원은 없다 => 마지막에서 4번째 축이 없다 => axis = -4는 에러가 난다.\n\n- 0차원은 축이 없으므로 concatenate를 쓸 수 없다.\n\na= np.array(1)\nb= np.array(-1) \n\n\na.shape, b.shape\n\n((), ())\n\n\n\nnp.concatenate([a,b])\n\nValueError: zero-dimensional arrays cannot be concatenated\n\n\n- 꼭 a,b가 같은 차원일 필요는 없다.\n\na=np.array(range(4)).reshape(2,2) \nb=np.array(range(2)).reshape(2,1)  \n\n\nnp.concatenate([a,b],axis=1)\n\narray([[0, 1, 0],\n       [2, 3, 1]])\n\n\n\na.shape, b.shape, np.concatenate([a,b],axis=1).shape\n\n((2, 2), (2, 1), (2, 3))\n\n\n\n\nnp.stack\n- 혹시 아래가 가능할까?\n\n(3,) 결합 (3,) => (3,2)\n\n\na=np.array([1,2,3])\nb=-a\n\n\na,b\n\n(array([1, 2, 3]), array([-1, -2, -3]))\n\n\n\nnp.concatenate([a,b],axis=1)\n\nAxisError: axis 1 is out of bounds for array of dimension 1\n\n\n\n불가능\n\n- 아래와 같이 하면 해결가능\n\na=np.array([1,2,3]).reshape(3,1) \nb=-a\n\n\na,b\n\n(array([[1],\n        [2],\n        [3]]),\n array([[-1],\n        [-2],\n        [-3]]))\n\n\n\nnp.concatenate([a,b],axis=1)\n\narray([[ 1, -1],\n       [ 2, -2],\n       [ 3, -3]])\n\n\n\n분석: (3) (3) => (3,1) (3,1) => (3,1) concat (3,1)\n\n- 위의 과정을 줄여서 아래와 같이 할 수 있다.\n\na=np.array([1,2,3])\nb=-a\n\n\nnp.stack([a,b],axis=1)\n\narray([[ 1, -1],\n       [ 2, -2],\n       [ 3, -3]])\n\n\n- 아래도 가능\n\nnp.stack([a,b],axis=0)\n\narray([[ 1,  2,  3],\n       [-1, -2, -3]])\n\n\n- 분석해보고 외우자\n(분석1)\n\na=np.array([1,2,3])\nb=-a\n\n\na.shape, b.shape, np.stack([a,b],axis=0).shape\n\n((3,), (3,), (2, 3))\n\n\n\n\n\n=> 첫 위치에 축을 추가 (axis=0) => (1,3) (1,3) => (2,3)\n\n\n\n(분석2)\n\na=np.array([1,2,3])\nb=-a\n\n\na.shape, b.shape, np.stack([a,b],axis=1).shape\n\n((3,), (3,), (3, 2))\n\n\n\n\n\n=> 두 위치에 축을 추가 (axis=1) => (3,1) (3,1) => (3,2)\n\n\n\n- 고차원예제\n\na=np.arange(3*4*5).reshape(3,4,5) \nb=-a\n\n\na.shape, b.shape\n\n((3, 4, 5), (3, 4, 5))\n\n\n\nnp.stack([a,b],axis=0).shape # (3,4,5) => (1,3,4,5) // 첫 위치에 축이 추가되고 스택 \n\n(2, 3, 4, 5)\n\n\n\nnp.stack([a,b],axis=1).shape # (3,4,5) => (3,1,4,5) // 두번째 위치에 축이 추가되고 스택 \n\n(3, 2, 4, 5)\n\n\n\nnp.stack([a,b],axis=2).shape # (3,4,5) => (3,4,1,5) // 세번째 위치에 축이 추가되고 스택 \n\n(3, 4, 2, 5)\n\n\n\nnp.stack([a,b],axis=3).shape # (3,4,5) => (3,4,5,1) // 네번째 위치에 축이 추가되고 스택 \n\n(3, 4, 5, 2)\n\n\n\nnp.stack([a,b],axis=-1).shape # axis=-1 <=> axis=3 \n\n(3, 4, 5, 2)\n\n\n\nnp.stack([a,b],axis=-2).shape # axis=-2 <=> axis=2\n\n(3, 4, 2, 5)\n\n\nnp.concatenate 는 축의 총 갯수를 유지하면서 결합, np.stack은 축의 갯수를 하나 증가시키면서 결합\n\n\nsum\n- 1차원\n\na = np.array([1,2,3]) \na\n\narray([1, 2, 3])\n\n\n\na.sum()\n\n6\n\n\n\na.sum(axis=0)\n\n6\n\n\n- 2차원\n\na=np.array(range(6)).reshape(2,3)\na\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n\na.sum() # 전체합\n\n15\n\n\n\na.sum(axis=0) \n\narray([3, 5, 7])\n\n\n\na.sum(axis=1) \n\narray([ 3, 12])\n\n\n- 2차원 결과 분석\n\na.shape, a.sum(axis=0).shape\n\n((2, 3), (3,))\n\n\n\n첫번째 축이 삭제됨 => axis=0\n\n\na.shape, a.sum(axis=1).shape\n\n((2, 3), (2,))\n\n\n\n두번째 축이 삭제됨 => axis=1\n\n- 연습\n\na=np.array(range(10)).reshape(5,2) \na\n\narray([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9]])\n\n\n(문제1) 1열의 합, 2열의 합을 계산하고 싶다면?\n(풀이) 차원이 (5,2) => (2,) 로 나와야 한다. (그럼 첫번째 축이 삭제되어야 하네?)\n\na.sum(axis=0)\n\narray([20, 25])\n\n\n(문제2) 1행의 합, 2행의 합, … , 5행의 합을 계산하고 싶다면?\n(풀이) 차원이 (5,2) => (5,)로 나와야 한다. (그럼 두번째 축이 삭제되어야 하네?)\n\na.sum(axis=1)\n\narray([ 1,  5,  9, 13, 17])\n\n\n(문제3) a의 모든원소의 합을 계산하고 싶다면?\n(풀이) 차원이 (5,2) => () 로 나와야 한다. (첫번째축, 두번째축이 모두 삭제되어야 하네?)\n\na.sum(axis=(0,1))\n\n45\n\n\n\na.sum() # 즉 a.sum(axis=(0,1))이 디폴트값임 \n\n45\n\n\n\n\nmean, std, max, min, prod\n- 모두 sum이랑 유사한 논리\n\na=np.array(range(10)).reshape(5,2)\na\n\narray([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9]])\n\n\n\na.mean(axis=0), a.std(axis=0), a.max(axis=0), a.min(axis=0), a.prod(axis=0)\n\n(array([4., 5.]),\n array([2.82842712, 2.82842712]),\n array([8, 9]),\n array([0, 1]),\n array([  0, 945]))\n\n\n\na.mean(axis=1), a.std(axis=1), a.max(axis=1), a.min(axis=1), a.prod(axis=1)\n\n(array([0.5, 2.5, 4.5, 6.5, 8.5]),\n array([0.5, 0.5, 0.5, 0.5, 0.5]),\n array([1, 3, 5, 7, 9]),\n array([0, 2, 4, 6, 8]),\n array([ 0,  6, 20, 42, 72]))\n\n\n- 참고로 std는 분포를 n으로 나눈다.\n\na=np.array([1,2,3,4])\na.std()\n\n1.118033988749895\n\n\n\nnp.sqrt(sum((a-a.mean())**2)/4)\n\n1.118033988749895\n\n\n- 분모를 n-1로 나눌려면?\n\na=np.array([1,2,3,4])\na.std(ddof=1)\n\n1.2909944487358056\n\n\n\nnp.sqrt(sum((a-a.mean())**2)/3)\n\n1.2909944487358056\n\n\n\n\nargmax, argmin\n- 1차원\n\na= np.array([1,-2,3,10,4])\na\n\narray([ 1, -2,  3, 10,  4])\n\n\n\na.argmax() # 가장 큰 값이 위치한 원소의 인덱스를 리턴 \n\n3\n\n\n\na.argmin() # 가장 작은 값이 위치한 원소의 인덱스를 리턴 \n\n1\n\n\n- 2차원\n\nnp.random.seed(43052)\na=np.random.randn(4*5).reshape(4,5)\na\n\narray([[ 0.38342049,  1.0841745 ,  1.14277825,  0.30789368,  0.23778744],\n       [ 0.35595116, -1.66307542, -1.38277318, -1.92684484, -1.4862163 ],\n       [ 0.00692519, -0.03488725, -0.34357323,  0.70895648, -1.55100608],\n       [ 1.34565583, -0.05654272, -0.83017342, -1.46395159, -0.35459593]])\n\n\n\na.argmin(), a.min()\n\n(8, -1.9268448358915802)\n\n\n\na.argmax(), a.max()\n\n(15, 1.3456558341738827)\n\n\n\na.argmin(axis=0), a.argmin(axis=1)\n\n(array([2, 1, 1, 1, 2]), array([4, 3, 4, 3]))\n\n\n\na.argmax(axis=0), a.argmax(axis=1)\n\n(array([3, 0, 0, 2, 0]), array([2, 0, 3, 0]))\n\n\n\n\ncumsum, cumprod\n- 1차원\n\na=np.array([1,2,3,4])\na\n\narray([1, 2, 3, 4])\n\n\n\na.cumsum()\n\narray([ 1,  3,  6, 10])\n\n\n\na.cumprod()\n\narray([ 1,  2,  6, 24])\n\n\n- 2차원\n\na=np.array(range(3*4)).reshape(3,4)\na\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n\n\n\na.cumsum(axis=0), a.cumsum(axis=1)\n\n(array([[ 0,  1,  2,  3],\n        [ 4,  6,  8, 10],\n        [12, 15, 18, 21]]),\n array([[ 0,  1,  3,  6],\n        [ 4,  9, 15, 22],\n        [ 8, 17, 27, 38]]))\n\n\n\na.cumprod(axis=0), a.cumprod(axis=1)\n\n(array([[  0,   1,   2,   3],\n        [  0,   5,  12,  21],\n        [  0,  45, 120, 231]]),\n array([[   0,    0,    0,    0],\n        [   4,   20,  120,  840],\n        [   8,   72,  720, 7920]]))\n\n\n\n\ndiff\n- 1차차분\n\na=np.array([1,2,4,6,7])\na\n\narray([1, 2, 4, 6, 7])\n\n\n\nnp.diff(a)\n\narray([1, 2, 2, 1])\n\n\n- 2차차분\n\nnp.diff(np.diff(a))\n\narray([ 1,  0, -1])\n\n\n- prepend, append\n\na=np.array([1,2,4,6,7])\na\n\narray([1, 2, 4, 6, 7])\n\n\n\nnp.diff(a,prepend=100)\n#np.diff(np.array([100]+a.tolist()) )\n\narray([-99,   1,   2,   2,   1])\n\n\n\n[1,2,4,6,7] -> [100,1,2,3,4,6] -> np.diff\n\n\nnp.diff(a,append=100)\n#np.diff(np.array(a.tolist()+[100]) )\n\narray([ 1,  2,  2,  1, 93])\n\n\n(예제) a=[1,2,4,6,7]의 앞에 1을 추가하여 차분하라.\n\nnp.diff(a,prepend=a[0])\n#np.diff(a,prepend=1)\n\narray([0, 1, 2, 2, 1])\n\n\n(예제) a=[1,2,4,6,7]의 뒤에 7을 추가하여 차분하라.\n\nnp.diff(a,append=a[-1])\n#np.diff(a,append=7)\n\narray([1, 2, 2, 1, 0])\n\n\n- 2차원 array의 차분\n\na=np.arange(24).reshape(4,6)\na\n\narray([[ 0,  1,  2,  3,  4,  5],\n       [ 6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17],\n       [18, 19, 20, 21, 22, 23]])\n\n\n\nnp.diff(a,axis=0) \n\narray([[6, 6, 6, 6, 6, 6],\n       [6, 6, 6, 6, 6, 6],\n       [6, 6, 6, 6, 6, 6]])\n\n\n\nnp.diff(a,axis=1) \n\narray([[1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1]])\n\n\n(숙제)\n\na=np.arange(24).reshape(4,6)\na\n\narray([[ 0,  1,  2,  3,  4,  5],\n       [ 6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17],\n       [18, 19, 20, 21, 22, 23]])\n\n\n에서 axis=1 옵션으로 np.diff를 적용하여 (4,5) array를 만들고 왼쪽열에 1이 포함된 column을 추가하여 최종 결과가 아래와 같이 되도록 하라.\narray([[1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1]])"
  },
  {
    "objectID": "posts/Applied statistics/CH0304.html",
    "href": "posts/Applied statistics/CH0304.html",
    "title": "2. SLR실습",
    "section": "",
    "text": "library(ggplot2)\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\nAttaching package: ‘zoo’\n\n\nThe following objects are masked from ‘package:base’:\n\n    as.Date, as.Date.numeric\n\n\n\n\n\ndt <- data.frame(x = c(4,8,9,8,8,12,6,10,6,9),\n                 y = c(9,20,22,15,17,30,18,25,10,20))\ndt\n\n\n\nA data.frame: 10 × 2\n\n    xy\n    <dbl><dbl>\n\n\n     4 9\n     820\n     922\n     815\n     817\n    1230\n     618\n    1025\n     610\n     920\n\n\n\n\n\ncor(dt$x, dt$y) #상관계수\n\n0.921812343945765\n\n\n\n산점도 그리기\n\n\nplot(y~x,\n     data=dt,\n     lxab=\"광고료\",\n     ylab=\"총판매액\",\n     pch = 16,\n     cex = 2,\n     col = \"darkorange\")\n\nWarning message in plot.window(...):\n“\"lxab\" is not a graphical parameter”\nWarning message in plot.xy(xy, type, ...):\n“\"lxab\" is not a graphical parameter”\nWarning message in axis(side = side, at = at, labels = labels, ...):\n“\"lxab\" is not a graphical parameter”\nWarning message in axis(side = side, at = at, labels = labels, ...):\n“\"lxab\" is not a graphical parameter”\nWarning message in box(...):\n“\"lxab\" is not a graphical parameter”\nWarning message in title(...):\n“\"lxab\" is not a graphical parameter”\n\n\n\n\n\n\nggplot(dt, aes(x, y)) +\n  geom_point(col='darkorange', size=3) +    \n  xlab(\"광고료\")+ylab(\"총판매액\")+\n  theme_bw() +\n  theme(axis.title = element_text(size = 14))\n\n\n\n\n- 단순선형회귀모형 \\[y= β_0 + β_1 x + ϵ\\]\n- 추정된 회귀직선\n\\[\\widehat{y}=\\widehat{E(y|X=x)}=\\widehat{\\beta_0}+\\widehat{\\beta_1}x\\]\n- 모형 적합\n\nlm(formula, data)\nformula: \\(y=f(x)\\), \\(y\\)(반응변수) ~ \\(x\\)(설명변수)\n\n\nmodel1 <- lm(y~x, dt)\n\n- 직접계산\n\\[\\widehat{\\beta_0} = \\bar y - \\widehat{\\beta_1} + \\bar x \\]\n\\[\\widehat{\\beta_1} = \\dfrac{S_{xy}}{S_{xx}}\\]\n\nSxy <-sum((dt$x - mean(dt$x))*(dt$y - mean(dt$y)))\nSxx <- sum((dt$x - mean(dt$x))^2)\nSxy/Sxx\n\n2.60869565217391\n\n\n\nsummary(model1)\n\n\nCall:\nlm(formula = y ~ x, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.600 -1.502  0.813  1.128  4.617 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -2.2696     3.2123  -0.707 0.499926    \nx             2.6087     0.3878   6.726 0.000149 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.631 on 8 degrees of freedom\nMultiple R-squared:  0.8497,    Adjusted R-squared:  0.831 \nF-statistic: 45.24 on 1 and 8 DF,  p-value: 0.0001487\n\n\n- summary\n\n제일 먼저 F검정의 p-value값을 확인하자.\n\\(H_0\\): \\(\\beta_1=0\\) vs \\(H_1\\): not \\(H_0\\)\n\n\n\nnames(model1)\n\n\n'coefficients''residuals''effects''rank''fitted.values''assign''qr''df.residual''xlevels''call''terms''model'\n\n\n\nmodel1$coefficients\n\n(Intercept)-2.2695652173913x2.60869565217391\n\n\n- 아래 값들은 모두 동일한 값을 리턴한다.\n\\[\\widehat y = \\widehat \\beta_0 + \\widehat \\beta_1 x\\]\n\nmodel1$fitted.values\n\n18.16521739130434218.6321.2086956521739418.6518.6629.0347826086957713.3826086956522823.8173913043478913.38260869565221021.2086956521739\n\n\n\ncoef(model1)[1] + coef(model1)[2]*dt$x\n\n\n8.1652173913043518.621.208695652173918.618.629.034782608695713.382608695652223.817391304347813.382608695652221.2086956521739\n\n\n\nfitted(model1)\n\n18.16521739130434218.6321.2086956521739418.6518.6629.0347826086957713.3826086956522823.8173913043478913.38260869565221021.2086956521739\n\n\n\nfitted.values(model1) \n\n18.16521739130434218.6321.2086956521739418.6518.6629.0347826086957713.3826086956522823.8173913043478913.38260869565221021.2086956521739\n\n\n- 아래 값들은 모두 동일한 값을 리턴한다.\n\\[e = y- \\widehat y\\]\n\nmodel1$residuals\n\n10.83478260869565621.430.7913043478260874-3.65-1.660.9652173913043574.6173913043478281.182608695652179-3.3826086956521810-1.20869565217391\n\n\n\ndt$y - model1$fitted.values\n\n10.83478260869565621.430.7913043478260884-3.65-1.660.9652173913043574.6173913043478281.182608695652179-3.3826086956521810-1.20869565217391\n\n\n\nresid(model1)\n\n10.83478260869565621.430.7913043478260874-3.65-1.660.9652173913043574.6173913043478281.182608695652179-3.3826086956521810-1.20869565217391\n\n\n\n\n\nanova(model1)\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x1313.04348313.04347845.240340.0001486582\n    Residuals8 55.35652  6.919565      NA          NA\n\n\n\n\n\na <- summary(model1)\n\n\nls(a)\n\n\n'adj.r.squared''aliased''call''coefficients''cov.unscaled''df''fstatistic''r.squared''residuals''sigma''terms'\n\n\n\na$r.squared\n\n0.849737997450786\n\n\n\na$adj.r.squared\n\n0.830955247132134\n\n\n\na$fstatistic\n\nvalue45.2403393025447numdf1dendf8\n\n\n\na$coef ## 회귀계수의 유의성 검정\n\n\n\nA matrix: 2 × 4 of type dbl\n\n    EstimateStd. Errort valuePr(>|t|)\n\n\n    (Intercept)-2.2695653.212348-0.70651290.4999255886\n    x 2.6086960.387847 6.72609390.0001486582\n\n\n\n\n\na$coef[,2] # s.e\n\n(Intercept)3.21234769191659x0.387847045641519\n\n\n\n\n\n\nconfint(model1, level=0.95)\n\n\n\nA matrix: 2 × 2 of type dbl\n\n    2.5 %97.5 %\n\n\n    (Intercept)-9.6772525.138122\n    x 1.7143193.503073\n\n\n\n\n\\[\\widehat \\beta \\pm t_{\\alpha/2}(n-2) s.e(\\widehat \\beta)\\]\n\nqt(0.025,8)\n\n-2.30600413520417\n\n\n\nqt(0.975,8) #왼쪽에 있는거 t_alpha/2 (n-2)\n\n2.30600413520417\n\n\n\ncoef(model1) + qt(0.975, 8) * summary(model1)$coef[,2]\n\n(Intercept)5.1381218438819x3.50307254324997\n\n\n\ncoef(model1) - qt(0.975, 8) * summary(model1)$coef[,2]\n\n(Intercept)-9.6772522786645x1.71431876109785\n\n\n\n\n\n\\[y=\\beta_1 x + \\epsilon\\]\n\nmodel2 <- lm(y ~ 0 + x, dt)\nsummary(model2)\n\n\nCall:\nlm(formula = y ~ 0 + x, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0641 -1.5882  0.2638  1.4818  3.9359 \n\nCoefficients:\n  Estimate Std. Error t value Pr(>|t|)    \nx   2.3440     0.0976   24.02  1.8e-09 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.556 on 9 degrees of freedom\nMultiple R-squared:  0.9846,    Adjusted R-squared:  0.9829 \nF-statistic: 576.8 on 1 and 9 DF,  p-value: 1.798e-09\n\n\n\nsummary(model1)$r.squared\nsummary(model2)$r.squared\n\n0.849737997450786\n\n\n0.984636756628312\n\n\n\n절편이 없는 회귀모형의 R스퀘어가 더 크므로 model2가 더 좋은 것 같다.\n\n\nanova(model2)\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x13769.18953769.1895576.81381.79763e-09\n    Residuals9  58.8105   6.5345      NA         NA\n\n\n\n\n\n\n\n\ndt1 <- data.frame(\n  i = 1:nrow(dt),\n  x = dt$x,\n  y = dt$y,\n  x_barx = dt$x - mean(dt$x),\n  y_bary = dt$y - mean(dt$y))\n\n\ndt1$x_barx2 <- dt1$x_barx^2\ndt1$y_bary2 <- dt1$y_bary^2\ndt1$xy <-dt1$x_barx * dt1$y_bary\n\n\ndt1\n\n\n\nA data.frame: 10 × 8\n\n    ixyx_barxy_baryx_barx2y_bary2xy\n    <int><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n     1 4 9-4-9.616 92.1638.4\n     2 820 0 1.4 0  1.96 0.0\n     3 922 1 3.4 1 11.56 3.4\n     4 815 0-3.6 0 12.96 0.0\n     5 817 0-1.6 0  2.56 0.0\n     61230 411.416129.9645.6\n     7 618-2-0.6 4  0.36 1.2\n     81025 2 6.4 4 40.9612.8\n     9 610-2-8.6 4 73.9617.2\n    10 920 1 1.4 1  1.96 1.4\n\n\n\n\n\nround(colSums(dt1),3)\n\ni55x80y186x_barx0y_bary0x_barx246y_bary2368.4xy120\n\n\n- 직접계산\n\\[\\widehat{\\beta_0} = \\bar y - \\widehat{\\beta_1} + \\bar x \\]\n\\[\\widehat{\\beta_1} = \\dfrac{S_{xy}}{S_{xx}}\\]\n\nbeta1 <- as.numeric(colSums(dt1)[8]/colSums(dt1)[6])\n\nbeta0 <- mean(dt$y) - beta1 *  mean(dt$x)\n\n\ncat(\"hat beta0 = \", beta0)\n\nhat beta0 =  -2.269565\n\n\n\ncat(\"hat beta1 = \", beta1)\n\nhat beta1 =  2.608696\n\n\n\n\n\n\nplot(y~x, data = dt,\n     xlab = \"광고료\",\n     ylab = \"총판매액\",\n     pch  = 20,\n     cex  = 2,\n     col  = \"darkorange\",\n     ylim = c(0,35),\n     xlim = c(0, 12))\nabline(model1, col='steelblue', lwd=2)\nabline(model2, col='violet', lwd=2)\n\n\n\n\n\nco <- coef(model1)\nco_2 <- coef(model2) \n\n\nggplot(dt, aes(x, y)) +\n   geom_point(col='steelblue', lwd=3) +\n   geom_abline(intercept = co[1], slope = co[2], col='darkorange', lwd=1) +\n   geom_abline(intercept = 0, slope = co_2, col='darkgreen', lwd=1) +\n   xlab(\"광고료\")+ylab(\"총판매액\")+\n   theme_bw()+\n   theme(axis.title = element_text(size = 16))\n\nWarning message in geom_point(col = \"steelblue\", lwd = 3):\n“Ignoring unknown parameters: `linewidth`”\n\n\n\n\n\n\n\n\n\nbb <- summary(model1)$sigma *\n  sqrt( 1 + 1/10 + (dt$x - 8)^2/46) ## 개별 y에 대한 추정량의 표준오차\ndt$ma95y <- model1$fitted + 2.306*bb\ndt$mi95y <- model1$fitted - 2.306*bb\n\n\nqt(0.975,8)\n\n2.30600413520417\n\n\n\nggplot(dt, aes(x=x, y=y)) +\n  geom_point() +\n  geom_smooth(method=lm,\n              color=\"red\", fill=\"#69b3a2\", se=TRUE)+\n  geom_line(aes(x,mi95y), col='darkgrey', lty=2)+\n  geom_line(aes(x,ma95y), col='darkgrey', lty=2) +\n  theme_bw() +\n  theme(axis.title = element_blank())\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\\[E(Y|x_0)\\]\n\\[y=E(Y|x_0)+\\epsilon\\]\n\n\\(x_0\\)=4.5\n\n\nnew_dt <- data.frame(x = 4.5)\n\n\\[\\widehat \\mu_0 = \\widehat y_0 = \\widehat \\beta_0 + \\widehat \\beta_1 4.5\\]\n\nmodel1$coefficients[1] + model1$coefficients[2]*4.5\n\n(Intercept): 9.46956521739131\n\n\n\npredict(model1, newdata = new_dt)\n\n1: 9.46956521739131\n\n\n\npredict(model1, \n        newdata = new_dt,\n        interval = c(\"confidence\"),  #구간추정\n        level = 0.95)  ##평균반응\n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    19.4695655.7982613.14087\n\n\n\n\n\npredict(model1, newdata = new_dt, \n        interval = c(\"prediction\"),  \n        level = 0.95)  ## 개별 y\n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    19.4695652.37912516.56001\n\n\n\n\n\ndt_pred <- data.frame(\n  x = c(1:12, 20, 35, 50),\n  predict(model1, \n          newdata=data.frame(x=c(1:12, 20, 35, 50)), \n          interval=\"confidence\", level = 0.95),\n  predict(model1, \n          newdata=data.frame(x=c(1:12, 20, 35, 50)), \n          interval=\"prediction\", level = 0.95)[,-1])\ndt_pred\n\n\n\nA data.frame: 15 × 6\n\n    xfitlwruprlwr.1upr.1\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    1 1  0.3391304-6.2087835  6.887044-8.5867330  9.264994\n    2 2  2.9478261-2.7509762  8.646628-5.3751666 11.270819\n    3 3  5.5565217 0.6905854 10.422458-2.2199297 13.332973\n    4 4  8.1652174 4.1058891 12.224546 0.8663128 15.464122\n    5 5 10.7739130 7.4756140 14.072212 3.8692308 17.678595\n    6 6 13.382608710.7597808 16.005437 6.7738957 19.991322\n    7 7 15.991304313.8748223 18.107786 9.5667143 22.415894\n    8 8 18.600000016.6817753 20.51822512.2379683 24.962032\n    9 9 21.208695719.0922136 23.32517814.7841056 27.633286\n    1010 23.817391321.1945634 26.44021917.2086783 30.426104\n    1111 26.426087023.1277880 29.72438619.5214047 33.330769\n    1212 29.034782624.9754543 33.09411121.7358781 36.333687\n    1320 49.904347839.0017505 60.80694537.4278703 62.380825\n    1435 89.034782664.8105387113.25902764.0626010114.006964\n    1550128.165217490.5524421165.77799390.0664414166.263993\n\n\n\n\n\nnames(dt_pred)[5:6] <- c('plwr', 'pupr')\ndt_pred\n\n\n\nA data.frame: 15 × 6\n\n    xfitlwruprplwrpupr\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    1 1  0.3391304-6.2087835  6.887044-8.5867330  9.264994\n    2 2  2.9478261-2.7509762  8.646628-5.3751666 11.270819\n    3 3  5.5565217 0.6905854 10.422458-2.2199297 13.332973\n    4 4  8.1652174 4.1058891 12.224546 0.8663128 15.464122\n    5 5 10.7739130 7.4756140 14.072212 3.8692308 17.678595\n    6 6 13.382608710.7597808 16.005437 6.7738957 19.991322\n    7 7 15.991304313.8748223 18.107786 9.5667143 22.415894\n    8 8 18.600000016.6817753 20.51822512.2379683 24.962032\n    9 9 21.208695719.0922136 23.32517814.7841056 27.633286\n    1010 23.817391321.1945634 26.44021917.2086783 30.426104\n    1111 26.426087023.1277880 29.72438619.5214047 33.330769\n    1212 29.034782624.9754543 33.09411121.7358781 36.333687\n    1320 49.904347839.0017505 60.80694537.4278703 62.380825\n    1435 89.034782664.8105387113.25902764.0626010114.006964\n    1550128.165217490.5524421165.77799390.0664414166.263993\n\n\n\n\n\nbarx <- mean(dt$x)\nbary <- mean(dt$y)\n\n\n\n\n\nplot(y~x, data = dt,\n     xlab = \"광고료\",\n     ylab = \"총판매액\",\n     ylim = c(min(dt_pred$plwr), max(dt_pred$pupr)),\n     xlim = c(1,50),\n     pch  = 20,\n     cex  = 2,\n     col  = \"grey\"\n     )\nabline(model1, lwd = 5, col = \"darkorange\")\nlines(dt_pred$x, dt_pred$lwr, col = \"dodgerblue\", lwd = 3, lty = 2)\nlines(dt_pred$x, dt_pred$upr, col = \"dodgerblue\", lwd = 3, lty = 2)\nlines(dt_pred$x, dt_pred$plwr, col = \"darkgreen\", lwd = 3, lty = 3)\nlines(dt_pred$x, dt_pred$pupr, col = \"darkgreen\", lwd = 3, lty = 3)\n\nabline(v=barx, lty=2, lwd=0.2, col='dark grey')\n\n\n\n\n\n\n\n- \\(\\epsilon\\) : 선형성, 등분산성, 정규성, 독립성\n\ndt$yhat <- model1$fitted\ndt$resid <- model1$residuals\n\n\npar(mfrow=c(1,2))\nplot(resid ~ x, dt, pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\nplot(resid ~ yhat, dt, pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\npar(mfrow=c(1,1))\n\n\n\n\n\n\n\n\\(H_0\\):등분산 vs \\(H_1\\):이분산 (Heteroscedesticity)\n\n\nbptest(model1)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model1\nBP = 1.6727, df = 1, p-value = 0.1959\n\n\n\n\n\n\nqqnorm(dt$resid, pch=16)\nqqline(dt$resid, col = 2)\n\nhist(dt$resid)\n\n\n\n\n\n\n\n\n\n\n\n\\(H_0\\):normal distribution vs \\(H_1\\): not \\(H_0\\)\n\n\nshapiro.test(resid(model1))\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(model1)\nW = 0.92426, p-value = 0.3939\n\n\n\n\n\n\ndwtest(model1, alternative = \"two.sided\")  #H0 : uncorrelated vs H1 : rho != 0\n\n\n    Durbin-Watson test\n\ndata:  model1\nDW = 1.4679, p-value = 0.3916\nalternative hypothesis: true autocorrelation is not 0\n\n\n\ndwtest(model1, alternative = \"greater\")  #H0 : uncorrelated vs H1 : rho > 0\n\n\n    Durbin-Watson test\n\ndata:  model1\nDW = 1.4679, p-value = 0.1958\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\ndwtest(model1, alternative = \"less\")     #H0 : uncorrelated vs H1 : rho < 0\n\n\n    Durbin-Watson test\n\ndata:  model1\nDW = 1.4679, p-value = 0.8042\nalternative hypothesis: true autocorrelation is less than 0"
  },
  {
    "objectID": "posts/Applied statistics/AS1_3.html",
    "href": "posts/Applied statistics/AS1_3.html",
    "title": "AS HW1_",
    "section": "",
    "text": "library(dplyr)\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\nAttaching package: ‘zoo’\n\n\nThe following objects are masked from ‘package:base’:\n\n    as.Date, as.Date.numeric\n\n\n\n\n\n데이터셋\n\nexams <- read.csv(\"~/Dropbox/coco/posts/Applied statistics/exams.csv\")\nhead(exams)\n\n\n\nA data.frame: 6 × 8\n\n    genderrace.ethnicityparental.level.of.educationlunchtest.preparation.coursemath.scorereading.scorewriting.score\n    <chr><chr><chr><chr><chr><int><int><int>\n\n\n    1femalegroup Dsome college      standard    completed597078\n    2male  group Dassociate's degreestandard    none     969387\n    3femalegroup Dsome college      free/reducednone     577677\n    4male  group Bsome college      free/reducednone     707063\n    5femalegroup Dassociate's degreestandard    none     838586\n    6male  group Csome high school  standard    none     685754\n\n\n\n\n\nsummary(exams)\n\n    gender          race.ethnicity     parental.level.of.education\n Length:1000        Length:1000        Length:1000                \n Class :character   Class :character   Class :character           \n Mode  :character   Mode  :character   Mode  :character           \n                                                                  \n                                                                  \n                                                                  \n    lunch           test.preparation.course   math.score     reading.score   \n Length:1000        Length:1000             Min.   : 15.00   Min.   : 25.00  \n Class :character   Class :character        1st Qu.: 58.00   1st Qu.: 61.00  \n Mode  :character   Mode  :character        Median : 68.00   Median : 70.50  \n                                            Mean   : 67.81   Mean   : 70.38  \n                                            3rd Qu.: 79.25   3rd Qu.: 80.00  \n                                            Max.   :100.00   Max.   :100.00  \n writing.score   \n Min.   : 15.00  \n 1st Qu.: 59.00  \n Median : 70.00  \n Mean   : 69.14  \n 3rd Qu.: 80.00  \n Max.   :100.00  \n\n\n\ndata <- filter(exams,race.ethnicity== 'group E')\nhead(data)\n\n\n\nA data.frame: 6 × 8\n\n    genderrace.ethnicityparental.level.of.educationlunchtest.preparation.coursemath.scorereading.scorewriting.score\n    <chr><chr><chr><chr><chr><int><int><int>\n\n\n    1femalegroup Eassociate's degreestandard    none828380\n    2male  group Emaster's degree   free/reducednone564643\n    3femalegroup Eassociate's degreefree/reducednone808285\n    4male  group Eassociate's degreestandard    none898886\n    5femalegroup Eassociate's degreestandard    none807971\n    6femalegroup Esome college      free/reducednone697475\n\n\n\n\n- 데이터: Kaggle의 exams 데이터\n\ngroup E의 아래 두 데이터 상관관계를 보고자 함\nReading score: The student’s score on a standardized reading test\nWriting score: The student’s score on a standardized writing test\n\n\nnrow(data)\nncol(data)\n\n143\n\n\n8\n\n\n\n\n산점도\n\ndt <- data.frame(\n  i = 1:nrow(data),\n  x = data$reading.score,\n  y = data$writing.score,\n  x_barx = data$reading.score - mean(data$reading.score),\n  y_bary = data$writing.score - mean(data$writing.score)) \nhead(dt)\n\n\n\nA data.frame: 6 × 5\n\n    ixyx_barxy_bary\n    <int><int><int><dbl><dbl>\n\n\n    118380  6.384615  4.96503497\n    224643-30.615385-32.03496503\n    338285  5.384615  9.96503497\n    448886 11.384615 10.96503497\n    557971  2.384615 -4.03496503\n    667475 -2.615385 -0.03496503\n\n\n\n\n\ndt$x_barx2 <- dt$x_barx^2\ndt$y_bary2 <- dt$y_bary^2\ndt$x_barxy_bary <-dt$x_barx * dt$y_bary\nhead(dt)\n\n\n\nA data.frame: 6 × 8\n\n    ixyx_barxy_baryx_barx2y_bary2x_barxy_bary\n    <int><int><int><dbl><dbl><dbl><dbl><dbl>\n\n\n    118380  6.384615  4.96503497 40.7633142.465157e+01 31.69983862\n    224643-30.615385-32.03496503937.3017751.026239e+03980.76277569\n    338285  5.384615  9.96503497 28.9940839.930192e+01 53.65788058\n    448886 11.384615 10.96503497129.6094671.202320e+02124.83270576\n    557971  2.384615 -4.03496503  5.6863911.628094e+01 -9.62183970\n    667475 -2.615385 -0.03496503  6.8402371.222554e-03  0.09144701\n\n\n\n\n\n\nplot(y~x,\n     data=dt,\n     xlab=\"reading.score\",\n     ylab=\"writing.score\",\n     pch=16,\n     cex=2,\n     col=\"darkorange\")\n\n\n\n\n\n양의 상관관계가 있어 보인다.\n\n\n\n회귀직선\n\nmodel_ <- lm(y~x,dt)\nmodel_\n\n\nCall:\nlm(formula = y ~ x, data = dt)\n\nCoefficients:\n(Intercept)            x  \n     -2.506        1.012  \n\n\n\\(\\widehat y =-2.506 + 1.012 x\\)\n\nsummary(model_)\n\n\nCall:\nlm(formula = y ~ x, data = dt)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.5572  -3.4544   0.3703   3.3341  12.7208 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -2.5063     2.2880  -1.095    0.275    \nx             1.0121     0.0294  34.419   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.778 on 141 degrees of freedom\nMultiple R-squared:  0.8936,    Adjusted R-squared:  0.8929 \nF-statistic:  1185 on 1 and 141 DF,  p-value: < 2.2e-16\n\n\n\n\nplot(y~x,\n     data=dt,\n     xlab=\"reading.score\",\n     ylab=\"writing.score\",\n     pch=16,\n     cex=2,\n     col=\"darkorange\")\n\n\nabline(model_, col='steelblue', lwd=2)\n\n\n\n\n\n\n분산분석\n\nanova(model_)\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x  127045.85627045.855881184.6851.731866e-70\n    Residuals141 3218.969   22.82957      NA          NA\n\n\n\n\n\n가정: \\(H_0: \\beta_1 = 0\\) vs \\(H_1: \\beta_1 \\neq 0\\)\n\n\nqf(0.95,1,141)\n\n3.90825811075366\n\n\n\n\\(F_0 > F_{0.05}(0.95,1,141) = 3.91\\) 이므로 귀무가설을 기각할 수 잇다. 즉 회귀직선이 유의하다.\n\n\n\n결정계수, 상관계수\n\nsummary(model_)$r.squared\n\n0.893639917777383\n\n\n\nSxy <- sum((dt$x - mean(dt$x))*(dt$y - mean(dt$y)))\nSxx <- sum((dt$x - mean(dt$x))^2)\nSyy <- sum((dt$y - mean(dt$y))^2)\n\n\nrxy<-Sxy/sqrt(Sxx*Syy)\n\n\nrxy**2\n\n0.893639917777383\n\n\n\n\n개별 회귀계수 유의성 검정\n\\(β_0, β_1\\)에 대한 개별 회귀계수의 유의성검정을 수행하시오.\n가설 \\(H_0: \\beta_1 = 0\\) vs \\(H_1: not H_0\\)\n\nsummary(model_)$coef\n\n\n\nA matrix: 2 × 4 of type dbl\n\n    EstimateStd. Errort valuePr(>|t|)\n\n\n    (Intercept)-2.5062772.2880027-1.095402.752092e-01\n    x 1.0120840.029404634.419261.731866e-70\n\n\n\n\n\nqt(0.975,141)\n\n1.97693148863425\n\n\n\\(\\beta_0\\)의 t-value= -1.09540 < 1.97693148863425 이므로 귀무가설을 기각할 수 없다.\n\n\n신뢰구간\n\nconfint(model_, level=0.9)\n\n\n\nA matrix: 2 × 2 of type dbl\n\n    5 %95 %\n\n\n    (Intercept)-6.29459711.282043\n    x 0.96339831.060771\n\n\n\n\n\n\n평균반응\nreading score가 61.2 인 학생의 평균 wiring score 예측하고, 95% 신뢰구간을 구하시오.\n\nnew_score <- data.frame(x=61.2)\n\n- 코드\n\nmodel_$coefficients[1] + model_$coefficients[2]*61.2\n\n(Intercept): 59.4332934119049\n\n\n\npredict(model_, \n        newdata = new_score,\n        interval = c(\"confidence\"),  #구간추정\n        level = 0.95)  ##평균반응\n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    159.4332958.2387460.62785\n\n\n\n\n\n\n개별 y\nreading score가 61.2 인 학생의 개별 wiring score 예측하고, 95% 신뢰구간을 구하시오.\n\npredict(model_, newdata = new_score, \n        interval = c(\"prediction\"),  \n        level = 0.95)  ## 개별 y\n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    159.4332949.9122268.95437\n\n\n\n\n\n\n잔차\n\ndata$yhat <- model_$fitted\ndata$resid <- model_$residuals\n\n\npar(mfrow=c(1,2))\nplot(resid ~ reading.score, data, pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\nplot(resid ~ yhat, data, pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\n\n\n\n\n\nbptest(model_)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model_\nBP = 0.057101, df = 1, p-value = 0.8111\n\n\n\nqqnorm(data$resid, pch=16)\nqqline(data$resid, col=2)\nhist(data$resid)\n\n\n\n\n\n\n\n\nshapiro.test(resid(model_))\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(model_)\nW = 0.9925, p-value = 0.6551\n\n\n\n\\(H_0\\): 정규성 만족, \\(H_1\\): 정규성만족X\np-value의 값이 0.05 보다 크므로 귀무가설 채택. 즉 정규성 가정을 만족한다.\n\n\ndwtest(model_, alternative = \"two.sided\") \n\n\n    Durbin-Watson test\n\ndata:  model_\nDW = 2.1808, p-value = 0.275\nalternative hypothesis: true autocorrelation is not 0\n\n\n\nH0 : uncorrelated vs H1 : rho != 0\n\np-value값이 0.275로 0.05보다 크므로 독립성을 먼족한다."
  },
  {
    "objectID": "posts/Applied statistics/선형회귀분석 CH0607.html",
    "href": "posts/Applied statistics/선형회귀분석 CH0607.html",
    "title": "4. MLR실습",
    "section": "",
    "text": "library(MASS)\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\nAttaching package: ‘zoo’\n\n\nThe following objects are masked from ‘package:base’:\n\n    as.Date, as.Date.numeric"
  },
  {
    "objectID": "posts/Applied statistics/선형회귀분석 CH0607.html#산점도",
    "href": "posts/Applied statistics/선형회귀분석 CH0607.html#산점도",
    "title": "4. MLR실습",
    "section": "산점도",
    "text": "산점도\n\npairs(Boston[,which(names(Boston) %in% \n                      c('medv', 'rm', 'lstat'))], \n      pch=16, col='darkorange')\n\n\n\n\n\n\n마지막 행을 봐보자. 맨아래 왼쪽의 X축은 rm, Y축은 medv를 의미하고, 맨아래의 중간의 x축은 lstat를 의미한다.\nrm이 클수록 집값이 증가하고(직선관계처럼 보인다)\nlstat가 높아질수록 집값은 떨어지는 경향이 있다(곡선관계처럼 보인다. 처음엔 뚝 떨어지다가 천천히 감소)\nrm과 lstat의 다중공산성을 봐야해! > 맨위 가운데 그림을 봐보자.."
  },
  {
    "objectID": "posts/Applied statistics/선형회귀분석 CH0607.html#상관관계",
    "href": "posts/Applied statistics/선형회귀분석 CH0607.html#상관관계",
    "title": "4. MLR실습",
    "section": "상관관계",
    "text": "상관관계\n\n# pairs(Boston, pch=16, col='darkorange')\ncor(Boston[,which(names(Boston) %in% \n                    c('medv', 'rm', 'lstat'))])\n\n\n\nA matrix: 3 × 3 of type dbl\n\n    rmlstatmedv\n\n\n    rm 1.0000000-0.6138083 0.6953599\n    lstat-0.6138083 1.0000000-0.7376627\n    medv 0.6953599-0.7376627 1.0000000\n\n\n\n\n\nrm과 medv는 양의 상관관계\nlstat와 mdev는 음의 상관관계\nrm과 lstat는 음의 상관관계"
  },
  {
    "objectID": "posts/Applied statistics/선형회귀분석 CH0607.html#회귀모형-적합",
    "href": "posts/Applied statistics/선형회귀분석 CH0607.html#회귀모형-적합",
    "title": "4. MLR실습",
    "section": "회귀모형 적합",
    "text": "회귀모형 적합\n\nfit_Boston<-lm(medv~rm+lstat, data=Boston)\nsummary(fit_Boston)\n\n\nCall:\nlm(formula = medv ~ rm + lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.076  -3.516  -1.010   1.909  28.131 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.35827    3.17283  -0.428    0.669    \nrm           5.09479    0.44447  11.463   <2e-16 ***\nlstat       -0.64236    0.04373 -14.689   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 5.54 on 503 degrees of freedom\nMultiple R-squared:  0.6386,    Adjusted R-squared:  0.6371 \nF-statistic: 444.3 on 2 and 503 DF,  p-value: < 2.2e-16\n\n\n\n\\(y=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon , \\  \\epsilon\\) ~ \\(N(0,\\sigma^2)\\)\n모형을 적합하라는 것은, \\(\\hat \\beta_0, \\hat \\beta_1, \\hat \\beta_2\\)를 구해서 \\(\\hat y=\\)꼴로 적어주기\n\n- summary 결과 해석\n\n회귀직선의 유의성에 대한 가설검정이다.\n\\(H_0: \\beta_1=\\beta_2=0\\) vs \\(H_1: \\beta_1 \\neq 0\\) or \\(\\beta_2 \\neq 0\\)\nF-statistic: 444.3, F=MSR/MSE, p-value: < 2.2e-16:p-value가 아주 작으므로 \\(H_0\\)를 기각할 수 있다. 즉 회귀직선은 유의하다.\nResidual standard error: 5.54 = \\(\\sqrt{MSE} = \\sqrt{\\widehat \\sigma^2} = \\widehat \\sigma^2\\)\nPr(>|t|):양측 검정에 대한 유의성 검정\nEstimate 추정량, \\(\\hat \\beta_0=-1.35827, \\hat \\beta_1=5.09479, \\hat \\beta_2=-0.64236\\)\nStd. Error = \\(s.e(\\widehat \\beta_i) = \\dfrac{\\sigma^2}{c_{ii}}\\) 이고, \\(\\widehat {s.e}(\\widehat \\beta_i)\\sqrt{\\dfrac{\\widehat \\sigma^2}{c_{ii}}}\\)\nt value는 \\(t_0 = \\dfrac{\\widehat \\beta_i - 0}{\\widehat {s.e}(\\widehat \\beta_i)}\\)\n절편은 유의하지 않다.\nResiduals: 잔차해석. 0을 기준으로 대칭인가? 봤는데 max가 훨씬 더 커서 오른쪽으로 꼬리가 더 길 수 있겠네? 생각 가능"
  },
  {
    "objectID": "posts/Applied statistics/선형회귀분석 CH0607.html#matrix",
    "href": "posts/Applied statistics/선형회귀분석 CH0607.html#matrix",
    "title": "4. MLR실습",
    "section": "matrix",
    "text": "matrix\n\n\nn = nrow(Boston)\nX = cbind(rep(1,n), Boston$rm, Boston$lstat)\ny = Boston$medv\n\n\nhead(X) # 506X3 행렬\n\n\n\nA matrix: 6 × 3 of type dbl\n\n    16.5754.98\n    16.4219.14\n    17.1854.03\n    16.9982.94\n    17.1475.33\n    16.4305.21\n\n\n\n\n\nhead(y) # medv값, 506x1행렬\n\n\n2421.634.733.436.228.7\n\n\n\n# beta = 3x1 행렬"
  },
  {
    "objectID": "posts/Applied statistics/선형회귀분석 CH0607.html#y-xbeta-epsilon-rightarrow-widehat-beta-xtx-1xty",
    "href": "posts/Applied statistics/선형회귀분석 CH0607.html#y-xbeta-epsilon-rightarrow-widehat-beta-xtx-1xty",
    "title": "4. MLR실습",
    "section": "y = X\\(\\beta\\) + \\(\\epsilon \\rightarrow \\widehat \\beta = (X^TX)^{-1}X^Ty\\)",
    "text": "y = X\\(\\beta\\) + \\(\\epsilon \\rightarrow \\widehat \\beta = (X^TX)^{-1}X^Ty\\)\n\nbeta_hat = solve(t(X)%*%X) %*% t(X) %*% y   # t(X): X^T를 의미함 \nbeta_hat\ncoef(fit_Boston)\n\n\n\nA matrix: 3 × 1 of type dbl\n\n    -1.3582728\n     5.0947880\n    -0.6423583\n\n\n\n\n(Intercept)-1.35827281187452rm5.09478798433654lstat-0.642358334244129\n\n\n\nt(X): \\(X^T\\)를 의미\n%*% : 행렬곱의미\nsolve() : 역행렬 구하는 함수\n\\(\\widehat y = X \\widehat \\beta\\)\n\n\ny_hat = X %*% beta_hat\ny_hat[1:5]\nfitted(fit_Boston)[1:5]\n\n\n28.941013680602525.484205660559132.659074768579832.406519999834931.6304069906576\n\n\n128.941013680603225.4842056605591332.6590747685798432.4065199998349531.6304069906576\n\n\n\ny_hat[1:5] 과 fitted(fit_Boston)[1:5] 값이 동일한 것을 확인 가능\n\n\nsse <- sum((y - y_hat)^2) ##SSE\nsqrt(sse/(n-2-1)) ##RMSE\nsummary(fit_Boston)$sigma\n\n5.54025736698867\n\n\n5.54025736698867\n\n\n\n\\(SSE = \\sum (y_i - \\widehat y_i)^2\\)\n\\(RMSE = \\sqrt{SSE/(n-p-1)} = \\widehat \\sigma\\)"
  },
  {
    "objectID": "posts/Applied statistics/선형회귀분석 CH0607.html#lm사용",
    "href": "posts/Applied statistics/선형회귀분석 CH0607.html#lm사용",
    "title": "4. MLR실습",
    "section": "lm사용",
    "text": "lm사용\n\ndt <- Boston[,which(names(Boston) %in% c('medv', 'rm', 'lstat'))]\nhead(dt)\n\n\n\nA data.frame: 6 × 3\n\n    rmlstatmedv\n    <dbl><dbl><dbl>\n\n\n    16.5754.9824.0\n    26.4219.1421.6\n    37.1854.0334.7\n    46.9982.9433.4\n    57.1475.3336.2\n    66.4305.2128.7\n\n\n\n\n\nfit_Boston<-lm(medv~., data=dt)           # boston의 설명변수 13개 모두 다 사용하고 싶을때 (물결 뒤에 점을 찍어주기!!)\nfit_Boston<-lm(medv~rm+lstat, data=dt)    # 설명변수 중 원하는 것만 사용하고 싶을때\n\n\nsummary(fit_Boston)\n\n\nCall:\nlm(formula = medv ~ rm + lstat, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.076  -3.516  -1.010   1.909  28.131 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.35827    3.17283  -0.428    0.669    \nrm           5.09479    0.44447  11.463   <2e-16 ***\nlstat       -0.64236    0.04373 -14.689   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 5.54 on 503 degrees of freedom\nMultiple R-squared:  0.6386,    Adjusted R-squared:  0.6371 \nF-statistic: 444.3 on 2 and 503 DF,  p-value: < 2.2e-16\n\n\n\nhat y = -1.3583 + 5.0948rm - 0.6424lstat"
  },
  {
    "objectID": "posts/Applied statistics/선형회귀분석 CH0607.html#분산분석회귀직선의-유의성-검정",
    "href": "posts/Applied statistics/선형회귀분석 CH0607.html#분산분석회귀직선의-유의성-검정",
    "title": "4. MLR실습",
    "section": "분산분석:회귀직선의 유의성 검정",
    "text": "분산분석:회귀직선의 유의성 검정\n\n\\(H_0: \\beta_1=\\beta_2=0\\) vs \\(H_1: \\beta_1 \\neq 0\\) or \\(\\beta_2 \\neq 0\\)\n\\(H_0\\): 귀무가설, null hypothesis, 영가설 -> \\(y=\\beta_0\\)\n\\(H_1\\): 대립가설 -> \\(y= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\)\n\n\nanova(fit_Boston) ## XXX\n\n\n\nA anova: 3 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    rm  120654.4220654.41622672.90398.266887e-95\n    lstat  1 6622.57 6622.56999215.75796.669365e-41\n    Residuals50315439.31   30.69445      NA          NA\n\n\n\n\n\n설명변수가 두개로 쪼개져서 나온다.\n\n\nnull_model <- lm(medv~1, data=dt)  #H0\nfit_Boston <- lm(medv~., data=dt)  #H1\n\nanova(null_model, fit_Boston) ##***\n\n\n\nA anova: 2 × 6\n\n    Res.DfRSSDfSum of SqFPr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    150542716.30NA      NA      NA           NA\n    250315439.31 227276.99444.33097.008455e-112\n\n\n\n\n\nnull_model : 설명모델을 안쓰고 절편만 가져가는 모델. 1만 쓴다.(절편만)\nRSS의 \\(15439.31=SSE=\\sum(y_i - \\widehat y_i)^2\\)이고 \\(42716.30 = \\sum(y_i - \\bar y)^2=SST\\)\nSST와 SSE를 비교해서 적합이 잘 되어있는지 확인\nSum of Sq의 \\(27276.99=SSR\\)\nF = \\(\\dfrac{SSR/P}{SSE/(n-p-1)}\\)"
  },
  {
    "objectID": "posts/Applied statistics/선형회귀분석 CH0607.html#beta의-신뢰구간",
    "href": "posts/Applied statistics/선형회귀분석 CH0607.html#beta의-신뢰구간",
    "title": "4. MLR실습",
    "section": "\\(\\beta\\)의 신뢰구간",
    "text": "\\(\\beta\\)의 신뢰구간\n\\(\\widehat \\beta_i \\pm t_{a/2} (n-p-1) \\widehat{s.e}(\\widehat \\beta_i)\\)\n\nvcov(fit_Boston)  ##var(hat beta) = (X^TX)^-1 \\sigma^2\n\n\n\nA matrix: 3 × 3 of type dbl\n\n    (Intercept)rmlstat\n\n\n    (Intercept)10.06683612-1.39248641-0.099178133\n    rm-1.39248641 0.19754958 0.011930670\n    lstat-0.09917813 0.01193067 0.001912441\n\n\n\n\n\n공분산 행렬 값\n\\(Var(\\widehat \\beta_1)=0.19754958, Var(\\widehat \\beta_2)= 0.001912441\\)\n\n\n#코드\nconfint(fit_Boston, level = 0.90)\n\n#수식(직접계산)\ncoef(fit_Boston) + qt(0.975, 503) * summary(fit_Boston)$coef[,2]\ncoef(fit_Boston) - qt(0.975, 503) * summary(fit_Boston)$coef[,2]\n\n\n\nA matrix: 3 × 2 of type dbl\n\n    5 %95 %\n\n\n    (Intercept)-6.5867396 3.8701939\n    rm 4.3623583 5.8272176\n    lstat-0.7144229-0.5702938\n\n\n\n\n(Intercept)4.87535465808391rm5.9680255329079lstat-0.556439501179164\n\n\n(Intercept)-7.59190028183295rm4.22155043576519lstat-0.728277167309094\n\n\n\n\\(n=506, p=2\\)\nsummary(fit_Boston)$coef[,2] : s.e"
  },
  {
    "objectID": "posts/Applied statistics/선형회귀분석 CH0607.html#평균반응-개별-y-추정",
    "href": "posts/Applied statistics/선형회귀분석 CH0607.html#평균반응-개별-y-추정",
    "title": "4. MLR실습",
    "section": "평균반응, 개별 y 추정",
    "text": "평균반응, 개별 y 추정\n\nE(Y|x0), y = E(Y|x0) + epsilon\n\n\nnew_dt <- data.frame(rm=7, lstat=10)\n\n\npredict(fit_Boston, newdata = new_dt)\nc(1,7,10)%*%beta_hat  # hat y0 = -1.3583 + 5.0948*7 - 0.6424*10\n\n1: 27.88165973604\n\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    27.88166\n\n\n\n\n\n\\(x_{0}=\\begin{pmatrix} 1 \\\\ 7 \\\\ 10 \\end{pmatrix}, \\widehat{y}_{0}=x_{0}^{T}\\beta\\)\n\n\npredict(fit_Boston, \n        newdata = new_dt,\n        interval = c(\"confidence\"), \n        level = 0.95)  ##평균반응\n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    127.8816627.1734728.58985\n\n\n\n\n\npredict(fit_Boston, newdata = new_dt, \n        interval = c(\"prediction\"), \n        level = 0.95)  ## 개별 y\n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    127.8816616.9737538.78957"
  },
  {
    "objectID": "posts/Applied statistics/선형회귀분석 CH0607.html#절편을-포함하지-않는-회귀직선-원점을-지나는-회귀직선",
    "href": "posts/Applied statistics/선형회귀분석 CH0607.html#절편을-포함하지-않는-회귀직선-원점을-지나는-회귀직선",
    "title": "4. MLR실습",
    "section": "절편을 포함하지 않는 회귀직선 (원점을 지나는 회귀직선)",
    "text": "절편을 포함하지 않는 회귀직선 (원점을 지나는 회귀직선)\n\n\\(y=\\beta_1 x_1 + \\beta_2 x_2\\)\n\n\nfit_Boston0 <- lm(medv ~ 0 + rm + lstat, dt)\nsummary(fit_Boston0)\nsummary(fit_Boston)\n\n\nCall:\nlm(formula = medv ~ 0 + rm + lstat, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.714  -3.498  -1.075   1.877  27.750 \n\nCoefficients:\n      Estimate Std. Error t value Pr(>|t|)    \nrm     4.90691    0.07019   69.91   <2e-16 ***\nlstat -0.65574    0.03056  -21.46   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 5.536 on 504 degrees of freedom\nMultiple R-squared:  0.9485,    Adjusted R-squared:  0.9482 \nF-statistic:  4637 on 2 and 504 DF,  p-value: < 2.2e-16\n\n\n\nCall:\nlm(formula = medv ~ ., data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.076  -3.516  -1.010   1.909  28.131 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.35827    3.17283  -0.428    0.669    \nrm           5.09479    0.44447  11.463   <2e-16 ***\nlstat       -0.64236    0.04373 -14.689   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 5.54 on 503 degrees of freedom\nMultiple R-squared:  0.6386,    Adjusted R-squared:  0.6371 \nF-statistic: 444.3 on 2 and 503 DF,  p-value: < 2.2e-16\n\n\n\n\\(Adjusted R-squared: 0.9482\\) vs \\(0.6371\\)\n\\(R^2=\\dfrac{SSR}{SST}=\\dfrac{\\sum(\\widehat y_i - \\bar y)^2}{\\sum(y_i-\\bar y)^2}\\)=\\(\\dfrac{설명변수 없을때와 있을때의 차이}{y의변동이 평균으로부터 얼마나 떨어져 있는지}\\)\n절편이 없는 모형의 \\(R^2=\\dfrac{\\sum(\\widehat y_i - 0)^2}{\\sum(y_i-0)^2}\\): 원점으로부터 얼마나 떨어져있는가. 기본적으로 엄청 큰 값을 가지게 된다.\n\n\n절편이 있다 vs 절편이 없다 에서는 \\(R^2\\)과 \\(RMSE=\\widehat \\sigma\\)를 확인해주는 게 좋다.\nRMSE비교 \\(5.536\\) vs \\(5.54\\) -> 별로 차이가 없네?"
  },
  {
    "objectID": "posts/Applied statistics/선형회귀분석 CH0607.html#잔차분석",
    "href": "posts/Applied statistics/선형회귀분석 CH0607.html#잔차분석",
    "title": "4. MLR실습",
    "section": "잔차분석",
    "text": "잔차분석\n\nepsilon : 선형성, 등분산성, 정규성, 독립성\n\n\nyhat <- fitted(fit_Boston)\nres <- resid(fit_Boston)\n\n\nplot(res ~ yhat,pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\n\n\n\n\n\n선형성 애매함\n\n\n등분산성\n\nH0 : 등분산 vs. H1 : 이분산 (Heteroscedasticity)\n\n\nbptest(fit_Boston)\n\n\n    studentized Breusch-Pagan test\n\ndata:  fit_Boston\nBP = 1.5297, df = 2, p-value = 0.4654\n\n\n\np-value가 커서 기각을 못했다. 즉 등분산이다.\n\n\n\n정규성\n\n잔차의 QQ plot\n\n\npar(mfrow=c(1,2))\nqqnorm(res, pch=16)\nqqline(res, col = 2)\n\nhist(res)\npar(mfrow=c(1,1))\n\n\n\n\n\n이상치가 있는 듯 하다ㅡ\nShapiro-Wilk Test\n\n\n\n## H0 : normal distribution  vs. H1 : not H0\nshapiro.test(res)\n\n\n    Shapiro-Wilk normality test\n\ndata:  res\nW = 0.9098, p-value < 2.2e-16\n\n\n\n정규분포 아니다\n\n\n\n독립성 검정 DW test\n\nH0 : uncorrelated vs H1 : rho != 0\n\n\ndwtest(fit_Boston, alternative = \"two.sided\") \n\n\n    Durbin-Watson test\n\ndata:  fit_Boston\nDW = 0.83421, p-value < 2.2e-16\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n독립이라고 할 수 없다."
  },
  {
    "objectID": "posts/Applied statistics/선형회귀분석 CH0607.html#가설검정-fm-rm",
    "href": "posts/Applied statistics/선형회귀분석 CH0607.html#가설검정-fm-rm",
    "title": "4. MLR실습",
    "section": "가설검정: FM, RM",
    "text": "가설검정: FM, RM\n\nreduced_model = lm(medv ~ rm+lstat, data = Boston) #(q=2) \nfull_model = lm(medv ~ ., data=Boston) #(P=13) full model\n\n# r=p-q=11\n\n\nsummary(full_model)\nsummary(reduced_model)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  < 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  < 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: < 2.2e-16\n\n\n\nCall:\nlm(formula = medv ~ rm + lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.076  -3.516  -1.010   1.909  28.131 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.35827    3.17283  -0.428    0.669    \nrm           5.09479    0.44447  11.463   <2e-16 ***\nlstat       -0.64236    0.04373 -14.689   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 5.54 on 503 degrees of freedom\nMultiple R-squared:  0.6386,    Adjusted R-squared:  0.6371 \nF-statistic: 444.3 on 2 and 503 DF,  p-value: < 2.2e-16\n\n\n\nreduced_model보다는 full_model이 더 좋아 보인다. (R^2와 RMSE확인햇을떄)\n13개 중 11개 변수가 유의함을 확인 가능\n\n\n가설검정\nRM : \\(H_0: \\beta_1= \\dots = \\beta_5 = \\beta_7 = \\dots = \\beta_{12} = 0\\)\n\nanova(reduced_model, full_model)\n\n\n\nA anova: 2 × 6\n\n    Res.DfRSSDfSum of SqFPr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    150315439.31NA      NA      NA          NA\n    249211078.78114360.52517.604311.425983e-29\n\n\n\n\n\\(F = \\dfrac{(SSE_{RM}-SSE_{RM})/r}{SSE_{FM}/(n-p-1)}=\\dfrac{(SSR_{FM}-SSR_{RM})/r}{SSE_{FM}/(n-p-1)}=17.60431\\)\n\nRSS : SSE를 의미. RM의 \\(SSE=15439.31\\), FM의 \\(SSE=11078.78\\)\nRes.Df: \\(n-q-1=503, n-p-1=492\\)\n\n\n#강의록에 있는 수식\np <- full_model$rank-1\nq <- reduced_model$rank-1\nSSE_FM <- anova(full_model)$Sum[p+1] #SSE_FM\nSSE_RM <- anova(reduced_model)$Sum[q+1]  #SSE_RM\n\nF0 <- ((SSE_RM-SSE_FM)/(p-q))/(SSE_FM/(nrow(Boston)-p-1))\nF0\n\n17.60431143783\n\n\n\n#기각역 F_{0.05}(p-q,n-p-1)\nqf(0.95,p-q,nrow(Boston)-p-1)\n# p-value -> 해당강의 20분쯤.. 어렵\n1-pf(F0, p-q,nrow(Boston)-p-1)\n\n1.80811652913556\n\n\n0\n\n\n\n#################################\nreduced_model = lm(medv ~ .-age-indus, data = Boston)  # 유의하지 않은 2개 변수 제거(-age-indus)\nfull_model = lm(medv ~ ., data=Boston) # 13개 변수\n\nanova(reduced_model, full_model)\n\n\n\nA anova: 2 × 6\n\n    Res.DfRSSDfSum of SqFPr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    149411081.36NA      NA        NA       NA\n    249211078.78 22.5793740.057273980.9443416\n\n\n\n\n\n\\(H_0: \\beta_{indus} = \\beta_{age} = 0\\) 이고 H0기각 못하므로 빼도 된다."
  },
  {
    "objectID": "posts/Applied statistics/선형회귀분석 CH0607.html#general-linear-hypothesis",
    "href": "posts/Applied statistics/선형회귀분석 CH0607.html#general-linear-hypothesis",
    "title": "4. MLR실습",
    "section": "General linear hypothesis",
    "text": "General linear hypothesis\n\n선형 가설 검정\n\n\nx1<-c(4,8,9,8,8,12,6,10,6,9)\nx2<-c(4,10,8,5,10,15,8,13,5,12)\ny<-c(9,20,22,15,17,30,18,25,10,20)\nfit<-lm(y~x1+x2)  ##FM\nsummary(fit)\n\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4575 -1.9100  0.3314  0.6388  3.2628 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  -0.6507     2.9075  -0.224   0.8293  \nx1            1.5515     0.6462   2.401   0.0474 *\nx2            0.7599     0.3968   1.915   0.0970 .\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.278 on 7 degrees of freedom\nMultiple R-squared:  0.9014,    Adjusted R-squared:  0.8732 \nF-statistic:    32 on 2 and 7 DF,  p-value: 0.0003011\n\n\n\n\\(H_0 : T\\beta = c\\)\n\n\n# install.packages(\"car\")\n\n\n\n\\(H_0 : \\beta_1 = 1\\)\n\n\\(y=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\) 에서\n\\(y=\\beta_0 + x_1 + \\beta_2 x_2 + \\epsilon\\) 이 된다.\n즉, \\(y-x_1 = \\beta_0 + \\beta_2 x_2 + \\epsilon\\)\n단순성형회귀모형이 되는것..(z)\n\n\nlibrary(car)\nlinearHypothesis(fit, c(0,1,0), 1)\n\nERROR: Error in library(car): there is no package called ‘car’\n\n\n\n오류가 나넹.. Rstudio에서 돌린거\n\n\n\n기각 못한다. beta1=1\n\\(H_0 : \\beta_1 = \\beta_2\\)\n\n#b1-b2=0 => (0,1,-1) *beta \n#H_0 : beta_1 = beta2\nlinearHypothesis(fit, c(0,1,-1), 0)\n\n\n\\(H_0 : \\beta_1 = \\beta_2+1\\)\n\n#H_0 : beta_1 = beta2 + 1\nlinearHypothesis(fit, c(0,1,-1), 1)\n\n\n\\(H_0 : \\beta_1 = \\beta_2+5\\)\n\n#H_0 : beta_1 = beta2 + 5\nlinearHypothesis(fit, c(0,1,-1), 5)\n\n\n5로 바꾸고 나니까 기각할 수 있다.\n\n- 강의노트 코드\n\n##H_0 : beta_1 = beta2 + 1\n#y=b0 + b1x1 + b2x2 + e = b0+x1 + b2(x1+x2)+e\n#y-x1 = b0+b2(x1+x2)+e :   RM\n\n\ny1 <- y-x1\nz1 <- x1 + x2\n\n\nfit2 <- lm(y1~z1)\nsummary(fit2)\nanova(fit2)\n\n\nCall:\nlm(formula = y1 ~ z1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5054 -1.9294  0.4236  0.6821  3.4473 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -1.0014     2.2175  -0.452 0.663574    \nz1            0.6824     0.1242   5.493 0.000578 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.137 on 8 degrees of freedom\nMultiple R-squared:  0.7904,    Adjusted R-squared:  0.7642 \nF-statistic: 30.17 on 1 and 8 DF,  p-value: 0.0005785\n\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    z11137.85135137.85135130.173780.0005784583\n    Residuals8 36.54865  4.568581      NA          NA\n\n\n\n\n\nanova(fit)  ##FM\nanova(fit2)  #RM\n\n\n\nA anova: 3 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x11313.04348313.04347860.3231030.0001100467\n    x21 19.03040 19.030400 3.6671350.0970444465\n    Residuals7 36.32612  5.189446       NA          NA\n\n\n\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    z11137.85135137.85135130.173780.0005784583\n    Residuals8 36.54865  4.568581      NA          NA\n\n\n\n\n\n# F = {(SSE_RM - SSE_FM)/r} / {SSE_FM/(n-p-1)}\np <- fit$rank-1\nq <- fit2$rank-1\nSSE_FM <- anova(fit)$Sum[p+1] #SSE_FM\nSSE_RM <- anova(fit2)$Sum[q+1]  #SSE_RM\nF0 <- ((SSE_RM-SSE_FM)/(p-q))/(SSE_FM/(length(y)-p-1))\nF0\n\n0.0428807391894599\n\n\n\n#기각역 F_{0.05}(p-q,n-p-1)\nqf(0.95,p-q,length(y)-p-1)\n# p-value\npf(F0, p-q,length(y)-p-1,lower.tail = F)\n\n5.59144785122073\n\n\n0.841844969417543"
  },
  {
    "objectID": "posts/Applied statistics/AS1.html",
    "href": "posts/Applied statistics/AS1.html",
    "title": "AS HW1",
    "section": "",
    "text": "원점을 지나는 회귀모형은 다음과 같이 정의할 수 있다.\n\\[y_i = β_1x_i + ϵ_i,  ϵ_i ∼_{i.i.d.} N(0, σ^2), i = 1, \\dots , n\\]\n\n\n오차제곱합을 정의하고 \\(β_1\\)의 최소제곱추정량 \\((\\hatβ_1)\\)을 구하여라.\n\\(S=\\sum_{i=1}^n \\epsilon^2 = \\sum_{i=1}^n (y_i-\\beta_1 x_i)^2 = \\sum(y_i^2 + \\beta_1^2 x_i^2 -2 \\beta_1 x_i y_i)\\)\n\\(\\widehat \\beta_1 = argmin \\sum_{i=1}^n(y_i - \\beta_1 x_i)^2\\)\n\\(\\dfrac{\\partial S}{\\partial \\beta_1}= -2 \\sum_{i=1}^n x_i(y_i-\\beta_1 x_i)\\)\n= \\(\\sum x_iy_i - \\beta_1 \\sum x_i^2 = 0\\)\n\\(\\therefore \\widehat \\beta_1 = \\dfrac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2}\\)\n\n\n\n\\(E(\\hatβ_1)\\)을 구하여라.\n\\(a_i = \\dfrac{x_i}{\\sum_{i=1}^n x_i^2}\\)라고 놓자.\n즉, \\(\\widehat \\beta_1 = \\sum_{i=1}^n a_i y_i\\)\n\\(E(\\widehat \\beta_1)= E(\\sum_{i=1}^n a_i y_i) = \\sum_{i=1}^n a_i E(y_i)=\\sum_{i=1}^n a_i E(\\beta_1 x_i + \\epsilon_i)=\\sum_{i=1}^n a_i \\beta_1 x_i = \\dfrac{\\sum_{i=1}^n x_i^2}{\\sum_{i=1}^n x_i^2} \\beta_1 = \\beta_1\\)\n\\(E(\\hatβ_1)=\\beta_1\\)이므로 불편추정량\n\n\n\n\\(Var(\\hatβ_1)\\)을 구하여라.\n\\(Var(\\widehat \\beta_1)= Var(\\sum_{i=1}^n a_i y_i) = \\sum_{i=1}^n a_i^2 Var(y_i)=\\dfrac{\\sigma^2}{\\sum_{i=1}^n x_i^2}\\)\n\\(\\because Var(y_i) = \\sigma^2\\)\n\n\n\n제곱합에 대한 분산분석표를 작성하여라.\n\n\n\n\n\n\n\n\n\n\n\n요인\n제곱합(SS)\n자유도(df)\n평균제곱(MS)\n\\(F_0\\)\n유의확률\n\n\n\n\n회귀\n\\(SSR\\)\n1\n\\(MSR=\\dfrac{SSR}{1}\\)\n\\(\\dfrac{MSR}{MSE}\\)\n\\(P(F \\geq F_0)\\)\n\n\n잔차\n\\(SSE\\)\n\\(n-1\\)\n\\(MSE=\\dfrac{SSE}{n-1}\\)\n\n\n\n\n계\n\\(SST\\)\n\\(n\\)\n\n\n\n\n\n\n\\(SSR=\\sum_{i=1}^n (\\widehat y_i)^2 = \\sum (\\widehat \\beta_1 x_i)^2 = \\widehat \\beta_1^2 \\sum x_i^2\\)\n\\(SSE=\\sum_{i=1}^n(y_i - \\widehat y)^2 = \\sum(y_i - \\widehat \\beta_1 x_i)^2 = \\sum y_i^2 - SSR\\)\n\\(SST=SSE+SSR=\\sum y_i^2\\)\n절편이 없는 모형은 평균이 0인 느낌\n\\(R^2=\\dfrac{\\sum \\widehat y_i^2}{\\sum y_i^2}\\)\n\n\n\n회귀모형의 유의성 검정을 하기 위한 가설을 설정하고, 검정통계량을 제시하여라.\n\n가설 \\(H_0: \\beta_1 = 0 \\ vs \\ H_1:\\beta_1 \\neq 0\\)\n검정통계량 \\(F=\\dfrac{MSR}{MSE}=\\dfrac{SSR/1}{SSE/(n-1)} \\sim_{H_0} F(1,n-1)\\)\n\n\n\n\n위의 가설에 대해, 유의수준 \\(α\\)에서 검정하는 방법을 기술하여라.\n\\(F_0 > F_\\alpha(1,n-1)\\)이면 귀무가설을 기각(유의함)하고 그 외는 귀무가설을 채택\n혹은,\n유의확률 = \\(P(F>F_0) < \\alpha \\to H_0\\)기각\n유의확률 = \\(P(F>F_0) > \\alpha \\to H_0\\)기각못함\n\n\n\n다음의 가설에 대한 검정통계량을 제시하고, 유의수준 \\(α\\)에서 가설 검정하는 방법을 기술하여라.\n\\[H_0 : β_1 = 0        \\  vs   \\      H_1 : β_1 > 0\\]\n검정통계량 \\(T=\\dfrac{\\widehat \\beta_1 - 0}{\\widehat{s.e}(\\widehat \\beta_1)} \\sim_{H_0} t(n-1)\\)\n\\(s.e(\\widehat \\beta_1)=\\sqrt{Var(\\widehat \\beta_1)} = \\dfrac{\\sigma}{\\sqrt{\\sum x_i^2}}\\)\n\\(\\widehat{s.e}(\\widehat \\beta_1)= \\sqrt{\\dfrac{\\hat {\\sigma^2}}{\\sum x_i^2}}, {\\hat \\sigma^2}=MSE\\) $\n유의확률 = \\(P(T > t_0) < \\alpha \\to H_0\\)기각\n혹은\n\\(t_0>t_{\\alpha}(n-1) \\to H_0\\) 기각\n\\(t_0<t_{\\alpha}(n-1) \\to H_0\\) 기각 못함"
  },
  {
    "objectID": "posts/Applied statistics/AS1.html#산점도",
    "href": "posts/Applied statistics/AS1.html#산점도",
    "title": "AS HW1",
    "section": "(1) 산점도",
    "text": "(1) 산점도\n이 데이터의 산점도를 그리고 두 변수 사이의 관계를 설명하시오.\n\nplot(dist~speed,\n     data=cars,\n     xlab=\"speed\",\n     ylab=\"dist\",\n     pch=16,\n     cex=2,\n     col=\"darkorange\")\n\n\n\n\n\n두 변수 사이에 선형관계가 있어 보인다."
  },
  {
    "objectID": "posts/Applied statistics/AS1.html#회귀직선",
    "href": "posts/Applied statistics/AS1.html#회귀직선",
    "title": "AS HW1",
    "section": "(2) 회귀직선",
    "text": "(2) 회귀직선\n최소제곱법의 의한 회귀직선을 적합시키시키고, 모형 적합 결과를 설명하시오.\n\ndt <- data.frame(\n  i = 1:nrow(cars),\n  x = cars$speed,\n  y = cars$dist,\n  x_barx = cars$speed - mean(cars$speed),\n  y_bary = cars$dist - mean(cars$dist)) \ndt\n\n\n\nA data.frame: 50 × 5\n\n    ixyx_barxy_bary\n    <int><dbl><dbl><dbl><dbl>\n\n\n     1 4  2-11.4-40.98\n     2 4 10-11.4-32.98\n     3 7  4 -8.4-38.98\n     4 7 22 -8.4-20.98\n     5 8 16 -7.4-26.98\n     6 9 10 -6.4-32.98\n     710 18 -5.4-24.98\n     810 26 -5.4-16.98\n     910 34 -5.4 -8.98\n    1011 17 -4.4-25.98\n    1111 28 -4.4-14.98\n    1212 14 -3.4-28.98\n    1312 20 -3.4-22.98\n    1412 24 -3.4-18.98\n    1512 28 -3.4-14.98\n    1613 26 -2.4-16.98\n    1713 34 -2.4 -8.98\n    1813 34 -2.4 -8.98\n    1913 46 -2.4  3.02\n    2014 26 -1.4-16.98\n    2114 36 -1.4 -6.98\n    2214 60 -1.4 17.02\n    2314 80 -1.4 37.02\n    2415 20 -0.4-22.98\n    2515 26 -0.4-16.98\n    2615 54 -0.4 11.02\n    2716 32  0.6-10.98\n    2816 40  0.6 -2.98\n    2917 32  1.6-10.98\n    3017 40  1.6 -2.98\n    3117 50  1.6  7.02\n    3218 42  2.6 -0.98\n    3318 56  2.6 13.02\n    3418 76  2.6 33.02\n    3518 84  2.6 41.02\n    3619 36  3.6 -6.98\n    3719 46  3.6  3.02\n    3819 68  3.6 25.02\n    3920 32  4.6-10.98\n    4020 48  4.6  5.02\n    4120 52  4.6  9.02\n    4220 56  4.6 13.02\n    4320 64  4.6 21.02\n    4422 66  6.6 23.02\n    4523 54  7.6 11.02\n    4624 70  8.6 27.02\n    4724 92  8.6 49.02\n    4824 93  8.6 50.02\n    4924120  8.6 77.02\n    5025 85  9.6 42.02\n\n\n\n\n\ndt$x_barx2 <- dt$x_barx^2\ndt$y_bary2 <- dt$y_bary^2\ndt$x_barxy_bary <-dt$x_barx * dt$y_bary\ndt\n\n\n\nA data.frame: 50 × 8\n\n    ixyx_barxy_baryx_barx2y_bary2x_barxy_bary\n    <int><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n     1 4  2-11.4-40.98129.961679.3604467.172\n     2 4 10-11.4-32.98129.961087.6804375.972\n     3 7  4 -8.4-38.98 70.561519.4404327.432\n     4 7 22 -8.4-20.98 70.56 440.1604176.232\n     5 8 16 -7.4-26.98 54.76 727.9204199.652\n     6 9 10 -6.4-32.98 40.961087.6804211.072\n     710 18 -5.4-24.98 29.16 624.0004134.892\n     810 26 -5.4-16.98 29.16 288.3204 91.692\n     910 34 -5.4 -8.98 29.16  80.6404 48.492\n    1011 17 -4.4-25.98 19.36 674.9604114.312\n    1111 28 -4.4-14.98 19.36 224.4004 65.912\n    1212 14 -3.4-28.98 11.56 839.8404 98.532\n    1312 20 -3.4-22.98 11.56 528.0804 78.132\n    1412 24 -3.4-18.98 11.56 360.2404 64.532\n    1512 28 -3.4-14.98 11.56 224.4004 50.932\n    1613 26 -2.4-16.98  5.76 288.3204 40.752\n    1713 34 -2.4 -8.98  5.76  80.6404 21.552\n    1813 34 -2.4 -8.98  5.76  80.6404 21.552\n    1913 46 -2.4  3.02  5.76   9.1204 -7.248\n    2014 26 -1.4-16.98  1.96 288.3204 23.772\n    2114 36 -1.4 -6.98  1.96  48.7204  9.772\n    2214 60 -1.4 17.02  1.96 289.6804-23.828\n    2314 80 -1.4 37.02  1.961370.4804-51.828\n    2415 20 -0.4-22.98  0.16 528.0804  9.192\n    2515 26 -0.4-16.98  0.16 288.3204  6.792\n    2615 54 -0.4 11.02  0.16 121.4404 -4.408\n    2716 32  0.6-10.98  0.36 120.5604 -6.588\n    2816 40  0.6 -2.98  0.36   8.8804 -1.788\n    2917 32  1.6-10.98  2.56 120.5604-17.568\n    3017 40  1.6 -2.98  2.56   8.8804 -4.768\n    3117 50  1.6  7.02  2.56  49.2804 11.232\n    3218 42  2.6 -0.98  6.76   0.9604 -2.548\n    3318 56  2.6 13.02  6.76 169.5204 33.852\n    3418 76  2.6 33.02  6.761090.3204 85.852\n    3518 84  2.6 41.02  6.761682.6404106.652\n    3619 36  3.6 -6.98 12.96  48.7204-25.128\n    3719 46  3.6  3.02 12.96   9.1204 10.872\n    3819 68  3.6 25.02 12.96 626.0004 90.072\n    3920 32  4.6-10.98 21.16 120.5604-50.508\n    4020 48  4.6  5.02 21.16  25.2004 23.092\n    4120 52  4.6  9.02 21.16  81.3604 41.492\n    4220 56  4.6 13.02 21.16 169.5204 59.892\n    4320 64  4.6 21.02 21.16 441.8404 96.692\n    4422 66  6.6 23.02 43.56 529.9204151.932\n    4523 54  7.6 11.02 57.76 121.4404 83.752\n    4624 70  8.6 27.02 73.96 730.0804232.372\n    4724 92  8.6 49.02 73.962402.9604421.572\n    4824 93  8.6 50.02 73.962502.0004430.172\n    4924120  8.6 77.02 73.965932.0804662.372\n    5025 85  9.6 42.02 92.161765.6804403.392\n\n\n\n\n\ncolSums(dt)\n\ni1275x770y2149x_barx-1.77635683940025e-14y_bary1.63424829224823e-13x_barx21370y_bary232538.98x_barxy_bary5387.4\n\n\n\\(\\beta_1 = \\dfrac{S_{xy}}{S_{xx}}\\)\n\\(\\beta_0 = \\bar y - \\beta_1 \\bar x\\)\n\nbeta1 <- as.numeric(colSums(dt)[8]/colSums(dt)[6])\nbeta0 <- mean(cars$dist) - beta1 * mean(cars$speed)\nbeta1\nbeta0\n\n3.93240875912409\n\n\n-17.579094890511\n\n\n\nmodel <- lm(dist~speed, cars)\nmodel\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n    -17.579        3.932  \n\n\n\nsummary(model)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\n\np-value의 값이 1.49\\(e^{-12}\\)로 0.05보다 작아 유의하다.\nF검정은 89.57\n\\(\\mathbb{R^2}\\)는 0.6511\n\\(\\beta_0, \\beta_1\\)의 p-value값은 유의하다."
  },
  {
    "objectID": "posts/Applied statistics/AS1.html#산점도-위에-회귀직선",
    "href": "posts/Applied statistics/AS1.html#산점도-위에-회귀직선",
    "title": "AS HW1",
    "section": "(3) 산점도 위에 회귀직선",
    "text": "(3) 산점도 위에 회귀직선\n데이터의 산점도를 그리고 추정한 회귀직선을 (1)에서 그린 산점도 위에 그리시오.\n\nplot(dist~speed,\n     data=cars,\n     xlab=\"speed\",\n     ylab=\"dist\",\n     pch=16,\n     cex=2,\n     col=\"darkorange\")\nabline(model, col='steelblue', lwd=2)"
  },
  {
    "objectID": "posts/Applied statistics/AS1.html#분산분석",
    "href": "posts/Applied statistics/AS1.html#분산분석",
    "title": "AS HW1",
    "section": "(4) 분산분석",
    "text": "(4) 분산분석\n분산분석표를 작성하고 회귀직선의 유의 여부를 검정하시오.\n\nanova(model)\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    speed 121185.4621185.458989.567111.489836e-12\n    Residuals4811353.52  236.5317      NA          NA\n\n\n\n\n\n가정: \\(H_0: \\beta_1 = 0\\) vs \\(H_1: \\beta_1 \\neq 0\\)\n\n\nqf(0.95,1,48)\n\n4.04265212856665\n\n\n\n\\(F_0 > F_{0.05}(0.95,1,48) = 4.04\\) 이므로 귀무가설을 기각할 수 잇다. 즉 회귀직선이 유의하다."
  },
  {
    "objectID": "posts/Applied statistics/AS1.html#결정계수-상관계수",
    "href": "posts/Applied statistics/AS1.html#결정계수-상관계수",
    "title": "AS HW1",
    "section": "(5) 결정계수, 상관계수",
    "text": "(5) 결정계수, 상관계수\n결정계수와 상관계수를 구하고 이 둘의 관계를 설명하시오.\n\\(\\mathbb{R^2} = \\dfrac{SSR}{SST} = \\dfrac{21185.46}{32538.98} = 0.651079413060889\\)\n- 결정계수 직접계산\n\nSST = sum((dt$y- mean(dt$y))^2)\nSSR = sum(((-17.57909 + 3.932409*dt$x)-mean(dt$y))^2)\n\n\nSST\nSSR\n\n32538.98\n\n\n21185.4615442987\n\n\n\nR2=SSR/SST\nR2\n\n0.651079460520848\n\n\n- 결정계수 코드\n\nsummary(model)$r.squared\n\n0.651079380758251\n\n\n\\(r_{xy} = \\dfrac{S_{xy}}{\\sqrt{S{(xx)}S{(yy)}}}\\)\n\nSxy <- sum((dt$x - mean(dt$x))*(dt$y - mean(dt$y)))\nSxx <- sum((dt$x - mean(dt$x))^2)\nSyy <- sum((dt$y - mean(dt$y))^2)\n\n\nrxy<-Sxy/sqrt(Sxx*Syy)\n\n\nrxy**2\n\n0.651079380758251\n\n\n단순선형회귀모형에서는 표본상관계수와 결정계수가 같다.\n\\(\\mathbb{R^2} = r_{xy}^2\\)"
  },
  {
    "objectID": "posts/Applied statistics/AS1.html#개별-회귀계수-유의성검정",
    "href": "posts/Applied statistics/AS1.html#개별-회귀계수-유의성검정",
    "title": "AS HW1",
    "section": "(6) 개별 회귀계수 유의성검정",
    "text": "(6) 개별 회귀계수 유의성검정\n\\(β_0, β_1\\)에 대한 개별 회귀계수의 유의성검정을 수행하시오.\n가설 \\(H_0: \\beta_1 = 0\\) vs \\(H_1: not H_0\\)\n- 직접구현\n\nSSE=SST-SSR\nMSE=SSE/48\n\n\ntvalue1 = beta1/(sqrt((MSE/sum((dt$x-mean(dt$x))^2))))\ntvalue1\n\n9.46399107202371\n\n\n\ntvalue0 = beta0/(sqrt((MSE*((1/48)+((mean(dt$x))^2/sum((dt$x-mean(dt$x))^2))))))\ntvalue0\n\n-2.59546417223485\n\n\n- 코드구현\n\nsummary(model)$coef\n\n\n\nA matrix: 2 × 4 of type dbl\n\n    EstimateStd. Errort valuePr(>|t|)\n\n\n    (Intercept)-17.5790956.7584402-2.6010581.231882e-02\n    speed  3.9324090.4155128 9.4639901.489836e-12\n\n\n\n\n- 결과\n\nqt(0.975,48)\n\n2.01063475762423\n\n\n\nqt(0.025,48)\n\n-2.01063475762423\n\n\n\n\\(\\beta_0\\)에 대한 \\(t-vlaue\\)값이 \\(-2.601058\\)\n\\(\\beta_1\\)에 대한 \\(t-vlaue\\)값이 \\(9.463990\\)\n\\(\\beta_0, \\beta_1\\)의 t-value는 유의수준 \\(\\alpha=0.05\\)에서의 \\(tvalue=-2.011\\)보다 크기 때문에 유의하다. 즉 귀무가설을 기각한다. \\(\\beta_0, \\beta_1\\)은 모두 0이 아니다."
  },
  {
    "objectID": "posts/Applied statistics/AS1.html#개별-회귀계수-신뢰구간",
    "href": "posts/Applied statistics/AS1.html#개별-회귀계수-신뢰구간",
    "title": "AS HW1",
    "section": "(7) 개별 회귀계수 신뢰구간",
    "text": "(7) 개별 회귀계수 신뢰구간\n\\(β_0, β_1\\)에 대한 90% 신뢰구간을 구하시오.\n\\[\\widehat \\beta_0 \\pm t_{\\alpha/2}(n-2) \\widehat \\sigma \\sqrt{\\dfrac{1}{n}+\\dfrac{\\bar x^2}{S_{xx}}}\\]\n\\[\\widehat \\beta_1 \\pm t_{\\alpha/2}(n-2) \\dfrac{\\widehat \\sigma}{\\sqrt{S_{xx}}}\\]\n\n함수사용\n\n\nconfint(model, level=0.9)\n\n\n\nA matrix: 2 × 2 of type dbl\n\n    5 %95 %\n\n\n    (Intercept)-28.914514-6.243676\n    speed  3.235501 4.629317\n\n\n\n\n\n직접계산\n\n\ncoef(model) + qt(0.95, 48) * summary(model)$coef[,2]\n\n(Intercept)-6.24367551036937speed4.62931684193222\n\n\n\ncoef(model) - qt(0.95, 48) * summary(model)$coef[,2]\n\n(Intercept)-28.9145142706524speed3.23550067631595"
  },
  {
    "objectID": "posts/Applied statistics/AS1.html#평균반응-신뢰구간",
    "href": "posts/Applied statistics/AS1.html#평균반응-신뢰구간",
    "title": "AS HW1",
    "section": "(8) 평균반응, 신뢰구간",
    "text": "(8) 평균반응, 신뢰구간\n속도가 18.5mph 인 차량의 평균 제동거리를 예측하고, 95% 신뢰구간을 구하시오.\n개별 speed = 18,5\n\nnew_speed <- data.frame(speed=18.5)\n\n- 코드\n\nmodel$coefficients[1] + model$coefficients[2]*18.5\n\n(Intercept): 55.1704671532847\n\n\n\npredict(model, \n        newdata = new_speed,\n        interval = c(\"confidence\"),  #구간추정\n        level = 0.95)  ##평균반응\n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    155.1704750.0879760.25296\n\n\n\n\n- 직접계산\n- 평균반응 추정량\n\\[\\widehat \\mu_0 = \\widehat \\beta_0 + \\widehat \\beta_1 x_0\\]\n\nmu0 = beta0+beta1*18.5\nmu0\n\n55.1704671532847\n\n\n\\[Var(\\widehat\\mu_0))=\\sigma^2(\\dfrac{1}{n} + \\dfrac{(x_0 - \\bar x)^2}{S_{xx}})\\]\n\nvarmu=((MSE*((1/48)+((18.5-mean(dt$x))^2/sum((dt$x-mean(dt$x))^2)))))\nsemu=sqrt(varmu)\n\n\nsemu\n\n2.5664989466792\n\n\n\nmu0+qt(0.975,48)*semu    \n\n60.3307591408838\n\n\n\nmu0-qt(0.975,48)*semu          \n\n50.0101751656855"
  },
  {
    "objectID": "posts/Applied statistics/AS1.html#개별반응-신뢰구간",
    "href": "posts/Applied statistics/AS1.html#개별반응-신뢰구간",
    "title": "AS HW1",
    "section": "(9) 개별반응, 신뢰구간",
    "text": "(9) 개별반응, 신뢰구간\n속도가 18.5mph 인 차량의 개별 제동거리를 예측하고, 95% 신뢰구간을 구하시오.\n- 코드\n\npredict(model, newdata = new_speed, \n        interval = c(\"prediction\"),  \n        level = 0.95)  ## 개별 y\n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    155.1704723.8328486.5081\n\n\n\n\n- 직접계산\n\nvary0=MSE*(1+(1/48)+(18.5-mean(dt$x))^2/sum((dt$x-mean(dt$x))^2))\nvary0\n\n243.118551337083\n\n\n\nsqrt(vary0)\n\n15.5922593403613\n\n\n\nmu0+qt(0.975,48)*sqrt(vary0)\n\n86.5208057329061\n\n\n\nmu0-qt(0.975,48)*sqrt(vary0)\n\n23.8201285736632\n\n\n\nqt(0.95,8)\n\n1.8595480375309"
  },
  {
    "objectID": "posts/Applied statistics/AS1.html#원점-지나는-회귀직선",
    "href": "posts/Applied statistics/AS1.html#원점-지나는-회귀직선",
    "title": "AS HW1",
    "section": "(10) 원점 지나는 회귀직선",
    "text": "(10) 원점 지나는 회귀직선\n원점을 지나는 회귀직선을 구하시오.\n- 코드\n\nmodel2 <- lm(dist ~ 0 + speed, cars)\nsummary(model2)\n\n\nCall:\nlm(formula = dist ~ 0 + speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-26.183 -12.637  -5.455   4.590  50.181 \n\nCoefficients:\n      Estimate Std. Error t value Pr(>|t|)    \nspeed   2.9091     0.1414   20.58   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 16.26 on 49 degrees of freedom\nMultiple R-squared:  0.8963,    Adjusted R-squared:  0.8942 \nF-statistic: 423.5 on 1 and 49 DF,  p-value: < 2.2e-16\n\n\n\\(\\widehat {dist} = 2.9091 \\widehat {speed}\\)\n- 직접계산\n\nbeta1_0 <- sum(dt$x * dt$y)/sum((dt$x^2))\nbeta1_0 \n\n2.9091321439371"
  },
  {
    "objectID": "posts/Applied statistics/AS1.html#원점-지나는-회귀계수-신뢰구간",
    "href": "posts/Applied statistics/AS1.html#원점-지나는-회귀계수-신뢰구간",
    "title": "AS HW1",
    "section": "(11) 원점 지나는 회귀계수 신뢰구간",
    "text": "(11) 원점 지나는 회귀계수 신뢰구간\n위 회귀직선에서 회귀계수(기울기)의 90% 신뢰구간을 구하시오.\n-코드\n\nconfint(model2, level=0.9)\n\n\n\nA matrix: 1 × 2 of type dbl\n\n    5 %95 %\n\n\n    speed2.672123.146144\n\n\n\n\n- 직접계산\n\ncolSums(dt)[6] #Sxx\n\nx_barx2: 1370\n\n\n\nSSR_0 = sum(((2.909132*dt$x))^2 )\n\n\nSSE_0 = sum((dt$y - 2.909132*dt$x)^2)\n\n\nSST_0 = sum(dt$y^2)\n\n\nMSE_0 = SSE_0/49\n\n\nsigma_0 = sqrt(MSE_0)\n\n\nbeta1_0 + qt(0.95,49) * sigma_0/sqrt(1370)\n\n3.64560479353767\n\n\n\nbeta1_0 - qt(0.95,49) * sigma_0/sqrt(1370)\n\n2.17265949433653\n\n\n\n qt(0.95,49)\n\n1.67655089261685"
  },
  {
    "objectID": "posts/Applied statistics/AS1.html#원점-지난-회귀직선-분산분석",
    "href": "posts/Applied statistics/AS1.html#원점-지난-회귀직선-분산분석",
    "title": "AS HW1",
    "section": "(12) 원점 지난 회귀직선 분산분석",
    "text": "(12) 원점 지난 회귀직선 분산분석\n원점을 지나는 회귀직선에 대한 분산분석표를 작성하고, 회귀직선의 유의 여부를 검정하시오.\n\nanova(model2)\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    speed 1111949.22111949.2232423.46829.227817e-26\n    Residuals49 12953.78   264.3628      NA          NA\n\n\n\n\n\n가정: \\(H_0: \\beta_1 = 0\\) vs \\(H_1: \\beta_1 \\neq 0\\)\n\n\nqf(0.95,1,49)\n\n4.03839263368304\n\n\n\n\\(F_0 > F_{0.05}(0.95,1,49) = 4.04\\) 이므로 귀무가설을 기각할 수 잇다. 즉 회귀직선이 유의하다."
  },
  {
    "objectID": "posts/Applied statistics/AS1.html#원점-지나는-회귀직선-결정계수",
    "href": "posts/Applied statistics/AS1.html#원점-지나는-회귀직선-결정계수",
    "title": "AS HW1",
    "section": "(13) 원점 지나는 회귀직선 결정계수",
    "text": "(13) 원점 지나는 회귀직선 결정계수\n원점을 지나는 회귀직선의 결정계수를 구하시오.\n- 코드\n\nsummary(model2)$r.squared\n\n0.896289305805206\n\n\n- 직접계산\n\nSSR_0/SST_0\n\n0.896289217112581"
  },
  {
    "objectID": "posts/Applied statistics/AS1.html#회귀직선-결과-비교",
    "href": "posts/Applied statistics/AS1.html#회귀직선-결과-비교",
    "title": "AS HW1",
    "section": "(14) 회귀직선 결과 비교",
    "text": "(14) 회귀직선 결과 비교\n원점을 포함한 회귀직선과 포함하지 않은 회귀직선의 결과를 비교하여라.\n\nsummary(model)$r.squared\nsummary(model2)$r.squared\n\n0.651079380758251\n\n\n0.896289305805206\n\n\n원점을 포함한 회귀직선의 R스퀘어값이 더 크므로 model2가 더 좋은 것 같다."
  },
  {
    "objectID": "posts/Applied statistics/AS1.html#잔차-산점도",
    "href": "posts/Applied statistics/AS1.html#잔차-산점도",
    "title": "AS HW1",
    "section": "(15) 잔차 산점도",
    "text": "(15) 잔차 산점도\n잔차에 대한 산점도를 그리고, 결과를 설명하여라.\n\ncars$disthat <- model$fitted\ncars$resid <- model$residuals\n\n\npar(mfrow=c(1,2))\nplot(resid ~ speed, cars, pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\nplot(resid ~ disthat, cars, pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')"
  },
  {
    "objectID": "posts/Applied statistics/AS1.html#잔차-등분산성-검정",
    "href": "posts/Applied statistics/AS1.html#잔차-등분산성-검정",
    "title": "AS HW1",
    "section": "(16) 잔차 등분산성 검정",
    "text": "(16) 잔차 등분산성 검정\n잔차에 대한 등분산성 검정을 수행하시오.\n가설:\n\nbptest(model)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model\nBP = 3.2149, df = 1, p-value = 0.07297"
  },
  {
    "objectID": "posts/Applied statistics/AS1.html#잔차-정규성-검정",
    "href": "posts/Applied statistics/AS1.html#잔차-정규성-검정",
    "title": "AS HW1",
    "section": "(17) 잔차 정규성 검정",
    "text": "(17) 잔차 정규성 검정\n잔차에 대한 히스토그램, QQ plot을 그리고, 정규성 검정을 수행하여라.\n가설:\n\nqqnorm(cars$resid, pch=16)\nqqline(cars$resid, col=2)\n\nhist(cars$resid)\n\n\n\n\n\n\n\n\nshapiro.test(resid(model))\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(model)\nW = 0.94509, p-value = 0.02152\n\n\n\n\\(H_0\\): 정규성 만족, \\(H_1\\): 정규성만족X\np-value의 값이 0.05 보다 작으므로 귀무가설을 기각"
  },
  {
    "objectID": "posts/Applied statistics/AS1.html#잔차-독립성-검정",
    "href": "posts/Applied statistics/AS1.html#잔차-독립성-검정",
    "title": "AS HW1",
    "section": "(18) 잔차 독립성 검정",
    "text": "(18) 잔차 독립성 검정\n잔차에 대한 독립성 검정을 수행하시오.\n\ndwtest(model, alternative = \"two.sided\")  #H0 : uncorrelated vs H1 : rho != 0\n\n\n    Durbin-Watson test\n\ndata:  model\nDW = 1.6762, p-value = 0.1904\nalternative hypothesis: true autocorrelation is not 0\n\n\n\nDW TEST의 p-value=0.1904이므로 H0를 기각할 수 없다. 채택. 즉 서로 독립이다.\n\n\ndwtest(model, alternative = \"greater\")  #H0 : uncorrelated vs H1 : rho > 0\n\n\n    Durbin-Watson test\n\ndata:  model\nDW = 1.6762, p-value = 0.09522\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\ndwtest(model, alternative = \"less\")     #H0 : uncorrelated vs H1 : rho < 0\n\n\n    Durbin-Watson test\n\ndata:  model\nDW = 1.6762, p-value = 0.9048\nalternative hypothesis: true autocorrelation is less than 0"
  },
  {
    "objectID": "posts/Applied statistics/CH0304_simulation.html",
    "href": "posts/Applied statistics/CH0304_simulation.html",
    "title": "3. SLR실습_simulation",
    "section": "",
    "text": "y = 10 - 3x + epsilon\nepsilon ~ N(0, 3^2)\n\n\nnum_obs <- 30\nbeta0 <- 10\nbeta1 <- -3\nsigma <- 3\n\n\n\nset.seed(1)\nepsilon <-  rnorm(n = num_obs, mean = 0, sd = sigma)\n\n\nx <- 1:num_obs\ny <- beta0 + beta1*x + epsilon\n\n\nsim_model1 <- lm(y~x)\nsummary(sim_model1)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.9439 -1.3193  0.3063  1.8845  4.1365 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 10.78911    1.05002   10.28 5.27e-11 ***\nx           -3.03495    0.05915  -51.31  < 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.804 on 28 degrees of freedom\nMultiple R-squared:  0.9895,    Adjusted R-squared:  0.9891 \nF-statistic:  2633 on 1 and 28 DF,  p-value: < 2.2e-16\n\n\n\nplot(y ~ x, pch=16, col='darkorange')\nabline(sim_model1, col='steelblue', lwd=2)\n\n\n\n\n\nconfint(sim_model1, level = 0.95)\n\n\n\nA matrix: 2 × 2 of type dbl\n\n    2.5 %97.5 %\n\n\n    (Intercept) 8.63823512.939986\n    x-3.156107-2.913795\n\n\n\n\n\n\n\ngenerating_slr = function(x, beta0, beta1, sigma) {\n  set.seed(1)\n  n = length(x)\n  epsilon = rnorm(n, mean = 0, sd = sigma)\n  y = beta0 + beta1 * x + epsilon\n  data.frame(x,y)\n}\n\n\npar(mfrow=c(1,2))\nx <- 1:10 \nbeta0 <- 10\nbeta1 <- -3\nsigma <- 3\n\n\nsim_dat <- generating_slr(x, beta0, beta1, sigma)\nsim_fit <- lm(y~x, data = sim_dat)\n\n\nplot(y ~ x, data = sim_dat, \n     pch=16, col='darkorange',\n     main =bquote(hat(beta)[0] ~ \"=\"~.(beta0) ~ \", \" ~\n                    hat(beta)[1] ~ \"=\"~.(beta1)~ \", \" ~\n                    sigma ~\"=\"~ .(sigma)))\nabline(sim_fit, col='steelblue', lwd=2)\nabline(10, -3, col='red', lwd=2)\n\n\n\n\n\nsummary(sim_fit)\n\n\nCall:\nlm(formula = y ~ x, data = sim_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9401 -1.9230  0.7015  0.8036  4.6355 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   9.4935     1.6581   5.726 0.000441 ***\nx            -2.8358     0.2672 -10.612 5.44e-06 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.427 on 8 degrees of freedom\nMultiple R-squared:  0.9337,    Adjusted R-squared:  0.9254 \nF-statistic: 112.6 on 1 and 8 DF,  p-value: 5.438e-06\n\n\n\n\n\nhat beta0 ~ N(beta0, Var(hat beta0))\nhat beta1 ~ N(beta1, Var(hat beta1))\n\nnum_samples = 10000\n\n\nbeta0_hats = rep(0, num_samples)\nbeta1_hats = rep(0, num_samples)\n\n\nx1 <- seq(3,7, lenth.out=20)\nx2 <- seq(1,10, lenth.out=20)\nbeta0 <- 10\nbeta1 <- -3\nsigma <- 3\n\nWarning message:\n“In seq.default(3, 7, lenth.out = 20) :\n extra argument ‘lenth.out’ will be disregarded”\nWarning message:\n“In seq.default(1, 10, lenth.out = 20) :\n extra argument ‘lenth.out’ will be disregarded”\n\n\n\ntmp_dt1 <- generating_slr(x1, beta0, beta1, sigma)\ntmp_dt2 <- generating_slr(x2, beta0, beta1, sigma)\n\n\npar(mfrow=c(1,2))\nplot(y~x, tmp_dt1)\n\n\n\n\n\nm1 <- lm(y~x, tmp_dt1)\nm2 <- lm(y~x, tmp_dt2)\n\n\n\nsummary(m1)$coef\nsummary(m2)$coef\n\n\n\nA matrix: 2 × 4 of type dbl\n\n    EstimateStd. Errort valuePr(>|t|)\n\n\n    (Intercept) 5.4024694.5800907 1.1795550.3231998\n    x-2.0029320.8814389-2.2723430.1076921\n\n\n\n\n\n\nA matrix: 2 × 4 of type dbl\n\n    EstimateStd. Errort valuePr(>|t|)\n\n\n    (Intercept) 9.4935291.6580904  5.725584.411935e-04\n    x-2.8358040.2672255-10.612035.438402e-06\n\n\n\n\n\n\nnum_samples = 10000\n\nbeta0_hats = rep(0, num_samples)\nbeta1_hats = rep(0, num_samples)\n\n\n\n\nx <- 1:10 \nbeta0 <- 10\nbeta1 <- -3\nsigma <- 3\n\nset.seed(1004)\n\n\n\nfor (i in 1:num_samples) {\n  tmp_dt <- generating_slr(x, beta0, beta1, sigma)\n  \n  sim_fit = lm(y ~ x, tmp_dt)\n  \n  beta0_hats[i] = coef(sim_fit)[1]\n  beta1_hats[i] = coef(sim_fit)[2]\n}\n\n\n\n## beta1\nhead(beta1_hats)\n\n\n-2.83580381478312-2.83580381478312-2.83580381478312-2.83580381478312-2.83580381478312-2.83580381478312\n\n\n\n\nmean(beta1_hats)  ## empirical mean of beta1_hat\n\n-2.83580381478312\n\n\n\nbeta1 ## true mean of beta1_hat\n\n-3\n\n\n\n\nvar(beta1_hats)  ## empirical variance of beta1_hat\n\n0\n\n\n\nvar_beta1_hat <- sigma^2/ sum((x - mean(x))^2)  ## true variance of beta1_hat\nvar_beta1_hat\n\n0.109090909090909\n\n\n\n\nhist(beta1_hats, prob = TRUE, breaks = 20, \n     xlab = expression(hat(beta)[1]), main = \"\", border = \"steelblue\")  ## empirical distribution of beta1_hat\ncurve(dnorm(x, mean = beta1, sd = sqrt(var_beta1_hat)), \n      col = \"darkorange\", add = TRUE, lwd = 3)  ## true distribution of beta1_hat\n\n\n\n\n\n\n\n\n## Model1 : y = 3 + 5x + epsilon,  epsilon~N(0,1)\n## Model2 : y = 3 + 5x + epsilon,  epsilon~N(0,x^2)\n## Model3 : y = 3 + 5x^2 + epsilon,  epsilon~N(0,25)\n\n\ngenerating function\n\n\n\nsim_1 = function(n) {\n  x = runif(n = n) * 5\n  y = 3 + 5 * x + rnorm(n = n, mean = 0, sd = 1)\n  data.frame(x, y)\n}\n\n\nsim_2 = function(n) {\n  x = runif(n = n) * 5\n  y = 3 + 5 * x + rnorm(n = n, mean = 0, sd = x)\n  data.frame(x, y)\n}\n\n\nsim_3 = function(n) {\n  x = runif(n = n) * 5\n  y = 3 + 5 * x ^ 2 + rnorm(n = n, mean = 0, sd = 5)\n  data.frame(x, y)\n}\n\n\n\nn <- 200\nset.seed(1004)\ndt1 <- sim_1(n)\ndt2 <- sim_2(n)\ndt3 <- sim_3(n)\n\n\n\n\nhead(dt1)\n\n\n\nA data.frame: 6 × 2\n\n    xy\n    <dbl><dbl>\n\n\n    11.358233 9.348186\n    21.229829 8.768066\n    33.89323121.079976\n    44.89505828.165052\n    52.17379311.262647\n    64.57066125.031025\n\n\n\n\n\nhead(dt2)\n\n\n\nA data.frame: 6 × 2\n\n    xy\n    <dbl><dbl>\n\n\n    13.62281018.645942\n    21.04530610.137068\n    33.16852517.546195\n    43.58500023.911858\n    53.98967918.895470\n    61.211359 8.320489\n\n\n\n\n\n\nhead(dt3)\n\n\n\nA data.frame: 6 × 2\n\n    xy\n    <dbl><dbl>\n\n\n    14.01684582.23803\n    23.87421684.05972\n    32.67535431.63082\n    44.22004285.95100\n    53.42650757.40688\n    62.88132041.75378\n\n\n\n\n\n\npar(mfrow=c(1,3))\nplot(y~x, dt1, col='grey', pch=16, main = \"Model1\")\nplot(y~x, dt2, col='grey', pch=16, main = \"Model2\")\nplot(y~x, dt3, col='grey', pch=16, main = \"Model3\")\n\n\n\n\n\n\n##### model fitting\n\nfit1 <- lm(y~x, dt1)\nfit2 <- lm(y~x, dt2)\nfit3 <- lm(y~x, dt3)\nfit4 <- lm(y~x+I(x^2), dt3)\n\n\nfit4의 경우는 x^2을 추가 해서.. 사용한다.\nfit4 <- lm(y~x+I(x^2), dt3) 로 하면 된다.ㅇI 안쓰면 안된ㅁ\n\n\nsummary(fit3)\n\n\nCall:\nlm(formula = y ~ x, data = dt3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.526  -8.086  -1.802   6.021  27.172 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -18.396      1.550  -11.87   <2e-16 ***\nx             25.177      0.518   48.60   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 10.3 on 198 degrees of freedom\nMultiple R-squared:  0.9227,    Adjusted R-squared:  0.9223 \nF-statistic:  2362 on 1 and 198 DF,  p-value: < 2.2e-16\n\n\n\n\npar(mfrow=c(1,3))\nplot(y~x, dt1, col='grey', pch=16, main = \"Model1\")\nabline(fit1, col='darkorange', lwd=2)\nplot(y~x, dt2, col='grey', pch=16, main = \"Model2\")\nabline(fit2, col='darkorange', lwd=2)\nplot(y~x, dt3, col='grey', pch=16, main = \"Model3\")\nabline(fit3, col='darkorange', lwd=2)\n\n\n\n\n\n\n##### residual plot\npar(mfrow=c(1,3))\nplot(fitted(fit1),resid(fit1), col = 'grey', pch=16, \n     xlab = expression(hat(y)),\n     ylab = \"Residual\",\n     main = \"Residual plot in Model1\")\nabline(h=0, col='darkorange', lty=2, lwd=2)\n\n# 0에 대해서 대칭인지 확인하자!\n\nplot(fitted(fit2),resid(fit2), col = 'grey', pch=16, \n     xlab = expression(hat(y)),\n     ylab = \"Residual\",\n     main = \"Residual plot in Model2\")\nabline(h=0, col='darkorange', lty=2, lwd=2)\n\nplot(fitted(fit3),resid(fit3), col = 'grey', pch=16, \n     xlab = expression(hat(y)),\n     ylab = \"Residual\",\n     main = \"Residual plot in Model3\")\nabline(h=0, col='darkorange', lty=2, lwd=2)\n\n\n\n\n\n\n\n\n\n##### \n\n### Breusch-Pagan Test\n## H0 : 등분산  vs.  H1 : 이분산 (Heteroscedasticity)\nlibrary(lmtest)\nbptest(fit1) # 0.05보다 큰값이므로 H0를 기각할 수 없다. 등분산\nbptest(fit2) # H0기각 이분산이다. (오차에다가 가중치를 사용해서 분산을 안정화시켜줌. x에 비례하도록 가중치를.. 가중최소제곱추정량(WLSE))\nbptest(fit3) # H0채택\n\n\n    studentized Breusch-Pagan test\n\ndata:  fit1\nBP = 0.11269, df = 1, p-value = 0.7371\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  fit2\nBP = 26.728, df = 1, p-value = 2.342e-07\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  fit3\nBP = 0.19028, df = 1, p-value = 0.6627\n\n\n\n\n\n\n\n### Histogram of residuals\npar(mfrow = c(1, 3))\nhist(resid(fit1),\n     xlab   = \"Residuals\",\n     main   = \"Histogram of Residuals, fit1\",\n     col    = \"darkorange\",\n     border = \"steelblue\",\n     breaks = 10)\nhist(resid(fit2),\n     xlab   = \"Residuals\",\n     main   = \"Histogram of Residuals, fit2\",\n     col    = \"darkorange\",\n     border = \"steelblue\",\n     breaks = 10)\nhist(resid(fit3),  #오른쪽으로 꼬리가 길다.\n     xlab   = \"Residuals\",\n     main   = \"Histogram of Residuals, fit3\",\n     col    = \"darkorange\",\n     border = \"steelblue\",\n     breaks = 10)\n\n\n\n\n\n\n### QQplot\nNormal<- rnorm(500)\nChisquare <- rchisq(500, 3)\nhist(Normal)\nhist(Chisquare)\n\n\n\n\n\n\n\n\n\n# 빨간색 직선에 붙어있으면 정규분포다.\nt2 <- rt(500, 3)\n\n\npar(mfrow=c(1,3))\nqqnorm(Normal, pch=16)\nqqline(Normal, col = 2, lwd=2)\n\n\nqqnorm(Chisquare, pch=16)\nqqline(Chisquare, col = 2)\n\n\n\nqqnorm(t2, pch=16)\nqqline(t2, col = 2)\n\n\n\n\n\n\npar(mfrow=c(1,3))\nhist(Normal, breaks = 10)\nhist(Chisquare, breaks = 10)\nhist(t2, breaks = 10)\npar(mfrow=c(1,1))\n\n\n\n\n\n\n##\npar(mfrow=c(1,3))\nqqnorm(resid(fit1), main = \"Normal Q-Q Plot, fit1\", col = \"darkgrey\")\nqqline(resid(fit1), col = \"steelblue\", lwd = 2)\n\nqqnorm(resid(fit2), main = \"Normal Q-Q Plot, fit1\", col = \"darkgrey\")\nqqline(resid(fit2), col = \"steelblue\", lwd = 2)\n\nqqnorm(resid(fit3), main = \"Normal Q-Q Plot, fit1\", col = \"darkgrey\")\nqqline(resid(fit3), col = \"steelblue\", lwd = 2)\n\npar(mfrow=c(1,1))\n\n\n\n\n\n\n\n\n## H0 : normal distribution  vs. H1 : not H0\n\nshapiro.test(resid(fit1))\nshapiro.test(resid(fit2))\nshapiro.test(resid(fit3))\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(fit1)\nW = 0.99577, p-value = 0.8555\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(fit2)\nW = 0.96659, p-value = 0.0001095\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(fit3)\nW = 0.96354, p-value = 4.875e-05\n\n\n\n\n\n\nlibrary(lmtest)\n\n\nModel4 : y = 3 + 5x + epsilon,\nepsilon~N(0, var),correlated\nepsilon_i = 0.8 * epsilon_(i-1) + v_i, v_i ~ N(0,1)\n\n\n\nsim_4 = function(n) {\n  e <- rep(0,n); e[1] <- rnorm(1)\n  \n  for(k in 2:n) {\n    e[k] <- e[(k-1)]*(0.8) + rnorm(1,0,1)\n  } # 오차들이 앞의 오차에 영향을 받고 있어!\n  \n  x = runif(n = n) * 5\n  y = 3 + 5 * x  + e\n  data.frame(x, y)\n}\n\n\n\nn <- 100\ndt4 <- sim_4(n)\nfit4 <- lm(y~x, dt4)\n\n\npar(mfrow=c(1,3))\nplot(y~x, dt4, col='grey', pch=16, main = \"Model4\")\nabline(fit4, col='darkorange', lwd=2)\n\nplot(fitted(fit4),resid(fit4), col = 'grey', pch=16, \n     xlab = expression(hat(y)),\n     ylab = \"Residual\",\n     main = \"Residual plot in Model4\")\nabline(h=0, col='darkorange', lty=2, lwd=2)\n\n\nplot(1:n,resid(fit4), col = 'grey', pch=16, type='l',\n     xlab = expression(hat(y)),\n     ylab = \"Residual\",\n     main = \"Residual plot in Model4\")\nabline(h=0, col='darkorange', lty=2, lwd=2)\npar(mfrow=c(1,1))\n\n\n\n\n\n\n## DWtest\n#H0 : 오차들은 독립이다. \ndwtest(fit1, alternative = \"two.sided\")  #H0 : uncorrelated vs H1 : rho != 0\ndwtest(fit2, alternative = \"two.sided\")  #H0 : uncorrelated vs H1 : rho != 0\ndwtest(fit3, alternative = \"two.sided\")  #H0 : uncorrelated vs H1 : rho != 0\ndwtest(fit4, alternative = \"two.sided\")  #H0 : uncorrelated vs H1 : rho != 0\n\n\n\n    Durbin-Watson test\n\ndata:  fit1\nDW = 2.2176, p-value = 0.1241\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n    Durbin-Watson test\n\ndata:  fit2\nDW = 2.3155, p-value = 0.02485\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n    Durbin-Watson test\n\ndata:  fit3\nDW = 2.1464, p-value = 0.2966\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n    Durbin-Watson test\n\ndata:  fit4\nDW = 0.55926, p-value = 3.234e-13\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n\ndwtest(fit4, alternative = \"two.sided\")  #H0 : uncorrelated vs H1 : rho != 0\ndwtest(fit4, alternative = \"greater\")  #H0 : uncorrelated vs H1 : rho > 0 양의상관관계\ndwtest(fit4, alternative = \"less\")     #H0 : uncorrelated vs H1 : rho < 0 음의상관관계\n\n\n# DW가 4에 가까울수록 음의상관관계가 크고 0에 가까울수록 양의 상관관계가 크다.\n\n\n\n    Durbin-Watson test\n\ndata:  fit4\nDW = 0.55926, p-value = 3.234e-13\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n    Durbin-Watson test\n\ndata:  fit4\nDW = 0.55926, p-value = 1.617e-13\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\n    Durbin-Watson test\n\ndata:  fit4\nDW = 0.55926, p-value = 1\nalternative hypothesis: true autocorrelation is less than 0"
  },
  {
    "objectID": "posts/Applied statistics/alpha.html",
    "href": "posts/Applied statistics/alpha.html",
    "title": "alpha",
    "section": "",
    "text": ": 제 1종 오류를 범할 확률\n즉, \\(H_0\\)가 참임에도 \\(H_0\\)를 기각할 확률\n1 귀무가설 \\(H_0\\)와 대립가설 \\(H_1\\)를 설정\n2 검정통계랑 구하기\n3 검정통계량의 기각역 구하기\n4 검정통계량 값이 기각역에 있으면 귀무가설을 기각\n\n\n임곘값: 주어진 유의수준을 검정통계량의 값으로 환산한 값\nF검정시 R에서 쓰는 코드\nqf(100(1-𝛼),df1,df2)\n검정통계량 \\(F_0\\)\n\\(F_0 > F_𝛼\\) 이면 귀무가설을 기각\n만약 df=1, 8, 𝛼=0.05이면\n\nqf(0.05,1,8)\n\n0.00418615505332731\n\n\n\nqf(0.95,1,8)\n\n5.31765507157871\n\n\n\n오시 pf라는것도 있는데..\n\n\n\n\nt검정시 R에서 쓰는 코드\nqt(𝛼,df)\nt분포에서는 \\(𝛼/2\\)를 해줘야 한다.\n검정통계량 \\(t_0\\)\n\\(t_0 > t_{𝛼/2}\\) 이면 귀무가설을 기각\n만약 df=8, 𝛼=0.05이면, t분포는 𝛼/2=0.025\n\nqt(0.975,8)\n\n2.30600413520417\n\n\n\nqt(0.025,8)\n\n-2.30600413520417\n\n\n\n\n\n잔차의 독립성 검정을 위한 test\n\\(H_0\\):독립이다. \\(H_1\\): not \\(H_0\\)\nDW값이 2에 가까울수록 양의 상관관계, 4에 가까울수록 음의 상관관계, 2를 기준으로 2이면 보류\np-value의 값이 유의수준(𝛼) 보다 크다면 귀무가설을 기각할 수 없음. 즉 독립\npvalue > 𝛼 이면 귀무가설을 채택한다.\n\n\n\n정규성을 검증\n\\(H_0\\):정규분포이다. \\(H_1\\): not \\(H_0\\)\npvalue > 𝛼 이면 귀무가설을 채택한다."
  },
  {
    "objectID": "posts/Applied statistics/가변수 실습.html",
    "href": "posts/Applied statistics/가변수 실습.html",
    "title": "5. 가변수 실습",
    "section": "",
    "text": "library(ggplot2)\n\n\n\n\ndt <- data.frame(\n  y = c(17,26,21,30,22,1,12,19,4,16,\n        28,15,11,38,31,21,20,13,30,14),\n  x1 = c(151,92,175,31,104,277,210,120,290,238,\n         164,272,295,68,85,224,166,305,124,246),\n  x2 = rep(c('M','F'), each=10)\n)\n\n\nhead(dt)\n\n\n\nA data.frame: 6 × 3\n\n    yx1x2\n    <dbl><dbl><chr>\n\n\n    117151M\n    226 92M\n    321175M\n    430 31M\n    522104M\n    6 1277M\n\n\n\n\n\n\n\nmodel_1 <- lm(y~x1, dt)\nsummary(model_1)\n\n\nCall:\nlm(formula = y ~ x1, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.579 -4.737  0.721  4.224  7.936 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 36.40361    2.78580  13.068 1.26e-10 ***\nx1          -0.09323    0.01396  -6.677 2.91e-06 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 5.124 on 18 degrees of freedom\nMultiple R-squared:  0.7124,    Adjusted R-squared:  0.6964 \nF-statistic: 44.58 on 1 and 18 DF,  p-value: 2.906e-06\n\n\n\n모형은 유의하다.\nMSE=5.124\n\n\nggplot(dt, aes(x1, y)) + \n  geom_point() + \n  geom_abline(slope = coef(model_1)[2], \n              intercept = coef(model_1)[1], col= 'darkorange')+\n  theme_bw()\n\n\n\n\n\nM,F상관없이 모든 데이터 퉁으로!!\n\\(\\widehat y = 36.40361-0.09323 x_1\\)\n시험성적 1점 올라갈때마다 시간이 0.0932 감소한다.\n\n\n\n\n\nmodel_2 : \\(y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\epsilon\\)\n\\(x_2=0 \\ if F, x_2=1 \\ if M\\)\n\\(E(y|F) : \\beta_0 + \\beta_1x_1\\) 여자가 기준\n\\(E(y|M) : \\beta_0+\\beta_1x_1 + \\beta_2 = (\\beta_0+\\beta_2)+\\beta_1x_1\\)\n\\(\\beta_2 = E(y|M)-E(y|F) = \\beta_0 + \\beta_2 + \\beta_1x_1 - {\\beta_0+\\beta_1x_1}\\)\n즉, \\(\\beta_2\\)는 시험성적이 동일할 때 여자와 남자의 소요시간의 평균의 차이\n\n\ncontrasts(factor(dt$x2))\n\n\n\nA matrix: 2 × 1 of type dbl\n\n    M\n\n\n    F0\n    M1\n\n\n\n\n\n\\(x_2\\)를 factor로 인식했을 때 무엇이 0이고 무엇이 1인지 알려준다.\n\\(H_0: \\beta_1=\\beta_2=0\\)\n\n\n################################\n# x2 = factor(rep(c('M','F'), each=10)) 로 입력한 경우 \n#y = b0 + b1x1 + b2x2 \n# x2 = 0,  F\n# x2 = 1,  M\n#E(y|M) : b0 + b1x1 + b2 = (b0 + b2) + b1x1\n#E(y|F) : b0 + b1x1\n\n# x2 = factor(rep(c(0,1), each=10))로 입력한 경우 \n# y = b0 + b1x1 + b2x2 \n# x2 = 0,  M\n# x2 = 1,  F\n#E(y|M) : b0 + b1x1\n#E(y|F) : b0 + b1x1+ b2 = = (b0 + b2) + b1x1\n\n\nmodel_2 <- lm(y~x1+x2, dt)\nsummary(model_2)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0165 -1.7450 -0.6055  1.8803  6.1835 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 41.768865   1.948930  21.432 9.64e-14 ***\nx1          -0.100918   0.008621 -11.707 1.47e-09 ***\nx2M         -7.933953   1.414702  -5.608 3.13e-05 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.123 on 17 degrees of freedom\nMultiple R-squared:  0.8991,    Adjusted R-squared:  0.8872 \nF-statistic: 75.72 on 2 and 17 DF,  p-value: 3.42e-09\n\n\n\np-value가 유의하다.\nmodel1보다 \\(R^2\\)값이 많이 올랐다.\nmodel1보다 MSE보다 감소했다.\nx2M: x2가 남자 그룹에 있는 계수, F=0이고 M=1이라는 것을 알려준다.\n\\(\\beta_2=-7.933953\\) 값이 나오는데 강의록에는 F=1,M=0이여서 강의록과는 부호가 바뀐것.\n\n\nggplot(dt, aes(x1, y, col=x2)) + \n  geom_point() + \n  theme_bw() + \n  geom_abline(slope = coef(model_2)[2], \n              intercept = coef(model_2)[1], col= 'darkorange')+\n  geom_abline(slope = coef(model_2)[2], \n              intercept = coef(model_2)[1]+coef(model_2)[3], col= 'steelblue')+\n  guides(col=guide_legend(title=\"성별\")) +\n  scale_color_manual(labels = c(\"여자\", \"남자\"), values = c(\"darkorange\", \"steelblue\"))\n\n\n\n\n\n\\(H_0:\\beta_2=0 \\ vs. \\ H_1:\\beta_2 \\neq 0\\)\n\n\nsummary(model_2)$coefficients\n\n\n\nA matrix: 3 × 4 of type dbl\n\n    EstimateStd. Errort valuePr(>|t|)\n\n\n    (Intercept)41.76886461.948929636 21.4316949.640000e-14\n    x1-0.10091770.008620641-11.7065221.468240e-09\n    x2M-7.93395261.414702366 -5.6082133.134533e-05\n\n\n\n\n\n개별회귀계수에 대한 유의성검정 , 유의확률 값이 작으므로 \\(\\beta_2=0\\)이라고 할 수 있다.\nPr(>|t|)은 양측검정에 대한 유의확률 값이다.\n\\(H_0:\\beta_2=0 \\ vs. \\ H_1:\\beta_2 < 0\\)\n이 때의 유의확률? -> 위의 표와 t-value는 똑같다. -5.608213\n\\(t=\\dfrac{\\widehat \\beta_2}{\\widehat{s.e}(\\widehat \\beta_2)}\\)\nH_1:\\beta_2 < 0 단측 검정에 대한 유의확률 값은 Pr(>|t|)/2\nt-value의 -5.608213 값을 제곱하면 아래 표의 F값 31.45206 이 나온다. Pr(>F)값은 똑같음\n\n\n-5.608213^2\n\n-31.452053053369\n\n\n\n자유도가 1개일때 t-value와 F검정의 값은 동일하다\n\n\nanova(model_1, model_2)\n\n\n\nA anova: 2 × 6\n\n    Res.DfRSSDfSum of SqFPr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    118472.5913NA      NA      NA          NA\n    217165.8145 1306.776831.452063.134533e-05\n\n\n\n\n\nRM: model_1\nFM: model_2\n472.5913 : \\(SSE_{RM}\\)\n165.8145 : \\(SSE_{FM}\\)\n\n\n\n\n\\(y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_1x_2+\\epsilon\\)\n\\(x_2 = 0 if F, x_2 = 1 if M\\)\n\\(E(y|F): \\beta_0 + \\beta_1 x_1\\)\n\\(E(y|M): \\beta_0 + \\beta_1x_1 + \\beta_2 + \\beta_3 x_1 = (\\beta_0+\\beta_2) + (\\beta_1+\\beta_3)x_1\\)\n\nmodel_3 <- lm(y~x1*x2, dt) #교호작용 보고 싶을 떈 x1*x2 곱하기\n# 혹은 lm(y~x1+x2+x1:x2,dt)\nsummary(model_3)\n\n\nCall:\nlm(formula = y ~ x1 * x2, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0463 -1.7591 -0.6232  1.9311  6.1102 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 41.969620   2.635580  15.924 3.11e-11 ***\nx1          -0.101948   0.012474  -8.173 4.20e-07 ***\nx2M         -8.313516   3.541379  -2.348   0.0321 *  \nx1:x2M       0.002089   0.017766   0.118   0.9078    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.218 on 16 degrees of freedom\nMultiple R-squared:  0.8992,    Adjusted R-squared:  0.8803 \nF-statistic: 47.56 on 3 and 16 DF,  p-value: 3.405e-08\n\n\n\n모형 자첸는 유의하고,\nmodel2에 비하면 \\(R^2\\)가 더 감소했다.\n\\(H_0: \\beta_3=0 \\ vs. \\ H_1:\\beta_3 \\neq 0\\)에서 \\(H_0\\)기각 못함\n\n\n## y = b0 + b1x1 + b2x2 + b3x1x2\n## M : x2=0 => E(y|M) = b0+b1x1\n## F : x2=1 => E(y|F) = b0 + b1x1 + b2 + b3x1 \n##                    = (b0+b2) + (b1+b3)x1\n\nggplot(dt, aes(x1, y, col=x2)) + \n  geom_point() + \n  theme_bw() + \n  geom_abline(slope = coef(model_3)[2], \n              intercept = coef(model_3)[1], col= 'darkorange')+\n  geom_abline(slope = coef(model_3)[2]+coef(model_3)[4], \n              intercept = coef(model_3)[1]+coef(model_3)[3], col= 'steelblue')+\n  guides(col=guide_legend(title=\"성별\")) +\n  scale_color_manual(labels = c(\"여자\", \"남자\"), values = c(\"darkorange\", \"steelblue\"))\n\n\n\n\n\n\\(H_0: \\beta_3=0 \\ vs. \\ H_1:\\beta_3 \\neq 0\\)\n\n\nsummary(model_3)$coefficients\n\n\n\nA matrix: 4 × 4 of type dbl\n\n    EstimateStd. Errort valuePr(>|t|)\n\n\n    (Intercept)41.969619602.6355804515.92424153.106803e-11\n    x1-0.101947770.01247420-8.17268934.198832e-07\n    x2M-8.313515643.54137909-2.34753623.209176e-02\n    x1:x2M 0.002089330.01776597 0.11760299.078460e-01\n\n\n\n\n\nanova(model_2, model_3)\n\n\n\nA anova: 2 × 6\n\n    Res.DfRSSDfSum of SqFPr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    117165.8145NA       NA        NA      NA\n    216165.6713 10.14320670.013830450.907846\n\n\n\n\n\\(H_0: \\beta_2=\\beta_3=0 \\ vs. \\ H_1: not H_0\\) 에서\nRM: model_1 (x1), FM: model_3 (x1*x2) 아래표 보자\n\nanova(model_1, model_3)\n\n\n\nA anova: 2 × 6\n\n    Res.DfRSSDfSum of SqFPr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    118472.5913NA    NA      NA          NA\n    216165.6713 2306.9214.820680.0002280824\n\n\n\n\n472.5913=SSE_RM\n165.6713=SSE_FM\n2=3-1\n\n\n\n\ndt2 <- data.frame(y=dt$y,\n                  x1=dt$x1,\n                  x2=as.numeric(dt$x2=='M'),\n                  x3=as.numeric(dt$x2=='F'))\nhead(dt2)\n                                    \n\n\n\nA data.frame: 6 × 4\n\n    yx1x2x3\n    <dbl><dbl><dbl><dbl>\n\n\n    11715110\n    226 9210\n    32117510\n    430 3110\n    52210410\n    6 127710\n\n\n\n\n\nmodel_4 <-lm(y~., dt2)\nsummary(model_4)\n\n\nCall:\nlm(formula = y ~ ., data = dt2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0165 -1.7450 -0.6055  1.8803  6.1835 \n\nCoefficients: (1 not defined because of singularities)\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 41.768865   1.948930  21.432 9.64e-14 ***\nx1          -0.100918   0.008621 -11.707 1.47e-09 ***\nx2          -7.933953   1.414702  -5.608 3.13e-05 ***\nx3                 NA         NA      NA       NA    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.123 on 17 degrees of freedom\nMultiple R-squared:  0.8991,    Adjusted R-squared:  0.8872 \nF-statistic: 75.72 on 2 and 17 DF,  p-value: 3.42e-09\n\n\n\n\\(y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_3+\\epsilon\\)\nfull Rank가 아니여서 구할 수 없다..\n1 = x2(M)+x3(F) 가 되서 full rank가 안되는데 이중 하나를 날리면 된다.\n아래 model_5는 절편이 없는 모델을 만들어서 돌려보자\n\n\nmodel_5 <- lm(y~0+x1+x2+x3,dt2)\nsummary(model_5)\n\n\nCall:\nlm(formula = y ~ 0 + x1 + x2 + x3, data = dt2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0165 -1.7450 -0.6055  1.8803  6.1835 \n\nCoefficients:\n    Estimate Std. Error t value Pr(>|t|)    \nx1 -0.100918   0.008621  -11.71 1.47e-09 ***\nx2 33.834912   1.758659   19.24 5.64e-13 ***\nx3 41.768865   1.948930   21.43 9.64e-14 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.123 on 17 degrees of freedom\nMultiple R-squared:  0.982, Adjusted R-squared:  0.9788 \nF-statistic:   309 on 3 and 17 DF,  p-value: 5.047e-15\n\n\n\nmodle5 : \\(y=\\beta_1x_1+\\beta_2x_2 + \\beta_3x_3 + \\epsilon\\)\n\\(x_2 = 1 if M, x_2=0 if F\\)\n\\(x_3 = 0 if M, x_3=1 if F\\)\n\\(E(y|M) = \\beta_1x_1 + \\beta_2\\) - (*)\n\\(E(y|F) = \\beta_1 + \\beta_3\\)\n기울기는 동일한데, 절편이 다르다. 잎에서 쓴 모형과 비교해 본다면,\n\\(E(y|M) = (\\beta_0 + \\beta_2) + \\beta_1x_1\\)에서 (*)의 \\(\\beta_2 = \\beta_0+\\beta_2\\)\n\n\n\n\n\n\ninstall.packages(\"ISLR\")\n\nInstalling package into ‘/home/coco/R/x86_64-pc-linux-gnu-library/4.2’\n(as ‘lib’ is unspecified)\n\n\n\n\nlibrary(ISLR)\n\n\nhead(Carseats)\ndim(Carseats)\n\n\n\nA data.frame: 6 × 11\n\n    SalesCompPriceIncomeAdvertisingPopulationPriceShelveLocAgeEducationUrbanUS\n    <dbl><dbl><dbl><dbl><dbl><dbl><fct><dbl><dbl><fct><fct>\n\n\n    1 9.50138 7311276120Bad   4217YesYes\n    211.22111 4816260 83Good  6510YesYes\n    310.06113 3510269 80Medium5912YesYes\n    4 7.40117100 4466 97Medium5514YesYes\n    5 4.15141 64 3340128Bad   3813YesNo \n    610.8112411313501 72Bad   7816No Yes\n\n\n\n\n\n40011\n\n\n• Sales : 판매량 (단위: 1,000)\n• Price : 각 지점에서의 카시트 가격\n• ShelveLoc : 진열대의 등급 (Bad, Medium, Good)\n• Urban :도시 여부 (Yes, No)\n• US: 미국 여부 (Yes, No)\n\n판매량을 예측하자.\n\\(y=\\beta_0 + \\beta_1x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\epsilon\\)\n\\(x_1\\):Price, \\(x_2,x_3\\)는 가변수\n\\(x_2 = 1\\), if ShelveLoc = Good, \\(x_2=0\\), if o.w.\n\\(x_3 = 1\\), if ShelveLoc = Medium, \\(x_3=0\\), if o.w.\n\\(E(y|Bad) = \\beta_0+\\beta_1x_1\\) <- base\n\\(E(y|Med) = \\beta_0 + \\beta_1x_1+ \\beta_3 = (\\beta_0+\\beta_3)+\\beta_1x_1\\)\n\\(E(y|Good) = \\beta_0 + \\beta_1x_1 + \\beta_2 = (\\beta_0+\\beta_2) + \\beta_1x_1\\)\n\n\nfit <- lm(fit<-lm(Sales~Price+ShelveLoc, \n                  data=Carseats))\nsummary(fit)  \n\n\nCall:\nlm(formula = fit <- lm(Sales ~ Price + ShelveLoc, data = Carseats))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8229 -1.3930 -0.0179  1.3868  5.0780 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     12.001802   0.503447  23.839  < 2e-16 ***\nPrice           -0.056698   0.004059 -13.967  < 2e-16 ***\nShelveLocGood    4.895848   0.285921  17.123  < 2e-16 ***\nShelveLocMedium  1.862022   0.234748   7.932 2.23e-14 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.917 on 396 degrees of freedom\nMultiple R-squared:  0.5426,    Adjusted R-squared:  0.5391 \nF-statistic: 156.6 on 3 and 396 DF,  p-value: < 2.2e-16\n\n\n\n교호작용은 보지 않겠따.\n\n\ncontrasts(Carseats$ShelveLoc)\n\n\n\nA matrix: 3 × 2 of type dbl\n\n    GoodMedium\n\n\n    Bad00\n    Good10\n    Medium01\n\n\n\n\n\nggplot(Carseats, aes(Price, Sales, col=ShelveLoc)) + \n  geom_point() + \n  theme_bw() + \n  geom_abline(slope = coef(fit)[2], \n              intercept = coef(fit)[1], col= 'darkorange')+\n  geom_abline(slope = coef(fit)[2], \n              intercept = coef(fit)[1]+coef(fit)[3], col= 'steelblue')+\n  geom_abline(slope = coef(fit)[2], \n              intercept = coef(fit)[1]+coef(fit)[4], col= 'darkgreen')+\n  guides(col=guide_legend(title=\"ShelveLoc\")) +\n  scale_color_manual(labels = c(\"Bad\", \"Good\", \"Medium\"), \n                     values = c(\"darkorange\", \"steelblue\",\"darkgreen\"))\n\n\n\n\n\n\\(y=\\beta_0 + \\beta_1x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 \\epsilon\\)\n\\(x_2 = 1\\), if ShelveLoc = Good, \\(x_2=0\\), if o.w.\n\\(x_3 = 1\\), if ShelveLoc = Medium, \\(x_3=0\\), if o.w.\n\\(x_4 = 1\\), if US=yes, \\(x_4=0\\), if US=no\n\n\ncontrasts(Carseats$US)\n\n\n\nA matrix: 2 × 1 of type dbl\n\n    Yes\n\n\n    No0\n    Yes1\n\n\n\n\n\nfit1 <- lm(fit<-lm(Sales~Price+ShelveLoc+US, \n                  data=Carseats))\nsummary(fit1)  \n\n\nCall:\nlm(formula = fit <- lm(Sales ~ Price + ShelveLoc + US, data = Carseats))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1720 -1.2587 -0.0056  1.2815  4.7462 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     11.476347   0.498083  23.041  < 2e-16 ***\nPrice           -0.057825   0.003938 -14.683  < 2e-16 ***\nShelveLocGood    4.827167   0.277294  17.408  < 2e-16 ***\nShelveLocMedium  1.893360   0.227486   8.323 1.42e-15 ***\nUSYes            1.013071   0.195034   5.194 3.30e-07 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.857 on 395 degrees of freedom\nMultiple R-squared:  0.5718,    Adjusted R-squared:  0.5675 \nF-statistic: 131.9 on 4 and 395 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\ndt <- data.frame(\n  y = c(377,249,355,475,139,452,440,257),\n  x1 = c(480,720,570,300,800,400,340,650)\n)\n\n\nggplot(data = dt, aes(x = x1, y = y)) + \n  geom_point(color='steelblue') + \n  theme_bw()\n\n\n\n\n\n\n\n### threshould = 500\n## x2(x1-xw)=x2(x1-500) = (x1 - 500)+ := x2\n\ndt$x2 = sapply(dt$x1, function(x) max(0, x-500))\n\n\nm <- lm(y ~ x1+x2, dt)\nsummary(m)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = dt)\n\nResiduals:\n      1       2       3       4       5       6       7       8 \n-22.765  29.765  18.068   4.068 -17.463  20.605 -15.117 -17.160 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 589.5447    60.4213   9.757 0.000192 ***\nx1           -0.3954     0.1492  -2.650 0.045432 *  \nx2           -0.3893     0.2310  -1.685 0.152774    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 24.49 on 5 degrees of freedom\nMultiple R-squared:  0.9693,    Adjusted R-squared:  0.9571 \nF-statistic: 79.06 on 2 and 5 DF,  p-value: 0.0001645\n\n\n\n\ndt2 <- rbind(dt[,2:3], c(500,0))\ndt2$y <- predict(m, newdata = dt2)\n\n\n# this is the predicted line of multiple linear regression\nggplot(data = dt, aes(x = x1, y = y)) + \n  geom_point(color='steelblue') +\n  geom_line(color='darkorange',\n            data = dt2, aes(x=x1, y=y))+\n  geom_vline(xintercept = 500, lty=2, col='red')+\n  theme_bw()"
  },
  {
    "objectID": "posts/Applied statistics/Simple Linear Regression.html",
    "href": "posts/Applied statistics/Simple Linear Regression.html",
    "title": "1. SLR(Simple Linear Regression)",
    "section": "",
    "text": "- 모수추정\n\n\\(\\widehat\\beta_0, \\widehat\\beta_1\\) : 최소제곱추정량(LSE)\n\\(\\widehat\\sigma^2\\) : MSE\n\n\n\n\n- 회귀직선의 유의성 검정\n\nF검정\n\\(H_0 : \\beta_1 = 0 \\ vs \\  H_1 : \\beta_1 \\neq 0\\)\n\n- 개별 회귀 계수의 유의성 검정\n\nt검정\n\\(H_0 : \\beta_0 = 0 \\ vs \\  H_1 : \\beta _{0}=\\begin{cases} >0\\\\ \\neq 0\\\\ <0\\end{cases}\\)\n\\(H_0 : \\beta_1 = 0 \\ vs \\  H_1 : \\beta _{1}=\\begin{cases} >0\\\\ \\neq 0\\\\ <0\\end{cases}\\)\n\n\nNOTE: 단순회귀모형에서는 회귀직선이 유의성검정, 개별회귀 계수의 유의성 검정이 같다. 중회귀모형에서는 다르다.\n\n\n\n\n\n\\(\\mathbb{R} ^{2}\\), MSE, \\(\\dots\\)\n\n\n\n\n\n잔차분석(오차항에 대한 가정 검토)\n이상점(leverage point)\n변수선택, 다중곤산성"
  },
  {
    "objectID": "posts/Applied statistics/Simple Linear Regression.html#beta_1에-대한-추론",
    "href": "posts/Applied statistics/Simple Linear Regression.html#beta_1에-대한-추론",
    "title": "1. SLR(Simple Linear Regression)",
    "section": "\\(\\beta_1\\)에 대한 추론",
    "text": "\\(\\beta_1\\)에 대한 추론\n\\[\\widehat \\beta_1 = \\dfrac{S_{(xy)}}{S_{(xx)}}\\]\n분자 \\(S_{(xy)}= \\sum(x_i - \\bar x)(y_i - \\bar y) = \\sum(x_i - \\bar x)y_i - \\sum(x_i- \\bar x)\\bar y= \\sum(x_i - \\bar x)y_i\\)\n\\(\\widehat \\beta_1 = \\dfrac{S_{(xy)}}{S_{(xx)}}=\\dfrac{\\sum(x_i-\\bar x)y_i}{S_{(xx)}}=\\sum \\dfrac{x_i- \\bar x}{S_{(xx)}} y_i= \\sum a_i y_i\\)\n\\[\\widehat \\beta_1 \\sim N(\\beta_1, \\dfrac{\\sigma^2}{S_{(xx)}})\\]\n- \\(E(\\widehat \\beta_1)=\\beta_1\\) : 불편추정량(unbiase-)\n\n\\(E(\\widehat \\beta_1)\\)\n\n= \\(\\sum a_i E(y_i)\\)\n= \\(\\sum \\dfrac{(x_i - \\bar x)}{S_{xx}}(\\beta_0 + \\beta_1 x_i)\\)\n= \\(\\dfrac{1}{S_{xx}}[\\beta_0 \\sum(x_i - \\bar x) + \\beta_1 \\sum(x_i - \\bar x) x_i]\\)\n\\(\\because \\sum(x_i - \\bar x)=0\\)\n\\(\\because \\sum(x_i - \\bar x)(x_i -\\bar x + \\bar x) = \\sum(x_i- \\bar x)^2 + \\bar x \\sum(x_i - \\bar x) =\\sum(x_i- \\bar x)^2= S_{(xx)}\\)\n= \\(\\dfrac{\\beta_1 S_{xx}}{S_{xx}} = \\beta_1\\)\n- \\(Var(\\widehat \\beta_1)\\)\n\n\\(Var(\\widehat \\beta_1)\\)\n\n= \\(Var(\\sum a_i y_i)\\)\n= \\(\\sum a_i^2 Var(y_i)\\)\n\\(\\because Var(y_i)=\\sigma^2\\)\n= \\(\\sigma^2 \\sum a_i^2\\)\n= \\(\\sigma^2 \\sum \\dfrac{(x_i- \\bar x)^2}{S_{xx}^2}\\)\n= \\(\\sigma^2 \\dfrac{S_{xx}}{S_{xx}^2}\\)\n= \\(\\dfrac{\\sigma^2}{S_{xx}}\\)\n- BLUE\n\nBest Linear Unbiased Estimation\n\n\\[\\widehat{Var}(\\widehat \\beta_1) = \\dfrac{MSE}{S_{xx}}\\]\n\\[\\widehat \\sigma_{\\widehat \\beta_1} = \\sqrt{\\dfrac{MSE}{S_{xx}}}\\]\n- stuendtized \\(\\widehat \\beta_1\\)의 분포\n\\[\\dfrac{\\widehat \\beta_1 - \\beta_1}{\\widehat \\sigma / \\sqrt{S_{xx}}} \\sim t(n-2), \\widehat \\sigma = \\sqrt{MSE}\\]\n- \\(\\widehat \\beta_1\\)의 \\(100(1-\\alpha)\\)% 신뢰구간\n\\[\\widehat \\beta_1 \\pm t_{\\alpha/2}(n-2) \\dfrac{\\widehat \\sigma}{\\sqrt{S_{xx}}}\\]\n\n\\(\\sigma\\)를 몰라 추정하므로 t분포로 바뀌고 분산이 더 커진다.\n\n- 모회귀계수(기울기) \\(\\beta_1\\)에 대한 추론\n\nt검정\n\\(H_0 : \\beta_1 = 0 \\ vs \\  H_1 : \\beta _{1}=\\begin{cases} >0\\\\ \\neq 0\\\\ <0\\end{cases}\\)\n검정통계량\n\n\\[\\dfrac{\\widehat \\beta_1 - \\beta_1}{\\widehat \\sigma / \\sqrt{S_{xx}}} \\sim t(n-2)\\]"
  },
  {
    "objectID": "posts/Applied statistics/Simple Linear Regression.html#beta_0에-대한-추론",
    "href": "posts/Applied statistics/Simple Linear Regression.html#beta_0에-대한-추론",
    "title": "1. SLR(Simple Linear Regression)",
    "section": "\\(\\beta_0\\)에 대한 추론",
    "text": "\\(\\beta_0\\)에 대한 추론\n- \\(\\beta_0\\)의 최소제곱추정량\n\\[\\widehat \\beta_0 = \\bar y - \\widehat \\beta_1 \\bar x\\]\n\\[\\widehat \\beta_0 \\sim N(\\beta_0, \\sigma^2(\\dfrac{1}{n} + \\dfrac{\\bar x^2}{S_{xx}}))\\]\n- stuendtized \\(\\widehat \\beta_0\\)의 분포\n\\[\\dfrac{\\widehat \\beta_0 - \\beta_0}{\\widehat \\sigma_{\\widehat \\beta_0}} \\sim t(n-2)   ,  \\widehat \\sigma_{\\widehat \\beta_0} = \\widehat \\sigma \\sqrt{\\dfrac{1}{n}+\\dfrac{\\bar x^2}{S_{xx}}}\\]\n- \\(\\widehat \\beta_1\\)의 \\(100(1-\\alpha)\\)% 신뢰구간\n\\[\\widehat \\beta_0 \\pm t_{\\alpha/2}(n-2) \\widehat \\sigma \\sqrt{\\dfrac{1}{n}+\\dfrac{\\bar x^2}{S_{xx}}}\\]"
  },
  {
    "objectID": "posts/Applied statistics/AS2.html",
    "href": "posts/Applied statistics/AS2.html",
    "title": "AS HW2",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "posts/Applied statistics/AS2.html#section",
    "href": "posts/Applied statistics/AS2.html#section",
    "title": "AS HW2",
    "section": "(1)",
    "text": "(1)\n이 데이터의 산점도 행렬을 그리시오.\n\ndt <- read_csv('dt.csv')\nhead(dt)\n\nRows: 400 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (8): Sales, CompPrice, Income, Advertising, Population, Price, Age, Educ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nA tibble: 6 × 8\n\n    SalesCompPriceIncomeAdvertisingPopulationPriceAgeEducation\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n     9.50138 73112761204217\n    11.22111 4816260 836510\n    10.06113 3510269 805912\n     7.40117100 4466 975514\n     4.15141 64 33401283813\n    10.8112411313501 727816\n\n\n\n\n\npairs(dt, pch=16, col='darkorange')\n\n\n\n\n\n선형관계가 있어보이는 데이터는, “sales와 price”,“compprice와 porice”,\n\n\npairs(dt[,which(names(dt) %in% \n                      c('Sales', 'CompPrice', 'Price'))], \n      pch=16, col='darkorange')\n\n\n\n\n\ncor(dt[,which(names(dt) %in% \n                      c('Sales', 'CompPrice', 'Price'))])\n\n\n\nA matrix: 3 × 3 of type dbl\n\n    SalesCompPricePrice\n\n\n    Sales 1.000000000.06407873-0.4449507\n    CompPrice 0.064078731.00000000 0.5848478\n    Price-0.444950730.58484777 1.0000000\n\n\n\n\n\n2,3번 문제에서 축소모형 하니까.. 1번 문제에서는 걍 전체로 돌리자"
  },
  {
    "objectID": "posts/Applied statistics/AS2.html#section-1",
    "href": "posts/Applied statistics/AS2.html#section-1",
    "title": "AS HW2",
    "section": "(2)",
    "text": "(2)\nSales를 예측하기 위한 중회귀분석을 하려고 한다. 이를 위한 모형을 설정하시오.\n\nfit_dt<-lm(Sales~., data=dt)\nsummary(fit_dt)\n\n\nCall:\nlm(formula = Sales ~ ., data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0598 -1.3515 -0.1739  1.1331  4.8304 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  7.7076934  1.1176260   6.896 2.15e-11 ***\nCompPrice    0.0939149  0.0078395  11.980  < 2e-16 ***\nIncome       0.0128717  0.0034757   3.703 0.000243 ***\nAdvertising  0.1308637  0.0151219   8.654  < 2e-16 ***\nPopulation  -0.0001239  0.0006877  -0.180 0.857092    \nPrice       -0.0925226  0.0050521 -18.314  < 2e-16 ***\nAge         -0.0449743  0.0060083  -7.485 4.75e-13 ***\nEducation   -0.0399844  0.0371257  -1.077 0.282142    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.929 on 392 degrees of freedom\nMultiple R-squared:  0.5417,    Adjusted R-squared:  0.5335 \nF-statistic: 66.18 on 7 and 392 DF,  p-value: < 2.2e-16\n\n\n\\(\\widehat {Sales} = 7.7076934 + 0.0939149 \\widehat {CompPrice} + 0.012871 \\widehat {Income} +0.1308637 \\widehat {Advertising} -0.0001239 \\widehat {Population} -0.0925226 \\widehat {Price} -0.0449743 \\widehat {Age} -0.0399844 \\widehat {Education}\\)\n\nfit__<-lm(Sales~CompPrice+Price, data=dt)\nsummary(fit__)\n\n\nCall:\nlm(formula = Sales ~ CompPrice + Price, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.5285 -1.6207 -0.2404  1.5269  6.2437 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  6.278692   0.932774   6.731 5.91e-11 ***\nCompPrice    0.090777   0.009132   9.941  < 2e-16 ***\nPrice       -0.087458   0.005914 -14.788  < 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.269 on 397 degrees of freedom\nMultiple R-squared:  0.3578,    Adjusted R-squared:  0.3546 \nF-statistic: 110.6 on 2 and 397 DF,  p-value: < 2.2e-16\n\n\n\\(\\widehat {Sales} = 6.278692 + 0.090777 \\widehat {CompPrice} -0.087458 \\widehat {Price}\\)"
  },
  {
    "objectID": "posts/Applied statistics/AS2.html#section-2",
    "href": "posts/Applied statistics/AS2.html#section-2",
    "title": "AS HW2",
    "section": "(3)",
    "text": "(3)\n최소제곱법의 의한 회귀직선을 적합시키시키고, 모형 적합 결과를 설명하시오."
  },
  {
    "objectID": "posts/Applied statistics/AS2.html#section-3",
    "href": "posts/Applied statistics/AS2.html#section-3",
    "title": "AS HW2",
    "section": "(4)",
    "text": "(4)\n회귀직선의 유의성 검정을 위한 가설을 설정하고, 분산분석표를 이용하여 가설 검정을 수행하시오.\n\\(H_0:\\beta_0=\\dots=\\beta_7=0\\) vs. \\(H_1:not H_0\\)\n\nanova(fit_dt)\n\n\n\nA anova: 8 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    CompPrice  1  13.0666859  13.0666859  3.51177516.167778e-02\n    Income  1  79.0733616  79.0733616 21.25159065.458487e-06\n    Advertising  1 219.3512681 219.3512681 58.95238621.300900e-13\n    Population  1   0.3824026   0.3824026  0.10277377.486970e-01\n    Price  11198.86688361198.8668836322.20494605.144277e-53\n    Age  1 208.6564283 208.6564283 56.07806354.652175e-13\n    Education  1   4.3158913   4.3158913  1.15992992.821424e-01\n    Residuals3921458.5617763   3.7208209         NA          NA\n\n\n\n\n\nnull_model <- lm(Sales~1, data=dt)  \nfit_dt <- lm(Sales~., data=dt) \n\nanova(null_model, fit_dt) \n\n\n\nA anova: 2 × 6\n\n    Res.DfRSSDfSum of SqFPr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    13993182.275NA      NA      NA          NA\n    23921458.562 71723.71366.180211.413772e-62\n\n\n\n\n\n회귀직선은 유의하다.\n\n\n(1723.713/7)/(1458.562/392)\n\n66.1802021443038"
  },
  {
    "objectID": "posts/Applied statistics/AS2.html#section-4",
    "href": "posts/Applied statistics/AS2.html#section-4",
    "title": "AS HW2",
    "section": "(5)",
    "text": "(5)\n오차의 분산에 대한 추정량을 구하시오.\n\nmatrix\n\nn = nrow(dt)\nX = cbind(rep(1,n), dt$CompPrice, dt$Income, dt$Advertising, dt$Population, dt$Price, dt$Age, dt$Education)\ny = dt$Sales\n\n\nbeta_hat = solve(t(X)%*%X) %*% t(X) %*% y   # t(X): X^T를 의미함 \nbeta_hat\ncoef(fit_dt)\n\n\n\nA matrix: 8 × 1 of type dbl\n\n     7.7076934384\n     0.0939149066\n     0.0128717129\n     0.1308636707\n    -0.0001239252\n    -0.0925226099\n    -0.0449743402\n    -0.0399844437\n\n\n\n\n(Intercept)7.70769343844283CompPrice0.0939149066067561Income0.0128717128971186Advertising0.130863670692396Population-0.000123925156795604Price-0.0925226098938757Age-0.0449743402082049Education-0.0399844437382063\n\n\n\ny_hat = X %*% beta_hat\ny_hat[1:5]\nfitted(fit_dt)[1:5]\n\n\n9.34151160644639.809135309058769.510780487498658.440550277355868.05222459125309\n\n\n19.3415116064467129.8091353090589339.5107804874987748.4405502773558958.05222459125317\n\n\n\nsse <- sum((y - y_hat)^2) ##SSE\nsqrt(sse/(n-7-1)) ##RMSE\nsummary(fit_dt)$sigma\n\n1.92894293798154\n\n\n1.92894293798154\n\n\n\nmse <- sse/(n-7-1)\nmse\n\n3.72082085798886"
  },
  {
    "objectID": "posts/Applied statistics/AS2.html#section-5",
    "href": "posts/Applied statistics/AS2.html#section-5",
    "title": "AS HW2",
    "section": "(6)",
    "text": "(6)\n결정계수와 수정된 결정계수를 구하시오.\n\nsummary(fit_dt)\n\n\nCall:\nlm(formula = Sales ~ ., data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0598 -1.3515 -0.1739  1.1331  4.8304 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  7.7076934  1.1176260   6.896 2.15e-11 ***\nCompPrice    0.0939149  0.0078395  11.980  < 2e-16 ***\nIncome       0.0128717  0.0034757   3.703 0.000243 ***\nAdvertising  0.1308637  0.0151219   8.654  < 2e-16 ***\nPopulation  -0.0001239  0.0006877  -0.180 0.857092    \nPrice       -0.0925226  0.0050521 -18.314  < 2e-16 ***\nAge         -0.0449743  0.0060083  -7.485 4.75e-13 ***\nEducation   -0.0399844  0.0371257  -1.077 0.282142    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.929 on 392 degrees of freedom\nMultiple R-squared:  0.5417,    Adjusted R-squared:  0.5335 \nF-statistic: 66.18 on 7 and 392 DF,  p-value: < 2.2e-16\n\n\n\n\\(R^2:0.5417, R^2_{adj}: 0.5335\\)"
  },
  {
    "objectID": "posts/Applied statistics/AS2.html#section-6",
    "href": "posts/Applied statistics/AS2.html#section-6",
    "title": "AS HW2",
    "section": "(7)",
    "text": "(7)\n개별 회귀계수의 유의성검정을 수행하시오.\n\nsummary(fit_dt)$coef\n\n\n\nA matrix: 8 × 4 of type dbl\n\n    EstimateStd. Errort valuePr(>|t|)\n\n\n    (Intercept) 7.70769343841.1176259965  6.89648732.145154e-11\n    CompPrice 0.09391490660.0078395225 11.97967182.153866e-28\n    Income 0.01287171290.0034756701  3.70337592.432641e-04\n    Advertising 0.13086367070.0151219066  8.65391341.302560e-16\n    Population-0.00012392520.0006877272 -0.18019528.570924e-01\n    Price-0.09252260990.0050520870-18.31374031.409811e-54\n    Age-0.04497434020.0060082977 -7.48537154.751078e-13\n    Education-0.03998444370.0371257460 -1.07700042.821424e-01"
  },
  {
    "objectID": "posts/Applied statistics/AS2.html#section-7",
    "href": "posts/Applied statistics/AS2.html#section-7",
    "title": "AS HW2",
    "section": "(8)",
    "text": "(8)\n회귀계수에 대한 90% 신뢰구간을 구하시오.\n\nconfint(fit_dt, level = 0.90)\n\n\n\nA matrix: 8 × 2 of type dbl\n\n    5 %95 %\n\n\n    (Intercept) 5.865007519 9.550379358\n    CompPrice 0.080989493 0.106840320\n    Income 0.007141202 0.018602224\n    Advertising 0.105931426 0.155795915\n    Population-0.001257815 0.001009965\n    Price-0.100852239-0.084192981\n    Age-0.054880521-0.035068159\n    Education-0.101195519 0.021226632"
  },
  {
    "objectID": "posts/Applied statistics/AS2.html#section-8",
    "href": "posts/Applied statistics/AS2.html#section-8",
    "title": "AS HW2",
    "section": "(9)",
    "text": "(9)\nCompPrice = 100, Income = 70, Advertising = 20, Population = 300, Price = 80, Education = 12인 지역에 위치한 매장의 평균 판매액을 예측하고, 95% 신뢰구간을 구하시오.\n\nnew_dt <- data.frame(CompPrice=100, Income=70, Advertising=20, Population=300, Price=80, Age=53, Education=12)\n\n\npredict(fit_dt, \n        newdata = new_dt,\n        interval = c(\"confidence\"), \n        level = 0.95)  ##평균반응\n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    110.315049.74614710.88393\n\n\n\n\n\n문제에서 Age에 대한 값이 명시되지 않아서.. 일단 age는 평균 값 넣어서 계산함"
  },
  {
    "objectID": "posts/Applied statistics/AS2.html#section-9",
    "href": "posts/Applied statistics/AS2.html#section-9",
    "title": "AS HW2",
    "section": "(10)",
    "text": "(10)\n위 매장에 대하여 개별 판매액 예측하고, 95% 신뢰구간을 구하시오.\n\npredict(fit_dt, newdata = new_dt, \n        interval = c(\"prediction\"), \n        level = 0.95)  ## 개별 y\n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    110.315046.48023814.14984"
  },
  {
    "objectID": "posts/Applied statistics/AS2.html#section-10",
    "href": "posts/Applied statistics/AS2.html#section-10",
    "title": "AS HW2",
    "section": "(11)",
    "text": "(11)\n잔차에 대한 산점도를 그리고, 결과를 설명하여라.\n\nyhat <- fitted(fit_dt)\nres <- resid(fit_dt)\n\n\nplot(res ~ yhat,pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\n\n\n\n\n선형성은 없어보이고, 등분산성이 있어보인다."
  },
  {
    "objectID": "posts/Applied statistics/AS2.html#section-11",
    "href": "posts/Applied statistics/AS2.html#section-11",
    "title": "AS HW2",
    "section": "(12)",
    "text": "(12)\n잔차에 대한 등분산성 검정을 수행하여라.\n\\(H_0\\):등분산 VS. \\(H_1\\):이분산\n\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\nAttaching package: ‘zoo’\n\n\nThe following objects are masked from ‘package:base’:\n\n    as.Date, as.Date.numeric\n\n\n\n\n\nbptest(fit_dt)\n\n\n    studentized Breusch-Pagan test\n\ndata:  fit_dt\nBP = 1.1751, df = 7, p-value = 0.9915\n\n\n\np-valeur가 커서 \\(H_0\\)를 채택한다. 즉 등분산성이다."
  },
  {
    "objectID": "posts/Applied statistics/AS2.html#section-12",
    "href": "posts/Applied statistics/AS2.html#section-12",
    "title": "AS HW2",
    "section": "(13)",
    "text": "(13)\n잔차에 대한 히스토그램, QQ plot을 그리고, 정규성 검정을 수행하여라.\n\npar(mfrow=c(1,2))\nqqnorm(res, pch=16)\nqqline(res, col = 2)\n\nhist(res)\npar(mfrow=c(1,1))\n\n\n\n\n\n정규성을 만족해 보인다.\n\n\n## H0 : normal distribution  vs. H1 : not H0\nshapiro.test(res)\n\n\n    Shapiro-Wilk normality test\n\ndata:  res\nW = 0.98602, p-value = 0.0006715"
  },
  {
    "objectID": "posts/Applied statistics/AS2.html#section-13",
    "href": "posts/Applied statistics/AS2.html#section-13",
    "title": "AS HW2",
    "section": "(14)",
    "text": "(14)\n잔차에 대한 독립성 검정을 수행하시오.\n\\(H_0\\):uncorrelated\n\ndwtest(fit_dt, alternative = \"two.sided\") \n\n\n    Durbin-Watson test\n\ndata:  fit_dt\nDW = 1.9694, p-value = 0.7622\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n독립이다."
  },
  {
    "objectID": "posts/Applied statistics/AS2.html#section-14",
    "href": "posts/Applied statistics/AS2.html#section-14",
    "title": "AS HW2",
    "section": "(1)",
    "text": "(1)\n위에서 적합한 모형에서 개별 회귀계수의 유의성 검정 결과 유의하지 않은 변수는 무엇인가?\nPopulation, Education은 유의하지 않은 변수이다."
  },
  {
    "objectID": "posts/Applied statistics/AS2.html#section-15",
    "href": "posts/Applied statistics/AS2.html#section-15",
    "title": "AS HW2",
    "section": "(2)",
    "text": "(2)\n위에서 유의하지 않았던 변수를 제외한 모형을 축소모형(Reduced Model)으로 하는 부분 F검정을 수행하여라. 검정에 필요한 가설을 설정하고, 검정 결과를 설명하시오.\n\nreduced_model<-lm(Sales~.-Population-Education, data=dt)\n\n\nanova(reduced_model, fit_dt)\n\n\n\nA anova: 2 × 6\n\n    Res.DfRSSDfSum of SqFPr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    13941462.897NA      NA       NA       NA\n    23921458.562 24.3350870.58254450.5589583"
  },
  {
    "objectID": "posts/Applied statistics/AS2.html#section-16",
    "href": "posts/Applied statistics/AS2.html#section-16",
    "title": "AS HW2",
    "section": "(3)",
    "text": "(3)\n1번에서 설정한 모형과, 축소모형 중 어느 모형이 이 데이터에 대한 설명을 잘 하고 있는지를 비교하시오.\n\nsummary(reduced_model)\nsummary(fit_dt)\n\n\nCall:\nlm(formula = Sales ~ . - Population - Education, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.9071 -1.3081 -0.1892  1.1495  4.6980 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  7.109190   0.943940   7.531 3.46e-13 ***\nCompPrice    0.093904   0.007792  12.051  < 2e-16 ***\nIncome       0.013092   0.003465   3.779 0.000182 ***\nAdvertising  0.130611   0.014572   8.963  < 2e-16 ***\nPrice       -0.092543   0.005044 -18.347  < 2e-16 ***\nAge         -0.044971   0.005994  -7.503 4.20e-13 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.927 on 394 degrees of freedom\nMultiple R-squared:  0.5403,    Adjusted R-squared:  0.5345 \nF-statistic: 92.62 on 5 and 394 DF,  p-value: < 2.2e-16\n\n\n\nCall:\nlm(formula = Sales ~ ., data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0598 -1.3515 -0.1739  1.1331  4.8304 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  7.7076934  1.1176260   6.896 2.15e-11 ***\nCompPrice    0.0939149  0.0078395  11.980  < 2e-16 ***\nIncome       0.0128717  0.0034757   3.703 0.000243 ***\nAdvertising  0.1308637  0.0151219   8.654  < 2e-16 ***\nPopulation  -0.0001239  0.0006877  -0.180 0.857092    \nPrice       -0.0925226  0.0050521 -18.314  < 2e-16 ***\nAge         -0.0449743  0.0060083  -7.485 4.75e-13 ***\nEducation   -0.0399844  0.0371257  -1.077 0.282142    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.929 on 392 degrees of freedom\nMultiple R-squared:  0.5417,    Adjusted R-squared:  0.5335 \nF-statistic: 66.18 on 7 and 392 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "posts/Applied statistics/AS2.html#section-17",
    "href": "posts/Applied statistics/AS2.html#section-17",
    "title": "AS HW2",
    "section": "(1)",
    "text": "(1)\n\\(H_0 : CompPrice=Income\\) vs. \\(H_1 : not H_0\\)\n\ninstall.packages(\"car\")\n\nInstalling package into ‘/home/coco/R/x86_64-pc-linux-gnu-library/4.2’\n(as ‘lib’ is unspecified)\n\n\n\nlinearHypothesis(dt_fit, c(0,1,-1,0,0,0,0,0),0)"
  },
  {
    "objectID": "posts/Applied statistics/AS2.html#section-18",
    "href": "posts/Applied statistics/AS2.html#section-18",
    "title": "AS HW2",
    "section": "(2)",
    "text": "(2)\n\\(H_0 : CompPrice=-Price\\) vs. \\(H_1 : not H_0\\)\nlinearHypothesis(fit_dt, c(0,1,0,0,0,1,0,0),0)"
  },
  {
    "objectID": "posts/Applied statistics/AS2.html#section-19",
    "href": "posts/Applied statistics/AS2.html#section-19",
    "title": "AS HW2",
    "section": "(3)",
    "text": "(3)\n\\(H_0\\)를 기각할 수 있는 제약조건을 만들어 보시오.(단 2개 이상의 변수 사용)"
  },
  {
    "objectID": "posts/est/콰트로 블로그 만드는 법.html",
    "href": "posts/est/콰트로 블로그 만드는 법.html",
    "title": "콰트로 블로그",
    "section": "",
    "text": "git에서 Repositories 생성\n\n\n생성시 Read.Me 체크\n\n\n만들어진 레퍼토리의 HTTPS주소 복사\n\n3.명령 프롬프트에서 Dropbox로 들어가기\ncd Dropbox\n\ngit clone “2번복사한주소”\n콰트로 블로그 설정파일 자동 생성\n\nquarto create-project --type website:blog\n\n유저등록 (최초 한번)\n\ngit config --global user.email \"내 깃허브 이메일\"\n\ngit config --global user.name \"내 깃허브 닉네임\"\n\ngit config credential.helper store  #(아이디와 비번 저장)\n\n\n\ngit add .\ngit commit -m .\ngit push   (실행시 id/password작성)\n\npublish\n\nquarto publish gh-pages --no-prompt --no-bropwer"
  },
  {
    "objectID": "posts/est/Python Data Analysis/경사하강법.html",
    "href": "posts/est/Python Data Analysis/경사하강법.html",
    "title": "경사하강법",
    "section": "",
    "text": "파이썬 데이터 분석"
  },
  {
    "objectID": "posts/est/Python Data Analysis/경사하강법.html#패키지-설정",
    "href": "posts/est/Python Data Analysis/경사하강법.html#패키지-설정",
    "title": "경사하강법",
    "section": "1. 패키지 설정",
    "text": "1. 패키지 설정\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/est/Python Data Analysis/경사하강법.html#데이터-준비",
    "href": "posts/est/Python Data Analysis/경사하강법.html#데이터-준비",
    "title": "경사하강법",
    "section": "2. 데이터 준비",
    "text": "2. 데이터 준비\n\nX_train = np.array([10,22,30,38,50])\ny_train = np.array([41,45,62,75,85])"
  },
  {
    "objectID": "posts/est/Python Data Analysis/경사하강법.html#탐색적-데이터-분석",
    "href": "posts/est/Python Data Analysis/경사하강법.html#탐색적-데이터-분석",
    "title": "경사하강법",
    "section": "3. 탐색적 데이터 분석",
    "text": "3. 탐색적 데이터 분석\n\nplt.scatter(X_train, y_train, color='b')\nplt.show()\n\n\n\n\n\nnp.corrcoef(X_train, y_train)\n\narray([[1.        , 0.97319891],\n       [0.97319891, 1.        ]])"
  },
  {
    "objectID": "posts/est/Python Data Analysis/경사하강법.html#피처-스케일링",
    "href": "posts/est/Python Data Analysis/경사하강법.html#피처-스케일링",
    "title": "경사하강법",
    "section": "4. 피처 스케일링",
    "text": "4. 피처 스케일링\n\nX_train = X_train.reshape(-1,1)   # NX1 매트릭스\nprint(X_train)\ny_train= y_train.reshape(-1,1)\nprint(y_train)\n\n[[10]\n [22]\n [30]\n [38]\n [50]]\n[[41]\n [45]\n [62]\n [75]\n [85]]\n\n\n- 정규화\n\nscalerX=StandardScaler()\nscalerX.fit(X_train)\nX_train=scalerX.transform(X_train)\nprint(X_train)\n\n[[-1.46805055e+00]\n [-5.87220220e-01]\n [-8.88178420e-17]\n [ 5.87220220e-01]\n [ 1.46805055e+00]]\n\n\n\nscalerY=StandardScaler()\nscalerY.fit(y_train)\ny_train=scalerY.transform(y_train)\nprint(y_train)\n\n[[-1.21929784]\n [-0.98254098]\n [ 0.02367569]\n [ 0.79313549]\n [ 1.38502764]]"
  },
  {
    "objectID": "posts/est/Python Data Analysis/경사하강법.html#모형화-및-학습",
    "href": "posts/est/Python Data Analysis/경사하강법.html#모형화-및-학습",
    "title": "경사하강법",
    "section": "5. 모형화 및 학습",
    "text": "5. 모형화 및 학습\n\nmodel=SGDRegressor(verbose=1)\n\n\nmodel.fit(X_train, y_train)\n\n-- Epoch 1\nNorm: 0.04, NNZs: 1, Bias: -0.001467, T: 5, Avg. loss: 0.489615\nTotal training time: 0.00 seconds.\n-- Epoch 2\nNorm: 0.07, NNZs: 1, Bias: -0.002592, T: 10, Avg. loss: 0.454060\nTotal training time: 0.00 seconds.\n-- Epoch 3\nNorm: 0.09, NNZs: 1, Bias: -0.002601, T: 15, Avg. loss: 0.429691\nTotal training time: 0.00 seconds.\n-- Epoch 4\nNorm: 0.11, NNZs: 1, Bias: -0.002910, T: 20, Avg. loss: 0.408928\nTotal training time: 0.00 seconds.\n-- Epoch 5\nNorm: 0.13, NNZs: 1, Bias: -0.002887, T: 25, Avg. loss: 0.390899\nTotal training time: 0.00 seconds.\n-- Epoch 6\nNorm: 0.15, NNZs: 1, Bias: -0.003017, T: 30, Avg. loss: 0.374571\nTotal training time: 0.00 seconds.\n-- Epoch 7\nNorm: 0.17, NNZs: 1, Bias: -0.002987, T: 35, Avg. loss: 0.359796\nTotal training time: 0.00 seconds.\n-- Epoch 8\nNorm: 0.19, NNZs: 1, Bias: -0.003047, T: 40, Avg. loss: 0.346121\nTotal training time: 0.00 seconds.\n-- Epoch 9\nNorm: 0.20, NNZs: 1, Bias: -0.003015, T: 45, Avg. loss: 0.333511\nTotal training time: 0.00 seconds.\n-- Epoch 10\nNorm: 0.22, NNZs: 1, Bias: -0.003040, T: 50, Avg. loss: 0.321704\nTotal training time: 0.00 seconds.\n-- Epoch 11\nNorm: 0.23, NNZs: 1, Bias: -0.003007, T: 55, Avg. loss: 0.310696\nTotal training time: 0.00 seconds.\n-- Epoch 12\nNorm: 0.24, NNZs: 1, Bias: -0.003012, T: 60, Avg. loss: 0.300312\nTotal training time: 0.00 seconds.\n-- Epoch 13\nNorm: 0.26, NNZs: 1, Bias: -0.002979, T: 65, Avg. loss: 0.290560\nTotal training time: 0.00 seconds.\n-- Epoch 14\nNorm: 0.27, NNZs: 1, Bias: -0.002972, T: 70, Avg. loss: 0.281311\nTotal training time: 0.00 seconds.\n-- Epoch 15\nNorm: 0.28, NNZs: 1, Bias: -0.002940, T: 75, Avg. loss: 0.272580\nTotal training time: 0.00 seconds.\n-- Epoch 16\nNorm: 0.29, NNZs: 1, Bias: -0.002925, T: 80, Avg. loss: 0.264268\nTotal training time: 0.00 seconds.\n-- Epoch 17\nNorm: 0.30, NNZs: 1, Bias: -0.002893, T: 85, Avg. loss: 0.256388\nTotal training time: 0.00 seconds.\n-- Epoch 18\nNorm: 0.31, NNZs: 1, Bias: -0.002873, T: 90, Avg. loss: 0.248864\nTotal training time: 0.00 seconds.\n-- Epoch 19\nNorm: 0.33, NNZs: 1, Bias: -0.002843, T: 95, Avg. loss: 0.241709\nTotal training time: 0.00 seconds.\n-- Epoch 20\nNorm: 0.34, NNZs: 1, Bias: -0.002819, T: 100, Avg. loss: 0.234860\nTotal training time: 0.00 seconds.\n-- Epoch 21\nNorm: 0.35, NNZs: 1, Bias: -0.002790, T: 105, Avg. loss: 0.228330\nTotal training time: 0.00 seconds.\n-- Epoch 22\nNorm: 0.36, NNZs: 1, Bias: -0.002764, T: 110, Avg. loss: 0.222066\nTotal training time: 0.00 seconds.\n-- Epoch 23\nNorm: 0.36, NNZs: 1, Bias: -0.002736, T: 115, Avg. loss: 0.216080\nTotal training time: 0.00 seconds.\n-- Epoch 24\nNorm: 0.37, NNZs: 1, Bias: -0.002709, T: 120, Avg. loss: 0.210329\nTotal training time: 0.00 seconds.\n-- Epoch 25\nNorm: 0.38, NNZs: 1, Bias: -0.002681, T: 125, Avg. loss: 0.204824\nTotal training time: 0.00 seconds.\n-- Epoch 26\nNorm: 0.39, NNZs: 1, Bias: -0.002653, T: 130, Avg. loss: 0.199526\nTotal training time: 0.00 seconds.\n-- Epoch 27\nNorm: 0.40, NNZs: 1, Bias: -0.002626, T: 135, Avg. loss: 0.194447\nTotal training time: 0.00 seconds.\n-- Epoch 28\nNorm: 0.41, NNZs: 1, Bias: -0.002598, T: 140, Avg. loss: 0.189553\nTotal training time: 0.00 seconds.\n-- Epoch 29\nNorm: 0.42, NNZs: 1, Bias: -0.002572, T: 145, Avg. loss: 0.184854\nTotal training time: 0.00 seconds.\n-- Epoch 30\nNorm: 0.42, NNZs: 1, Bias: -0.002543, T: 150, Avg. loss: 0.180321\nTotal training time: 0.00 seconds.\n-- Epoch 31\nNorm: 0.43, NNZs: 1, Bias: -0.002518, T: 155, Avg. loss: 0.175964\nTotal training time: 0.00 seconds.\n-- Epoch 32\nNorm: 0.44, NNZs: 1, Bias: -0.002489, T: 160, Avg. loss: 0.171756\nTotal training time: 0.00 seconds.\n-- Epoch 33\nNorm: 0.45, NNZs: 1, Bias: -0.002465, T: 165, Avg. loss: 0.167707\nTotal training time: 0.00 seconds.\n-- Epoch 34\nNorm: 0.45, NNZs: 1, Bias: -0.002436, T: 170, Avg. loss: 0.163793\nTotal training time: 0.00 seconds.\n-- Epoch 35\nNorm: 0.46, NNZs: 1, Bias: -0.002413, T: 175, Avg. loss: 0.160023\nTotal training time: 0.00 seconds.\n-- Epoch 36\nNorm: 0.47, NNZs: 1, Bias: -0.002385, T: 180, Avg. loss: 0.156377\nTotal training time: 0.00 seconds.\n-- Epoch 37\nNorm: 0.48, NNZs: 1, Bias: -0.002361, T: 185, Avg. loss: 0.152861\nTotal training time: 0.00 seconds.\n-- Epoch 38\nNorm: 0.48, NNZs: 1, Bias: -0.002334, T: 190, Avg. loss: 0.149458\nTotal training time: 0.00 seconds.\n-- Epoch 39\nNorm: 0.49, NNZs: 1, Bias: -0.002311, T: 195, Avg. loss: 0.146173\nTotal training time: 0.00 seconds.\n-- Epoch 40\nNorm: 0.50, NNZs: 1, Bias: -0.002284, T: 200, Avg. loss: 0.142992\nTotal training time: 0.00 seconds.\n-- Epoch 41\nNorm: 0.50, NNZs: 1, Bias: -0.002262, T: 205, Avg. loss: 0.139920\nTotal training time: 0.00 seconds.\n-- Epoch 42\nNorm: 0.51, NNZs: 1, Bias: -0.002236, T: 210, Avg. loss: 0.136943\nTotal training time: 0.00 seconds.\n-- Epoch 43\nNorm: 0.51, NNZs: 1, Bias: -0.002214, T: 215, Avg. loss: 0.134066\nTotal training time: 0.00 seconds.\n-- Epoch 44\nNorm: 0.52, NNZs: 1, Bias: -0.002188, T: 220, Avg. loss: 0.131275\nTotal training time: 0.00 seconds.\n-- Epoch 45\nNorm: 0.53, NNZs: 1, Bias: -0.002168, T: 225, Avg. loss: 0.128577\nTotal training time: 0.00 seconds.\n-- Epoch 46\nNorm: 0.53, NNZs: 1, Bias: -0.002142, T: 230, Avg. loss: 0.125959\nTotal training time: 0.00 seconds.\n-- Epoch 47\nNorm: 0.54, NNZs: 1, Bias: -0.002122, T: 235, Avg. loss: 0.123425\nTotal training time: 0.00 seconds.\n-- Epoch 48\nNorm: 0.54, NNZs: 1, Bias: -0.002097, T: 240, Avg. loss: 0.120966\nTotal training time: 0.00 seconds.\n-- Epoch 49\nNorm: 0.55, NNZs: 1, Bias: -0.002077, T: 245, Avg. loss: 0.118586\nTotal training time: 0.00 seconds.\n-- Epoch 50\nNorm: 0.55, NNZs: 1, Bias: -0.002053, T: 250, Avg. loss: 0.116273\nTotal training time: 0.00 seconds.\n-- Epoch 51\nNorm: 0.56, NNZs: 1, Bias: -0.002034, T: 255, Avg. loss: 0.114034\nTotal training time: 0.00 seconds.\n-- Epoch 52\nNorm: 0.56, NNZs: 1, Bias: -0.002010, T: 260, Avg. loss: 0.111858\nTotal training time: 0.00 seconds.\n-- Epoch 53\nNorm: 0.57, NNZs: 1, Bias: -0.001992, T: 265, Avg. loss: 0.109749\nTotal training time: 0.00 seconds.\n-- Epoch 54\nNorm: 0.57, NNZs: 1, Bias: -0.001968, T: 270, Avg. loss: 0.107699\nTotal training time: 0.00 seconds.\n-- Epoch 55\nNorm: 0.58, NNZs: 1, Bias: -0.001950, T: 275, Avg. loss: 0.105712\nTotal training time: 0.00 seconds.\n-- Epoch 56\nNorm: 0.58, NNZs: 1, Bias: -0.001927, T: 280, Avg. loss: 0.103779\nTotal training time: 0.00 seconds.\n-- Epoch 57\nNorm: 0.59, NNZs: 1, Bias: -0.001910, T: 285, Avg. loss: 0.101905\nTotal training time: 0.00 seconds.\n-- Epoch 58\nNorm: 0.59, NNZs: 1, Bias: -0.001887, T: 290, Avg. loss: 0.100081\nTotal training time: 0.00 seconds.\n-- Epoch 59\nNorm: 0.60, NNZs: 1, Bias: -0.001870, T: 295, Avg. loss: 0.098312\nTotal training time: 0.00 seconds.\n-- Epoch 60\nNorm: 0.60, NNZs: 1, Bias: -0.001849, T: 300, Avg. loss: 0.096591\nTotal training time: 0.00 seconds.\n-- Epoch 61\nNorm: 0.61, NNZs: 1, Bias: -0.001832, T: 305, Avg. loss: 0.094920\nTotal training time: 0.00 seconds.\n-- Epoch 62\nNorm: 0.61, NNZs: 1, Bias: -0.001811, T: 310, Avg. loss: 0.093293\nTotal training time: 0.00 seconds.\n-- Epoch 63\nNorm: 0.62, NNZs: 1, Bias: -0.001795, T: 315, Avg. loss: 0.091713\nTotal training time: 0.00 seconds.\n-- Epoch 64\nNorm: 0.62, NNZs: 1, Bias: -0.001774, T: 320, Avg. loss: 0.090175\nTotal training time: 0.00 seconds.\n-- Epoch 65\nNorm: 0.62, NNZs: 1, Bias: -0.001758, T: 325, Avg. loss: 0.088681\nTotal training time: 0.00 seconds.\n-- Epoch 66\nNorm: 0.63, NNZs: 1, Bias: -0.001738, T: 330, Avg. loss: 0.087226\nTotal training time: 0.00 seconds.\n-- Epoch 67\nNorm: 0.63, NNZs: 1, Bias: -0.001723, T: 335, Avg. loss: 0.085812\nTotal training time: 0.00 seconds.\n-- Epoch 68\nNorm: 0.64, NNZs: 1, Bias: -0.001703, T: 340, Avg. loss: 0.084435\nTotal training time: 0.00 seconds.\n-- Epoch 69\nNorm: 0.64, NNZs: 1, Bias: -0.001688, T: 345, Avg. loss: 0.083096\nTotal training time: 0.00 seconds.\n-- Epoch 70\nNorm: 0.64, NNZs: 1, Bias: -0.001669, T: 350, Avg. loss: 0.081791\nTotal training time: 0.00 seconds.\n-- Epoch 71\nNorm: 0.65, NNZs: 1, Bias: -0.001654, T: 355, Avg. loss: 0.080522\nTotal training time: 0.00 seconds.\n-- Epoch 72\nNorm: 0.65, NNZs: 1, Bias: -0.001635, T: 360, Avg. loss: 0.079286\nTotal training time: 0.00 seconds.\n-- Epoch 73\nNorm: 0.65, NNZs: 1, Bias: -0.001621, T: 365, Avg. loss: 0.078083\nTotal training time: 0.00 seconds.\n-- Epoch 74\nNorm: 0.66, NNZs: 1, Bias: -0.001603, T: 370, Avg. loss: 0.076910\nTotal training time: 0.00 seconds.\n-- Epoch 75\nNorm: 0.66, NNZs: 1, Bias: -0.001589, T: 375, Avg. loss: 0.075769\nTotal training time: 0.00 seconds.\n-- Epoch 76\nNorm: 0.67, NNZs: 1, Bias: -0.001571, T: 380, Avg. loss: 0.074657\nTotal training time: 0.00 seconds.\n-- Epoch 77\nNorm: 0.67, NNZs: 1, Bias: -0.001558, T: 385, Avg. loss: 0.073574\nTotal training time: 0.00 seconds.\n-- Epoch 78\nNorm: 0.67, NNZs: 1, Bias: -0.001540, T: 390, Avg. loss: 0.072518\nTotal training time: 0.00 seconds.\n-- Epoch 79\nNorm: 0.68, NNZs: 1, Bias: -0.001527, T: 395, Avg. loss: 0.071490\nTotal training time: 0.00 seconds.\n-- Epoch 80\nNorm: 0.68, NNZs: 1, Bias: -0.001510, T: 400, Avg. loss: 0.070487\nTotal training time: 0.00 seconds.\n-- Epoch 81\nNorm: 0.68, NNZs: 1, Bias: -0.001497, T: 405, Avg. loss: 0.069511\nTotal training time: 0.00 seconds.\n-- Epoch 82\nNorm: 0.69, NNZs: 1, Bias: -0.001480, T: 410, Avg. loss: 0.068558\nTotal training time: 0.00 seconds.\n-- Epoch 83\nNorm: 0.69, NNZs: 1, Bias: -0.001468, T: 415, Avg. loss: 0.067630\nTotal training time: 0.00 seconds.\n-- Epoch 84\nNorm: 0.69, NNZs: 1, Bias: -0.001452, T: 420, Avg. loss: 0.066725\nTotal training time: 0.00 seconds.\n-- Epoch 85\nNorm: 0.69, NNZs: 1, Bias: -0.001440, T: 425, Avg. loss: 0.065843\nTotal training time: 0.00 seconds.\nConvergence after 85 epochs took 0.00 seconds\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nSGDRegressor(verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SGDRegressorSGDRegressor(verbose=1)"
  },
  {
    "objectID": "posts/est/Python Data Analysis/경사하강법.html#예측",
    "href": "posts/est/Python Data Analysis/경사하강법.html#예측",
    "title": "경사하강법",
    "section": "6.예측",
    "text": "6.예측\n\nX_test = np.array([45]).reshape(-1,1)\nX_test=scalerX.transform(X_test)\nX_test\n\narray([[45.]])\n\n\n\ny_pred=model.predict(X_test)\nprint(y_pred)\n\n[31.27270054]\n\n\ny_pred_inverse=scalerY.inverse_transform(y_pred)\nprint(y_pred_inverse)"
  },
  {
    "objectID": "posts/est/Python Data Analysis/경사하강법.html#패키지-설정-1",
    "href": "posts/est/Python Data Analysis/경사하강법.html#패키지-설정-1",
    "title": "경사하강법",
    "section": "1. 패키지 설정",
    "text": "1. 패키지 설정\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd"
  },
  {
    "objectID": "posts/est/Python Data Analysis/경사하강법.html#데이터-준비-1",
    "href": "posts/est/Python Data Analysis/경사하강법.html#데이터-준비-1",
    "title": "경사하강법",
    "section": "2.데이터 준비",
    "text": "2.데이터 준비\n\ndiabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n\n\nprint(np.shape(diabetes_X))\nprint(np.shape(diabetes_y))\n\n(442, 10)\n(442,)\n\n\n\nprint(diabetes_X)\n\n[[ 0.03807591  0.05068012  0.06169621 ... -0.00259226  0.01990749\n  -0.01764613]\n [-0.00188202 -0.04464164 -0.05147406 ... -0.03949338 -0.06833155\n  -0.09220405]\n [ 0.08529891  0.05068012  0.04445121 ... -0.00259226  0.00286131\n  -0.02593034]\n ...\n [ 0.04170844  0.05068012 -0.01590626 ... -0.01107952 -0.04688253\n   0.01549073]\n [-0.04547248 -0.04464164  0.03906215 ...  0.02655962  0.04452873\n  -0.02593034]\n [-0.04547248 -0.04464164 -0.0730303  ... -0.03949338 -0.00422151\n   0.00306441]]\n\n\n\nprint(diabetes_y)\n\n[151.  75. 141. 206. 135.  97. 138.  63. 110. 310. 101.  69. 179. 185.\n 118. 171. 166. 144.  97. 168.  68.  49.  68. 245. 184. 202. 137.  85.\n 131. 283. 129.  59. 341.  87.  65. 102. 265. 276. 252.  90. 100.  55.\n  61.  92. 259.  53. 190. 142.  75. 142. 155. 225.  59. 104. 182. 128.\n  52.  37. 170. 170.  61. 144.  52. 128.  71. 163. 150.  97. 160. 178.\n  48. 270. 202. 111.  85.  42. 170. 200. 252. 113. 143.  51.  52. 210.\n  65. 141.  55. 134.  42. 111.  98. 164.  48.  96.  90. 162. 150. 279.\n  92.  83. 128. 102. 302. 198.  95.  53. 134. 144. 232.  81. 104.  59.\n 246. 297. 258. 229. 275. 281. 179. 200. 200. 173. 180.  84. 121. 161.\n  99. 109. 115. 268. 274. 158. 107.  83. 103. 272.  85. 280. 336. 281.\n 118. 317. 235.  60. 174. 259. 178. 128.  96. 126. 288.  88. 292.  71.\n 197. 186.  25.  84.  96. 195.  53. 217. 172. 131. 214.  59.  70. 220.\n 268. 152.  47.  74. 295. 101. 151. 127. 237. 225.  81. 151. 107.  64.\n 138. 185. 265. 101. 137. 143. 141.  79. 292. 178.  91. 116.  86. 122.\n  72. 129. 142.  90. 158.  39. 196. 222. 277.  99. 196. 202. 155.  77.\n 191.  70.  73.  49.  65. 263. 248. 296. 214. 185.  78.  93. 252. 150.\n  77. 208.  77. 108. 160.  53. 220. 154. 259.  90. 246. 124.  67.  72.\n 257. 262. 275. 177.  71.  47. 187. 125.  78.  51. 258. 215. 303. 243.\n  91. 150. 310. 153. 346.  63.  89.  50.  39. 103. 308. 116. 145.  74.\n  45. 115. 264.  87. 202. 127. 182. 241.  66.  94. 283.  64. 102. 200.\n 265.  94. 230. 181. 156. 233.  60. 219.  80.  68. 332. 248.  84. 200.\n  55.  85.  89.  31. 129.  83. 275.  65. 198. 236. 253. 124.  44. 172.\n 114. 142. 109. 180. 144. 163. 147.  97. 220. 190. 109. 191. 122. 230.\n 242. 248. 249. 192. 131. 237.  78. 135. 244. 199. 270. 164.  72.  96.\n 306.  91. 214.  95. 216. 263. 178. 113. 200. 139. 139.  88. 148.  88.\n 243.  71.  77. 109. 272.  60.  54. 221.  90. 311. 281. 182. 321.  58.\n 262. 206. 233. 242. 123. 167.  63. 197.  71. 168. 140. 217. 121. 235.\n 245.  40.  52. 104. 132.  88.  69. 219.  72. 201. 110.  51. 277.  63.\n 118.  69. 273. 258.  43. 198. 242. 232. 175.  93. 168. 275. 293. 281.\n  72. 140. 189. 181. 209. 136. 261. 113. 131. 174. 257.  55.  84.  42.\n 146. 212. 233.  91. 111. 152. 120.  67. 310.  94. 183.  66. 173.  72.\n  49.  64.  48. 178. 104. 132. 220.  57.]\n\n\n- 10열(s6-혈당수치) 할당\n\nX_data= diabetes_X[:,9]\nprint(X_data)\n\n[-0.01764613 -0.09220405 -0.02593034 -0.00936191 -0.04664087 -0.09634616\n -0.03835666  0.00306441  0.01134862 -0.01350402 -0.03421455 -0.05906719\n -0.04249877 -0.01350402 -0.07563562 -0.04249877  0.02791705 -0.0010777\n -0.01764613 -0.05492509  0.01549073 -0.01764613 -0.01350402  0.13561183\n -0.05492509 -0.03421455 -0.0052198   0.04034337 -0.05492509  0.05276969\n -0.00936191 -0.04249877  0.02791705  0.00306441 -0.06735141  0.01963284\n -0.02593034 -0.0052198   0.02377494  0.00306441  0.09419076 -0.00936191\n -0.03421455  0.07348023 -0.01764613 -0.05078298 -0.08806194 -0.08391984\n -0.02178823 -0.01350402  0.00720652 -0.02178823 -0.01350402  0.04862759\n  0.00720652  0.00720652 -0.01764613 -0.06735141 -0.05078298  0.01963284\n -0.03421455  0.01963284 -0.02593034 -0.07149352  0.01134862  0.01549073\n  0.01963284  0.00306441  0.00306441  0.01963284  0.04034337  0.06105391\n -0.01764613 -0.00936191  0.07348023  0.02791705  0.01549073 -0.0010777\n -0.08391984 -0.02593034 -0.0052198  -0.0010777  -0.07977773 -0.04249877\n -0.12948301 -0.07149352 -0.03421455 -0.03835666  0.06933812  0.01963284\n  0.01134862  0.01963284  0.01134862 -0.05492509 -0.09220405 -0.0052198\n  0.03205916  0.07762233 -0.06735141 -0.05492509 -0.0010777   0.03620126\n -0.0010777  -0.03007245 -0.0632093   0.01963284 -0.08391984 -0.0052198\n  0.04034337  0.10661708  0.01549073 -0.04664087  0.00306441  0.09004865\n  0.02377494  0.00306441  0.02791705  0.13561183  0.0569118   0.02791705\n -0.02178823  0.01134862  0.04862759  0.07348023 -0.01350402  0.00720652\n -0.05492509 -0.09634616 -0.07149352  0.1190434   0.07348023 -0.07977773\n  0.02377494 -0.03007245 -0.04664087  0.0569118  -0.02178823 -0.01350402\n  0.07348023 -0.0052198   0.01549073  0.13146972 -0.0010777  -0.04249877\n  0.00720652 -0.05078298  0.00720652  0.04034337 -0.0010777  -0.03835666\n -0.00936191 -0.01350402  0.07348023  0.01134862  0.00306441  0.04862759\n -0.03421455  0.02791705 -0.05078298 -0.01764613 -0.04664087  0.08590655\n -0.0010777   0.06105391 -0.01350402 -0.01350402 -0.10463037  0.10661708\n  0.12732762  0.01963284 -0.01764613 -0.04664087  0.08590655 -0.05492509\n  0.00720652  0.00306441  0.01963284  0.03205916 -0.03007245 -0.00936191\n -0.0010777  -0.0632093  -0.00936191  0.0569118   0.03205916 -0.01764613\n -0.03835666 -0.10463037 -0.00936191 -0.02593034 -0.0052198  -0.00936191\n -0.05906719  0.01963284  0.02377494  0.02377494  0.01134862  0.03620126\n -0.05906719  0.00720652  0.04448548  0.02791705 -0.03007245  0.02791705\n  0.07762233 -0.02593034  0.00306441  0.03620126  0.1190434  -0.01350402\n -0.05906719 -0.02178823  0.01963284  0.00720652 -0.05492509  0.10661708\n  0.04862759  0.04448548 -0.01350402 -0.03007245 -0.06735141 -0.03835666\n  0.01963284 -0.05078298 -0.00936191  0.00306441 -0.05492509  0.00720652\n -0.05078298  0.01134862  0.06933812  0.01134862  0.03205916 -0.03835666\n  0.0569118   0.04862759  0.01134862 -0.0010777  -0.02178823  0.04862759\n  0.01963284  0.0569118  -0.07149352 -0.04249877 -0.0052198  -0.12948301\n -0.05906719 -0.09220405  0.04034337  0.00720652  0.03620126  0.04034337\n -0.03007245  0.06933812  0.09004865  0.00720652  0.01134862 -0.05078298\n  0.0569118   0.00720652 -0.01764613  0.01549073  0.00306441 -0.01764613\n -0.02178823  0.04034337  0.00306441  0.02791705  0.07348023 -0.03835666\n  0.01549073  0.00720652 -0.09220405  0.03620126  0.01549073  0.06933812\n -0.02178823  0.01549073  0.03205916 -0.02593034 -0.01764613  0.00306441\n  0.02791705 -0.03007245 -0.05492509 -0.02178823 -0.0052198  -0.0052198\n -0.01764613 -0.03421455  0.04448548 -0.05906719 -0.03835666 -0.0010777\n -0.03421455  0.00306441 -0.05906719 -0.05492509 -0.01350402 -0.0010777\n  0.04448548  0.03620126  0.02791705 -0.02178823 -0.01350402 -0.06735141\n -0.01350402  0.08176444 -0.07977773  0.08176444  0.03205916  0.02377494\n -0.03007245 -0.0010777  -0.03835666 -0.0010777   0.04034337  0.06933812\n  0.03205916  0.06105391  0.00306441  0.06105391  0.08176444  0.0569118\n  0.01963284  0.04862759  0.02791705 -0.0052198   0.01963284  0.04448548\n  0.01549073 -0.0052198   0.03620126  0.03205916 -0.05492509 -0.05906719\n  0.05276969  0.00306441 -0.05906719  0.01549073 -0.03007245 -0.01764613\n  0.02377494  0.00720652 -0.03007245 -0.0010777   0.01549073 -0.03007245\n -0.0010777  -0.07149352  0.13561183 -0.04249877 -0.03835666  0.00306441\n  0.06105391  0.01134862 -0.07563562 -0.05906719 -0.04664087  0.06105391\n  0.04034337  0.02377494  0.04034337  0.01134862  0.09833287  0.09833287\n  0.04862759  0.03205916  0.08176444  0.02791705 -0.07563562 -0.01764613\n -0.02178823 -0.0010777  -0.07977773  0.01963284 -0.0010777  -0.0632093\n -0.03835666  0.02377494 -0.02593034 -0.05492509  0.03620126  0.00720652\n -0.03835666 -0.01350402  0.00306441 -0.07563562 -0.03835666 -0.08806194\n  0.07348023 -0.05078298  0.06519601 -0.02178823  0.00720652 -0.05492509\n -0.03421455  0.02377494 -0.06735141  0.01963284 -0.00936191  0.04034337\n  0.04448548  0.04034337 -0.00936191 -0.03007245 -0.13776723  0.01963284\n  0.04034337 -0.0052198  -0.04249877 -0.02593034  0.08590655  0.01963284\n  0.00720652  0.00306441  0.03620126  0.00306441  0.00720652 -0.0052198\n  0.06105391  0.01549073 -0.00936191 -0.04664087  0.02377494 -0.05078298\n  0.08590655  0.03205916  0.13146972 -0.03835666 -0.01764613 -0.0010777\n  0.10661708 -0.00936191  0.03205916 -0.03835666 -0.04664087  0.00720652\n  0.04448548  0.01549073 -0.02593034  0.00306441]"
  },
  {
    "objectID": "posts/est/Python Data Analysis/경사하강법.html#탐색적-데이터-분석-1",
    "href": "posts/est/Python Data Analysis/경사하강법.html#탐색적-데이터-분석-1",
    "title": "경사하강법",
    "section": "3.탐색적 데이터 분석",
    "text": "3.탐색적 데이터 분석\n\n#입력데이터의 박스플롯\ng1=plt.subplot(1,2,1)\ng1.boxplot(X_data, labels=['s6'])\nplt.title('Diabetes')\nplt.xlabel('Features')\n\ng2=plt.subplot(1,2,2)\ng2.boxplot(diabetes_y, labels=['disease progression'])\nplt.title('Diabetes')\nplt.xlabel('Target')\n\nplt.show()\n\n\n\n\n\nplt.scatter(X_data, diabetes_y, color='b')\nplt.title('Diabetes')\nplt.xlabel('s6')\nplt.ylabel('Disease progression')\nplt.show()\n\n\n\n\n\nnp.corrcoef(X_data,diabetes_y)\n\narray([[1.        , 0.38248348],\n       [0.38248348, 1.        ]])"
  },
  {
    "objectID": "posts/est/Python Data Analysis/경사하강법.html#데이터-분리",
    "href": "posts/est/Python Data Analysis/경사하강법.html#데이터-분리",
    "title": "경사하강법",
    "section": "4.데이터 분리",
    "text": "4.데이터 분리\n\nX_train, X_test, y_train, y_test = train_test_split(X_data,diabetes_y,\n                                                    test_size=0.5, random_state=1234)"
  },
  {
    "objectID": "posts/est/Python Data Analysis/경사하강법.html#피처-스케일링-1",
    "href": "posts/est/Python Data Analysis/경사하강법.html#피처-스케일링-1",
    "title": "경사하강법",
    "section": "5.피처 스케일링",
    "text": "5.피처 스케일링\n\n# 학습 데이터 정규화\n# 2차원 행렬 변환\nX_train = X_train.reshape(-1,1)\ny_train = y_train.reshape(-1,1)\n\n# 입력 데이터 정규화\nscalerX=MinMaxScaler()\nscalerX.fit(X_train)\nX_train_scaled = scalerX.transform(X_train)\n\n# 목표 데이터 정규화\nscalerY=MinMaxScaler()\nscalerY.fit(y_train)\ny_train_scaled = scalerY.transform(y_train)\n\n\n# 테스트 데이터 정규화\nX_test = X_test.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\n\n# 학습용 데이터의 정규화 스케일에 맞추어 입력과 목표 데이터를 정규화\nX_test_scaled=scalerX.transform(X_test)\ny_test_scaled=scalerY.transform(y_test)"
  },
  {
    "objectID": "posts/est/Python Data Analysis/경사하강법.html#모형화-및-학습-1",
    "href": "posts/est/Python Data Analysis/경사하강법.html#모형화-및-학습-1",
    "title": "경사하강법",
    "section": "6.모형화 및 학습",
    "text": "6.모형화 및 학습\n\nmodel = SGDRegressor(verbose=1)\n\n\nmodel.fit(X_train_scaled, y_train_scaled)\n\n-- Epoch 1\nNorm: 0.11, NNZs: 1, Bias: 0.188126, T: 221, Avg. loss: 0.058981\nTotal training time: 0.00 seconds.\n-- Epoch 2\nNorm: 0.14, NNZs: 1, Bias: 0.242482, T: 442, Avg. loss: 0.035508\nTotal training time: 0.00 seconds.\n-- Epoch 3\nNorm: 0.16, NNZs: 1, Bias: 0.268429, T: 663, Avg. loss: 0.030988\nTotal training time: 0.00 seconds.\n-- Epoch 4\nNorm: 0.17, NNZs: 1, Bias: 0.280449, T: 884, Avg. loss: 0.029699\nTotal training time: 0.00 seconds.\n-- Epoch 5\nNorm: 0.18, NNZs: 1, Bias: 0.285992, T: 1105, Avg. loss: 0.029280\nTotal training time: 0.00 seconds.\n-- Epoch 6\nNorm: 0.18, NNZs: 1, Bias: 0.290268, T: 1326, Avg. loss: 0.029095\nTotal training time: 0.00 seconds.\n-- Epoch 7\nNorm: 0.19, NNZs: 1, Bias: 0.291204, T: 1547, Avg. loss: 0.028998\nTotal training time: 0.00 seconds.\n-- Epoch 8\nNorm: 0.19, NNZs: 1, Bias: 0.291239, T: 1768, Avg. loss: 0.028947\nTotal training time: 0.00 seconds.\n-- Epoch 9\nNorm: 0.19, NNZs: 1, Bias: 0.290878, T: 1989, Avg. loss: 0.028905\nTotal training time: 0.00 seconds.\nConvergence after 9 epochs took 0.00 seconds\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nSGDRegressor(verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SGDRegressorSGDRegressor(verbose=1)\n\n\n\nprint('y절편:', model.intercept_)\nprint('기울기:', model.coef_)\n\ny절편: [0.29087809]\n기울기: [0.19422995]\n\n\n\nplt.scatter(X_train_scaled, y_train_scaled, color='b')\nx=np.array([min(X_train_scaled),max(X_train_scaled)])\ny=model.coef_*x+model.intercept_\nplt.plot(x,y,c='orange', label='regression line')\nplt.show()"
  },
  {
    "objectID": "posts/est/Python Data Analysis/경사하강법.html#예측-1",
    "href": "posts/est/Python Data Analysis/경사하강법.html#예측-1",
    "title": "경사하강법",
    "section": "7.예측",
    "text": "7.예측\n\n# 테스트 데이터 예측\ny_pred=model.predict(X_test_scaled)\n\n# 테스트 데이터 실제 값\nplt.scatter(X_test_scaled, y_test_scaled, color='g')\n\n# 테스트 데이터 예측 값\nplt.scatter(X_test_scaled, y_pred, color='b')\nplt.show()\n\n\n\n\n\n# 예측 값의 역변환(실제 스케일)\ny_pred = y_pred.reshape(-1,1)\ny_pred_inverse = scalerY.inverse_transform(y_pred)"
  },
  {
    "objectID": "posts/est/Python Data Analysis/딥러닝 회귀분석.html",
    "href": "posts/est/Python Data Analysis/딥러닝 회귀분석.html",
    "title": "딥러닝: 회귀분석",
    "section": "",
    "text": "파이썬 데이터 분석"
  },
  {
    "objectID": "posts/est/Python Data Analysis/딥러닝 회귀분석.html#패키지-설정",
    "href": "posts/est/Python Data Analysis/딥러닝 회귀분석.html#패키지-설정",
    "title": "딥러닝: 회귀분석",
    "section": "1. 패키지 설정",
    "text": "1. 패키지 설정\nsklearn에서 2.1 이후로 해당 데이터는 사용할 수 없다.\nfrom sklearn.datasets import load_boston\nIn this special case, you can fetch the dataset from the original source::\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split  #학습용과 테스트용 분리\nfrom sklearn.metrics import mean_absolute_error       #정규화\n#딥러닝 모형화를 위해 keras를 사용\nfrom keras.models import Sequential                   #Sequential 한층씩 추가하여 네트워크를 만든다.    \nfrom keras.layers import Dense                        #Dense 층 간 노드들은 모두 연결되는 모형구조를 만든다.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nimport pandas as pd\nimport numpy as np\n\ndata_url = \"http://lib.stat.cmu.edu/datasets/boston\"\nraw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22)\ndata = raw_df.values[:, :-1]\ntarget = raw_df.values[:, -1]\n\n\nimport pandas as pd\nimport numpy as np\n\ndata_url = \"http://lib.stat.cmu.edu/datasets/boston\"\nraw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=21, header=None)\n\n# 데이터셋에서 14번째 변수(MEDV)는 target 변수로 사용\n# 데이터와 target을 분리\ndata2 = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :3]])\n\nfeautre_names=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT']\n# 데이터셋의 크기를 확인\nprint(data2.shape)  # (506, 14)\n\n(506, 14)\n\n\n\ndata=data2[:,:-1]\ntarget=data2[:,-1]\n\nprint(data.shape)  # (506, 13)\nprint(target.shape)  # (506,)\n\n(506, 13)\n(506,)"
  },
  {
    "objectID": "posts/est/Python Data Analysis/딥러닝 회귀분석.html#데이터준비",
    "href": "posts/est/Python Data Analysis/딥러닝 회귀분석.html#데이터준비",
    "title": "딥러닝: 회귀분석",
    "section": "2. 데이터준비",
    "text": "2. 데이터준비\n\nX=pd.DataFrame(data, columns=feautre_names)\nprint(X)\n\n        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n\n     PTRATIO       B  LSTAT  \n0       15.3  396.90   4.98  \n1       17.8  396.90   9.14  \n2       17.8  392.83   4.03  \n3       18.7  394.63   2.94  \n4       18.7  396.90   5.33  \n..       ...     ...    ...  \n501     21.0  391.99   9.67  \n502     21.0  396.90   9.08  \n503     21.0  396.90   5.64  \n504     21.0  393.45   6.48  \n505     21.0  396.90   7.88  \n\n[506 rows x 13 columns]\n\n\n- 더미 변수 CHAS제외\n\nX=X.drop(['CHAS'],axis=1)\nprint(X.head())\n\n      CRIM    ZN  INDUS    NOX     RM   AGE     DIS  RAD    TAX  PTRATIO  \\\n0  0.00632  18.0   2.31  0.538  6.575  65.2  4.0900  1.0  296.0     15.3   \n1  0.02731   0.0   7.07  0.469  6.421  78.9  4.9671  2.0  242.0     17.8   \n2  0.02729   0.0   7.07  0.469  7.185  61.1  4.9671  2.0  242.0     17.8   \n3  0.03237   0.0   2.18  0.458  6.998  45.8  6.0622  3.0  222.0     18.7   \n4  0.06905   0.0   2.18  0.458  7.147  54.2  6.0622  3.0  222.0     18.7   \n\n        B  LSTAT  \n0  396.90   4.98  \n1  396.90   9.14  \n2  392.83   4.03  \n3  394.63   2.94  \n4  396.90   5.33  \n\n\n\nX.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 506 entries, 0 to 505\nData columns (total 12 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   CRIM     506 non-null    float64\n 1   ZN       506 non-null    float64\n 2   INDUS    506 non-null    float64\n 3   NOX      506 non-null    float64\n 4   RM       506 non-null    float64\n 5   AGE      506 non-null    float64\n 6   DIS      506 non-null    float64\n 7   RAD      506 non-null    float64\n 8   TAX      506 non-null    float64\n 9   PTRATIO  506 non-null    float64\n 10  B        506 non-null    float64\n 11  LSTAT    506 non-null    float64\ndtypes: float64(12)\nmemory usage: 47.6 KB\n\n\n\ny=pd.DataFrame(target)\nprint(y)\n\n        0\n0    24.0\n1    21.6\n2    34.7\n3    33.4\n4    36.2\n..    ...\n501  22.4\n502  20.6\n503  23.9\n504  22.0\n505  11.9\n\n[506 rows x 1 columns]"
  },
  {
    "objectID": "posts/est/Python Data Analysis/딥러닝 회귀분석.html#탐색적-데이터-분석",
    "href": "posts/est/Python Data Analysis/딥러닝 회귀분석.html#탐색적-데이터-분석",
    "title": "딥러닝: 회귀분석",
    "section": "3. 탐색적 데이터 분석",
    "text": "3. 탐색적 데이터 분석\n\nboston_df=pd.DataFrame(data=X)\nboston_df[\"MEDIV\"]=target\nprint(boston_df)\n\n        CRIM    ZN  INDUS    NOX     RM   AGE     DIS  RAD    TAX  PTRATIO  \\\n0    0.00632  18.0   2.31  0.538  6.575  65.2  4.0900  1.0  296.0     15.3   \n1    0.02731   0.0   7.07  0.469  6.421  78.9  4.9671  2.0  242.0     17.8   \n2    0.02729   0.0   7.07  0.469  7.185  61.1  4.9671  2.0  242.0     17.8   \n3    0.03237   0.0   2.18  0.458  6.998  45.8  6.0622  3.0  222.0     18.7   \n4    0.06905   0.0   2.18  0.458  7.147  54.2  6.0622  3.0  222.0     18.7   \n..       ...   ...    ...    ...    ...   ...     ...  ...    ...      ...   \n501  0.06263   0.0  11.93  0.573  6.593  69.1  2.4786  1.0  273.0     21.0   \n502  0.04527   0.0  11.93  0.573  6.120  76.7  2.2875  1.0  273.0     21.0   \n503  0.06076   0.0  11.93  0.573  6.976  91.0  2.1675  1.0  273.0     21.0   \n504  0.10959   0.0  11.93  0.573  6.794  89.3  2.3889  1.0  273.0     21.0   \n505  0.04741   0.0  11.93  0.573  6.030  80.8  2.5050  1.0  273.0     21.0   \n\n          B  LSTAT  MEDIV  \n0    396.90   4.98   24.0  \n1    396.90   9.14   21.6  \n2    392.83   4.03   34.7  \n3    394.63   2.94   33.4  \n4    396.90   5.33   36.2  \n..      ...    ...    ...  \n501  391.99   9.67   22.4  \n502  396.90   9.08   20.6  \n503  396.90   5.64   23.9  \n504  393.45   6.48   22.0  \n505  396.90   7.88   11.9  \n\n[506 rows x 13 columns]\n\n\n- 히스토그램\n\n#목표변수 값 히스토그램\nsns.set(rc={'figure.figsize':(10,5)})\nsns.distplot(boston_df['MEDIV'],bins=10)\nplt.show()\n\n\n\n\n- 상관계수\n\n# 각 변수별 상관계수\ncorrelation_matrix=boston_df.corr().round(2)\ncorrelation_matrix\n\n\n\n\n\n  \n    \n      \n      CRIM\n      ZN\n      INDUS\n      NOX\n      RM\n      AGE\n      DIS\n      RAD\n      TAX\n      PTRATIO\n      B\n      LSTAT\n      MEDIV\n    \n  \n  \n    \n      CRIM\n      1.00\n      -0.20\n      0.41\n      0.42\n      -0.22\n      0.35\n      -0.38\n      0.63\n      0.58\n      0.29\n      -0.39\n      0.46\n      -0.39\n    \n    \n      ZN\n      -0.20\n      1.00\n      -0.53\n      -0.52\n      0.31\n      -0.57\n      0.66\n      -0.31\n      -0.31\n      -0.39\n      0.18\n      -0.41\n      0.36\n    \n    \n      INDUS\n      0.41\n      -0.53\n      1.00\n      0.76\n      -0.39\n      0.64\n      -0.71\n      0.60\n      0.72\n      0.38\n      -0.36\n      0.60\n      -0.48\n    \n    \n      NOX\n      0.42\n      -0.52\n      0.76\n      1.00\n      -0.30\n      0.73\n      -0.77\n      0.61\n      0.67\n      0.19\n      -0.38\n      0.59\n      -0.43\n    \n    \n      RM\n      -0.22\n      0.31\n      -0.39\n      -0.30\n      1.00\n      -0.24\n      0.21\n      -0.21\n      -0.29\n      -0.36\n      0.13\n      -0.61\n      0.70\n    \n    \n      AGE\n      0.35\n      -0.57\n      0.64\n      0.73\n      -0.24\n      1.00\n      -0.75\n      0.46\n      0.51\n      0.26\n      -0.27\n      0.60\n      -0.38\n    \n    \n      DIS\n      -0.38\n      0.66\n      -0.71\n      -0.77\n      0.21\n      -0.75\n      1.00\n      -0.49\n      -0.53\n      -0.23\n      0.29\n      -0.50\n      0.25\n    \n    \n      RAD\n      0.63\n      -0.31\n      0.60\n      0.61\n      -0.21\n      0.46\n      -0.49\n      1.00\n      0.91\n      0.46\n      -0.44\n      0.49\n      -0.38\n    \n    \n      TAX\n      0.58\n      -0.31\n      0.72\n      0.67\n      -0.29\n      0.51\n      -0.53\n      0.91\n      1.00\n      0.46\n      -0.44\n      0.54\n      -0.47\n    \n    \n      PTRATIO\n      0.29\n      -0.39\n      0.38\n      0.19\n      -0.36\n      0.26\n      -0.23\n      0.46\n      0.46\n      1.00\n      -0.18\n      0.37\n      -0.51\n    \n    \n      B\n      -0.39\n      0.18\n      -0.36\n      -0.38\n      0.13\n      -0.27\n      0.29\n      -0.44\n      -0.44\n      -0.18\n      1.00\n      -0.37\n      0.33\n    \n    \n      LSTAT\n      0.46\n      -0.41\n      0.60\n      0.59\n      -0.61\n      0.60\n      -0.50\n      0.49\n      0.54\n      0.37\n      -0.37\n      1.00\n      -0.74\n    \n    \n      MEDIV\n      -0.39\n      0.36\n      -0.48\n      -0.43\n      0.70\n      -0.38\n      0.25\n      -0.38\n      -0.47\n      -0.51\n      0.33\n      -0.74\n      1.00\n    \n  \n\n\n\n\n\nsns.set(rc={'figure.figsize':(10,8)})\nsns.heatmap(data=correlation_matrix,annot=True)\nplt.show()"
  },
  {
    "objectID": "posts/est/Python Data Analysis/딥러닝 회귀분석.html#데이터분리",
    "href": "posts/est/Python Data Analysis/딥러닝 회귀분석.html#데이터분리",
    "title": "딥러닝: 회귀분석",
    "section": "4. 데이터분리",
    "text": "4. 데이터분리\n\n학습용과 테스트용 데이터를 7:3으로 분리하자.\n\n\nX.shape\n\n(506, 13)\n\n\n\nX=X.drop(['MEDIV'],axis=1)\nX\n\n\n\n\n\n  \n    \n      \n      CRIM\n      ZN\n      INDUS\n      NOX\n      RM\n      AGE\n      DIS\n      RAD\n      TAX\n      PTRATIO\n      B\n      LSTAT\n    \n  \n  \n    \n      0\n      0.00632\n      18.0\n      2.31\n      0.538\n      6.575\n      65.2\n      4.0900\n      1.0\n      296.0\n      15.3\n      396.90\n      4.98\n    \n    \n      1\n      0.02731\n      0.0\n      7.07\n      0.469\n      6.421\n      78.9\n      4.9671\n      2.0\n      242.0\n      17.8\n      396.90\n      9.14\n    \n    \n      2\n      0.02729\n      0.0\n      7.07\n      0.469\n      7.185\n      61.1\n      4.9671\n      2.0\n      242.0\n      17.8\n      392.83\n      4.03\n    \n    \n      3\n      0.03237\n      0.0\n      2.18\n      0.458\n      6.998\n      45.8\n      6.0622\n      3.0\n      222.0\n      18.7\n      394.63\n      2.94\n    \n    \n      4\n      0.06905\n      0.0\n      2.18\n      0.458\n      7.147\n      54.2\n      6.0622\n      3.0\n      222.0\n      18.7\n      396.90\n      5.33\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      501\n      0.06263\n      0.0\n      11.93\n      0.573\n      6.593\n      69.1\n      2.4786\n      1.0\n      273.0\n      21.0\n      391.99\n      9.67\n    \n    \n      502\n      0.04527\n      0.0\n      11.93\n      0.573\n      6.120\n      76.7\n      2.2875\n      1.0\n      273.0\n      21.0\n      396.90\n      9.08\n    \n    \n      503\n      0.06076\n      0.0\n      11.93\n      0.573\n      6.976\n      91.0\n      2.1675\n      1.0\n      273.0\n      21.0\n      396.90\n      5.64\n    \n    \n      504\n      0.10959\n      0.0\n      11.93\n      0.573\n      6.794\n      89.3\n      2.3889\n      1.0\n      273.0\n      21.0\n      393.45\n      6.48\n    \n    \n      505\n      0.04741\n      0.0\n      11.93\n      0.573\n      6.030\n      80.8\n      2.5050\n      1.0\n      273.0\n      21.0\n      396.90\n      7.88\n    \n  \n\n506 rows × 12 columns\n\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)"
  },
  {
    "objectID": "posts/est/Python Data Analysis/딥러닝 회귀분석.html#피처-스케일링",
    "href": "posts/est/Python Data Analysis/딥러닝 회귀분석.html#피처-스케일링",
    "title": "딥러닝: 회귀분석",
    "section": "5. 피처 스케일링",
    "text": "5. 피처 스케일링\n\n학습용 입력 데이터에 대한 정규화 스케일러 만들고 입력데이터를 0~1로 정규화하기\n\n\n# 정규화 스케일러 생성\nscalerX=MinMaxScaler()\n\n# 정규화 스케일러를 학습용 데이터에 맞춤\nscalerX.fit(X_train)\n\n# 정규화 스케일러로 학습용 데이터 변환\nX_train_norm=scalerX.transform(X_train)\n\n# 정규화 스케일러로 테스트용 데이터 변환\nX_test_norm = scalerX.transform(X_test)\n\n\nprint(X_train_norm)\n\n[[0.00680253 0.2        0.11962963 ... 0.04255319 0.98184477 0.08724646]\n [0.07235853 0.         0.64296296 ... 0.80851064 0.24617984 0.27924423]\n [0.1610363  0.         0.64296296 ... 0.80851064 0.93953301 0.7957766 ]\n ...\n [0.11494542 0.         0.64296296 ... 0.80851064 0.95662918 0.44595721]\n [0.1717624  0.         0.64296296 ... 0.80851064 0.91456957 0.59071964]\n [0.00187693 0.         0.33148148 ... 0.70212766 1.         0.27868852]]\n\n\n\nX_train_norm.shape\n\n(354, 12)\n\n\n\nprint(X_test_norm)\n\n[[ 2.05400974e-02  0.00000000e+00  6.97777778e-01 ...  2.23404255e-01\n   9.81617832e-01 -1.66712976e-03]\n [ 1.09008802e-02  0.00000000e+00  7.83333333e-01 ...  9.14893617e-01\n   6.61758031e-01  4.25951653e-01]\n [ 4.66707160e-01  0.00000000e+00  6.42962963e-01 ...  8.08510638e-01\n   8.29946039e-01  7.05751598e-01]\n ...\n [ 1.61036297e-01  0.00000000e+00  6.42962963e-01 ...  8.08510638e-01\n   9.65757224e-01  3.09252570e-01]\n [ 9.47848868e-04  2.00000000e-01  2.30370370e-01 ...  6.38297872e-01\n   9.85980130e-01  3.24256738e-01]\n [ 4.01371790e-04  2.80000000e-01  5.29629630e-01 ...  5.95744681e-01\n   9.95234253e-01  1.71714365e-01]]\n\n\n\nX_test_norm.shape\n\n(152, 12)\n\n\n\n# 정규화 스케일러 생성\nscalerY=MinMaxScaler()\n\n# 정규화 스케일러를 학습용 데이터에 맞춤\nscalerY.fit(y_train)\n\n# 정규화 스케일러로 학습용 데이터 변환\ny_train_norm=scalerY.transform(y_train)\n\n# 정규화 스케일러로 테스트용 데이터 변환\ny_test_norm = scalerY.transform(y_test)\n\n\nprint(y_train_norm[0:10])\n\n[[1.        ]\n [0.24666667]\n [0.11555556]\n [0.3       ]\n [0.17111111]\n [0.30888889]\n [0.28444444]\n [0.32666667]\n [0.33111111]\n [0.52444444]]\n\n\n\ny_train_norm.shape\n\n(354, 1)\n\n\n\nprint(y_test_norm[0:10])\n\n[[1.        ]\n [0.23555556]\n [0.07777778]\n [0.12222222]\n [0.53555556]\n [0.92666667]\n [0.40222222]\n [0.58      ]\n [0.57333333]\n [0.34888889]]\n\n\n\ny_test_norm.shape\n\n(152, 1)"
  },
  {
    "objectID": "posts/est/Python Data Analysis/딥러닝 회귀분석.html#모형화-및-학습",
    "href": "posts/est/Python Data Analysis/딥러닝 회귀분석.html#모형화-및-학습",
    "title": "딥러닝: 회귀분석",
    "section": "6. 모형화 및 학습",
    "text": "6. 모형화 및 학습\n\nmodel=Sequential() # 순차모형\nmodel.add(Dense(60,activation='relu', input_shape=(12,))) # 제 1은닉충과 입력층\nmodel.add(Dense(60,activation='relu')) # 제 2은닉충\nmodel.add(Dense(30,activation='relu')) # 제 3은닉충\nmodel.add(Dense(1)) # 출력층 (선형 활섬화함수)\n\n\nmodel.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_12 (Dense)            (None, 60)                780       \n                                                                 \n dense_13 (Dense)            (None, 60)                3660      \n                                                                 \n dense_14 (Dense)            (None, 30)                1830      \n                                                                 \n dense_15 (Dense)            (None, 1)                 31        \n                                                                 \n=================================================================\nTotal params: 6,301\nTrainable params: 6,301\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nParam : 가중치 12x60=720, 편향:60\n\n\nmodel.compile(optimizer='adam',\n              loss='mse',\n              metrics=['mae'])\n\n- 학습\n\nresults=model.fit(X_train_norm, y_train_norm,\n                  validation_data=(X_test_norm, y_test_norm),\n                  epochs=200, batch_size=32)\n\nEpoch 1/200\n12/12 [==============================] - 0s 7ms/step - loss: 0.1031 - mae: 0.2534 - val_loss: 0.0570 - val_mae: 0.1836\nEpoch 2/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0472 - mae: 0.1690 - val_loss: 0.0401 - val_mae: 0.1482\nEpoch 3/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1291 - val_loss: 0.0295 - val_mae: 0.1203\nEpoch 4/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1095 - val_loss: 0.0234 - val_mae: 0.1124\nEpoch 5/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0224 - mae: 0.1113 - val_loss: 0.0205 - val_mae: 0.0991\nEpoch 6/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0176 - mae: 0.0876 - val_loss: 0.0162 - val_mae: 0.0880\nEpoch 7/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0144 - mae: 0.0785 - val_loss: 0.0135 - val_mae: 0.0801\nEpoch 8/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0123 - mae: 0.0711 - val_loss: 0.0118 - val_mae: 0.0791\nEpoch 9/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0110 - mae: 0.0675 - val_loss: 0.0108 - val_mae: 0.0707\nEpoch 10/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0100 - mae: 0.0645 - val_loss: 0.0105 - val_mae: 0.0756\nEpoch 11/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0097 - mae: 0.0650 - val_loss: 0.0095 - val_mae: 0.0690\nEpoch 12/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0089 - mae: 0.0612 - val_loss: 0.0092 - val_mae: 0.0682\nEpoch 13/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0082 - mae: 0.0588 - val_loss: 0.0090 - val_mae: 0.0672\nEpoch 14/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0080 - mae: 0.0565 - val_loss: 0.0102 - val_mae: 0.0783\nEpoch 15/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0089 - mae: 0.0635 - val_loss: 0.0090 - val_mae: 0.0702\nEpoch 16/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0121 - mae: 0.0826 - val_loss: 0.0117 - val_mae: 0.0826\nEpoch 17/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0088 - mae: 0.0666 - val_loss: 0.0088 - val_mae: 0.0703\nEpoch 18/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0544 - val_loss: 0.0077 - val_mae: 0.0625\nEpoch 19/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0067 - mae: 0.0540 - val_loss: 0.0073 - val_mae: 0.0614\nEpoch 20/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0062 - mae: 0.0518 - val_loss: 0.0072 - val_mae: 0.0605\nEpoch 21/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0063 - mae: 0.0513 - val_loss: 0.0070 - val_mae: 0.0599\nEpoch 22/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0059 - mae: 0.0507 - val_loss: 0.0069 - val_mae: 0.0594\nEpoch 23/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0064 - mae: 0.0518 - val_loss: 0.0075 - val_mae: 0.0646\nEpoch 24/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0061 - mae: 0.0507 - val_loss: 0.0065 - val_mae: 0.0573\nEpoch 25/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0057 - mae: 0.0495 - val_loss: 0.0064 - val_mae: 0.0568\nEpoch 26/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0055 - mae: 0.0504 - val_loss: 0.0067 - val_mae: 0.0580\nEpoch 27/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0058 - mae: 0.0512 - val_loss: 0.0066 - val_mae: 0.0570\nEpoch 28/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0056 - mae: 0.0487 - val_loss: 0.0064 - val_mae: 0.0564\nEpoch 29/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0052 - mae: 0.0484 - val_loss: 0.0063 - val_mae: 0.0554\nEpoch 30/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0049 - mae: 0.0469 - val_loss: 0.0063 - val_mae: 0.0560\nEpoch 31/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0049 - mae: 0.0472 - val_loss: 0.0061 - val_mae: 0.0545\nEpoch 32/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0045 - mae: 0.0448 - val_loss: 0.0063 - val_mae: 0.0546\nEpoch 33/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0045 - mae: 0.0452 - val_loss: 0.0061 - val_mae: 0.0537\nEpoch 34/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0044 - mae: 0.0456 - val_loss: 0.0061 - val_mae: 0.0540\nEpoch 35/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0045 - mae: 0.0454 - val_loss: 0.0061 - val_mae: 0.0533\nEpoch 36/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0041 - mae: 0.0436 - val_loss: 0.0060 - val_mae: 0.0520\nEpoch 37/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0043 - mae: 0.0449 - val_loss: 0.0062 - val_mae: 0.0529\nEpoch 38/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0041 - mae: 0.0444 - val_loss: 0.0060 - val_mae: 0.0529\nEpoch 39/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0040 - mae: 0.0439 - val_loss: 0.0060 - val_mae: 0.0518\nEpoch 40/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0046 - mae: 0.0481 - val_loss: 0.0087 - val_mae: 0.0648\nEpoch 41/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0046 - mae: 0.0467 - val_loss: 0.0060 - val_mae: 0.0524\nEpoch 42/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0041 - mae: 0.0453 - val_loss: 0.0060 - val_mae: 0.0533\nEpoch 43/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0043 - mae: 0.0454 - val_loss: 0.0067 - val_mae: 0.0548\nEpoch 44/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0045 - mae: 0.0501 - val_loss: 0.0060 - val_mae: 0.0503\nEpoch 45/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0039 - mae: 0.0451 - val_loss: 0.0063 - val_mae: 0.0548\nEpoch 46/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0038 - mae: 0.0446 - val_loss: 0.0056 - val_mae: 0.0494\nEpoch 47/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0035 - mae: 0.0423 - val_loss: 0.0057 - val_mae: 0.0496\nEpoch 48/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0037 - mae: 0.0426 - val_loss: 0.0060 - val_mae: 0.0509\nEpoch 49/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0036 - mae: 0.0425 - val_loss: 0.0071 - val_mae: 0.0555\nEpoch 50/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0092 - mae: 0.0708 - val_loss: 0.0089 - val_mae: 0.0701\nEpoch 51/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0050 - mae: 0.0526 - val_loss: 0.0069 - val_mae: 0.0579\nEpoch 52/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0041 - mae: 0.0456 - val_loss: 0.0060 - val_mae: 0.0510\nEpoch 53/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0041 - mae: 0.0462 - val_loss: 0.0062 - val_mae: 0.0528\nEpoch 54/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0033 - mae: 0.0409 - val_loss: 0.0063 - val_mae: 0.0516\nEpoch 55/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0414 - val_loss: 0.0064 - val_mae: 0.0529\nEpoch 56/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0414 - val_loss: 0.0064 - val_mae: 0.0545\nEpoch 57/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0035 - mae: 0.0401 - val_loss: 0.0062 - val_mae: 0.0504\nEpoch 58/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0387 - val_loss: 0.0065 - val_mae: 0.0525\nEpoch 59/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0030 - mae: 0.0385 - val_loss: 0.0068 - val_mae: 0.0541\nEpoch 60/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0404 - val_loss: 0.0061 - val_mae: 0.0506\nEpoch 61/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0425 - val_loss: 0.0071 - val_mae: 0.0564\nEpoch 62/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0033 - mae: 0.0405 - val_loss: 0.0070 - val_mae: 0.0545\nEpoch 63/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0395 - val_loss: 0.0068 - val_mae: 0.0534\nEpoch 64/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0399 - val_loss: 0.0064 - val_mae: 0.0527\nEpoch 65/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0382 - val_loss: 0.0062 - val_mae: 0.0517\nEpoch 66/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0379 - val_loss: 0.0063 - val_mae: 0.0512\nEpoch 67/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0379 - val_loss: 0.0062 - val_mae: 0.0505\nEpoch 68/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0365 - val_loss: 0.0063 - val_mae: 0.0513\nEpoch 69/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0383 - val_loss: 0.0066 - val_mae: 0.0514\nEpoch 70/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0370 - val_loss: 0.0066 - val_mae: 0.0505\nEpoch 71/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0386 - val_loss: 0.0066 - val_mae: 0.0520\nEpoch 72/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0373 - val_loss: 0.0067 - val_mae: 0.0515\nEpoch 73/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0367 - val_loss: 0.0063 - val_mae: 0.0509\nEpoch 74/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0364 - val_loss: 0.0064 - val_mae: 0.0519\nEpoch 75/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0422 - val_loss: 0.0060 - val_mae: 0.0496\nEpoch 76/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0353 - val_loss: 0.0068 - val_mae: 0.0534\nEpoch 77/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0381 - val_loss: 0.0064 - val_mae: 0.0499\nEpoch 78/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0357 - val_loss: 0.0060 - val_mae: 0.0495\nEpoch 79/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0353 - val_loss: 0.0059 - val_mae: 0.0490\nEpoch 80/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0366 - val_loss: 0.0065 - val_mae: 0.0517\nEpoch 81/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0363 - val_loss: 0.0064 - val_mae: 0.0507\nEpoch 82/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0350 - val_loss: 0.0062 - val_mae: 0.0499\nEpoch 83/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0342 - val_loss: 0.0064 - val_mae: 0.0513\nEpoch 84/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0351 - val_loss: 0.0062 - val_mae: 0.0499\nEpoch 85/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0335 - val_loss: 0.0061 - val_mae: 0.0500\nEpoch 86/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0346 - val_loss: 0.0062 - val_mae: 0.0517\nEpoch 87/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0345 - val_loss: 0.0061 - val_mae: 0.0507\nEpoch 88/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0331 - val_loss: 0.0061 - val_mae: 0.0500\nEpoch 89/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0331 - val_loss: 0.0062 - val_mae: 0.0512\nEpoch 90/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0330 - val_loss: 0.0062 - val_mae: 0.0498\nEpoch 91/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0327 - val_loss: 0.0062 - val_mae: 0.0496\nEpoch 92/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0328 - val_loss: 0.0061 - val_mae: 0.0496\nEpoch 93/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0319 - val_loss: 0.0063 - val_mae: 0.0506\nEpoch 94/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0329 - val_loss: 0.0062 - val_mae: 0.0506\nEpoch 95/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0318 - val_loss: 0.0061 - val_mae: 0.0503\nEpoch 96/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0315 - val_loss: 0.0062 - val_mae: 0.0507\nEpoch 97/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0317 - val_loss: 0.0061 - val_mae: 0.0501\nEpoch 98/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0315 - val_loss: 0.0063 - val_mae: 0.0505\nEpoch 99/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0330 - val_loss: 0.0062 - val_mae: 0.0498\nEpoch 100/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0313 - val_loss: 0.0067 - val_mae: 0.0527\nEpoch 101/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0310 - val_loss: 0.0063 - val_mae: 0.0507\nEpoch 102/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0318 - val_loss: 0.0066 - val_mae: 0.0519\nEpoch 103/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0343 - val_loss: 0.0063 - val_mae: 0.0517\nEpoch 104/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0317 - val_loss: 0.0063 - val_mae: 0.0508\nEpoch 105/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0325 - val_loss: 0.0065 - val_mae: 0.0513\nEpoch 106/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0339 - val_loss: 0.0060 - val_mae: 0.0501\nEpoch 107/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0311 - val_loss: 0.0068 - val_mae: 0.0528\nEpoch 108/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0386 - val_loss: 0.0067 - val_mae: 0.0512\nEpoch 109/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0335 - val_loss: 0.0070 - val_mae: 0.0530\nEpoch 110/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0404 - val_loss: 0.0065 - val_mae: 0.0517\nEpoch 111/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0407 - val_loss: 0.0058 - val_mae: 0.0501\nEpoch 112/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0328 - val_loss: 0.0060 - val_mae: 0.0507\nEpoch 113/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0331 - val_loss: 0.0059 - val_mae: 0.0495\nEpoch 114/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0341 - val_loss: 0.0064 - val_mae: 0.0505\nEpoch 115/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0324 - val_loss: 0.0065 - val_mae: 0.0518\nEpoch 116/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0303 - val_loss: 0.0061 - val_mae: 0.0500\nEpoch 117/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0303 - val_loss: 0.0060 - val_mae: 0.0502\nEpoch 118/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0320 - val_loss: 0.0062 - val_mae: 0.0509\nEpoch 119/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0317 - val_loss: 0.0060 - val_mae: 0.0499\nEpoch 120/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0309 - val_loss: 0.0064 - val_mae: 0.0512\nEpoch 121/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0355 - val_loss: 0.0063 - val_mae: 0.0514\nEpoch 122/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0379 - val_loss: 0.0075 - val_mae: 0.0617\nEpoch 123/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0388 - val_loss: 0.0055 - val_mae: 0.0484\nEpoch 124/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0330 - val_loss: 0.0065 - val_mae: 0.0531\nEpoch 125/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0325 - val_loss: 0.0063 - val_mae: 0.0504\nEpoch 126/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0306 - val_loss: 0.0065 - val_mae: 0.0512\nEpoch 127/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0317 - val_loss: 0.0061 - val_mae: 0.0503\nEpoch 128/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0292 - val_loss: 0.0060 - val_mae: 0.0502\nEpoch 129/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0297 - val_loss: 0.0065 - val_mae: 0.0531\nEpoch 130/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0302 - val_loss: 0.0063 - val_mae: 0.0503\nEpoch 131/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0293 - val_loss: 0.0066 - val_mae: 0.0525\nEpoch 132/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0305 - val_loss: 0.0060 - val_mae: 0.0499\nEpoch 133/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0315 - val_loss: 0.0061 - val_mae: 0.0505\nEpoch 134/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0299 - val_loss: 0.0066 - val_mae: 0.0515\nEpoch 135/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0301 - val_loss: 0.0065 - val_mae: 0.0520\nEpoch 136/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0321 - val_loss: 0.0064 - val_mae: 0.0509\nEpoch 137/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0343 - val_loss: 0.0059 - val_mae: 0.0505\nEpoch 138/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0303 - val_loss: 0.0058 - val_mae: 0.0489\nEpoch 139/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0308 - val_loss: 0.0061 - val_mae: 0.0500\nEpoch 140/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0287 - val_loss: 0.0062 - val_mae: 0.0494\nEpoch 141/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0285 - val_loss: 0.0062 - val_mae: 0.0496\nEpoch 142/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0280 - val_loss: 0.0061 - val_mae: 0.0497\nEpoch 143/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0279 - val_loss: 0.0061 - val_mae: 0.0499\nEpoch 144/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0300 - val_loss: 0.0061 - val_mae: 0.0492\nEpoch 145/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0293 - val_loss: 0.0064 - val_mae: 0.0512\nEpoch 146/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0304 - val_loss: 0.0065 - val_mae: 0.0515\nEpoch 147/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0374 - val_loss: 0.0062 - val_mae: 0.0497\nEpoch 148/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0316 - val_loss: 0.0061 - val_mae: 0.0515\nEpoch 149/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0305 - val_loss: 0.0062 - val_mae: 0.0523\nEpoch 150/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0281 - val_loss: 0.0059 - val_mae: 0.0492\nEpoch 151/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0283 - val_loss: 0.0062 - val_mae: 0.0512\nEpoch 152/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0289 - val_loss: 0.0062 - val_mae: 0.0501\nEpoch 153/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0283 - val_loss: 0.0061 - val_mae: 0.0488\nEpoch 154/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0277 - val_loss: 0.0061 - val_mae: 0.0492\nEpoch 155/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0272 - val_loss: 0.0063 - val_mae: 0.0489\nEpoch 156/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0268 - val_loss: 0.0063 - val_mae: 0.0494\nEpoch 157/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0269 - val_loss: 0.0062 - val_mae: 0.0494\nEpoch 158/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0259 - val_loss: 0.0063 - val_mae: 0.0489\nEpoch 159/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0260 - val_loss: 0.0062 - val_mae: 0.0500\nEpoch 160/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0263 - val_loss: 0.0073 - val_mae: 0.0537\nEpoch 161/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0360 - val_loss: 0.0071 - val_mae: 0.0572\nEpoch 162/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0360 - val_loss: 0.0069 - val_mae: 0.0546\nEpoch 163/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0342 - val_loss: 0.0066 - val_mae: 0.0527\nEpoch 164/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0356 - val_loss: 0.0061 - val_mae: 0.0509\nEpoch 165/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0331 - val_loss: 0.0065 - val_mae: 0.0516\nEpoch 166/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0319 - val_loss: 0.0063 - val_mae: 0.0499\nEpoch 167/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0325 - val_loss: 0.0077 - val_mae: 0.0570\nEpoch 168/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0374 - val_loss: 0.0066 - val_mae: 0.0519\nEpoch 169/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0365 - val_loss: 0.0066 - val_mae: 0.0520\nEpoch 170/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0377 - val_loss: 0.0058 - val_mae: 0.0501\nEpoch 171/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0311 - val_loss: 0.0061 - val_mae: 0.0517\nEpoch 172/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0300 - val_loss: 0.0060 - val_mae: 0.0501\nEpoch 173/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0322 - val_loss: 0.0060 - val_mae: 0.0498\nEpoch 174/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0274 - val_loss: 0.0061 - val_mae: 0.0494\nEpoch 175/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0269 - val_loss: 0.0063 - val_mae: 0.0495\nEpoch 176/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0263 - val_loss: 0.0061 - val_mae: 0.0495\nEpoch 177/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0258 - val_loss: 0.0060 - val_mae: 0.0501\nEpoch 178/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0401 - val_loss: 0.0057 - val_mae: 0.0493\nEpoch 179/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0354 - val_loss: 0.0061 - val_mae: 0.0501\nEpoch 180/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0301 - val_loss: 0.0063 - val_mae: 0.0511\nEpoch 181/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0275 - val_loss: 0.0070 - val_mae: 0.0533\nEpoch 182/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0277 - val_loss: 0.0059 - val_mae: 0.0485\nEpoch 183/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0289 - val_loss: 0.0061 - val_mae: 0.0484\nEpoch 184/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0271 - val_loss: 0.0071 - val_mae: 0.0546\nEpoch 185/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0305 - val_loss: 0.0066 - val_mae: 0.0513\nEpoch 186/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0270 - val_loss: 0.0061 - val_mae: 0.0483\nEpoch 187/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0267 - val_loss: 0.0062 - val_mae: 0.0481\nEpoch 188/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0283 - val_loss: 0.0066 - val_mae: 0.0496\nEpoch 189/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0261 - val_loss: 0.0060 - val_mae: 0.0476\nEpoch 190/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0266 - val_loss: 0.0062 - val_mae: 0.0483\nEpoch 191/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0300 - val_loss: 0.0070 - val_mae: 0.0524\nEpoch 192/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0287 - val_loss: 0.0065 - val_mae: 0.0515\nEpoch 193/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0283 - val_loss: 0.0062 - val_mae: 0.0481\nEpoch 194/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0269 - val_loss: 0.0059 - val_mae: 0.0482\nEpoch 195/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0264 - val_loss: 0.0060 - val_mae: 0.0490\nEpoch 196/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0251 - val_loss: 0.0059 - val_mae: 0.0480\nEpoch 197/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0255 - val_loss: 0.0063 - val_mae: 0.0510\nEpoch 198/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0277 - val_loss: 0.0059 - val_mae: 0.0479\nEpoch 199/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0257 - val_loss: 0.0065 - val_mae: 0.0513\nEpoch 200/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0272 - val_loss: 0.0066 - val_mae: 0.0507\n\n\n\nprint(results.history.keys())\n\ndict_keys(['loss', 'mae', 'val_loss', 'val_mae'])\n\n\n\nloss : 학습 데이터 비용\nmae : 학습 데이터 오차\nval_loss : 테스트 데이터 비용\nval_mae : 테스트 데이터 오차\n\n\n# 학습 수에 따른 loss 변화\nplt.figure(figsize=(10,5))\nplt.plot(results.history['loss']) \nplt.plot(results.history['val_loss'])\nplt.title('loss')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['train','test'],loc='upper right')\nplt.show()\n\n\n\n\n\n# 학습 수에 따른 정확도(mae) 변화\nplt.figure(figsize=(10,5))\nplt.plot(results.history['mae']) \nplt.plot(results.history['val_loss'])\nplt.title('accuracy(MAE)')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend(['train','test'],loc='upper right')\nplt.show()"
  },
  {
    "objectID": "posts/est/Python Data Analysis/딥러닝 회귀분석.html#예측",
    "href": "posts/est/Python Data Analysis/딥러닝 회귀분석.html#예측",
    "title": "딥러닝: 회귀분석",
    "section": "7. 예측",
    "text": "7. 예측\n\n# 테스트 데이터 예측\ny_pred=model.predict(X_test_norm).flatten()\n\n# 예측 값의 역변환\ny_pred_inverse=scalerY.inverse_transform(y_pred.reshape(-1,1))\nprint(y_pred_inverse[0:10])\n\n5/5 [==============================] - 0s 540us/step\n[[50.411358 ]\n [13.942274 ]\n [ 6.1426053]\n [10.471625 ]\n [30.684546 ]\n [44.55355  ]\n [26.434183 ]\n [31.944752 ]\n [32.296173 ]\n [23.545477 ]]\n\n\n\n# 오차측정(MAE)\nprint('MAE:%.2f' %mean_absolute_error(y_test, y_pred_inverse))\n\nMAE:2.28\n\n\n\n# 실제 값 대비 예측 값의 산포도\nplt.figure(figsize=(7,7))\nplt.scatter(y_test_norm,y_pred,c='r')\nplt.xlabel('TrueValues')\nplt.ylabel('Predictions')\nplt.axis('equal')\nplt.xlim(0,1)\nplt.ylim(0,1)\nplt.plot([0,1],[0,1])\nplt.show()"
  },
  {
    "objectID": "posts/est/Python Data Analysis/딥러닝 회귀분석.html#드랍아웃-모형-추가",
    "href": "posts/est/Python Data Analysis/딥러닝 회귀분석.html#드랍아웃-모형-추가",
    "title": "딥러닝: 회귀분석",
    "section": "8. 드랍아웃 모형 추가",
    "text": "8. 드랍아웃 모형 추가\n\nfrom keras.layers import Dropout\n\n\nmodel=Sequential() # 순차모형\nmodel.add(Dense(60,activation='relu', input_shape=(12,))) # 제 1은닉충과 입력층\nmodel.add(Dropout(0.5)) # 제 1은닉충과 2은닉충 사이의 드롭 아웃 50%\nmodel.add(Dense(60,activation='relu')) # 제 2은닉충\nmodel.add(Dense(30,activation='relu')) # 제 3은닉충\nmodel.add(Dropout(0.2)) # 제 3은닉충과 출력층 사이의 드롭 아웃 20%\nmodel.add(Dense(1)) # 출력층 (선형 활섬화함수)\n\n\nmodel.summary()\n\nModel: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_20 (Dense)            (None, 60)                780       \n                                                                 \n dropout_2 (Dropout)         (None, 60)                0         \n                                                                 \n dense_21 (Dense)            (None, 60)                3660      \n                                                                 \n dense_22 (Dense)            (None, 30)                1830      \n                                                                 \n dropout_3 (Dropout)         (None, 30)                0         \n                                                                 \n dense_23 (Dense)            (None, 1)                 31        \n                                                                 \n=================================================================\nTotal params: 6,301\nTrainable params: 6,301\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nmodel.compile(optimizer='adam',\n              loss='mse',\n              metrics=['mae'])\n\n\n# results=model.fit(X_train_norm, y_train_norm,\n#                   validation_data=(X_test_norm, y_test_norm),\n#                   epochs=1000, batch_size=20)\n\n\nprint(results.history.keys())\n\ndict_keys(['loss', 'mae', 'val_loss', 'val_mae'])\n\n\n\n# 학습 수에 따른 loss 변화\nplt.figure(figsize=(10,5))\nplt.plot(results.history['loss']) \nplt.plot(results.history['val_loss'])\nplt.title('loss')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['train','test'],loc='upper right')\nplt.show()\n\n\n\n\n\n# 학습 수에 따른 정확도(mae) 변화\nplt.figure(figsize=(10,5))\nplt.plot(results.history['mae']) \nplt.plot(results.history['val_loss'])\nplt.title('accuracy(MAE)')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend(['train','test'],loc='upper right')\nplt.show()\n\n\n\n\n\n# 테스트 데이터 예측\ny_pred=model.predict(X_test_norm).flatten()\n\n# 예측 값의 역변환\ny_pred_inverse=scalerY.inverse_transform(y_pred.reshape(-1,1))\nprint(y_pred_inverse[0:10])\n\n5/5 [==============================] - 0s 517us/step\n[[36.150234]\n [15.184836]\n [ 8.98333 ]\n [12.427422]\n [26.98419 ]\n [34.612114]\n [20.946304]\n [27.669504]\n [28.082466]\n [23.544407]]\n\n\n\n# 오차측정(MAE)\nprint('MAE:%.2f' %mean_absolute_error(y_test, y_pred_inverse))\n\nMAE:3.00\n\n\n\n# 실제 값 대비 예측 값의 산포도\nplt.figure(figsize=(7,7))\nplt.scatter(y_test_norm,y_pred,c='r')\nplt.xlabel('TrueValues')\nplt.ylabel('Predictions')\nplt.axis('equal')\nplt.xlim(0,1)\nplt.ylim(0,1)\nplt.plot([0,1],[0,1])\nplt.show()"
  },
  {
    "objectID": "posts/est/Python Data Analysis/딥러닝 회귀분석.html#입력-노드의-드랍-아웃",
    "href": "posts/est/Python Data Analysis/딥러닝 회귀분석.html#입력-노드의-드랍-아웃",
    "title": "딥러닝: 회귀분석",
    "section": "9. 입력 노드의 드랍 아웃",
    "text": "9. 입력 노드의 드랍 아웃\nmodel=Sequential() # 순차모형\nmodel.add(Dropout(0.2, input_shape=(12,))) # 입력 노드에 대한 드랍아웃 비율 0.2\nmodel.add(Dense(60,activation='relu', input_shape=(12,))) # 제 1은닉충과 입력층\nmodel.add(Dense(60,activation='relu')) # 제 2은닉충\nmodel.add(Dense(30,activation='relu')) # 제 3은닉충\nmodel.add(Dense(1)) # 출력층 (선형 활섬화함수)"
  },
  {
    "objectID": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html",
    "href": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "",
    "text": "파이썬 데이터 분석"
  },
  {
    "objectID": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#패키지-설정",
    "href": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#패키지-설정",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "1. 패키지 설정",
    "text": "1. 패키지 설정\n\nfrom sklearn.neural_network import MLPClassifier\nimport numpy as np"
  },
  {
    "objectID": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#데이터-작성",
    "href": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#데이터-작성",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "2. 데이터 작성",
    "text": "2. 데이터 작성\n\nX=np.array([[0,0],[0,1],[1,0],[1,1]])\ny=np.array([0,1,1,0])\n\n\n입력 값, 목표 값 작성"
  },
  {
    "objectID": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#모형화",
    "href": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#모형화",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "3. 모형화",
    "text": "3. 모형화\n\nmodel=MLPClassifier(hidden_layer_sizes=(2),\n                    activation='logistic',\n                    solver='lbfgs',\n                    max_iter=100)"
  },
  {
    "objectID": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#학습",
    "href": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#학습",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "4. 학습",
    "text": "4. 학습\n\nmodel.fit(X,y)\n\nMLPClassifier(activation='logistic', hidden_layer_sizes=2, max_iter=100,\n              solver='lbfgs')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MLPClassifierMLPClassifier(activation='logistic', hidden_layer_sizes=2, max_iter=100,\n              solver='lbfgs')"
  },
  {
    "objectID": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#예측",
    "href": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#예측",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "5. 예측",
    "text": "5. 예측\n\nprint(model.predict(X))\n\n[0 0 1 0]"
  },
  {
    "objectID": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#패키지-설정-1",
    "href": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#패키지-설정-1",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "1. 패키지 설정",
    "text": "1. 패키지 설정\n\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_percentage_error\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#데이터-준비",
    "href": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#데이터-준비",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "2. 데이터 준비",
    "text": "2. 데이터 준비\n\nX=np.array(range(1,101))\nprint(X)\n\n[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n  91  92  93  94  95  96  97  98  99 100]\n\n\n\ny=0.5*(X-50)**3 - 50000/X + 120000\nprint(y)\n\n[ 11175.5         39704.          51421.83333333  58832.\n  64437.5         69074.66666667  73103.64285714  76706.\n  79983.94444444  83000.          85795.04545455  88397.33333333\n  90827.34615385  93100.57142857  95229.16666667  97223.\n  99090.32352941 100838.22222222 102472.92105263 104000.\n 105424.54761905 106751.27272727 107984.58695652 109128.66666667\n 110187.5        111164.92307692 112064.64814815 112890.28571429\n 113645.36206897 114333.33333333 114957.59677419 115521.5\n 116028.34848485 116481.41176471 116883.92857143 117239.11111111\n 117550.14864865 117820.21052632 118052.44871795 118250.\n 118415.98780488 118553.52380952 118665.70930233 118755.63636364\n 118826.38888889 118881.04347826 118922.67021277 118954.33333333\n 118979.09183673 119000.         119020.10784314 119042.46153846\n 119070.10377358 119106.07407407 119153.40909091 119215.14285714\n 119294.30701754 119393.93103448 119517.04237288 119666.66666667\n 119845.82786885 120057.5483871  120304.84920635 120590.75\n 120918.26923077 121290.42424242 121710.23134328 122180.70588235\n 122704.86231884 123285.71428571 123926.27464789 124629.55555556\n 125398.56849315 126236.32432432 127145.83333333 128130.10526316\n 129192.14935065 130334.97435897 131561.58860759 132875.\n 134278.21604938 135774.24390244 137366.09036145 139056.76190476\n 140849.26470588 142746.60465116 144751.78735632 146867.81818182\n 149097.70224719 151444.44444444 153911.04945055 156500.52173913\n 159215.8655914  162060.08510638 165036.18421053 168147.16666667\n 171396.03608247 174785.79591837 178319.44949495 182000.        ]"
  },
  {
    "objectID": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#탐색적-데이터-분석",
    "href": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#탐색적-데이터-분석",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "3. 탐색적 데이터 분석",
    "text": "3. 탐색적 데이터 분석\n\nplt.scatter(X, y, color='b')\nplt.title('y=0.5*(X-50)**3 - 50000/X + 120000')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.ylim(0,200000)\nplt.show()"
  },
  {
    "objectID": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#데이터-분리",
    "href": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#데이터-분리",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "4. 데이터 분리",
    "text": "4. 데이터 분리\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,\n                                                    random_state=1234)\n\n\nprint(X_train)\nprint(X_test)\n\n[  5  65  11  94  58  73  37   8  55  78  22  19  71  87  23   7  45   9\n  42  17  46  21  26  56  79  32  93   6  85  33  53  14  92  18  29  47\n  61  15  66  13  20   3   4   1  12  68  98  35  38  96  51 100  74  81\n  70  59  91  90  44  31  27  24  50  16  25  77  54  39  84  48]\n[41 36 82 62 99 69 86 28 40 43 34 60 64 95 57 88 97  2 72 83 10 52 30 89\n 76 75 63 67 80 49]\n\n\n\nprint(y_train)\nprint(y_test)\n\n[ 64437.5        120918.26923077  85795.04545455 162060.08510638\n 119393.93103448 125398.56849315 117550.14864865  76706.\n 119153.40909091 130334.97435897 106751.27272727 102472.92105263\n 123926.27464789 144751.78735632 107984.58695652  73103.64285714\n 118826.38888889  79983.94444444 118553.52380952  99090.32352941\n 118881.04347826 105424.54761905 111164.92307692 119215.14285714\n 131561.58860759 115521.5        159215.8655914   69074.66666667\n 140849.26470588 116028.34848485 119070.10377358  93100.57142857\n 156500.52173913 100838.22222222 113645.36206897 118922.67021277\n 119845.82786885  95229.16666667 121290.42424242  90827.34615385\n 104000.          51421.83333333  58832.          11175.5\n  88397.33333333 122180.70588235 174785.79591837 116883.92857143\n 117820.21052632 168147.16666667 119020.10784314 182000.\n 126236.32432432 134278.21604938 123285.71428571 119517.04237288\n 153911.04945055 151444.44444444 118755.63636364 114957.59677419\n 112064.64814815 109128.66666667 119000.          97223.\n 110187.5        129192.14935065 119106.07407407 118052.44871795\n 139056.76190476 118954.33333333]\n[118415.98780488 117239.11111111 135774.24390244 120057.5483871\n 178319.44949495 122704.86231884 142746.60465116 112890.28571429\n 118250.         118665.70930233 116481.41176471 119666.66666667\n 120590.75       165036.18421053 119294.30701754 146867.81818182\n 171396.03608247  39704.         124629.55555556 137366.09036145\n  83000.         119042.46153846 114333.33333333 149097.70224719\n 128130.10526316 127145.83333333 120304.84920635 121710.23134328\n 132875.         118979.09183673]"
  },
  {
    "objectID": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#피처-스케일링",
    "href": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#피처-스케일링",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "5. 피처 스케일링",
    "text": "5. 피처 스케일링\n\nX_train = X_train.reshape(-1,1)\nX_test=X_test.reshape(-1,1)\ny_train=y_train.reshape(-1,1)\ny_test=y_test.reshape(-1,1)\nprint(X_train)\n\n[[  5]\n [ 65]\n [ 11]\n [ 94]\n [ 58]\n [ 73]\n [ 37]\n [  8]\n [ 55]\n [ 78]\n [ 22]\n [ 19]\n [ 71]\n [ 87]\n [ 23]\n [  7]\n [ 45]\n [  9]\n [ 42]\n [ 17]\n [ 46]\n [ 21]\n [ 26]\n [ 56]\n [ 79]\n [ 32]\n [ 93]\n [  6]\n [ 85]\n [ 33]\n [ 53]\n [ 14]\n [ 92]\n [ 18]\n [ 29]\n [ 47]\n [ 61]\n [ 15]\n [ 66]\n [ 13]\n [ 20]\n [  3]\n [  4]\n [  1]\n [ 12]\n [ 68]\n [ 98]\n [ 35]\n [ 38]\n [ 96]\n [ 51]\n [100]\n [ 74]\n [ 81]\n [ 70]\n [ 59]\n [ 91]\n [ 90]\n [ 44]\n [ 31]\n [ 27]\n [ 24]\n [ 50]\n [ 16]\n [ 25]\n [ 77]\n [ 54]\n [ 39]\n [ 84]\n [ 48]]\n\n\n\nscalerX=MinMaxScaler()\nscalerX.fit(X_train)\nX_train_norm=scalerX.transform(X_train)\n\n\nscalerY=MinMaxScaler()\nscalerY.fit(y_train)\ny_train_norm=scalerY.transform(y_train)\n\n\nX_test_norm=scalerX.transform(X_test)\ny_test_norm=scalerY.transform(y_test)"
  },
  {
    "objectID": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#모형화-및-학습",
    "href": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#모형화-및-학습",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "6. 모형화 및 학습",
    "text": "6. 모형화 및 학습\n\nmodel=MLPRegressor(hidden_layer_sizes=(4),\n                   activation='logistic',\n                   solver='lbfgs',\n                   max_iter=500)\n\n\nmodel.fit(X_train_norm,y_train_norm)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nMLPRegressor(activation='logistic', hidden_layer_sizes=4, max_iter=500,\n             solver='lbfgs')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MLPRegressorMLPRegressor(activation='logistic', hidden_layer_sizes=4, max_iter=500,\n             solver='lbfgs')"
  },
  {
    "objectID": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#예측-1",
    "href": "posts/est/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#예측-1",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "7. 예측",
    "text": "7. 예측\n\ny_pred=model.predict(X_test_norm)\nprint(y_pred)\n\n[0.58532633 0.5596793  0.78666172 0.69076416 0.86381245 0.72489815\n 0.80520178 0.51832425 0.58021083 0.59553508 0.5493746  0.68090896\n 0.70057482 0.84605491 0.66604552 0.81438502 0.85496516 0.38241064\n 0.73934651 0.79131812 0.42434114 0.64106953 0.52869543 0.81895453\n 0.75843272 0.7536807  0.69567514 0.71520447 0.77730687 0.62596952]\n\n\n\n# 데이터 구조의 변형\ny_pred=y_pred.reshape(-1,1)\n\n\n# 예측 값의 역변환\ny_pred_inverse=scalerY.inverse_transform(y_pred)\nprint(y_pred_inverse)\n\n[[111163.57767477]\n [106782.43697013]\n [145556.59434008]\n [129174.94277171]\n [158735.82920511]\n [135005.86461054]\n [148723.69087489]\n [ 99717.9814556 ]\n [110289.72571243]\n [112907.48205149]\n [105022.14122011]\n [127491.43180009]\n [130850.84418785]\n [155702.4069167 ]\n [124952.39323735]\n [150292.41387152]\n [157224.4958677 ]\n [ 76500.60587199]\n [137473.99775244]\n [146352.02276425]\n [ 83663.3625764 ]\n [120685.88219775]\n [101489.63200544]\n [151072.99795266]\n [140734.38957096]\n [139922.62822513]\n [130013.85862295]\n [133349.94535149]\n [143958.55773291]\n [118106.43080755]]\n\n\n\nplt.scatter(X,y, color='g')\n# 원데이터\nplt.title('y=0.5*(X-50)**3 - 50000/X + 120000')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.ylim(0,200000)\n\n# 테스트데이터\nplt.scatter(X_test, y_pred_inverse, color='r')\nplt.show()\n\n\n\n\n\nprint('MAPE:%.2f' % \n      mean_absolute_percentage_error(y_test,y_pred_inverse))\n\nMAPE:0.10"
  },
  {
    "objectID": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html",
    "href": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html",
    "title": "인공신경망과 퍼셉트론",
    "section": "",
    "text": "파이썬 데이터 분석"
  },
  {
    "objectID": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#패키지-설정",
    "href": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#패키지-설정",
    "title": "인공신경망과 퍼셉트론",
    "section": "1. 패키지 설정",
    "text": "1. 패키지 설정\n\nfrom sklearn.linear_model import Perceptron\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#데이터-준비",
    "href": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#데이터-준비",
    "title": "인공신경망과 퍼셉트론",
    "section": "2. 데이터 준비",
    "text": "2. 데이터 준비\n\n# 데이터 작성(OR연산)\n\nX=np.array([[0,0],[0,1],[1,0],[1,1]])\ny=np.array([0,1,1,1])"
  },
  {
    "objectID": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#탐색적-데이터-분석",
    "href": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#탐색적-데이터-분석",
    "title": "인공신경망과 퍼셉트론",
    "section": "3. 탐색적 데이터 분석",
    "text": "3. 탐색적 데이터 분석\n\nplt.scatter(X[:,0], X[:,1], c=y)\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.show()"
  },
  {
    "objectID": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#모형화-및-학습",
    "href": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#모형화-및-학습",
    "title": "인공신경망과 퍼셉트론",
    "section": "4. 모형화 및 학습",
    "text": "4. 모형화 및 학습\n\nmodel=Perceptron(verbose=1)\nmodel.fit(X,y)\n\n-- Epoch 1\nNorm: 1.41, NNZs: 2, Bias: 0.000000, T: 4, Avg. loss: 0.250000\nTotal training time: 0.00 seconds.\n-- Epoch 2\nNorm: 2.24, NNZs: 2, Bias: 0.000000, T: 8, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\n-- Epoch 3\nNorm: 2.24, NNZs: 2, Bias: -1.000000, T: 12, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\n-- Epoch 4\nNorm: 2.83, NNZs: 2, Bias: 0.000000, T: 16, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\n-- Epoch 5\nNorm: 2.83, NNZs: 2, Bias: -1.000000, T: 20, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\n-- Epoch 6\nNorm: 2.83, NNZs: 2, Bias: -1.000000, T: 24, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\n-- Epoch 7\nNorm: 2.83, NNZs: 2, Bias: -1.000000, T: 28, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\nConvergence after 7 epochs took 0.00 seconds\n\n\nPerceptron(verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PerceptronPerceptron(verbose=1)\n\n\n\n학습계수 eta0=1\n학습 종료 기준 허용오차 0.001\n이전 단계 epoch의 평균 비용과 현재 epoch의 평균 비용과의 차이가 허용오차보다 작으면 학습 종료ㅡ\n\n\n# 가중치\nprint(model.coef_)\n# 편향\nprint(model.intercept_)\n# 학습 수(epoch)\nprint(model.n_iter_)\n\n[[2. 2.]]\n[-1.]\n7\n\n\n\\[x_2=\\dfrac{w_1}{w_2}x_1 - \\dfrac{b}{w_2}\\]\n\\[x_2=-\\dfrac{2}{2}x_1 - \\dfrac{-1}{2}= -x_1+\\dfrac{1}{2}\\]\n\n#산포도\nplt.scatter(X[:,0], X[:,1],c=y)\nplt.xlabel('x1')\nplt.ylabel('x2')\n\n#파라미터\nw1=model.coef_[0,0]\nw2=model.coef_[0,1]\nb=model.intercept_\n\n#x절편\nx_intercept=-b/w1\n#y절편\ny_intercept=-b/w2\n#선형 분류자 추가\nplt.plot([0,y_intercept],[x_intercept,0], c='red')\nplt.show()\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/core/shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  ary = asanyarray(ary)"
  },
  {
    "objectID": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#예측",
    "href": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#예측",
    "title": "인공신경망과 퍼셉트론",
    "section": "5. 예측",
    "text": "5. 예측\n\nX_test=X\npred=model.predict(X_test)\nprint(pred)\n\n[0 1 1 1]"
  },
  {
    "objectID": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#패키지-설정-1",
    "href": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#패키지-설정-1",
    "title": "인공신경망과 퍼셉트론",
    "section": "1. 패키지 설정",
    "text": "1. 패키지 설정\n\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns"
  },
  {
    "objectID": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#데이터-준비-1",
    "href": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#데이터-준비-1",
    "title": "인공신경망과 퍼셉트론",
    "section": "2. 데이터 준비",
    "text": "2. 데이터 준비\n\niris = datasets.load_iris()\nprint(iris.feature_names)\nprint(iris.target_names)\n\n['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n['setosa' 'versicolor' 'virginica']\n\n\n\nX=iris.data\ny=iris.target\nprint(X[:10])\nprint(y)\n\n[[5.1 3.5 1.4 0.2]\n [4.9 3.  1.4 0.2]\n [4.7 3.2 1.3 0.2]\n [4.6 3.1 1.5 0.2]\n [5.  3.6 1.4 0.2]\n [5.4 3.9 1.7 0.4]\n [4.6 3.4 1.4 0.3]\n [5.  3.4 1.5 0.2]\n [4.4 2.9 1.4 0.2]\n [4.9 3.1 1.5 0.1]]\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]"
  },
  {
    "objectID": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#탐색적-데이터-분석-1",
    "href": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#탐색적-데이터-분석-1",
    "title": "인공신경망과 퍼셉트론",
    "section": "3. 탐색적 데이터 분석",
    "text": "3. 탐색적 데이터 분석\n\nplt.figure(figsize=(8,4))\nplt.boxplot([X[:,0],X[:,1],X[:,2],X[:,3]],\n            labels=iris.feature_names)\nplt.title('Iris')\nplt.xlabel('Features')\nplt.ylabel('Length or Width(cm)')\nplt.show()\n\n\n\n\n\nX_df=pd.DataFrame(X)  #boxplot()함수 사용을 위해 데이터 프레임 변환\n\nX_df.columns=iris.feature_names # 특성 이름 열 이름으로 대체\n\nX_df['species']=y\n\nprint(X_df)\n\n     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n0                  5.1               3.5                1.4               0.2   \n1                  4.9               3.0                1.4               0.2   \n2                  4.7               3.2                1.3               0.2   \n3                  4.6               3.1                1.5               0.2   \n4                  5.0               3.6                1.4               0.2   \n..                 ...               ...                ...               ...   \n145                6.7               3.0                5.2               2.3   \n146                6.3               2.5                5.0               1.9   \n147                6.5               3.0                5.2               2.0   \n148                6.2               3.4                5.4               2.3   \n149                5.9               3.0                5.1               1.8   \n\n     species  \n0          0  \n1          0  \n2          0  \n3          0  \n4          0  \n..       ...  \n145        2  \n146        2  \n147        2  \n148        2  \n149        2  \n\n[150 rows x 5 columns]\n\n\n\nsns.pairplot(X_df, hue='species', height=2)\n\n<seaborn.axisgrid.PairGrid at 0x7fef3a0e3070>"
  },
  {
    "objectID": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#데이터-분리",
    "href": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#데이터-분리",
    "title": "인공신경망과 퍼셉트론",
    "section": "4. 데이터 분리",
    "text": "4. 데이터 분리\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)"
  },
  {
    "objectID": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#피처-스케일링",
    "href": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#피처-스케일링",
    "title": "인공신경망과 퍼셉트론",
    "section": "5. 피처 스케일링",
    "text": "5. 피처 스케일링\n\nscalerX=StandardScaler()\nscalerX.fit(X_train)\nX_train_std=scalerX.transform(X_train)\nX_test_std=scalerX.transform(X_test)"
  },
  {
    "objectID": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#모형화-및-학습-1",
    "href": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#모형화-및-학습-1",
    "title": "인공신경망과 퍼셉트론",
    "section": "6. 모형화 및 학습",
    "text": "6. 모형화 및 학습\n\nmodel=Perceptron(verbose=1)\nmodel.fit(X_train_std,y_train)\n\n-- Epoch 1\nNorm: 3.02, NNZs: 4, Bias: 0.000000, T: 105, Avg. loss: 0.004369\nTotal training time: 0.00 seconds.\n-- Epoch 2\nNorm: 3.02, NNZs: 4, Bias: 0.000000, T: 210, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\n-- Epoch 3\nNorm: 3.02, NNZs: 4, Bias: 0.000000, T: 315, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\n-- Epoch 4\nNorm: 3.02, NNZs: 4, Bias: 0.000000, T: 420, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\n-- Epoch 5\nNorm: 3.02, NNZs: 4, Bias: 0.000000, T: 525, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\n-- Epoch 6\nNorm: 3.02, NNZs: 4, Bias: 0.000000, T: 630, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\n-- Epoch 7\nNorm: 3.02, NNZs: 4, Bias: 0.000000, T: 735, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\nConvergence after 7 epochs took 0.00 seconds\n-- Epoch 1\nNorm: 1.85, NNZs: 4, Bias: -2.000000, T: 105, Avg. loss: 0.701329\nTotal training time: 0.00 seconds.\n-- Epoch 2\nNorm: 3.98, NNZs: 4, Bias: 1.000000, T: 210, Avg. loss: 0.545394\nTotal training time: 0.00 seconds.\n-- Epoch 3\nNorm: 4.14, NNZs: 4, Bias: -1.000000, T: 315, Avg. loss: 0.808579\nTotal training time: 0.00 seconds.\n-- Epoch 4\nNorm: 5.02, NNZs: 4, Bias: -1.000000, T: 420, Avg. loss: 0.676848\nTotal training time: 0.00 seconds.\n-- Epoch 5\nNorm: 6.59, NNZs: 4, Bias: -1.000000, T: 525, Avg. loss: 0.787455\nTotal training time: 0.00 seconds.\n-- Epoch 6\nNorm: 5.52, NNZs: 4, Bias: -1.000000, T: 630, Avg. loss: 0.698311\nTotal training time: 0.00 seconds.\n-- Epoch 7\nNorm: 5.54, NNZs: 4, Bias: -2.000000, T: 735, Avg. loss: 0.644139\nTotal training time: 0.00 seconds.\nConvergence after 7 epochs took 0.00 seconds\n-- Epoch 1\nNorm: 3.94, NNZs: 4, Bias: -3.000000, T: 105, Avg. loss: 0.119225\nTotal training time: 0.00 seconds.\n-- Epoch 2\nNorm: 4.58, NNZs: 4, Bias: -4.000000, T: 210, Avg. loss: 0.117215\nTotal training time: 0.00 seconds.\n-- Epoch 3\nNorm: 5.79, NNZs: 4, Bias: -4.000000, T: 315, Avg. loss: 0.104179\nTotal training time: 0.00 seconds.\n-- Epoch 4\nNorm: 5.86, NNZs: 4, Bias: -5.000000, T: 420, Avg. loss: 0.095289\nTotal training time: 0.00 seconds.\n-- Epoch 5\nNorm: 7.06, NNZs: 4, Bias: -5.000000, T: 525, Avg. loss: 0.056061\nTotal training time: 0.00 seconds.\n-- Epoch 6\nNorm: 7.47, NNZs: 4, Bias: -6.000000, T: 630, Avg. loss: 0.095611\nTotal training time: 0.00 seconds.\n-- Epoch 7\nNorm: 7.88, NNZs: 4, Bias: -7.000000, T: 735, Avg. loss: 0.093122\nTotal training time: 0.00 seconds.\n-- Epoch 8\nNorm: 7.17, NNZs: 4, Bias: -8.000000, T: 840, Avg. loss: 0.019793\nTotal training time: 0.00 seconds.\n-- Epoch 9\nNorm: 9.08, NNZs: 4, Bias: -7.000000, T: 945, Avg. loss: 0.082413\nTotal training time: 0.00 seconds.\n-- Epoch 10\nNorm: 8.29, NNZs: 4, Bias: -8.000000, T: 1050, Avg. loss: 0.045435\nTotal training time: 0.00 seconds.\n-- Epoch 11\nNorm: 8.41, NNZs: 4, Bias: -8.000000, T: 1155, Avg. loss: 0.067990\nTotal training time: 0.00 seconds.\n-- Epoch 12\nNorm: 8.56, NNZs: 4, Bias: -8.000000, T: 1260, Avg. loss: 0.062187\nTotal training time: 0.00 seconds.\n-- Epoch 13\nNorm: 7.87, NNZs: 4, Bias: -9.000000, T: 1365, Avg. loss: 0.082040\nTotal training time: 0.00 seconds.\nConvergence after 13 epochs took 0.00 seconds\n\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n\n\nPerceptron(verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PerceptronPerceptron(verbose=1)\n\n\n\n# 가중치\nprint(model.coef_)\n# 편향\nprint(model.intercept_)\n\n[[-1.35951428  1.97275265 -1.46733157 -1.0997214 ]\n [-0.91929061 -1.69928112  4.06213278 -3.23501377]\n [-1.60316749 -0.19414391  5.45511955  5.44362091]]\n[ 0. -2. -9.]"
  },
  {
    "objectID": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#예측-1",
    "href": "posts/est/Python Data Analysis/인공신경망과 퍼셉트론.html#예측-1",
    "title": "인공신경망과 퍼셉트론",
    "section": "7. 예측",
    "text": "7. 예측\n\ny_pred=model.predict(X_test_std)\n\n\nprint(y_pred) #예측값\nprint(y_test) #실제값\n\n[1 0 2 2 0 0 2 2 0 1 0 2 0 1 2 1 1 2 0 0 0 2 0 0 1 0 2 2 0 0 2 0 0 0 0 0 2\n 2 2 0 2 0 0 2 2]\n[1 1 2 2 1 0 2 2 0 2 1 2 0 1 2 1 1 2 0 0 0 2 0 0 1 1 2 2 0 0 2 0 0 0 1 0 2\n 2 2 0 2 0 1 2 2]\n\n\n\nprint('Accuracy:%.2f' % accuracy_score(y_test,y_pred))\n\nAccuracy:0.84"
  },
  {
    "objectID": "posts/est/imbalaced data/plot_comparison_under_sampling.html",
    "href": "posts/est/imbalaced data/plot_comparison_under_sampling.html",
    "title": "plot_comparison_under_sampling",
    "section": "",
    "text": "ref: imbalaced-learn"
  },
  {
    "objectID": "posts/est/imbalaced data/plot_comparison_under_sampling.html#prototype-generation-under-sampling-by-generating-new-samples",
    "href": "posts/est/imbalaced data/plot_comparison_under_sampling.html#prototype-generation-under-sampling-by-generating-new-samples",
    "title": "plot_comparison_under_sampling",
    "section": "Prototype generation: under-sampling by generating new samples",
    "text": "Prototype generation: under-sampling by generating new samples\n:class:~imblearn.under_sampling.ClusterCentroids under-samples by replacing the original samples by the centroids of the cluster found.\n\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import MiniBatchKMeans\n\nfrom imblearn import FunctionSampler\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.under_sampling import ClusterCentroids\n\nX, y = create_dataset(n_samples=400, weights=(0.05, 0.15, 0.8), class_sep=0.8)\n\nsamplers = {\n    FunctionSampler(),  # identity resampler\n    ClusterCentroids(\n        estimator=MiniBatchKMeans(n_init=1, random_state=0), random_state=0\n    ),\n}\n\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X, y, model, ax[0], title=f\"Decision function with {sampler.__class__.__name__}\"\n    )\n    plot_resampling(X, y, sampler, ax[1])\n\nfig.tight_layout()"
  },
  {
    "objectID": "posts/est/imbalaced data/plot_comparison_under_sampling.html#prototype-selection-under-sampling-by-selecting-existing-samples",
    "href": "posts/est/imbalaced data/plot_comparison_under_sampling.html#prototype-selection-under-sampling-by-selecting-existing-samples",
    "title": "plot_comparison_under_sampling",
    "section": "Prototype selection: under-sampling by selecting existing samples",
    "text": "Prototype selection: under-sampling by selecting existing samples\nThe algorithm performing prototype selection can be subdivided into two groups: (i) the controlled under-sampling methods and (ii) the cleaning under-sampling methods.\nWith the controlled under-sampling methods, the number of samples to be selected can be specified. :class:~imblearn.under_sampling.RandomUnderSampler is the most naive way of performing such selection by randomly selecting a given number of samples by the targetted class.\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\nX, y = create_dataset(n_samples=400, weights=(0.05, 0.15, 0.8), class_sep=0.8)\n\nsamplers = {\n    FunctionSampler(),  # identity resampler\n    RandomUnderSampler(random_state=0),\n}\n\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X, y, model, ax[0], title=f\"Decision function with {sampler.__class__.__name__}\"\n    )\n    plot_resampling(X, y, sampler, ax[1])\n\nfig.tight_layout()\n\n\n\n\n:class:~imblearn.under_sampling.NearMiss algorithms implement some heuristic rules in order to select samples. NearMiss-1 selects samples from the majority class for which the average distance of the \\(k\\)` nearest samples of the minority class is the smallest. NearMiss-2 selects the samples from the majority class for which the average distance to the farthest samples of the negative class is the smallest. NearMiss-3 is a 2-step algorithm: first, for each minority sample, their \\(m\\) nearest-neighbors will be kept; then, the majority samples selected are the on for which the average distance to the \\(k\\) nearest neighbors is the largest.\n\nfrom imblearn.under_sampling import NearMiss\n\nX, y = create_dataset(n_samples=1000, weights=(0.05, 0.15, 0.8), class_sep=1.5)\n\nsamplers = [NearMiss(version=1), NearMiss(version=2), NearMiss(version=3)]\n\nfig, axs = plt.subplots(nrows=3, ncols=2, figsize=(15, 25))\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X,\n        y,\n        model,\n        ax[0],\n        title=f\"Decision function for {sampler.__class__.__name__}-{sampler.version}\",\n    )\n    plot_resampling(\n        X,\n        y,\n        sampler,\n        ax[1],\n        title=f\"Resampling using {sampler.__class__.__name__}-{sampler.version}\",\n    )\nfig.tight_layout()\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:203: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:203: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:203: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:203: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n  warnings.warn(\n\n\n\n\n\n:class:~imblearn.under_sampling.EditedNearestNeighbours removes samples of the majority class for which their class differ from the one of their nearest-neighbors. This sieve can be repeated which is the principle of the :class:~imblearn.under_sampling.RepeatedEditedNearestNeighbours. :class:~imblearn.under_sampling.AllKNN is slightly different from the :class:~imblearn.under_sampling.RepeatedEditedNearestNeighbours by changing the \\(k\\) parameter of the internal nearest neighors algorithm, increasing it at each iteration.\n\nfrom imblearn.under_sampling import (\n    AllKNN,\n    EditedNearestNeighbours,\n    RepeatedEditedNearestNeighbours,\n)\n\nX, y = create_dataset(n_samples=500, weights=(0.2, 0.3, 0.5), class_sep=0.8)\n\nsamplers = [\n    EditedNearestNeighbours(),\n    RepeatedEditedNearestNeighbours(),\n    AllKNN(allow_minority=True),\n]\n\nfig, axs = plt.subplots(3, 2, figsize=(15, 25))\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X, y, clf, ax[0], title=f\"Decision function for \\n{sampler.__class__.__name__}\"\n    )\n    plot_resampling(\n        X, y, sampler, ax[1], title=f\"Resampling using \\n{sampler.__class__.__name__}\"\n    )\n\nfig.tight_layout()\n\n\n\n\n:class:~imblearn.under_sampling.CondensedNearestNeighbour makes use of a 1-NN to iteratively decide if a sample should be kept in a dataset or not. The issue is that :class:~imblearn.under_sampling.CondensedNearestNeighbour is sensitive to noise by preserving the noisy samples. :class:~imblearn.under_sampling.OneSidedSelection also used the 1-NN and use :class:~imblearn.under_sampling.TomekLinks to remove the samples considered noisy. The :class:~imblearn.under_sampling.NeighbourhoodCleaningRule use a :class:~imblearn.under_sampling.EditedNearestNeighbours to remove some sample. Additionally, they use a 3 nearest-neighbors to remove samples which do not agree with this rule.\n\nfrom imblearn.under_sampling import (\n    CondensedNearestNeighbour,\n    NeighbourhoodCleaningRule,\n    OneSidedSelection,\n)\n\nX, y = create_dataset(n_samples=500, weights=(0.2, 0.3, 0.5), class_sep=0.8)\n\nfig, axs = plt.subplots(nrows=3, ncols=2, figsize=(15, 25))\n\nsamplers = [\n    CondensedNearestNeighbour(random_state=0),\n    OneSidedSelection(random_state=0),\n    NeighbourhoodCleaningRule(),\n]\n\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X, y, clf, ax[0], title=f\"Decision function for \\n{sampler.__class__.__name__}\"\n    )\n    plot_resampling(\n        X, y, sampler, ax[1], title=f\"Resampling using \\n{sampler.__class__.__name__}\"\n    )\nfig.tight_layout()\n\n\n\n\n:class:~imblearn.under_sampling.InstanceHardnessThreshold uses the prediction of classifier to exclude samples. All samples which are classified with a low probability will be removed.\n\nfrom imblearn.under_sampling import InstanceHardnessThreshold\n\nsamplers = {\n    FunctionSampler(),  # identity resampler\n    InstanceHardnessThreshold(\n        estimator=LogisticRegression(),\n        random_state=0,\n    ),\n}\n\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X,\n        y,\n        model,\n        ax[0],\n        title=f\"Decision function with \\n{sampler.__class__.__name__}\",\n    )\n    plot_resampling(\n        X, y, sampler, ax[1], title=f\"Resampling using \\n{sampler.__class__.__name__}\"\n    )\n\nfig.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/est/imbalaced data/imbalaced data.html",
    "href": "posts/est/imbalaced data/imbalaced data.html",
    "title": "imbalanced data",
    "section": "",
    "text": "https://imbalanced-learn.org/stable/references/index.html#api\nhttps://towardsdatascience.com/imbalanced-classification-in-python-smote-tomek-links-method-6e48dfe69bbc\nhttps://www.kaggle.com/code/rafjaa/resampling-strategies-for-imbalanced-datasets/notebook"
  },
  {
    "objectID": "posts/est/imbalaced data/imbalaced data.html#generate-the-dataset",
    "href": "posts/est/imbalaced data/imbalaced data.html#generate-the-dataset",
    "title": "imbalanced data",
    "section": "Generate the dataset",
    "text": "Generate the dataset\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=200, shuffle=True, noise=0.5, random_state=10)\nX = pd.DataFrame(X, columns=[\"feature 1\", \"feature 2\"])\nax = X.plot.scatter(\n    x=\"feature 1\",\n    y=\"feature 2\",\n    c=y,\n    colormap=\"viridis\",\n    colorbar=False,\n)\nsns.despine(ax=ax, offset=10)\nplt.tight_layout()"
  },
  {
    "objectID": "posts/est/imbalaced data/imbalaced data.html#make-a-dataset-imbalanced",
    "href": "posts/est/imbalaced data/imbalaced data.html#make-a-dataset-imbalanced",
    "title": "imbalanced data",
    "section": "Make a dataset imbalanced",
    "text": "Make a dataset imbalanced\n\n# pip install imblearn\n\n\nfrom collections import Counter\n\n\ndef ratio_func(y, multiplier, minority_class):\n    target_stats = Counter(y)\n    return {minority_class: int(multiplier * target_stats[minority_class])}\n\n\nfrom imblearn.datasets import make_imbalance\n\nfig, axs = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n\nX.plot.scatter(\n    x=\"feature 1\",\n    y=\"feature 2\",\n    c=y,\n    ax=axs[0, 0],\n    colormap=\"viridis\",\n    colorbar=False,\n)\naxs[0, 0].set_title(\"Original set\")\nsns.despine(ax=axs[0, 0], offset=10)\n\nmultipliers = [0.9, 0.75, 0.5, 0.25, 0.1]\nfor ax, multiplier in zip(axs.ravel()[1:], multipliers):\n    X_resampled, y_resampled = make_imbalance(\n        X,\n        y,\n        sampling_strategy=ratio_func,\n        **{\"multiplier\": multiplier, \"minority_class\": 1},\n    )\n    X_resampled.plot.scatter(\n        x=\"feature 1\",\n        y=\"feature 2\",\n        c=y_resampled,\n        ax=ax,\n        colormap=\"viridis\",\n        colorbar=False,\n    )\n    ax.set_title(f\"Sampling ratio = {multiplier}\")\n    sns.despine(ax=ax, offset=10)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/est/imbalaced data/imbalaced data.html#effect-of-the-shrinkage-factor-in-random-over-sampling",
    "href": "posts/est/imbalaced data/imbalaced data.html#effect-of-the-shrinkage-factor-in-random-over-sampling",
    "title": "imbalanced data",
    "section": "Effect of the shrinkage factor in random over-sampling",
    "text": "Effect of the shrinkage factor in random over-sampling\n\nfrom collections import Counter\n\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_samples=100,\n    n_features=2,\n    n_redundant=0,\n    weights=[0.1, 0.9],\n    random_state=0,\n)\nCounter(y)\n\nCounter({1: 90, 0: 10})\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(7, 7))\nscatter = plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.4)\nclass_legend = ax.legend(*scatter.legend_elements(), loc=\"lower left\", title=\"Classes\")\nax.add_artist(class_legend)\nax.set_xlabel(\"Feature #1\")\n_ = ax.set_ylabel(\"Feature #2\")\nplt.tight_layout()\n\n\n\n\n\nfrom imblearn.over_sampling import RandomOverSampler\n\nsampler = RandomOverSampler(random_state=0)\nX_res, y_res = sampler.fit_resample(X, y)\nCounter(y_res)\n\nCounter({1: 90, 0: 90})\n\n\n\n부트스트랩 사용\n\n\nfig, ax = plt.subplots(figsize=(7, 7))\nscatter = plt.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.4)\nclass_legend = ax.legend(*scatter.legend_elements(), loc=\"lower left\", title=\"Classes\")\nax.add_artist(class_legend)\nax.set_xlabel(\"Feature #1\")\n_ = ax.set_ylabel(\"Feature #2\")\nplt.tight_layout()\n\n\n\n\n\nsampler = RandomOverSampler(shrinkage=1, random_state=0)\nX_res, y_res = sampler.fit_resample(X, y)\nCounter(y_res)\n\nCounter({1: 90, 0: 90})\n\n\n\nfig, ax = plt.subplots(figsize=(7, 7))\nscatter = plt.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.4)\nclass_legend = ax.legend(*scatter.legend_elements(), loc=\"lower left\", title=\"Classes\")\nax.add_artist(class_legend)\nax.set_xlabel(\"Feature #1\")\n_ = ax.set_ylabel(\"Feature #2\")\nplt.tight_layout()\n\n\n\n\n\nsampler = RandomOverSampler(shrinkage=3, random_state=0)\nX_res, y_res = sampler.fit_resample(X, y)\nCounter(y_res)\n\nCounter({1: 90, 0: 90})\n\n\n\nfig, ax = plt.subplots(figsize=(7, 7))\nscatter = plt.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.4)\nclass_legend = ax.legend(*scatter.legend_elements(), loc=\"lower left\", title=\"Classes\")\nax.add_artist(class_legend)\nax.set_xlabel(\"Feature #1\")\n_ = ax.set_ylabel(\"Feature #2\")\nplt.tight_layout()\n\n\n\n\n\nsampler = RandomOverSampler(shrinkage=0, random_state=0)\nX_res, y_res = sampler.fit_resample(X, y)\nCounter(y_res)\n\nCounter({1: 90, 0: 90})\n\n\n\nfig, ax = plt.subplots(figsize=(7, 7))\nscatter = plt.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.4)\nclass_legend = ax.legend(*scatter.legend_elements(), loc=\"lower left\", title=\"Classes\")\nax.add_artist(class_legend)\nax.set_xlabel(\"Feature #1\")\n_ = ax.set_ylabel(\"Feature #2\")\nplt.tight_layout()"
  },
  {
    "objectID": "posts/est/imbalaced data/imbalaced data.html#sample-generator-used-in-smote-like-samplers",
    "href": "posts/est/imbalaced data/imbalaced data.html#sample-generator-used-in-smote-like-samplers",
    "title": "imbalanced data",
    "section": "Sample generator used in SMOTE-like samplers",
    "text": "Sample generator used in SMOTE-like samplers\n\nprint(__doc__)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nsns.set_context(\"poster\")\n\nrng = np.random.RandomState(18)\n\nf, ax = plt.subplots(figsize=(8, 8))\n\n# generate some data points\ny = np.array([3.65284, 3.52623, 3.51468, 3.22199, 3.21])\nz = np.array([0.43, 0.45, 0.6, 0.4, 0.211])\ny_2 = np.array([3.3, 3.6])\nz_2 = np.array([0.58, 0.34])\n\n# plot the majority and minority samples\nax.scatter(z, y, label=\"Minority class\", s=100)\nax.scatter(z_2, y_2, label=\"Majority class\", s=100)\n\nidx = rng.randint(len(y), size=2)\nannotation = [r\"$x_i$\", r\"$x_{zi}$\"]\n\nfor a, i in zip(annotation, idx):\n    ax.annotate(a, (z[i], y[i]), xytext=tuple([z[i] + 0.01, y[i] + 0.005]), fontsize=15)\n\n# draw the circle in which the new sample will generated\nradius = np.sqrt((z[idx[0]] - z[idx[1]]) ** 2 + (y[idx[0]] - y[idx[1]]) ** 2)\ncircle = plt.Circle((z[idx[0]], y[idx[0]]), radius=radius, alpha=0.2)\nax.add_artist(circle)\n\n# plot the line on which the sample will be generated\nax.plot(z[idx], y[idx], \"--\", alpha=0.5)\n\n# create and plot the new sample\nstep = rng.uniform()\ny_gen = y[idx[0]] + step * (y[idx[1]] - y[idx[0]])\nz_gen = z[idx[0]] + step * (z[idx[1]] - z[idx[0]])\n\nax.scatter(z_gen, y_gen, s=100)\nax.annotate(\n    r\"$x_{new}$\",\n    (z_gen, y_gen),\n    xytext=tuple([z_gen + 0.01, y_gen + 0.005]),\n    fontsize=15,\n)\n\n# make the plot nicer with legend and label\nsns.despine(ax=ax, offset=10)\nax.set_xlim([0.2, 0.7])\nax.set_ylim([3.2, 3.7])\nplt.xlabel(r\"$X_1$\")\nplt.ylabel(r\"$X_2$\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nAutomatically created module for IPython interactive environment"
  },
  {
    "objectID": "posts/est/imbalaced data/imbalaced data.html#illustration-of-the-definition-of-a-tomek-link",
    "href": "posts/est/imbalaced data/imbalaced data.html#illustration-of-the-definition-of-a-tomek-link",
    "title": "imbalanced data",
    "section": "Illustration of the definition of a Tomek link",
    "text": "Illustration of the definition of a Tomek link\n\nprint(__doc__)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_context(\"poster\")\n\nAutomatically created module for IPython interactive environment\n\n\n\ndef make_plot_despine(ax):\n    sns.despine(ax=ax, offset=10)\n    ax.set_xlim([0, 3])\n    ax.set_ylim([0, 3])\n    ax.set_xlabel(r\"$X_1$\")\n    ax.set_ylabel(r\"$X_2$\")\n    ax.legend(loc=\"lower right\")\n\n\nimport numpy as np\n\nrng = np.random.RandomState(18)\n\nX_minority = np.transpose(\n    [[1.1, 1.3, 1.15, 0.8, 0.55, 2.1], [1.0, 1.5, 1.7, 2.5, 0.55, 1.9]]\n)\nX_majority = np.transpose(\n    [\n        [2.1, 2.12, 2.13, 2.14, 2.2, 2.3, 2.5, 2.45],\n        [1.5, 2.1, 2.7, 0.9, 1.0, 1.4, 2.4, 2.9],\n    ]\n)\n\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.scatter(\n    X_minority[:, 0],\n    X_minority[:, 1],\n    label=\"Minority class\",\n    s=200,\n    marker=\"_\",\n)\nax.scatter(\n    X_majority[:, 0],\n    X_majority[:, 1],\n    label=\"Majority class\",\n    s=200,\n    marker=\"+\",\n)\n\n# highlight the samples of interest\nax.scatter(\n    [X_minority[-1, 0], X_majority[1, 0]],\n    [X_minority[-1, 1], X_majority[1, 1]],\n    label=\"Tomek link\",\n    s=200,\n    alpha=0.3,\n)\nmake_plot_despine(ax)\nfig.suptitle(\"Illustration of a Tomek link\")\nfig.tight_layout()\n\n\n\n\n\nfrom imblearn.under_sampling import TomekLinks\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n\nsamplers = {\n    \"Removing only majority samples\": TomekLinks(sampling_strategy=\"auto\"),\n    \"Removing all samples\": TomekLinks(sampling_strategy=\"all\"),\n}\n\nfor ax, (title, sampler) in zip(axs, samplers.items()):\n    X_res, y_res = sampler.fit_resample(\n        np.vstack((X_minority, X_majority)),\n        np.array([0] * X_minority.shape[0] + [1] * X_majority.shape[0]),\n    )\n    ax.scatter(\n        X_res[y_res == 0][:, 0],\n        X_res[y_res == 0][:, 1],\n        label=\"Minority class\",\n        s=200,\n        marker=\"_\",\n    )\n    ax.scatter(\n        X_res[y_res == 1][:, 0],\n        X_res[y_res == 1][:, 1],\n        label=\"Majority class\",\n        s=200,\n        marker=\"+\",\n    )\n\n    # highlight the samples of interest\n    ax.scatter(\n        [X_minority[-1, 0], X_majority[1, 0]],\n        [X_minority[-1, 1], X_majority[1, 1]],\n        label=\"Tomek link\",\n        s=200,\n        alpha=0.3,\n    )\n\n    ax.set_title(title)\n    make_plot_despine(ax)\nfig.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "posts/est/imbalaced data/imbalaced data.html#sample-selection-in-nearmiss",
    "href": "posts/est/imbalaced data/imbalaced data.html#sample-selection-in-nearmiss",
    "title": "imbalanced data",
    "section": "Sample selection in NearMiss",
    "text": "Sample selection in NearMiss\n\nprint(__doc__)\n\nimport seaborn as sns\n\nsns.set_context(\"poster\")\n\nAutomatically created module for IPython interactive environment\n\n\n\ndef make_plot_despine(ax):\n    sns.despine(ax=ax, offset=10)\n    ax.set_xlim([0, 3.5])\n    ax.set_ylim([0, 3.5])\n    ax.set_xticks(np.arange(0, 3.6, 0.5))\n    ax.set_yticks(np.arange(0, 3.6, 0.5))\n    ax.set_xlabel(r\"$X_1$\")\n    ax.set_ylabel(r\"$X_2$\")\n    ax.legend(loc=\"upper left\", fontsize=16)\n\n\nimport numpy as np\n\nrng = np.random.RandomState(18)\n\nX_minority = np.transpose(\n    [[1.1, 1.3, 1.15, 0.8, 0.8, 0.6, 0.55], [1.0, 1.5, 1.7, 2.5, 2.0, 1.2, 0.55]]\n)\nX_majority = np.transpose(\n    [\n        [2.1, 2.12, 2.13, 2.14, 2.2, 2.3, 2.5, 2.45],\n        [1.5, 2.1, 2.7, 0.9, 1.0, 1.4, 2.4, 2.9],\n    ]\n)\n\n\nNearMiss-1\n\n가장 가까운 이웃의 평균 거리가 가장 작은 다수 클래스에서 표본 선택\n3-NN사용하여 특정 샘플 2개에 대한 평균 거리 계산\n평균거리가 더 작아 녹색 점선으로 연결된 점 선택\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import NearestNeighbors\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.scatter(\n    X_minority[:, 0],\n    X_minority[:, 1],\n    label=\"Minority class\",\n    s=200,\n    marker=\"_\",\n)\nax.scatter(\n    X_majority[:, 0],\n    X_majority[:, 1],\n    label=\"Majority class\",\n    s=200,\n    marker=\"+\",\n)\n\nnearest_neighbors = NearestNeighbors(n_neighbors=3)\nnearest_neighbors.fit(X_minority)\ndist, ind = nearest_neighbors.kneighbors(X_majority[:2, :])\ndist_avg = dist.sum(axis=1) / 3\n\nfor positive_idx, (neighbors, distance, color) in enumerate(\n    zip(ind, dist_avg, [\"g\", \"r\"])\n):\n    for make_plot, sample_idx in enumerate(neighbors):\n        ax.plot(\n            [X_majority[positive_idx, 0], X_minority[sample_idx, 0]],\n            [X_majority[positive_idx, 1], X_minority[sample_idx, 1]],\n            \"--\" + color,\n            alpha=0.3,\n            label=f\"Avg. dist.={distance:.2f}\" if make_plot == 0 else \"\",\n        )\nax.set_title(\"NearMiss-1\")\nmake_plot_despine(ax)\nplt.tight_layout()\n\n\n\n\n\n\nNearMiss-2\n\n가장 먼 이웃의 평균 거리가 가장 작은 샘플 선택\n멀리 있는 세 이웃의 거리가 가장 작은 녹색 점 선택\n\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.scatter(\n    X_minority[:, 0],\n    X_minority[:, 1],\n    label=\"Minority class\",\n    s=200,\n    marker=\"_\",\n)\nax.scatter(\n    X_majority[:, 0],\n    X_majority[:, 1],\n    label=\"Majority class\",\n    s=200,\n    marker=\"+\",\n)\n\nnearest_neighbors = NearestNeighbors(n_neighbors=X_minority.shape[0])\nnearest_neighbors.fit(X_minority)\ndist, ind = nearest_neighbors.kneighbors(X_majority[:2, :])\ndist = dist[:, -3::]\nind = ind[:, -3::]\ndist_avg = dist.sum(axis=1) / 3\n\nfor positive_idx, (neighbors, distance, color) in enumerate(\n    zip(ind, dist_avg, [\"g\", \"r\"])\n):\n    for make_plot, sample_idx in enumerate(neighbors):\n        ax.plot(\n            [X_majority[positive_idx, 0], X_minority[sample_idx, 0]],\n            [X_majority[positive_idx, 1], X_minority[sample_idx, 1]],\n            \"--\" + color,\n            alpha=0.3,\n            label=f\"Avg. dist.={distance:.2f}\" if make_plot == 0 else \"\",\n        )\nax.set_title(\"NearMiss-2\")\nmake_plot_despine(ax)\nplt.tight_layout()\n\n\n\n\n\n\nNearMiss-3\n\n가장 가까운 이웃은 다수 클래스의 샘플을 short-list 하는데 사용\n가장 가까운 이웃의 평균 거리가 가장 큰 표본 선택\n\n\nfig, ax = plt.subplots(figsize=(8.5, 8.5))\nax.scatter(\n    X_minority[:, 0],\n    X_minority[:, 1],\n    label=\"Minority class\",\n    s=200,\n    marker=\"_\",\n)\nax.scatter(\n    X_majority[:, 0],\n    X_majority[:, 1],\n    label=\"Majority class\",\n    s=200,\n    marker=\"+\",\n)\n\nnearest_neighbors = NearestNeighbors(n_neighbors=3)\nnearest_neighbors.fit(X_majority)\n\n# select only the majority point of interest\nselected_idx = nearest_neighbors.kneighbors(X_minority, return_distance=False)\nX_majority = X_majority[np.unique(selected_idx), :]\nax.scatter(\n    X_majority[:, 0],\n    X_majority[:, 1],\n    label=\"Short-listed samples\",\n    s=200,\n    alpha=0.3,\n    color=\"g\",\n)\nnearest_neighbors = NearestNeighbors(n_neighbors=3)\nnearest_neighbors.fit(X_minority)\ndist, ind = nearest_neighbors.kneighbors(X_majority[:2, :])\ndist_avg = dist.sum(axis=1) / 3\n\nfor positive_idx, (neighbors, distance, color) in enumerate(\n    zip(ind, dist_avg, [\"r\", \"g\"])\n):\n    for make_plot, sample_idx in enumerate(neighbors):\n        ax.plot(\n            [X_majority[positive_idx, 0], X_minority[sample_idx, 0]],\n            [X_majority[positive_idx, 1], X_minority[sample_idx, 1]],\n            \"--\" + color,\n            alpha=0.3,\n            label=f\"Avg. dist.={distance:.2f}\" if make_plot == 0 else \"\",\n        )\nax.set_title(\"NearMiss-3\")\nmake_plot_despine(ax)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/est/imbalaced data/plot_comparison_over_sampling.html",
    "href": "posts/est/imbalaced data/plot_comparison_over_sampling.html",
    "title": "plot_comparison_over_sampling",
    "section": "",
    "text": "ref: imbalaced-learn"
  },
  {
    "objectID": "posts/est/imbalaced data/plot_comparison_over_sampling.html#illustration-of-the-influence-of-the-balancing-ratio",
    "href": "posts/est/imbalaced data/plot_comparison_over_sampling.html#illustration-of-the-influence-of-the-balancing-ratio",
    "title": "plot_comparison_over_sampling",
    "section": "Illustration of the influence of the balancing ratio",
    "text": "Illustration of the influence of the balancing ratio\nWe will first illustrate the influence of the balancing ratio on some toy data using a logistic regression classifier which is a linear model.\n\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\n\nWe will fit and show the decision boundary model to illustrate the impact of dealing with imbalanced classes.\n\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 12))\n\nweights_arr = (\n    (0.01, 0.01, 0.98),\n    (0.01, 0.05, 0.94),\n    (0.2, 0.1, 0.7),\n    (0.33, 0.33, 0.33),\n)\nfor ax, weights in zip(axs.ravel(), weights_arr):\n    X, y = create_dataset(n_samples=300, weights=weights)\n    clf.fit(X, y)\n    plot_decision_function(X, y, clf, ax, title=f\"weight={weights}\")\n    fig.suptitle(f\"Decision function of {clf.__class__.__name__}\")\nfig.tight_layout()\n\n\n\n\nGreater is the difference between the number of samples in each class, poorer are the classification results."
  },
  {
    "objectID": "posts/est/imbalaced data/plot_comparison_over_sampling.html#random-over-sampling-to-balance-the-data-set",
    "href": "posts/est/imbalaced data/plot_comparison_over_sampling.html#random-over-sampling-to-balance-the-data-set",
    "title": "plot_comparison_over_sampling",
    "section": "Random over-sampling to balance the data set",
    "text": "Random over-sampling to balance the data set\nRandom over-sampling can be used to repeat some samples and balance the number of samples between the dataset. It can be seen that with this trivial approach the boundary decision is already less biased toward the majority class. The class :class:~imblearn.over_sampling.RandomOverSampler implements such of a strategy.\n\nfrom imblearn.over_sampling import RandomOverSampler\n\n\nfrom imblearn.pipeline import make_pipeline\n\nX, y = create_dataset(n_samples=100, weights=(0.05, 0.25, 0.7))\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 7))\n\nclf.fit(X, y)\nplot_decision_function(X, y, clf, axs[0], title=\"Without resampling\")\n\nsampler = RandomOverSampler(random_state=0)\nmodel = make_pipeline(sampler, clf).fit(X, y)\nplot_decision_function(X, y, model, axs[1], f\"Using {model[0].__class__.__name__}\")\n\nfig.suptitle(f\"Decision function of {clf.__class__.__name__}\")\nfig.tight_layout()\n\n\n\n\nBy default, random over-sampling generates a bootstrap. The parameter shrinkage allows adding a small perturbation to the generated data to generate a smoothed bootstrap instead. The plot below shows the difference between the two data generation strategies.\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 7))\n\nsampler.set_params(shrinkage=None)\nplot_resampling(X, y, sampler, ax=axs[0], title=\"Normal bootstrap\")\n\nsampler.set_params(shrinkage=0.3)\nplot_resampling(X, y, sampler, ax=axs[1], title=\"Smoothed bootstrap\")\n\nfig.suptitle(f\"Resampling with {sampler.__class__.__name__}\")\nfig.tight_layout()\n\n\n\n\nIt looks like more samples are generated with smoothed bootstrap. This is due to the fact that the samples generated are not superimposing with the original samples."
  },
  {
    "objectID": "posts/est/imbalaced data/plot_comparison_over_sampling.html#more-advanced-over-sampling-using-adasyn-and-smote",
    "href": "posts/est/imbalaced data/plot_comparison_over_sampling.html#more-advanced-over-sampling-using-adasyn-and-smote",
    "title": "plot_comparison_over_sampling",
    "section": "More advanced over-sampling using ADASYN and SMOTE",
    "text": "More advanced over-sampling using ADASYN and SMOTE\nInstead of repeating the same samples when over-sampling or perturbating the generated bootstrap samples, one can use some specific heuristic instead. :class:~imblearn.over_sampling.ADASYN and :class:~imblearn.over_sampling.SMOTE can be used in this case.\n\nfrom imblearn import FunctionSampler  # to use a idendity sampler\nfrom imblearn.over_sampling import ADASYN, SMOTE\n\nX, y = create_dataset(n_samples=150, weights=(0.1, 0.2, 0.7))\n\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\n\nsamplers = [\n    FunctionSampler(),\n    RandomOverSampler(random_state=0),\n    SMOTE(random_state=0),\n    ADASYN(random_state=0),\n]\n\nfor ax, sampler in zip(axs.ravel(), samplers):\n    title = \"Original dataset\" if isinstance(sampler, FunctionSampler) else None\n    plot_resampling(X, y, sampler, ax, title=title)\nfig.tight_layout()\n\n\n\n\nThe following plot illustrates the difference between :class:~imblearn.over_sampling.ADASYN and :class:~imblearn.over_sampling.SMOTE. :class:~imblearn.over_sampling.ADASYN will focus on the samples which are difficult to classify with a nearest-neighbors rule while regular :class:~imblearn.over_sampling.SMOTE will not make any distinction. Therefore, the decision function depending of the algorithm.\n\nX, y = create_dataset(n_samples=150, weights=(0.05, 0.25, 0.7))\n\nfig, axs = plt.subplots(nrows=1, ncols=3, figsize=(20, 6))\n\nmodels = {\n    \"Without sampler\": clf,\n    \"ADASYN sampler\": make_pipeline(ADASYN(random_state=0), clf),\n    \"SMOTE sampler\": make_pipeline(SMOTE(random_state=0), clf),\n}\n\nfor ax, (title, model) in zip(axs, models.items()):\n    model.fit(X, y)\n    plot_decision_function(X, y, model, ax=ax, title=title)\n\nfig.suptitle(f\"Decision function using a {clf.__class__.__name__}\")\nfig.tight_layout()\n\n\n\n\nDue to those sampling particularities, it can give rise to some specific issues as illustrated below.\n\nX, y = create_dataset(n_samples=5000, weights=(0.01, 0.05, 0.94), class_sep=0.8)\n\nsamplers = [SMOTE(random_state=0), ADASYN(random_state=0)]\n\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X, y, clf, ax[0], title=f\"Decision function with {sampler.__class__.__name__}\"\n    )\n    plot_resampling(X, y, sampler, ax[1])\n\nfig.suptitle(\"Particularities of over-sampling with SMOTE and ADASYN\")\nfig.tight_layout()\n\n\n\n\nSMOTE proposes several variants by identifying specific samples to consider during the resampling. The borderline version (:class:~imblearn.over_sampling.BorderlineSMOTE) will detect which point to select which are in the border between two classes. The SVM version (:class:~imblearn.over_sampling.SVMSMOTE) will use the support vectors found using an SVM algorithm to create new sample while the KMeans version (:class:~imblearn.over_sampling.KMeansSMOTE) will make a clustering before to generate samples in each cluster independently depending each cluster density.\n\nfrom sklearn.cluster import MiniBatchKMeans\n\nfrom imblearn.over_sampling import SVMSMOTE, BorderlineSMOTE, KMeansSMOTE\n\nX, y = create_dataset(n_samples=5000, weights=(0.01, 0.05, 0.94), class_sep=0.8)\n\nfig, axs = plt.subplots(5, 2, figsize=(15, 30))\n\nsamplers = [\n    SMOTE(random_state=0),\n    BorderlineSMOTE(random_state=0, kind=\"borderline-1\"),\n    BorderlineSMOTE(random_state=0, kind=\"borderline-2\"),\n    KMeansSMOTE(\n        kmeans_estimator=MiniBatchKMeans(n_init=1, random_state=0), random_state=0\n    ),\n    SVMSMOTE(random_state=0),\n]\n\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X, y, clf, ax[0], title=f\"Decision function for {sampler.__class__.__name__}\"\n    )\n    plot_resampling(X, y, sampler, ax[1])\n\nfig.suptitle(\"Decision function and resampling using SMOTE variants\")\nfig.tight_layout()\n\n\n\n\nWhen dealing with a mixed of continuous and categorical features, :class:~imblearn.over_sampling.SMOTENC is the only method which can handle this case.\n\nfrom collections import Counter\n\nfrom imblearn.over_sampling import SMOTENC\n\nrng = np.random.RandomState(42)\nn_samples = 50\n# Create a dataset of a mix of numerical and categorical data\nX = np.empty((n_samples, 3), dtype=object)\nX[:, 0] = rng.choice([\"A\", \"B\", \"C\"], size=n_samples).astype(object)\nX[:, 1] = rng.randn(n_samples)\nX[:, 2] = rng.randint(3, size=n_samples)\ny = np.array([0] * 20 + [1] * 30)\n\nprint(\"The original imbalanced dataset\")\nprint(sorted(Counter(y).items()))\nprint()\nprint(\"The first and last columns are containing categorical features:\")\nprint(X[:5])\nprint()\n\nsmote_nc = SMOTENC(categorical_features=[0, 2], random_state=0)\nX_resampled, y_resampled = smote_nc.fit_resample(X, y)\nprint(\"Dataset after resampling:\")\nprint(sorted(Counter(y_resampled).items()))\nprint()\nprint(\"SMOTE-NC will generate categories for the categorical features:\")\nprint(X_resampled[-5:])\nprint()\n\nThe original imbalanced dataset\n[(0, 20), (1, 30)]\n\nThe first and last columns are containing categorical features:\n[['C' -0.14021849735700803 2]\n ['A' -0.033193400066544886 2]\n ['C' -0.7490765234433554 1]\n ['C' -0.7783820070908942 2]\n ['A' 0.948842857719016 2]]\n\nDataset after resampling:\n[(0, 30), (1, 30)]\n\nSMOTE-NC will generate categories for the categorical features:\n[['A' 0.5246469549655818 2]\n ['B' -0.3657680728116921 2]\n ['B' 0.9344237230779993 2]\n ['B' 0.3710891618824609 2]\n ['B' 0.3327240726719727 2]]\n\n\n\nHowever, if the dataset is composed of only categorical features then one should use :class:~imblearn.over_sampling.SMOTEN.\n\nfrom imblearn.over_sampling import SMOTEN\n\n# Generate only categorical data\nX = np.array([\"A\"] * 10 + [\"B\"] * 20 + [\"C\"] * 30, dtype=object).reshape(-1, 1)\ny = np.array([0] * 20 + [1] * 40, dtype=np.int32)\n\nprint(f\"Original class counts: {Counter(y)}\")\nprint()\nprint(X[:5])\nprint()\n\nsampler = SMOTEN(random_state=0)\nX_res, y_res = sampler.fit_resample(X, y)\nprint(f\"Class counts after resampling {Counter(y_res)}\")\nprint()\nprint(X_res[-5:])\nprint()\n\nOriginal class counts: Counter({1: 40, 0: 20})\n\n[['A']\n ['A']\n ['A']\n ['A']\n ['A']]\n\nClass counts after resampling Counter({0: 40, 1: 40})\n\n[['B']\n ['B']\n ['A']\n ['B']\n ['A']]"
  },
  {
    "objectID": "posts/est/boostcourse/2. 데이터 분석 준비하기.html",
    "href": "posts/est/boostcourse/2. 데이터 분석 준비하기.html",
    "title": "3: 데이터 분석 준비하기",
    "section": "",
    "text": "Zen of Python\n\n파이썬의 철학이 잘 담겨있는 Zen of Python을 출력해 봅니다. (아래의 실습을 통해 확인해 보세요!)\n\n\nimport this # improt를 통해 파이썬의 라이브러리나 패키지를 가져올 수 있음\n\n\n\nboolean\n\n파이썬에서는 명시적인 것이 암시적인 것보다 낫다라는 철학이 있습니다.\nTrue나 False는 0과 1로도 표현할 수 있으나 명시적으로 표현하기 위해 True와 False를 사용합니다.\n\n\nTrue\n\nTrue\n\n\n\nFalse\n\nFalse\n\n\n\nTrue == 1\n\nTrue\n\n\n\nFalse == 0\n\nTrue\n\n\n\nTrue == \"1\" # True와 문자 1과는 다르다! 1따옴포=문자열\n\nFalse\n\n\n\nTrue != \"1\"\n\nTrue\n\n\n\nFalse == \"0\"\n\nFalse\n\n\n\nFalse != \"0\"\n\nTrue\n\n\n\nTrue and True\n\nTrue\n\n\n\nTrue and False\n\nFalse\n\n\n\nTrue or False #or연산자: 하나만 true여도 true\n\nTrue\n\n\n\n\nnumber and String\n\n숫자 1과 문자 “1”은 다르다! 데이터 타입 “type” 사용\n\n\n\"1\"\n\n'1'\n\n\n\ntype(1)\n\nint\n\n\n\ntype(\"1\")\n\nstr\n\n\n\n\nStrings and Lists\n\ntil = \"Today I learned\"\ntil\n\n'Today I learned'\n\n\n\ntil.lower() #다 소문자로 변경\n\n'today i learned'\n\n\n\ntil.upper() #대문자 변경\n\n'TODAY I LEARNED'\n\n\n\n# 비어있는 리스트 만들기. lang라는 변수에 담기\nlang = []\nlang\n\n[]\n\n\n\nlang.append(\"python\")\nlang.append(\"java\")\nlang.append(\"c\")\nlang\n\n['python', 'java', 'c']\n\n\n\nlang[0] #lnag이라는 변수에 담겨있는 언어명을 인덱싱을 통해 가져오기\n\n'python'\n\n\n\nlang[1]\n\n'java'\n\n\n\nlang[-1]\n\n'c'\n\n\n\n\nControl Flow\n\n제어문-조건문, 반복문\n\n\nfor i in lang:\n    print(i)\n\npython\njava\nc\n\n\n\nfor i in lang:\n    if i == \"python\":\n        print(\"python\")\n    else:\n            print(\"기타\") # indent를 맞춰줘야 한다.\n\npython\n기타\n기타\n\n\n\n# 특정 횟수만큼 반복문 실행\ncount = len(lang)\nfor i in range(count):\n    print(lang[i]) \n\npython\njava\nc\n\n\n\n# 짝수일때 python을 홀수일때 java출력\nfor i in range(1,10): # `1에서 9까지\n    if i % 2 == 0: #짝수만 출력\n        print(\"python\")\n    else:\n        print(\"java\")\n\njava\npython\njava\npython\njava\npython\njava\npython\njava\n\n\n\n# enumerate를 사용하면 인덱스 번호와 원소를 같이 가져온다\nfor i, val in enumerate(lang):\n    print(i,val)\n\n0 python\n1 java\n2 c\n\n\n\n문자열\n\naddress = \" 경기도 성남시 분당구 불정로 6 NAVER 그린팩토리 16층 \"\naddress\n\n' 경기도 성남시 분당구 불정로 6 NAVER 그린팩토리 16층 '\n\n\n\n# 앞뒤 공백 제거 \n# 데이터 전처리 시 주로 사용\naddress = address.strip() \naddress\n\n'경기도 성남시 분당구 불정로 6 NAVER 그린팩토리 16층'\n\n\n\n# 문자열 길이\nlen(address)\n\n33\n\n\n\n# 공백으로 문자열 분리\naddress_list = address.split()\naddress_list\n\n['경기도', '성남시', '분당구', '불정로', '6', 'NAVER', '그린팩토리', '16층']\n\n\n\n\n슬라이싱, startswith, in 을 통해 문자열에 경기아 있는지 확인하기\n\naddress[:2]\n\n'경기'\n\n\n\naddress.startswith(\"경기\")\n\nTrue\n\n\n\n\"경기\" in address \n\nTrue\n\n\n\n\n리스트\n\n문자열에서 쓰는 방법과 비슷한 메소드 등을 사용\n\n\naddress_list[2]\n\n'분당구'\n\n\n\nstreet = address_list[3]\nstreet\n\n'불정로'\n\n\n\naddress_list[-1]\n\n'16층'\n\n\n\n# \"\".join(리스트)를 사용하면 리스트를 공백 문자열로 연결 한다\n\"-\".join(address_list)\n\n'경기도-성남시-분당구-불정로-6-NAVER-그린팩토리-16층'\n\n\n\n\"경기\" in address_list\n\nFalse\n\n\n\n\"경기도\" in address_list\n\nTrue\n\n\n\n\"분당구\" in address_list\n\nTrue\n\n\n\n# pandas (패널 데이터 앤 시스템의 약자)\n# 수식과 그래프를 통해 계산 및 시각화하는 도구\n# 엑셀을 사용 했을때보다 대용량 데이터를 쓸수 있고 소스 코드 파일만 데이터 프레임 로드를 해서 소스코드들을 재사용 할 수 있다.\n# 월별 작업과 같은 반복작업.. \n# 아래에 첨부된 10 minutes to pandas를 한 번씩 실행해보시면 판다스의 전반적인 것을 익힐 수 있습니다.\n\n\n# 추가로 같이 첨부된 Pandas Cheat Sheet도 추천드립니다.\n\n\nimport pandas as pd\n\n\npd.DataFrame?\n\n\n# 공식문서 (도움말) 활용하기\npd.DataFrame()\n# shift+tab+tab\n\n\n\n\n\n  \n    \n      \n    \n  \n  \n  \n\n\n\n\n\n\nDataFrame\n\ndf = pd.DataFrame(\n{\"a\" : [4, 5, 6, 6],\n\"b\" : [7, 8, 9, 9],\n\"c\" : [10, 11, 12, 12]},\nindex = [1, 2, 3, 4])\ndf\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      1\n      4\n      7\n      10\n    \n    \n      2\n      5\n      8\n      11\n    \n    \n      3\n      6\n      9\n      12\n    \n    \n      4\n      6\n      9\n      12\n    \n  \n\n\n\n\n\n\nSeries\n\n# 1차원 자료구조.. (벡터!)\ndf[\"a\"]\n# 출력된 형태: series 데이터\n\n1    4\n2    5\n3    6\nName: a, dtype: int64\n\n\n\n# dataframe으로 변경 (2차원 자료구조) (행렬!)\ndf[[\"a\"]]\n\n\n\n\n\n  \n    \n      \n      a\n    \n  \n  \n    \n      1\n      4\n    \n    \n      2\n      5\n    \n    \n      3\n      6\n    \n  \n\n\n\n\n\n\nSubset\n\ndf[df[\"a\"] > 4]\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      2\n      5\n      8\n      11\n    \n    \n      3\n      6\n      9\n      12\n    \n  \n\n\n\n\n\ndf[\"a\"]\n\n1    4\n2    5\n3    6\nName: a, dtype: int64\n\n\n\ndf[[\"a\",\"b\"]]\n# df[\"a\",\"b\"] 이렇게 쓰면 오류가 난다. 대괄호를 또 해주기 \n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      1\n      4\n      7\n    \n    \n      2\n      5\n      8\n    \n    \n      3\n      6\n      9\n    \n  \n\n\n\n\n\n\nSummarize Data\n\ndf[\"a\"].value_counts()\n# 빈도수 계산\n\n6    2\n4    1\n5    1\nName: a, dtype: int64\n\n\n\nlen(df)\n\n4\n\n\n\n\nReahaping\n\nsort_values, drop\n\n\ndf.sort_values(\"a\",ascending=False)\n#ascending= 역수 \n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      3\n      6\n      9\n      12\n    \n    \n      4\n      6\n      9\n      12\n    \n    \n      2\n      5\n      8\n      11\n    \n    \n      1\n      4\n      7\n      10\n    \n  \n\n\n\n\n\ndf = df.drop([\"c\"], axis=1)\n# 기본 설정은 axis=0 으로 되어있으므로 axis를 바꿔줘야 함 \ndf\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      1\n      4\n      7\n    \n    \n      2\n      5\n      8\n    \n    \n      3\n      6\n      9\n    \n    \n      4\n      6\n      9\n    \n  \n\n\n\n\n\n\nGroup Data\n\nGroupby, pivot_table\n\n\n# \"a\" 컬럼값을 Groupby하여 \"b\"의 컬럼값 구하기\ndf.groupby([\"a\"])[\"b\"].mean()\n\na\n4    7.0\n5    8.0\n6    9.0\nName: b, dtype: float64\n\n\n\ndf.groupby([\"a\"])[\"b\"].agg([\"mean\", \"sum\", \"count\"])\n\n\n\n\n\n  \n    \n      \n      mean\n      sum\n      count\n    \n    \n      a\n      \n      \n      \n    \n  \n  \n    \n      4\n      7.0\n      7\n      1\n    \n    \n      5\n      8.0\n      8\n      1\n    \n    \n      6\n      9.0\n      18\n      2\n    \n  \n\n\n\n\n\ndf.groupby([\"a\"])[\"b\"].describe() #요약하는거\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      a\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      4\n      1.0\n      7.0\n      NaN\n      7.0\n      7.0\n      7.0\n      7.0\n      7.0\n    \n    \n      5\n      1.0\n      8.0\n      NaN\n      8.0\n      8.0\n      8.0\n      8.0\n      8.0\n    \n    \n      6\n      2.0\n      9.0\n      0.0\n      9.0\n      9.0\n      9.0\n      9.0\n      9.0\n    \n  \n\n\n\n\n\npd.pivot_table(df, index=\"a\", values=\"b\", aggfunc=\"sum\")\n\n\n\n\n\n  \n    \n      \n      b\n    \n    \n      a\n      \n    \n  \n  \n    \n      4\n      7\n    \n    \n      5\n      8\n    \n    \n      6\n      18\n    \n  \n\n\n\n\n\n\nPlotting\n\ndf.plot.bar()\n\n<AxesSubplot:>\n\n\n\n\n\n\n!conda env list\n\n# conda environments:\n#\nbase                     /home/koinup4/anaconda3\npy37                  *  /home/koinup4/anaconda3/envs/py37\npy39                     /home/koinup4/anaconda3/envs/py39"
  },
  {
    "objectID": "posts/est/boostcourse/0. jupyter basic.html",
    "href": "posts/est/boostcourse/0. jupyter basic.html",
    "title": "1: 주피터 노트북 사용법",
    "section": "",
    "text": "주피터 노트북 사용법\n\nShift+Enter 키를 누르면 셀이 실행되고 커서가 다음셀로 이동\nEnter 키 누르면 편집상태로 돌아온다.\nESC 키를 누르고\n\na 키를 누르면 위에 셀이 추가 된다.\nb 키를 누르면 아래에 셀이 추가 된다.\ndd 키를 누르면 셀이 삭제 된다.\nm 키를 누르면 문서 셀로 변경 된다.\ny 키를 누르면 코드 셀로 변경 된다.\n\n단축키는 h 버튼 눌러서 확인이 가능하다\n\n\n마크다운(Markdown)이란?\n\n코드와 함께 문서화를 할 수 있다.\n문서화를 할 수 있는 문법\n\n여러 줄의 설명을\n줄바꿈으로 쓰고자 할 때\n\nprint(\"Hello World\")\n\nHello World\n\n\n\na=1\nb=2\na+b\n\n3\n\n\n\nfor i in range(10000000):\n    print(i)"
  },
  {
    "objectID": "posts/est/boostcourse/3. 서울 종합병원 분포 확인하기.html",
    "href": "posts/est/boostcourse/3. 서울 종합병원 분포 확인하기.html",
    "title": "4: 서울 종합병원 분포 확인하기",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n\n\n\n\n# ctrl(cmd) + / : 주석처리를 풀었다 했다 \n\nimport matplotlib.pyplot as plt\n # window의 한글 폰트 설정\nplt.rc('font',family='Malgun Gothic') #윈도우의 경우\n\n# plt.rc('font', family='AppleGothic') #맥의 경우\n\nplt.rc('axes', unicode_minus=False) #마이너스 폰트 깨지는 것 대비\n\n# 그래프가 노트북 안에 보이게 하기 위해\n%matplotlib inline\n\n\nfrom IPython.display import set_matplotlib_formats\n#폰트가 선명하게 보이기 위해\n\n%config InlineBackend.figure_format = 'retina'\n\n\n\n\n\n!move \"C:\\Users\\user\\Downloads\\소상공인시장진흥공단_상가업소정보_의료기관_201909.csv\"\n\n지정된 파일을 찾을 수 없습니다.\n\n\n\ndf = pd.read_csv(\"data/소상공인시장진흥공단_상가업소정보_의료기관_201909.csv\", low_memory=False)\n# low_memory=False로 설정이 되어야 한다. 안그럼 오류남.\ndf.shape # 데이터의 행과 열 크기를 찍어볼 수 있따\n\n(91335, 39)\n\n\n\n\n\n\nhead, tail을 통해 데이터를 미리 볼수 있따\n\n\n# shift + tab 키를 누르면 docstring을 볼 수 있다\n# head: 데이터 미리보기\ndf.head(1)\n\n\n\n\n\n  \n    \n      \n      상가업소번호\n      상호명\n      지점명\n      상권업종대분류코드\n      상권업종대분류명\n      상권업종중분류코드\n      상권업종중분류명\n      상권업종소분류코드\n      상권업종소분류명\n      표준산업분류코드\n      ...\n      건물관리번호\n      건물명\n      도로명주소\n      구우편번호\n      신우편번호\n      동정보\n      층정보\n      호정보\n      경도\n      위도\n    \n  \n  \n    \n      0\n      19956873\n      하나산부인과\n      NaN\n      S\n      의료\n      S01\n      병원\n      S01B10\n      산부인과\n      Q86201\n      ...\n      4127310900110810000010857\n      산호한양아파트\n      경기도 안산시 단원구 달미로 10\n      425764.0\n      15236.0\n      NaN\n      NaN\n      NaN\n      126.814295\n      37.336344\n    \n  \n\n1 rows × 39 columns\n\n\n\n\n# tail: 마지막 데이터 불러오기\ndf.tail(1)\n\n\n\n\n\n  \n    \n      \n      상가업소번호\n      상호명\n      지점명\n      상권업종대분류코드\n      상권업종대분류명\n      상권업종중분류코드\n      상권업종중분류명\n      상권업종소분류코드\n      상권업종소분류명\n      표준산업분류코드\n      ...\n      건물관리번호\n      건물명\n      도로명주소\n      구우편번호\n      신우편번호\n      동정보\n      층정보\n      호정보\n      경도\n      위도\n    \n  \n  \n    \n      91334\n      16109073\n      천안김안과천안역본점의원\n      NaN\n      S\n      의료\n      S01\n      병원\n      S01B13\n      안과의원\n      Q86201\n      ...\n      4413110700102660017016314\n      김안과\n      충청남도 천안시 동남구 중앙로 92\n      330952.0\n      31127.0\n      NaN\n      NaN\n      NaN\n      127.152651\n      36.80664\n    \n  \n\n1 rows × 39 columns\n\n\n\n\n\n\n\n\n\n# 요약정보가 나타난다\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 91335 entries, 0 to 91334\nData columns (total 39 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   상가업소번호     91335 non-null  int64  \n 1   상호명        91335 non-null  object \n 2   지점명        1346 non-null   object \n 3   상권업종대분류코드  91335 non-null  object \n 4   상권업종대분류명   91335 non-null  object \n 5   상권업종중분류코드  91335 non-null  object \n 6   상권업종중분류명   91335 non-null  object \n 7   상권업종소분류코드  91335 non-null  object \n 8   상권업종소분류명   91335 non-null  object \n 9   표준산업분류코드   86413 non-null  object \n 10  표준산업분류명    86413 non-null  object \n 11  시도코드       90956 non-null  float64\n 12  시도명        90956 non-null  object \n 13  시군구코드      90956 non-null  float64\n 14  시군구명       90956 non-null  object \n 15  행정동코드      91335 non-null  int64  \n 16  행정동명       90956 non-null  object \n 17  법정동코드      91280 non-null  float64\n 18  법정동명       91280 non-null  object \n 19  지번코드       91335 non-null  int64  \n 20  대지구분코드     91335 non-null  int64  \n 21  대지구분명      91335 non-null  object \n 22  지번본번지      91335 non-null  int64  \n 23  지번부번지      72079 non-null  float64\n 24  지번주소       91335 non-null  object \n 25  도로명코드      91335 non-null  int64  \n 26  도로명        91335 non-null  object \n 27  건물본번지      91335 non-null  int64  \n 28  건물부번지      10604 non-null  float64\n 29  건물관리번호     91335 non-null  object \n 30  건물명        46453 non-null  object \n 31  도로명주소      91335 non-null  object \n 32  구우편번호      91323 non-null  float64\n 33  신우편번호      91333 non-null  float64\n 34  동정보        7406 non-null   object \n 35  층정보        44044 non-null  object \n 36  호정보        15551 non-null  object \n 37  경도         91335 non-null  float64\n 38  위도         91335 non-null  float64\ndtypes: float64(9), int64(7), object(23)\nmemory usage: 27.2+ MB\n\n\n\n# object : 문자열로 된 데이터 타입\n# int: 정수형\n# float : 실수형\n\n\n\n\n\n# 컬럼명만 추출해보기\ndf.columns\n\nIndex(['상가업소번호', '상호명', '지점명', '상권업종대분류코드', '상권업종대분류명', '상권업종중분류코드',\n       '상권업종중분류명', '상권업종소분류코드', '상권업종소분류명', '표준산업분류코드', '표준산업분류명', '시도코드',\n       '시도명', '시군구코드', '시군구명', '행정동코드', '행정동명', '법정동코드', '법정동명', '지번코드',\n       '대지구분코드', '대지구분명', '지번본번지', '지번부번지', '지번주소', '도로명코드', '도로명', '건물본번지',\n       '건물부번지', '건물관리번호', '건물명', '도로명주소', '구우편번호', '신우편번호', '동정보', '층정보',\n       '호정보', '경도', '위도'],\n      dtype='object')\n\n\n\n\n\n\n# 데이터 타입만 출력\ndf.dtypes\n\n상가업소번호         int64\n상호명           object\n지점명           object\n상권업종대분류코드     object\n상권업종대분류명      object\n상권업종중분류코드     object\n상권업종중분류명      object\n상권업종소분류코드     object\n상권업종소분류명      object\n표준산업분류코드      object\n표준산업분류명       object\n시도코드         float64\n시도명           object\n시군구코드        float64\n시군구명          object\n행정동코드          int64\n행정동명          object\n법정동코드        float64\n법정동명          object\n지번코드           int64\n대지구분코드         int64\n대지구분명         object\n지번본번지          int64\n지번부번지        float64\n지번주소          object\n도로명코드          int64\n도로명           object\n건물본번지          int64\n건물부번지        float64\n건물관리번호        object\n건물명           object\n도로명주소         object\n구우편번호        float64\n신우편번호        float64\n동정보           object\n층정보           object\n호정보           object\n경도           float64\n위도           float64\ndtype: object\n\n\n\n\n\n\n\ndf.isnull()\n# true로 표시되는 값은 null 값! \n\n\n\n\n\n  \n    \n      \n      상가업소번호\n      상호명\n      지점명\n      상권업종대분류코드\n      상권업종대분류명\n      상권업종중분류코드\n      상권업종중분류명\n      상권업종소분류코드\n      상권업종소분류명\n      표준산업분류코드\n      ...\n      건물관리번호\n      건물명\n      도로명주소\n      구우편번호\n      신우편번호\n      동정보\n      층정보\n      호정보\n      경도\n      위도\n    \n  \n  \n    \n      0\n      False\n      False\n      True\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      False\n      False\n      False\n      False\n      True\n      True\n      True\n      False\n      False\n    \n    \n      1\n      False\n      False\n      True\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      True\n      False\n      False\n      False\n      True\n      False\n      True\n      False\n      False\n    \n    \n      2\n      False\n      False\n      True\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      False\n      False\n      False\n      False\n      True\n      True\n      True\n      False\n      False\n    \n    \n      3\n      False\n      False\n      True\n      False\n      False\n      False\n      False\n      False\n      False\n      True\n      ...\n      False\n      True\n      False\n      False\n      False\n      True\n      False\n      True\n      False\n      False\n    \n    \n      4\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      True\n      False\n      False\n      False\n      True\n      False\n      True\n      False\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      91330\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      True\n      False\n      False\n      False\n      True\n      True\n      True\n      False\n      False\n    \n    \n      91331\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      False\n      False\n      False\n      False\n      True\n      True\n      True\n      False\n      False\n    \n    \n      91332\n      False\n      False\n      True\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      False\n      False\n      False\n      False\n      True\n      False\n      True\n      False\n      False\n    \n    \n      91333\n      False\n      False\n      True\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      True\n      False\n      False\n      False\n      True\n      True\n      True\n      False\n      False\n    \n    \n      91334\n      False\n      False\n      True\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      False\n      False\n      False\n      False\n      True\n      True\n      True\n      False\n      False\n    \n  \n\n91335 rows × 39 columns\n\n\n\n\nnull_count = df.isnull().sum()\nnull_count\n\n상가업소번호           0\n상호명              0\n지점명          89989\n상권업종대분류코드        0\n상권업종대분류명         0\n상권업종중분류코드        0\n상권업종중분류명         0\n상권업종소분류코드        0\n상권업종소분류명         0\n표준산업분류코드      4922\n표준산업분류명       4922\n시도코드           379\n시도명            379\n시군구코드          379\n시군구명           379\n행정동코드            0\n행정동명           379\n법정동코드           55\n법정동명            55\n지번코드             0\n대지구분코드           0\n대지구분명            0\n지번본번지            0\n지번부번지        19256\n지번주소             0\n도로명코드            0\n도로명              0\n건물본번지            0\n건물부번지        80731\n건물관리번호           0\n건물명          44882\n도로명주소            0\n구우편번호           12\n신우편번호            2\n동정보          83929\n층정보          47291\n호정보          75784\n경도               0\n위도               0\ndtype: int64\n\n\n\n# 결측치를 막대그래프로 표현\nnull_count.plot() # 선그래프로 표시되는데.. 적합하지 않은 거 같아!\n\n<AxesSubplot:>\n\n\n\n\n\n\nnull_count.plot.bar(rot=60)\n# rot = 글자 돌려보는것\n\n<AxesSubplot:>\n\n\n\n\n\n\nnull_count.plot.barh()\n\n<AxesSubplot:>\n\n\n\n\n\n\nnull_count.plot.barh(figsize=(5,7)) # 그래프 사이즈 지정\n\n<AxesSubplot:>\n\n\n\n\n\n\n# 위에서 계산한 결측치 수를 reset_index 통해 데이터 프레임으로 만들기\n# df_null_coount 변수에 결과를 담아 head로 미리보기 해보기\n\ndf_null_count = null_count.reset_index()\ndf_null_count.head()\n\n\n\n\n\n  \n    \n      \n      index\n      0\n    \n  \n  \n    \n      0\n      상가업소번호\n      0\n    \n    \n      1\n      상호명\n      0\n    \n    \n      2\n      지점명\n      89989\n    \n    \n      3\n      상권업종대분류코드\n      0\n    \n    \n      4\n      상권업종대분류명\n      0\n    \n  \n\n\n\n\n\n\n\n\n# 변수에 담겨있는 컬럼이름 변경\n\ndf_null_count.columns = [\"컬럼명\", \"결측치수\"]\ndf_null_count.head()\n\n\n\n\n\n  \n    \n      \n      컬럼명\n      결측치수\n    \n  \n  \n    \n      0\n      상가업소번호\n      0\n    \n    \n      1\n      상호명\n      0\n    \n    \n      2\n      지점명\n      89989\n    \n    \n      3\n      상권업종대분류코드\n      0\n    \n    \n      4\n      상권업종대분류명\n      0\n    \n  \n\n\n\n\n\n\n\n\n# sort_values 통해 정렬\n# 결측치가 많은 순으로 상위 10개 출력\n\ndf_null_count_top = df_null_count.sort_values(by=\"결측치수\", ascending=False).head(10)\ndf_null_count_top \n\n\n\n\n\n  \n    \n      \n      컬럼명\n      결측치수\n    \n  \n  \n    \n      2\n      지점명\n      89989\n    \n    \n      34\n      동정보\n      83929\n    \n    \n      28\n      건물부번지\n      80731\n    \n    \n      36\n      호정보\n      75784\n    \n    \n      35\n      층정보\n      47291\n    \n    \n      30\n      건물명\n      44882\n    \n    \n      23\n      지번부번지\n      19256\n    \n    \n      9\n      표준산업분류코드\n      4922\n    \n    \n      10\n      표준산업분류명\n      4922\n    \n    \n      11\n      시도코드\n      379\n    \n  \n\n\n\n\n\n\n\n\n# 지점명 컬럼 불러오기\n# NaN == Not a Number의 약자로 결측치를 의미한다.\n\ndf[\"지점명\"].head()\n\n0    NaN\n1    NaN\n2    NaN\n3    NaN\n4    수지점\nName: 지점명, dtype: object\n\n\n\n# \"컬럼명\" 이라는 컬럼 값만 가져와서 drop_columns 라는 변수에 담기\n\ndrop_columns = df_null_count_top[\"컬럼명\"].tolist()    # tolist : list로 만들어줌\ndrop_columns \n\n['지점명',\n '동정보',\n '건물부번지',\n '호정보',\n '층정보',\n '건물명',\n '지번부번지',\n '표준산업분류코드',\n '표준산업분류명',\n '시도코드']\n\n\n\n# drop_columns 변수로 해당 컬럼 정보만 데이터프레임에서 가져오기\ndf[drop_columns].head()\n\n\n\n\n\n  \n    \n      \n      지점명\n      동정보\n      건물부번지\n      호정보\n      층정보\n      건물명\n      지번부번지\n      표준산업분류코드\n      표준산업분류명\n      시도코드\n    \n  \n  \n    \n      0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      산호한양아파트\n      NaN\n      Q86201\n      일반 의원\n      41.0\n    \n    \n      1\n      NaN\n      NaN\n      NaN\n      NaN\n      4\n      NaN\n      14.0\n      Q86201\n      일반 의원\n      11.0\n    \n    \n      2\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      한라프라자\n      1.0\n      Q86201\n      일반 의원\n      41.0\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n      NaN\n      5\n      NaN\n      1.0\n      NaN\n      NaN\n      26.0\n    \n    \n      4\n      수지점\n      NaN\n      NaN\n      NaN\n      1\n      NaN\n      2.0\n      G47811\n      의약품 및 의료용품 소매업\n      41.0\n    \n  \n\n\n\n\n\n\n\n\nprint(df.shape)\n\ndf = df.drop(drop_columns, axis=1) \n# axis=1 컬럼기준으로 drop axis > 행(0), 열 (1)\n\nprint(df.shape)\n\n(91335, 39)\n(91335, 29)\n\n\n\n# 제거 결과 info로 확인\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 91335 entries, 0 to 91334\nData columns (total 29 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   상가업소번호     91335 non-null  int64  \n 1   상호명        91335 non-null  object \n 2   상권업종대분류코드  91335 non-null  object \n 3   상권업종대분류명   91335 non-null  object \n 4   상권업종중분류코드  91335 non-null  object \n 5   상권업종중분류명   91335 non-null  object \n 6   상권업종소분류코드  91335 non-null  object \n 7   상권업종소분류명   91335 non-null  object \n 8   시도명        90956 non-null  object \n 9   시군구코드      90956 non-null  float64\n 10  시군구명       90956 non-null  object \n 11  행정동코드      91335 non-null  int64  \n 12  행정동명       90956 non-null  object \n 13  법정동코드      91280 non-null  float64\n 14  법정동명       91280 non-null  object \n 15  지번코드       91335 non-null  int64  \n 16  대지구분코드     91335 non-null  int64  \n 17  대지구분명      91335 non-null  object \n 18  지번본번지      91335 non-null  int64  \n 19  지번주소       91335 non-null  object \n 20  도로명코드      91335 non-null  int64  \n 21  도로명        91335 non-null  object \n 22  건물본번지      91335 non-null  int64  \n 23  건물관리번호     91335 non-null  object \n 24  도로명주소      91335 non-null  object \n 25  구우편번호      91323 non-null  float64\n 26  신우편번호      91333 non-null  float64\n 27  경도         91335 non-null  float64\n 28  위도         91335 non-null  float64\ndtypes: float64(6), int64(7), object(16)\nmemory usage: 20.2+ MB\n\n\n\n\n\n\n\n\n\ndf.dtypes\n\n상가업소번호         int64\n상호명           object\n상권업종대분류코드     object\n상권업종대분류명      object\n상권업종중분류코드     object\n상권업종중분류명      object\n상권업종소분류코드     object\n상권업종소분류명      object\n시도명           object\n시군구코드        float64\n시군구명          object\n행정동코드          int64\n행정동명          object\n법정동코드        float64\n법정동명          object\n지번코드           int64\n대지구분코드         int64\n대지구분명         object\n지번본번지          int64\n지번주소          object\n도로명코드          int64\n도로명           object\n건물본번지          int64\n건물관리번호        object\n도로명주소         object\n구우편번호        float64\n신우편번호        float64\n경도           float64\n위도           float64\ndtype: object\n\n\n\n#평균값\ndf[\"위도\"].mean()\n\n36.62471119236673\n\n\n\n# 중앙값\ndf[\"위도\"].median()\n\n37.2346523177033\n\n\n\n# 최댓값\ndf[\"위도\"].max()\n\n38.4996585705598\n\n\n\n# 최솟값\ndf[\"위도\"].min()\n\n33.2192896688307\n\n\n\n# 갯수\ndf[\"위도\"].count()\n\n91335\n\n\n\n\n\n데이터를 요약해서 볼 수 있음\n\n# 위도를 descibe로 요약\n\ndf[\"위도\"].describe()\n\ncount    91335.000000\nmean        36.624711\nstd          1.041361\nmin         33.219290\n25%         35.811830\n50%         37.234652\n75%         37.507463\nmax         38.499659\nName: 위도, dtype: float64\n\n\n\n# 2개의 컬럼을 describe로 요약\ndf[\"위도\", \"경도\"] \n\n# pandas에서는 리스트 형태로 가져와야한다. 위와 같이 하면 오류남\n\nKeyError: ('위도', '경도')\n\n\n\ndf[[\"위도\", \"경도\"]]\n\n\n\n\n\n  \n    \n      \n      위도\n      경도\n    \n  \n  \n    \n      0\n      37.336344\n      126.814295\n    \n    \n      1\n      37.488742\n      127.053198\n    \n    \n      2\n      37.344955\n      126.734841\n    \n    \n      3\n      35.166872\n      129.115438\n    \n    \n      4\n      37.323528\n      127.095522\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      91330\n      36.352728\n      127.389865\n    \n    \n      91331\n      37.627530\n      126.830144\n    \n    \n      91332\n      35.227138\n      129.082790\n    \n    \n      91333\n      37.540993\n      127.143958\n    \n    \n      91334\n      36.806640\n      127.152651\n    \n  \n\n91335 rows × 2 columns\n\n\n\n\ndf[[\"위도\", \"경도\"]].describe()\n\n\n\n\n\n  \n    \n      \n      위도\n      경도\n    \n  \n  \n    \n      count\n      91335.000000\n      91335.000000\n    \n    \n      mean\n      36.624711\n      127.487524\n    \n    \n      std\n      1.041361\n      0.842877\n    \n    \n      min\n      33.219290\n      124.717632\n    \n    \n      25%\n      35.811830\n      126.914297\n    \n    \n      50%\n      37.234652\n      127.084550\n    \n    \n      75%\n      37.507463\n      128.108919\n    \n    \n      max\n      38.499659\n      130.909912\n    \n  \n\n\n\n\n\n# describe로 문자열 데이터타입 요약\n\ndf.describe() # 기본값은 수치형으로 되어있음!\n\n\n\n\n\n  \n    \n      \n      상가업소번호\n      시군구코드\n      행정동코드\n      법정동코드\n      지번코드\n      대지구분코드\n      지번본번지\n      도로명코드\n      건물본번지\n      구우편번호\n      신우편번호\n      경도\n      위도\n    \n  \n  \n    \n      count\n      9.133500e+04\n      90956.000000\n      9.133500e+04\n      9.128000e+04\n      9.133500e+04\n      91335.000000\n      91335.000000\n      9.133500e+04\n      91335.000000\n      91323.000000\n      91333.00000\n      91335.000000\n      91335.000000\n    \n    \n      mean\n      2.121818e+07\n      32898.381877\n      3.293232e+09\n      3.293385e+09\n      3.293191e+18\n      1.001336\n      587.534549\n      3.293207e+11\n      251.200482\n      428432.911085\n      28085.47698\n      127.487524\n      36.624711\n    \n    \n      std\n      5.042828e+06\n      12985.393171\n      1.297387e+09\n      1.297706e+09\n      1.297393e+18\n      0.036524\n      582.519364\n      1.297391e+11\n      477.456487\n      193292.339066\n      18909.01455\n      0.842877\n      1.041361\n    \n    \n      min\n      2.901108e+06\n      11110.000000\n      1.111052e+09\n      1.111010e+09\n      1.111010e+18\n      1.000000\n      1.000000\n      1.111020e+11\n      0.000000\n      100011.000000\n      1000.00000\n      124.717632\n      33.219290\n    \n    \n      25%\n      2.001931e+07\n      26350.000000\n      2.635065e+09\n      2.635011e+09\n      2.635011e+18\n      1.000000\n      162.000000\n      2.635042e+11\n      29.000000\n      302120.000000\n      11681.00000\n      126.914297\n      35.811830\n    \n    \n      50%\n      2.211900e+07\n      41117.000000\n      4.111758e+09\n      4.111710e+09\n      4.111711e+18\n      1.000000\n      462.000000\n      4.111743e+11\n      92.000000\n      440300.000000\n      24353.00000\n      127.084550\n      37.234652\n    \n    \n      75%\n      2.480984e+07\n      43113.000000\n      4.311370e+09\n      4.311311e+09\n      4.311311e+18\n      1.000000\n      858.000000\n      4.311332e+11\n      257.000000\n      602811.000000\n      46044.00000\n      128.108919\n      37.507463\n    \n    \n      max\n      2.852470e+07\n      50130.000000\n      5.013061e+09\n      5.013032e+09\n      5.013061e+18\n      2.000000\n      7338.000000\n      5.013049e+11\n      8795.000000\n      799801.000000\n      63643.00000\n      130.909912\n      38.499659\n    \n  \n\n\n\n\n\ndf.describe(include=\"object\")\n\n# top : 가장 많이 나타난걸 보여줌\n# freq : frequency : 빈도수.. 리원이라는 상호명이 152번 등장한다.\n# 결측치는 제외하고 보여줌! \n\n\n\n\n\n  \n    \n      \n      상호명\n      상권업종대분류코드\n      상권업종대분류명\n      상권업종중분류코드\n      상권업종중분류명\n      상권업종소분류코드\n      상권업종소분류명\n      시도명\n      시군구명\n      행정동명\n      법정동명\n      대지구분명\n      지번주소\n      도로명\n      건물관리번호\n      도로명주소\n    \n  \n  \n    \n      count\n      91335\n      91335\n      91335\n      91335\n      91335\n      91335\n      91335\n      90956\n      90956\n      90956\n      91280\n      91335\n      91335\n      91335\n      91335\n      91335\n    \n    \n      unique\n      56910\n      1\n      1\n      5\n      5\n      34\n      34\n      17\n      228\n      2791\n      2822\n      2\n      53118\n      16610\n      54142\n      54031\n    \n    \n      top\n      리원\n      S\n      의료\n      S01\n      병원\n      S02A01\n      약국\n      경기도\n      서구\n      중앙동\n      중동\n      대지\n      서울특별시 동대문구 제기동 965-1\n      서울특별시 강남구 강남대로\n      1123010300109650001031604\n      서울특별시 동대문구 약령중앙로8길 10\n    \n    \n      freq\n      152\n      91335\n      91335\n      60774\n      60774\n      18964\n      18964\n      21374\n      3165\n      1856\n      874\n      91213\n      198\n      326\n      198\n      198\n    \n  \n\n\n\n\n\n# 모든 데이터 요약\ndf.describe(include=\"all\")\n\n\n\n\n\n  \n    \n      \n      상가업소번호\n      상호명\n      상권업종대분류코드\n      상권업종대분류명\n      상권업종중분류코드\n      상권업종중분류명\n      상권업종소분류코드\n      상권업종소분류명\n      시도명\n      시군구코드\n      ...\n      지번주소\n      도로명코드\n      도로명\n      건물본번지\n      건물관리번호\n      도로명주소\n      구우편번호\n      신우편번호\n      경도\n      위도\n    \n  \n  \n    \n      count\n      9.133500e+04\n      91335\n      91335\n      91335\n      91335\n      91335\n      91335\n      91335\n      90956\n      90956.000000\n      ...\n      91335\n      9.133500e+04\n      91335\n      91335.000000\n      91335\n      91335\n      91323.000000\n      91333.00000\n      91335.000000\n      91335.000000\n    \n    \n      unique\n      NaN\n      56910\n      1\n      1\n      5\n      5\n      34\n      34\n      17\n      NaN\n      ...\n      53118\n      NaN\n      16610\n      NaN\n      54142\n      54031\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      top\n      NaN\n      리원\n      S\n      의료\n      S01\n      병원\n      S02A01\n      약국\n      경기도\n      NaN\n      ...\n      서울특별시 동대문구 제기동 965-1\n      NaN\n      서울특별시 강남구 강남대로\n      NaN\n      1123010300109650001031604\n      서울특별시 동대문구 약령중앙로8길 10\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      freq\n      NaN\n      152\n      91335\n      91335\n      60774\n      60774\n      18964\n      18964\n      21374\n      NaN\n      ...\n      198\n      NaN\n      326\n      NaN\n      198\n      198\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      mean\n      2.121818e+07\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      32898.381877\n      ...\n      NaN\n      3.293207e+11\n      NaN\n      251.200482\n      NaN\n      NaN\n      428432.911085\n      28085.47698\n      127.487524\n      36.624711\n    \n    \n      std\n      5.042828e+06\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      12985.393171\n      ...\n      NaN\n      1.297391e+11\n      NaN\n      477.456487\n      NaN\n      NaN\n      193292.339066\n      18909.01455\n      0.842877\n      1.041361\n    \n    \n      min\n      2.901108e+06\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      11110.000000\n      ...\n      NaN\n      1.111020e+11\n      NaN\n      0.000000\n      NaN\n      NaN\n      100011.000000\n      1000.00000\n      124.717632\n      33.219290\n    \n    \n      25%\n      2.001931e+07\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      26350.000000\n      ...\n      NaN\n      2.635042e+11\n      NaN\n      29.000000\n      NaN\n      NaN\n      302120.000000\n      11681.00000\n      126.914297\n      35.811830\n    \n    \n      50%\n      2.211900e+07\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      41117.000000\n      ...\n      NaN\n      4.111743e+11\n      NaN\n      92.000000\n      NaN\n      NaN\n      440300.000000\n      24353.00000\n      127.084550\n      37.234652\n    \n    \n      75%\n      2.480984e+07\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      43113.000000\n      ...\n      NaN\n      4.311332e+11\n      NaN\n      257.000000\n      NaN\n      NaN\n      602811.000000\n      46044.00000\n      128.108919\n      37.507463\n    \n    \n      max\n      2.852470e+07\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      50130.000000\n      ...\n      NaN\n      5.013049e+11\n      NaN\n      8795.000000\n      NaN\n      NaN\n      799801.000000\n      63643.00000\n      130.909912\n      38.499659\n    \n  \n\n11 rows × 29 columns\n\n\n\n\n\n\n\nunique로 중복 제거 nuique 갯수 세기\n\n\n# 상권업종대분류명\n\ndf[\"상권업종대분류명\"].unique()\n\narray(['의료'], dtype=object)\n\n\n\ndf[\"상권업종대분류명\"].nunique()\n\n1\n\n\n\n# 상권업종중분류명\ndf[\"상권업종중분류명\"].unique()\n\narray(['병원', '약국/한약방', '수의업', '유사의료업', '의료관련서비스업'], dtype=object)\n\n\n\ndf[\"상권업종중분류명\"].nunique()\n\n5\n\n\n\n# 상권업종소분류명\ndf[\"상권업종소분류명\"].unique()\n\narray(['산부인과', '내과/외과', '신경외과', '기타병원', '약국', '동물병원', '한약방', '탕제원',\n       '정형/성형외과', '소아과', '이비인후과의원', '노인/치매병원', '언어치료', '수의업-종합', '한의원',\n       '치과의원', '침구원', '일반병원', '안과의원', '조산원', '한방병원', '종합병원', '유사의료업기타',\n       '응급구조대', '혈액원', '치과병원', '척추교정치료', '피부과', '비뇨기과', '치과기공소', '산후조리원',\n       '접골원', '수의업-기타', '제대혈'], dtype=object)\n\n\n\ndf[\"상권업종소분류명\"].nunique()\n\n34\n\n\n\nlen(df[\"상권업종소분류명\"].unique())\n\n34\n\n\n\n\n\n\n카테고리 형태의 데이터 갯수를 셀 수 있다.\n\n\n# 시도코드 세어보기 -> 결측치...\ndf[\"시도명\"].head()\n\n0      경기도\n1    서울특별시\n2      경기도\n3    부산광역시\n4      경기도\nName: 시도명, dtype: object\n\n\n\n# 시도명 세보면\ncity = df[\"시도명\"].value_counts()\ncity\n\n경기도        21374\n서울특별시      18943\n부산광역시       6473\n경상남도        4973\n인천광역시       4722\n대구광역시       4597\n경상북도        4141\n전라북도        3894\n충청남도        3578\n전라남도        3224\n광주광역시       3214\n대전광역시       3067\n충청북도        2677\n강원도         2634\n울산광역시       1997\n제주특별자치도     1095\n세종특별자치시      353\nName: 시도명, dtype: int64\n\n\n\n# normalize=True 옵션 사용시 비율을 구할 수 있다.\n\ncity_normalize= df[\"시도명\"].value_counts(normalize=True)\ncity_normalize\n\n경기도        0.234993\n서울특별시      0.208266\n부산광역시      0.071166\n경상남도       0.054675\n인천광역시      0.051915\n대구광역시      0.050541\n경상북도       0.045528\n전라북도       0.042812\n충청남도       0.039338\n전라남도       0.035446\n광주광역시      0.035336\n대전광역시      0.033720\n충청북도       0.029432\n강원도        0.028959\n울산광역시      0.021956\n제주특별자치도    0.012039\n세종특별자치시    0.003881\nName: 시도명, dtype: float64\n\n\n\ncity.plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\n # 막대그래프 표현\ncity.plot.barh()\n\n<AxesSubplot:>\n\n\n\n\n\n\n# plto.pie() 사용하여 파이그래프 그리기\ncity_normalize.plot.pie(figsize=(7,7))\n\n<AxesSubplot:ylabel='시도명'>\n\n\n\n\n\n\n# seaborn의 countplot 그리기\nsns.countplot(data=df, y=\"시도명\" )\n\n<AxesSubplot:xlabel='count', ylabel='시도명'>\n\n\n\n\n\n\nc=sns.countplot(data=df, y=\"시도명\" ) \n# 변수명에 담아주면 밑에 글씨가 없어진당 \n\n\n\n\n\ndf[\"상권업종대분류명\"].value_counts()\n\n의료    91335\nName: 상권업종대분류명, dtype: int64\n\n\n\nd= df[\"상권업종중분류명\"].value_counts()\nd\n\n병원          60774\n약국/한약방      20923\n수의업          5323\n유사의료업        3774\n의료관련서비스업      541\nName: 상권업종중분류명, dtype: int64\n\n\n\nn =df[\"상권업종중분류명\"].value_counts(normalize=True)\nn\n\n병원          0.665397\n약국/한약방      0.229080\n수의업         0.058280\n유사의료업       0.041320\n의료관련서비스업    0.005923\nName: 상권업종중분류명, dtype: float64\n\n\n\nd.plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\nd.plot.bar()\n\n<AxesSubplot:>\n\n\n\n\n\n\nd.plot.bar(rot=0) # x축 레이블 값 회전\n\n<AxesSubplot:>\n\n\n\n\n\n\nn.plot.pie()\n\n<AxesSubplot:ylabel='상권업종중분류명'>\n\n\n\n\n\n\nc = df[\"상권업종소분류명\"].value_counts()\nc\n\n약국         18964\n치과의원       13731\n한의원        13211\n내과/외과      11374\n기타병원        4922\n일반병원        3385\n동물병원        3098\n정형/성형외과     2562\n소아과         2472\n수의업-종합      2216\n치과기공소       1724\n이비인후과의원     1486\n한약방         1442\n피부과         1273\n산부인과        1116\n노인/치매병원     1055\n안과의원        1042\n비뇨기과         809\n종합병원         762\n치과병원         756\n언어치료         664\n유사의료업기타      629\n탕제원          517\n산후조리원        511\n신경외과         421\n한방병원         397\n척추교정치료       338\n침구원          154\n혈액원          130\n응급구조대        125\n조산원           30\n접골원            9\n수의업-기타         9\n제대혈            1\nName: 상권업종소분류명, dtype: int64\n\n\n\nc.plot.barh(figsize=(7, 8), grid= True) #gird: 격자 표시\n\n<AxesSubplot:>\n\n\n\n\n\n\n\n\n\n\n# 상권업종분류명이 약국/한약방인 데이터만 가져와서\n# df_medical이라는 변수에 담고\n# head()통해 미리보기\n\ndf_medical = df[df[\"상권업종중분류명\"] == \"약국/한약방\"].copy()\ndf_medical.head(1)\n\n\n\n\n\n  \n    \n      \n      상가업소번호\n      상호명\n      상권업종대분류코드\n      상권업종대분류명\n      상권업종중분류코드\n      상권업종중분류명\n      상권업종소분류코드\n      상권업종소분류명\n      시도명\n      시군구코드\n      ...\n      지번주소\n      도로명코드\n      도로명\n      건물본번지\n      건물관리번호\n      도로명주소\n      구우편번호\n      신우편번호\n      경도\n      위도\n    \n  \n  \n    \n      4\n      20364049\n      더블유스토어수지점\n      S\n      의료\n      S02\n      약국/한약방\n      S02A01\n      약국\n      경기도\n      41465.0\n      ...\n      경기도 용인시 수지구 풍덕천동 712-2\n      414653205024\n      경기도 용인시 수지구 문정로\n      32\n      4146510100107120002026238\n      경기도 용인시 수지구 문정로 32\n      448170.0\n      16837.0\n      127.095522\n      37.323528\n    \n  \n\n1 rows × 29 columns\n\n\n\n\n# 상권업종대분류명이 의료만 가져오기\n# df.loc 사용하면 행, 열을 함께 가져온다.\n# 이 기능을 통해 상권업종중뷴려망만 가져온다\n# 가져온 결과를 value_counts를 통해 중분류의 갯수를 세본다.\n\ndf.loc[df[\"상권업종대분류명\"] == \"의료\", \"상권업종중분류명\"].value_counts()\n\n병원          60774\n약국/한약방      20923\n수의업          5323\n유사의료업        3774\n의료관련서비스업      541\nName: 상권업종중분류명, dtype: int64\n\n\n\n# 위와 같은 기능을 수행하는 코드\n# df.loc[df[\"상권업종대분류명\"] == \"의료\"][\"상권업종중분류명\"] 근데 이건 느리다!! \n\n\nm = df[\"상권업종대분류명\"] == \"의료\"\ndf.loc[m, \"상권업종중분류명\"].value_counts()\n\n병원          60774\n약국/한약방      20923\n수의업          5323\n유사의료업        3774\n의료관련서비스업      541\nName: 상권업종중분류명, dtype: int64\n\n\n\n# 유사의료업\ndf[df[\"상권업종중분류명\"] == \"유사의료업\"]\n\n\n\n\n\n  \n    \n      \n      상가업소번호\n      상호명\n      상권업종대분류코드\n      상권업종대분류명\n      상권업종중분류코드\n      상권업종중분류명\n      상권업종소분류코드\n      상권업종소분류명\n      시도명\n      시군구코드\n      ...\n      지번주소\n      도로명코드\n      도로명\n      건물본번지\n      건물관리번호\n      도로명주소\n      구우편번호\n      신우편번호\n      경도\n      위도\n    \n  \n  \n    \n      22\n      21013731\n      세종언어치료센터\n      S\n      의료\n      S03\n      유사의료업\n      S03B07\n      언어치료\n      부산광역시\n      26410.0\n      ...\n      부산광역시 금정구 구서동 84-1\n      264102000010\n      부산광역시 금정구 중앙대로\n      1817\n      2641010700100840001017686\n      부산광역시 금정구 중앙대로 1817-11\n      609310.0\n      46273.0\n      129.091662\n      35.246528\n    \n    \n      40\n      20933900\n      고려수지침학회\n      S\n      의료\n      S03\n      유사의료업\n      S03B03\n      침구원\n      경상남도\n      48123.0\n      ...\n      경상남도 창원시 성산구 상남동 5-2\n      481234784088\n      경상남도 창원시 성산구 마디미로4번길\n      9\n      4812312700100050002026799\n      경상남도 창원시 성산구 마디미로4번길 9\n      642832.0\n      51495.0\n      128.684678\n      35.224113\n    \n    \n      97\n      21717820\n      청명원\n      S\n      의료\n      S03\n      유사의료업\n      S03B09\n      유사의료업기타\n      충청북도\n      43760.0\n      ...\n      충청북도 괴산군 청안면 금신리 241\n      437604538132\n      충청북도 괴산군 청안면 금신로1길\n      93\n      4376037022102410000007293\n      충청북도 괴산군 청안면 금신로1길 93\n      367831.0\n      28050.0\n      127.635740\n      36.768935\n    \n    \n      102\n      21865854\n      응급환자이송센터\n      S\n      의료\n      S03\n      유사의료업\n      S03B01\n      응급구조대\n      대전광역시\n      30140.0\n      ...\n      대전광역시 중구 대사동 248-237\n      301404295026\n      대전광역시 중구 계룡로921번길\n      40\n      3014011000102480237013097\n      대전광역시 중구 계룡로921번길 40\n      301846.0\n      34946.0\n      127.417693\n      36.321801\n    \n    \n      108\n      21914637\n      태화아동발달지원센터\n      S\n      의료\n      S03\n      유사의료업\n      S03B07\n      언어치료\n      대전광역시\n      30140.0\n      ...\n      대전광역시 중구 문화동 27\n      301404295402\n      대전광역시 중구 보문산로333번길\n      29\n      3014011600100270000008172\n      대전광역시 중구 보문산로333번길 29\n      301130.0\n      35020.0\n      127.412725\n      36.312953\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      91300\n      16131218\n      으뜸치과기공소\n      S\n      의료\n      S03\n      유사의료업\n      S03B06\n      치과기공소\n      경상남도\n      48170.0\n      ...\n      경상남도 진주시 수정동 39-11\n      481704797625\n      경상남도 진주시 향교로18번길\n      8\n      4817011600100390011004490\n      경상남도 진주시 향교로18번길 8\n      660180.0\n      52753.0\n      128.084600\n      35.197029\n    \n    \n      91310\n      16199325\n      보령치과기공소\n      S\n      의료\n      S03\n      유사의료업\n      S03B06\n      치과기공소\n      서울특별시\n      11290.0\n      ...\n      서울특별시 성북구 동소문동4가 103-11\n      112903107003\n      서울특별시 성북구 동소문로\n      47\n      1129010700101030014050661\n      서울특별시 성북구 동소문로 47-15\n      136821.0\n      2832.0\n      127.010602\n      37.591455\n    \n    \n      91311\n      16199088\n      점프셈교실\n      S\n      의료\n      S03\n      유사의료업\n      S03B09\n      유사의료업기타\n      경상북도\n      47130.0\n      ...\n      경상북도 경주시 황성동 446\n      471304715895\n      경상북도 경주시 용담로104번길\n      16\n      4713012400104460000024894\n      경상북도 경주시 용담로104번길 16\n      780954.0\n      38084.0\n      129.211755\n      35.865600\n    \n    \n      91319\n      16108560\n      씨앤디자인치과기공소\n      S\n      의료\n      S03\n      유사의료업\n      S03B06\n      치과기공소\n      서울특별시\n      11545.0\n      ...\n      서울특별시 금천구 가산동 60-25\n      115453116013\n      서울특별시 금천구 벚꽃로\n      234\n      1154510100100600025000001\n      서울특별시 금천구 벚꽃로 234\n      153798.0\n      8513.0\n      126.886122\n      37.475986\n    \n    \n      91327\n      16190388\n      오피스알파\n      S\n      의료\n      S03\n      유사의료업\n      S03B06\n      치과기공소\n      경기도\n      41173.0\n      ...\n      경기도 안양시 동안구 호계동 970-24\n      411734349013\n      경기도 안양시 동안구 경수대로507번길\n      28\n      4117310400109700024005182\n      경기도 안양시 동안구 경수대로507번길 28\n      431849.0\n      14120.0\n      126.956365\n      37.367779\n    \n  \n\n3774 rows × 29 columns\n\n\n\n\n\ndf[df[\"상권업종중분류명\"] == \"유사의료업\"].shape\n\n(3774, 29)\n\n\n\n# 상호명 그룹화해서 갯수\n# value_counts를 사용해 상위 10개 출력\n\ndf[\"상호명\"].value_counts().head(10)\n\n리원       152\n온누리약국    149\n경희한의원    141\n우리약국     119\n중앙약국     111\n전자담배      98\n조은약국      95\n건강약국      87\n제일약국      79\n사랑약국      73\nName: 상호명, dtype: int64\n\n\n\n\ndf[\"상호명\"].value_counts().tail()\n\n메리디언치과          1\n이엘피부과성형외과       1\n금오중국한의원         1\n오케이연합의원         1\n천안김안과천안역본점의원    1\nName: 상호명, dtype: int64\n\n\n\n\ndf_medi = df[df[\"상권업종중분류명\"] == \"유사의료업\"]\ndf_medi\n\n\n\n\n\n  \n    \n      \n      상가업소번호\n      상호명\n      상권업종대분류코드\n      상권업종대분류명\n      상권업종중분류코드\n      상권업종중분류명\n      상권업종소분류코드\n      상권업종소분류명\n      시도명\n      시군구코드\n      ...\n      지번주소\n      도로명코드\n      도로명\n      건물본번지\n      건물관리번호\n      도로명주소\n      구우편번호\n      신우편번호\n      경도\n      위도\n    \n  \n  \n    \n      22\n      21013731\n      세종언어치료센터\n      S\n      의료\n      S03\n      유사의료업\n      S03B07\n      언어치료\n      부산광역시\n      26410.0\n      ...\n      부산광역시 금정구 구서동 84-1\n      264102000010\n      부산광역시 금정구 중앙대로\n      1817\n      2641010700100840001017686\n      부산광역시 금정구 중앙대로 1817-11\n      609310.0\n      46273.0\n      129.091662\n      35.246528\n    \n    \n      40\n      20933900\n      고려수지침학회\n      S\n      의료\n      S03\n      유사의료업\n      S03B03\n      침구원\n      경상남도\n      48123.0\n      ...\n      경상남도 창원시 성산구 상남동 5-2\n      481234784088\n      경상남도 창원시 성산구 마디미로4번길\n      9\n      4812312700100050002026799\n      경상남도 창원시 성산구 마디미로4번길 9\n      642832.0\n      51495.0\n      128.684678\n      35.224113\n    \n    \n      97\n      21717820\n      청명원\n      S\n      의료\n      S03\n      유사의료업\n      S03B09\n      유사의료업기타\n      충청북도\n      43760.0\n      ...\n      충청북도 괴산군 청안면 금신리 241\n      437604538132\n      충청북도 괴산군 청안면 금신로1길\n      93\n      4376037022102410000007293\n      충청북도 괴산군 청안면 금신로1길 93\n      367831.0\n      28050.0\n      127.635740\n      36.768935\n    \n    \n      102\n      21865854\n      응급환자이송센터\n      S\n      의료\n      S03\n      유사의료업\n      S03B01\n      응급구조대\n      대전광역시\n      30140.0\n      ...\n      대전광역시 중구 대사동 248-237\n      301404295026\n      대전광역시 중구 계룡로921번길\n      40\n      3014011000102480237013097\n      대전광역시 중구 계룡로921번길 40\n      301846.0\n      34946.0\n      127.417693\n      36.321801\n    \n    \n      108\n      21914637\n      태화아동발달지원센터\n      S\n      의료\n      S03\n      유사의료업\n      S03B07\n      언어치료\n      대전광역시\n      30140.0\n      ...\n      대전광역시 중구 문화동 27\n      301404295402\n      대전광역시 중구 보문산로333번길\n      29\n      3014011600100270000008172\n      대전광역시 중구 보문산로333번길 29\n      301130.0\n      35020.0\n      127.412725\n      36.312953\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      91300\n      16131218\n      으뜸치과기공소\n      S\n      의료\n      S03\n      유사의료업\n      S03B06\n      치과기공소\n      경상남도\n      48170.0\n      ...\n      경상남도 진주시 수정동 39-11\n      481704797625\n      경상남도 진주시 향교로18번길\n      8\n      4817011600100390011004490\n      경상남도 진주시 향교로18번길 8\n      660180.0\n      52753.0\n      128.084600\n      35.197029\n    \n    \n      91310\n      16199325\n      보령치과기공소\n      S\n      의료\n      S03\n      유사의료업\n      S03B06\n      치과기공소\n      서울특별시\n      11290.0\n      ...\n      서울특별시 성북구 동소문동4가 103-11\n      112903107003\n      서울특별시 성북구 동소문로\n      47\n      1129010700101030014050661\n      서울특별시 성북구 동소문로 47-15\n      136821.0\n      2832.0\n      127.010602\n      37.591455\n    \n    \n      91311\n      16199088\n      점프셈교실\n      S\n      의료\n      S03\n      유사의료업\n      S03B09\n      유사의료업기타\n      경상북도\n      47130.0\n      ...\n      경상북도 경주시 황성동 446\n      471304715895\n      경상북도 경주시 용담로104번길\n      16\n      4713012400104460000024894\n      경상북도 경주시 용담로104번길 16\n      780954.0\n      38084.0\n      129.211755\n      35.865600\n    \n    \n      91319\n      16108560\n      씨앤디자인치과기공소\n      S\n      의료\n      S03\n      유사의료업\n      S03B06\n      치과기공소\n      서울특별시\n      11545.0\n      ...\n      서울특별시 금천구 가산동 60-25\n      115453116013\n      서울특별시 금천구 벚꽃로\n      234\n      1154510100100600025000001\n      서울특별시 금천구 벚꽃로 234\n      153798.0\n      8513.0\n      126.886122\n      37.475986\n    \n    \n      91327\n      16190388\n      오피스알파\n      S\n      의료\n      S03\n      유사의료업\n      S03B06\n      치과기공소\n      경기도\n      41173.0\n      ...\n      경기도 안양시 동안구 호계동 970-24\n      411734349013\n      경기도 안양시 동안구 경수대로507번길\n      28\n      4117310400109700024005182\n      경기도 안양시 동안구 경수대로507번길 28\n      431849.0\n      14120.0\n      126.956365\n      37.367779\n    \n  \n\n3774 rows × 29 columns\n\n\n\n\ndf_medi[\"상호명\"].value_counts().head(10)\n\n리원          32\n고려수지침       22\n대한적십자사      17\n헌혈의집        12\n고려수지침학회     10\n수치과기공소      10\n제일치과기공소      9\n미소치과기공소      8\n아트치과기공소      8\n이사랑치과기공소     8\nName: 상호명, dtype: int64\n\n\n\n\n\n# 상권업종소분류명이 약국이고 시도명이 서울특별시인 데이터\ndf[\"상권업종소분류명\"] == \"약국\"  and df[\"시도명\"] == \"서울특별시\"\n# 오류! 판다스에서는 & 써야함. \n\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\n\n\n\ndf[\"상권업종소분류명\"] == \"약국\"  & df[\"시도명\"] == \"서울특별시\"\n# 오류! 연산자 우선순위 때문에 오류가 났다.\n\nTypeError: Cannot perform 'rand_' with a dtyped [object] array and scalar of type [bool]\n\n\n\n\ndf_seoul_drug = df[(df[\"상권업종소분류명\"] == \"약국\")  & (df[\"시도명\"] == \"서울특별시\")]\nprint(df_seoul_drug.shape)\ndf_seoul_drug.head(1)\n\n(3579, 29)\n\n\n\n\n\n\n  \n    \n      \n      상가업소번호\n      상호명\n      상권업종대분류코드\n      상권업종대분류명\n      상권업종중분류코드\n      상권업종중분류명\n      상권업종소분류코드\n      상권업종소분류명\n      시도명\n      시군구코드\n      ...\n      지번주소\n      도로명코드\n      도로명\n      건물본번지\n      건물관리번호\n      도로명주소\n      구우편번호\n      신우편번호\n      경도\n      위도\n    \n  \n  \n    \n      33\n      20816709\n      이즈타워약\n      S\n      의료\n      S02\n      약국/한약방\n      S02A01\n      약국\n      서울특별시\n      11680.0\n      ...\n      서울특별시 강남구 역삼동 821\n      116803122010\n      서울특별시 강남구 테헤란로\n      101\n      1168010100108210001000001\n      서울특별시 강남구 테헤란로 101\n      135080.0\n      6134.0\n      127.028023\n      37.498656\n    \n  \n\n1 rows × 29 columns\n\n\n\n\n\n\n\n# 시군구명으로 그룹화해서 갯수 세어보기\n# 구별로 약국이 몇개가 있는지 확인\nc = df_seoul_drug[\"시군구명\"].value_counts()\nc.head()\n\n강남구     374\n동대문구    261\n광진구     212\n서초구     191\n송파구     188\nName: 시군구명, dtype: int64\n\n\n\nn = df_seoul_drug[\"시군구명\"].value_counts(normalize=True)\nn.head()\n\n강남구     0.104498\n동대문구    0.072925\n광진구     0.059234\n서초구     0.053367\n송파구     0.052529\nName: 시군구명, dtype: float64\n\n\n\nc.plot.bar(rot=60) #rot:글씨를 기울인다\n\n<AxesSubplot:>\n\n\n\n\n\n\n# 상권업종소분류명이 종합병원\n# 시도명이 서울특별시인 데이터\n\ndf_seoul_hospital = df[(df[\"상권업종소분류명\"] == \"종합병원\") & (df[\"시도명\"] == \"서울특별시\")].copy()\n# copy를 해줘야 df_soeul_hospital 이 바뀐다. df까지 바뀌지 않는다. \ndf_seoul_hospital\n\n\n\n\n\n  \n    \n      \n      상가업소번호\n      상호명\n      상권업종대분류코드\n      상권업종대분류명\n      상권업종중분류코드\n      상권업종중분류명\n      상권업종소분류코드\n      상권업종소분류명\n      시도명\n      시군구코드\n      ...\n      지번주소\n      도로명코드\n      도로명\n      건물본번지\n      건물관리번호\n      도로명주소\n      구우편번호\n      신우편번호\n      경도\n      위도\n    \n  \n  \n    \n      305\n      25155642\n      대진의료재단\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11215.0\n      ...\n      서울특별시 광진구 중곡동 58-25\n      112153104006\n      서울특별시 광진구 긴고랑로\n      119\n      1121510100100580025000733\n      서울특별시 광진구 긴고랑로 119\n      143220.0\n      4944.0\n      127.088279\n      37.559048\n    \n    \n      353\n      20471487\n      홍익병원별관\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11470.0\n      ...\n      서울특별시 양천구 신정동 897-13\n      114702005008\n      서울특별시 양천구 국회대로\n      250\n      1147010100108970013001044\n      서울특별시 양천구 국회대로 250\n      158070.0\n      7937.0\n      126.862805\n      37.529213\n    \n    \n      385\n      20737057\n      SNUH\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11680.0\n      ...\n      서울특별시 강남구 역삼동 736-55\n      116804166727\n      서울특별시 강남구 테헤란로26길\n      10\n      1168010100107360055027688\n      서울특별시 강남구 테헤란로26길 10\n      135080.0\n      6236.0\n      127.035825\n      37.499630\n    \n    \n      1917\n      23210677\n      평화드림여의도성모병원의료기매장\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11560.0\n      ...\n      서울특별시 영등포구 여의도동 62\n      115603118001\n      서울특별시 영등포구 63로\n      10\n      1156011000100620000031477\n      서울특별시 영등포구 63로 10\n      150713.0\n      7345.0\n      126.936693\n      37.518296\n    \n    \n      2461\n      20024045\n      한양\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11200.0\n      ...\n      서울특별시 성동구 행당동 15-1\n      112003103002\n      서울특별시 성동구 마조로\n      22\n      1120010700100150001019623\n      서울특별시 성동구 마조로 22-2\n      133070.0\n      4763.0\n      127.041325\n      37.559469\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      71991\n      28505952\n      서울성모병원응급의료센터\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11650.0\n      ...\n      서울특별시 서초구 반포동 505\n      116502121003\n      서울특별시 서초구 반포대로\n      222\n      1165010700101230000017226\n      서울특별시 서초구 반포대로 222\n      137701.0\n      6591.0\n      127.005841\n      37.502382\n    \n    \n      76508\n      12292992\n      라마르의원\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11740.0\n      ...\n      서울특별시 강동구 천호동 453-8\n      117404172367\n      서울특별시 강동구 천호대로157길\n      18\n      1174010900104530021010314\n      서울특별시 강동구 천호대로157길 18\n      134864.0\n      5335.0\n      127.127466\n      37.538485\n    \n    \n      90492\n      16031909\n      가톨릭대학교여의도성모병원\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11140.0\n      ...\n      서울특별시 중구 명동2가 1-1\n      111404103165\n      서울특별시 중구 명동길\n      74\n      1114012700100010001019574\n      서울특별시 중구 명동길 74\n      100809.0\n      4537.0\n      126.986758\n      37.563662\n    \n    \n      90581\n      16332576\n      씨엠병원\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11560.0\n      ...\n      서울특별시 영등포구 영등포동4가 90\n      115604154717\n      서울특별시 영등포구 영등포로36길\n      13\n      1156010500100900000035097\n      서울특별시 영등포구 영등포로36길 13\n      150030.0\n      7301.0\n      126.903857\n      37.518807\n    \n    \n      90788\n      16162338\n      성베드로병원\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11680.0\n      ...\n      서울특별시 강남구 도곡동 910-27\n      116802000003\n      서울특별시 강남구 남부순환로\n      2649\n      1168011800109100027000895\n      서울특별시 강남구 남부순환로 2649\n      135859.0\n      6271.0\n      127.039567\n      37.485604\n    \n  \n\n91 rows × 29 columns\n\n\n\n\ndf_seoul_hospital[\"시군구명\"].value_counts()\n\n강남구     15\n영등포구     8\n광진구      6\n서초구      6\n강동구      5\n중구       5\n송파구      5\n강북구      4\n도봉구      4\n서대문구     4\n양천구      4\n성북구      3\n강서구      2\n중랑구      2\n종로구      2\n동대문구     2\n구로구      2\n노원구      2\n금천구      2\n성동구      2\n관악구      2\n동작구      1\n마포구      1\n용산구      1\n은평구      1\nName: 시군구명, dtype: int64\n\n\n\n\n\n\n# 색인 전 상호명 중에 종합병원이 아닌 데이터 찾기\ndf_seoul_hospital.loc[~df_seoul_hospital[\"상호명\"].str.contains(\"종합병원\"),\"상호명\"].unique()\n\n# str.contains하면 특정 값만 찾을 수 있다. \n# 앞에 물결 표시를 하게 되면 종합병원이 안들어간 것만 찾을 수 있다. \n\narray(['대진의료재단', '홍익병원별관', 'SNUH', '평화드림여의도성모병원의료기매장', '한양', '백산의료재단친구병원',\n       '서울보훈병원', '서울성모병원장례식장꽃배달', '서울대학교병원', '알콜중독및정신질환상담소',\n       '강남성모병원장례식장꽃배달', '제일병원', '이랜드클리닉', '사랑나눔의료재단', '우울증센터', '성심의료재단',\n       '다나의료재단', '서울아산병원신관', '원자력병원장례식장', '국민의원', '고려대학교구로병원', '학교법인일송학원',\n       '삼성의료원장례식장', '희명스포츠의학센터인공신장실', '연세대학교의과대학강남세브란스', '국립정신병원',\n       '코아클리닉', '수서제일의원', '사랑의의원', '한국전력공사부속한일병원', '신촌연세병원', '창동제일의원',\n       '영동세브란스병원', '제일성심의원', '삼성의료재단강북삼성태', '서울시립보라매병원', '서울이의원',\n       '서울대학교병원비상계획외래', '평화드림서울성모병원의료', '홍익병원', '사랑나눔의료재단서', '독일의원',\n       '서울연합의원', '우신향병원', '동부제일병원', '아산재단금강병원', '명곡안연구소', '아산재단서울중앙병원',\n       '메디힐특수여객', '삼성생명공익재단삼성서', '성광의료재단차병원', '한국건강관리협회서울특',\n       '정해복지부설한신메디피아', '성베드로병원', '성애의료재단', '실로암의원', 'Y&T성모마취과', '광진성모의원',\n       '서울현대의원', '이노신경과의원', '송정훼밀리의원', '서울중앙의원', '영남의료재단', '인제대학교서울백병원',\n       '한국필의료재단', '세브란스의원', '가톨릭대학교성바오로병원장례식장', '서울연세의원', '사랑의병원',\n       '성삼의료재단미즈메디병원', '씨엠충무병원', '성신의원', '원진재단부설녹색병원', '송파제일의원',\n       '카톨릭성모의원', '한양성심의원', '관악성모의원', '강남센트럴병원', '우이한솔의원', '우리들병원',\n       '서울성모병원어린이집', '건국대학교병원', '서울적십자병원', '북부성모의원', '한림대학교부속한강성심병원장례식장',\n       '서울성모병원응급의료센터', '라마르의원', '가톨릭대학교여의도성모병원', '씨엠병원'], dtype=object)\n\n\n\ndf_seoul_hospital[df_seoul_hospital[\"상호명\"].str.contains(\"꽃배달\")]\n\n\n\n\n\n  \n    \n      \n      상가업소번호\n      상호명\n      상권업종대분류코드\n      상권업종대분류명\n      상권업종중분류코드\n      상권업종중분류명\n      상권업종소분류코드\n      상권업종소분류명\n      시도명\n      시군구코드\n      ...\n      지번주소\n      도로명코드\n      도로명\n      건물본번지\n      건물관리번호\n      도로명주소\n      구우편번호\n      신우편번호\n      경도\n      위도\n    \n  \n  \n    \n      2803\n      20895655\n      서울성모병원장례식장꽃배달\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11650.0\n      ...\n      서울특별시 서초구 반포동 551\n      116504163330\n      서울특별시 서초구 사평대로28길\n      55\n      1165010700105510000017194\n      서울특별시 서초구 사평대로28길 55\n      137040.0\n      6578.0\n      127.000682\n      37.498257\n    \n    \n      4644\n      22020310\n      강남성모병원장례식장꽃배달\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11650.0\n      ...\n      서울특별시 서초구 반포동 547-6\n      116504163242\n      서울특별시 서초구 반포대로39길\n      56\n      1165010700105470006016762\n      서울특별시 서초구 반포대로39길 56-24\n      137040.0\n      6578.0\n      127.001756\n      37.499095\n    \n  \n\n2 rows × 29 columns\n\n\n\n\ndf_seoul_hospital[df_seoul_hospital[\"상호명\"].str.contains(\"의료기\")]\n\n\n\n\n\n  \n    \n      \n      상가업소번호\n      상호명\n      상권업종대분류코드\n      상권업종대분류명\n      상권업종중분류코드\n      상권업종중분류명\n      상권업종소분류코드\n      상권업종소분류명\n      시도명\n      시군구코드\n      ...\n      지번주소\n      도로명코드\n      도로명\n      건물본번지\n      건물관리번호\n      도로명주소\n      구우편번호\n      신우편번호\n      경도\n      위도\n    \n  \n  \n    \n      1917\n      23210677\n      평화드림여의도성모병원의료기매장\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11560.0\n      ...\n      서울특별시 영등포구 여의도동 62\n      115603118001\n      서울특별시 영등포구 63로\n      10\n      1156011000100620000031477\n      서울특별시 영등포구 63로 10\n      150713.0\n      7345.0\n      126.936693\n      37.518296\n    \n  \n\n1 rows × 29 columns\n\n\n\n\ndf_seoul_hospital[df_seoul_hospital[\"상호명\"].str.contains(\"꽃배달|의료기|장례식장|상담소|어린이집\")]\n\n\n\n\n\n  \n    \n      \n      상가업소번호\n      상호명\n      상권업종대분류코드\n      상권업종대분류명\n      상권업종중분류코드\n      상권업종중분류명\n      상권업종소분류코드\n      상권업종소분류명\n      시도명\n      시군구코드\n      ...\n      지번주소\n      도로명코드\n      도로명\n      건물본번지\n      건물관리번호\n      도로명주소\n      구우편번호\n      신우편번호\n      경도\n      위도\n    \n  \n  \n    \n      1917\n      23210677\n      평화드림여의도성모병원의료기매장\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11560.0\n      ...\n      서울특별시 영등포구 여의도동 62\n      115603118001\n      서울특별시 영등포구 63로\n      10\n      1156011000100620000031477\n      서울특별시 영등포구 63로 10\n      150713.0\n      7345.0\n      126.936693\n      37.518296\n    \n    \n      2803\n      20895655\n      서울성모병원장례식장꽃배달\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11650.0\n      ...\n      서울특별시 서초구 반포동 551\n      116504163330\n      서울특별시 서초구 사평대로28길\n      55\n      1165010700105510000017194\n      서울특별시 서초구 사평대로28길 55\n      137040.0\n      6578.0\n      127.000682\n      37.498257\n    \n    \n      4431\n      21781516\n      알콜중독및정신질환상담소\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11320.0\n      ...\n      서울특별시 도봉구 창동 181-52\n      113204127202\n      서울특별시 도봉구 마들로13길\n      153\n      1132010700101810052014414\n      서울특별시 도봉구 마들로13길 153\n      132040.0\n      1411.0\n      127.046203\n      37.657046\n    \n    \n      4644\n      22020310\n      강남성모병원장례식장꽃배달\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11650.0\n      ...\n      서울특별시 서초구 반포동 547-6\n      116504163242\n      서울특별시 서초구 반포대로39길\n      56\n      1165010700105470006016762\n      서울특별시 서초구 반포대로39길 56-24\n      137040.0\n      6578.0\n      127.001756\n      37.499095\n    \n    \n      7938\n      20625484\n      원자력병원장례식장\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11350.0\n      ...\n      서울특별시 노원구 공릉동 215-4\n      113503110002\n      서울특별시 노원구 노원로\n      75\n      1135010300102150004014400\n      서울특별시 노원구 노원로 75\n      139706.0\n      1812.0\n      127.082670\n      37.628808\n    \n    \n      10283\n      20024377\n      삼성의료원장례식장\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11680.0\n      ...\n      서울특별시 강남구 일원동 50\n      116803122009\n      서울특별시 강남구 일원로\n      81\n      1168011400100500000002609\n      서울특별시 강남구 일원로 81\n      135710.0\n      6351.0\n      127.089579\n      37.490334\n    \n    \n      47008\n      21738670\n      가톨릭대학교성바오로병원장례식장\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11230.0\n      ...\n      서울특별시 동대문구 전농동 620-56\n      112303105008\n      서울특별시 동대문구 왕산로\n      180\n      1123010400106200056027814\n      서울특별시 동대문구 왕산로 180\n      130709.0\n      2559.0\n      127.043471\n      37.579246\n    \n    \n      60645\n      27670796\n      서울성모병원어린이집\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11650.0\n      ...\n      서울특별시 서초구 반포동 505\n      116502121003\n      서울특별시 서초구 반포대로\n      222\n      1165010700101230000017226\n      서울특별시 서초구 반포대로 222\n      137701.0\n      6591.0\n      127.005841\n      37.502382\n    \n    \n      70177\n      11537223\n      한림대학교부속한강성심병원장례식장\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11560.0\n      ...\n      서울특별시 영등포구 영등포동7가 94-200\n      115604154428\n      서울특별시 영등포구 버드나루로7길\n      12\n      1156010800100940200033663\n      서울특별시 영등포구 버드나루로7길 12\n      150030.0\n      7247.0\n      126.909676\n      37.523168\n    \n  \n\n9 rows × 29 columns\n\n\n\n\ndf_seoul_hospital[df_seoul_hospital[\"상호명\"].str.contains(\"꽃배달|의료기|장례식장|상담소|어린이집\")].index\n\nInt64Index([1917, 2803, 4431, 4644, 7938, 10283, 47008, 60645, 70177], dtype='int64')\n\n\n\n# 종합병원과 무관한 데이터를 전처리를 위해 해당 텍스트 한번에 검색\n# 제거할 데이터의 인덱스만 drop_row에 담아주고 list 형태로 변환\n\ndrop_row = df_seoul_hospital[df_seoul_hospital[\"상호명\"].str.contains(\"꽃배달|의료기|장례식장|상담소|어린이집\")].index\ndrop_row = drop_row.tolist() \ndrop_row\n\n[1917, 2803, 4431, 4644, 7938, 10283, 47008, 60645, 70177]\n\n\n\n# 의원으로 끝나는 데이터 인덱스 찾기\n# drop_row2 에 담고 list 변환\n# str.endswith() : ~로 끝나는거\n\ndrop_row2 = df_seoul_hospital[df_seoul_hospital[\"상호명\"].str.endswith(\"의원\")].index\ndrop_row2 = drop_row2.tolist()\ndrop_row2\n\n[8479,\n 12854,\n 13715,\n 14966,\n 16091,\n 18047,\n 20200,\n 20415,\n 30706,\n 32889,\n 34459,\n 34720,\n 35696,\n 37251,\n 45120,\n 49626,\n 51575,\n 55133,\n 56320,\n 56404,\n 56688,\n 57551,\n 62113,\n 76508]\n\n\n\n# 삭제할 행을 drop_row에 합치기\ndrop_row = drop_row + drop_row2\nlen(drop_row)\n\n33\n\n\n\n# 해당 셀을 삭제하고 삭제 전 후의 행의 갯수 비교\nprint(df_seoul_hospital.shape)\ndf_seoul_hospital = df_seoul_hospital.drop(drop_row, axis=0)\nprint(df_seoul_hospital.shape)\n\n(91, 29)\n(58, 29)\n\n\n\n# 시군구명에 따라 종합병원의 숫자\ndf_seoul_hospital[\"시군구명\"].value_counts().plot.bar()\n\n<AxesSubplot:>\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.countplot(data=df_seoul_hospital, x=\"시군구명\", order=df_seoul_hospital[\"시군구명\"].value_counts().index)\n\n<AxesSubplot:xlabel='시군구명', ylabel='count'>\n\n\n\n\n\n\ndf_seoul_hospital[\"상호명\"].unique()\n\narray(['대진의료재단', '홍익병원별관', 'SNUH', '한양', '백산의료재단친구병원', '서울보훈병원',\n       '서울대학교병원', '제일병원', '이랜드클리닉', '사랑나눔의료재단', '우울증센터', '성심의료재단',\n       '다나의료재단', '서울아산병원신관', '고려대학교구로병원', '학교법인일송학원', '희명스포츠의학센터인공신장실',\n       '연세대학교의과대학강남세브란스', '국립정신병원', '코아클리닉', '한국전력공사부속한일병원', '신촌연세병원',\n       '영동세브란스병원', '삼성의료재단강북삼성태', '서울시립보라매병원', '서울대학교병원비상계획외래',\n       '평화드림서울성모병원의료', '홍익병원', '사랑나눔의료재단서', '우신향병원', '동부제일병원', '아산재단금강병원',\n       '명곡안연구소', '아산재단서울중앙병원', '메디힐특수여객', '삼성생명공익재단삼성서', '성광의료재단차병원',\n       '한국건강관리협회서울특', '정해복지부설한신메디피아', '성베드로병원', '성애의료재단', 'Y&T성모마취과',\n       '영남의료재단', '인제대학교서울백병원', '한국필의료재단', '사랑의병원', '성삼의료재단미즈메디병원',\n       '씨엠충무병원', '원진재단부설녹색병원', '강남센트럴병원', '우리들병원', '건국대학교병원', '서울적십자병원',\n       '서울성모병원응급의료센터', '가톨릭대학교여의도성모병원', '씨엠병원'], dtype=object)\n\n\n\n\n\n\n# 서울에 있는 데이터의 위도와 경도 보기\n# 결과를 df_seoul 이라는 df에 저장\n# 새로운 변수에 데이터프레임 저장시 copy()를 사용\n\ndf_seoul= df[df[\"시도명\"] == \"서울특별시\"].copy()\ndf_seoul.shape\n\n(18943, 29)\n\n\n\n# seaborn 의 countplot를 사용해 위에서 만든 데이터프레ㅣㅁ의 시군구명 시각화\ndf_seoul[\"시군구명\"].value_counts().head()\ndf_seoul[\"시군구명\"].value_counts().plot.bar(figsize=(10,4), rot=30)\n\n<AxesSubplot:>\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.countplot(data=df_seoul, x=\"시군구명\")\n# x축 y축 두개 중 하나만 지정해주면 된다.\n\n<AxesSubplot:xlabel='시군구명', ylabel='count'>\n\n\n\n\n\n\n# pandas의 plot.scatter를 통해 경도와 위도 표시\ndf_seoul[[\"경도\", \"위도\", \"시군구명\"]].plot.scatter()\n# scatter는 x축과 y축이 꼭 들어가야 한다!!\n\nTypeError: scatter() missing 2 required positional arguments: 'x' and 'y'\n\n\n\ndf_seoul[[\"경도\", \"위도\", \"시군구명\"]].plot.scatter(x=\"경도\", y=\"위도\", figsize=(8,7), grid=True)\n\n<AxesSubplot:xlabel='경도', ylabel='위도'>\n\n\n\n\n\n\nplt.figure(figsize=(9,8))\nsns.scatterplot(data=df_seoul,x=\"경도\", y=\"위도\", hue=\"시군구명\") \n# hue: 색상 다르게\n\n<AxesSubplot:xlabel='경도', ylabel='위도'>\n\n\n\n\n\n\nplt.figure(figsize=(9,8))\nsns.scatterplot(data=df_seoul,x=\"경도\", y=\"위도\", hue=\"상권업종중분류명\") \n\n<AxesSubplot:xlabel='경도', ylabel='위도'>\n\n\n\n\n\n\nplt.figure(figsize=(16,12))\nsns.scatterplot(data=df,x=\"경도\", y=\"위도\", hue=\"시도명\") \n\n<AxesSubplot:xlabel='경도', ylabel='위도'>\n\n\n\n\n\n\n\n\n\n\nimport folium\n# 아나콘다에서 folium 별도 설치해야함\n# conda install -c conda-forge folium\n# 지도 시각화를 위한 라이브러리\n\nm= folium.Map(location=[45.5236, -122.6750])\n\n\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\nfolium.Map()\n# 세계 지도 출력! \n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\nfolium.Map()\n\n\n# 지도의 중심을 지정하기 위해 위도와 경도의 평균을 구한다.\n\ndf_seoul_hospital[\"위도\"].mean()\ndf_seoul_hospital[\"경도\"].mean()\n\n126.9963589356625\n\n\n\nmap = folium.Map(location=[df_seoul_hospital[\"위도\"].mean(),df_seoul_hospital[\"경도\"].mean()], zoom_start=12)\n\n\ndf_seoul_hospital.head(1)\n\n\n\n\n\n  \n    \n      \n      상가업소번호\n      상호명\n      상권업종대분류코드\n      상권업종대분류명\n      상권업종중분류코드\n      상권업종중분류명\n      상권업종소분류코드\n      상권업종소분류명\n      시도명\n      시군구코드\n      ...\n      지번주소\n      도로명코드\n      도로명\n      건물본번지\n      건물관리번호\n      도로명주소\n      구우편번호\n      신우편번호\n      경도\n      위도\n    \n  \n  \n    \n      305\n      25155642\n      대진의료재단\n      S\n      의료\n      S01\n      병원\n      S01B01\n      종합병원\n      서울특별시\n      11215.0\n      ...\n      서울특별시 광진구 중곡동 58-25\n      112153104006\n      서울특별시 광진구 긴고랑로\n      119\n      1121510100100580025000733\n      서울특별시 광진구 긴고랑로 119\n      143220.0\n      4944.0\n      127.088279\n      37.559048\n    \n  \n\n1 rows × 29 columns\n\n\n\n\nfor n in df_seoul_hospital.index:\n    name = df_seoul_hospital.loc[n, \"상호명\"]\n    address = df_seoul_hospital.loc[n, \"도로명주소\"]\n    popup = f\"{name}-{address}\"\n    location = [df_seoul_hospital.loc[n, \"위도\"], df_seoul_hospital.loc[n, \"경도\"]]\n    folium.Marker(\n        location = location,\n        popup = popup,\n    ).add_to(map)\nmap\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "posts/est/boostcourse/1. file-path-setting.html",
    "href": "posts/est/boostcourse/1. file-path-setting.html",
    "title": "2: file-path-setting",
    "section": "",
    "text": "!move \"C:\\Users\\user\\Downloads\\도로교통공단_사망 교통사고 정보_20211231.csv\" .\n# mv: 다운로드 받은 파일을 같은 경로로 옮긴다. 근데 이건 맥에서만..\n# !move: 이게 윈도우즈 환경. 윈도우즈 환경에서는 맥과 다르게 작성하는듯 \n\n        1개 파일을 이동했습니다.\n\n\n\n# 주피터 노트북이 있는 폴더의 경로를 출력한다.\n%pwd\n\n'C:\\\\Users\\\\user\\\\Untitled Folder'\n\n\n\n%ls\n\n C 드라이브의 볼륨: system\n 볼륨 일련 번호: 0819-B8FC\n\n C:\\Users\\user\\Untitled Folder 디렉터리\n\n2022-10-27  오전 10:43    <DIR>          .\n2022-10-27  오전 10:43    <DIR>          ..\n2022-10-27  오전 10:35    <DIR>          .ipynb_checkpoints\n2022-10-26  오후 02:24            45,288 2. 데이터 분석 준비하기.ipynb\n2022-10-27  오전 10:43             1,498 file-path-setting.ipynb\n2022-10-25  오후 01:57             2,883 jupyter basic.ipynb\n2022-10-27  오전 10:37           459,343 도로교통공단_사망 교통사고 정보_20211231.csv\n               4개 파일             509,012 바이트\n               3개 디렉터리  113,083,060,224 바이트 남음\n\n\n\nimport pandas as pd\n\n\npd.read_csv(\"data/도로교통공단_사망 교통사고 정보_20211231.csv\", encoding=\"cp949\")\n# UnicodeDecodeError 이파일의 인코딩이 UTF8이 아니라는 뜻이다.\n# ()안에서 shift+tab키를 누르면 도움말 본다.\n# 옵션에 encording=None 으로 설정되어 있음. 인코딩 지정을 해줘야 한다. 엑셀은 cp949 \n\n# 파일 옮겨주면 FileNotFoundError 가 난다. -> 파일이 속해있는 곳을  적어주기 \n\n\n\n\n\n  \n    \n      \n      발생년\n      발생년월일시\n      주야\n      요일\n      사망자수\n      부상자수\n      중상자수\n      경상자수\n      부상신고자수\n      발생지시도\n      ...\n      사고유형\n      가해자법규위반\n      도로형태_대분류\n      도로형태\n      가해자_당사자종별\n      피해자_당사자종별\n      발생위치X(UTMK)\n      발생위치Y(UTMK)\n      경도\n      위도\n    \n  \n  \n    \n      0\n      2021\n      2021-01-01 03:00\n      야\n      금\n      1\n      3\n      0\n      3\n      0\n      경북\n      ...\n      추돌\n      안전운전 의무 불이행\n      교차로\n      교차로부근\n      승용차\n      승용차\n      1097010.0\n      1793385.0\n      128.578152\n      36.132653\n    \n    \n      1\n      2021\n      2021-01-01 09:00\n      주\n      금\n      1\n      0\n      0\n      0\n      0\n      충남\n      ...\n      공작물충돌\n      안전운전 의무 불이행\n      단일로\n      기타단일로\n      승용차\n      없음\n      902369.0\n      1847109.0\n      126.408201\n      36.616845\n    \n    \n      2\n      2021\n      2021-01-01 15:00\n      주\n      금\n      1\n      0\n      0\n      0\n      0\n      강원\n      ...\n      측면충돌\n      안전운전 의무 불이행\n      교차로\n      교차로내\n      원동기장치자전거\n      승용차\n      1123975.0\n      1974509.0\n      128.907484\n      37.761842\n    \n    \n      3\n      2021\n      2021-01-01 19:00\n      야\n      금\n      1\n      0\n      0\n      0\n      0\n      전남\n      ...\n      횡단중\n      안전운전 의무 불이행\n      단일로\n      기타단일로\n      화물차\n      보행자\n      886507.0\n      1613961.0\n      126.263573\n      34.513391\n    \n    \n      4\n      2021\n      2021-01-01 21:00\n      야\n      금\n      1\n      0\n      0\n      0\n      0\n      경기\n      ...\n      기타\n      기타\n      단일로\n      기타단일로\n      승용차\n      보행자\n      953522.0\n      1915403.0\n      126.976011\n      37.236327\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2811\n      2021\n      2021-12-31 16:00\n      주\n      금\n      1\n      0\n      0\n      0\n      0\n      경북\n      ...\n      정면충돌\n      안전운전 의무 불이행\n      교차로\n      교차로내\n      승용차\n      이륜차\n      1119020.0\n      1766895.0\n      128.818730\n      35.891434\n    \n    \n      2812\n      2021\n      2021-12-31 17:00\n      주\n      금\n      1\n      0\n      0\n      0\n      0\n      제주\n      ...\n      추돌\n      안전운전 의무 불이행\n      단일로\n      기타단일로\n      화물차\n      화물차\n      940588.0\n      1503049.6\n      126.860248\n      33.517699\n    \n    \n      2813\n      2021\n      2021-12-31 18:00\n      야\n      금\n      1\n      0\n      0\n      0\n      0\n      강원\n      ...\n      횡단중\n      보행자 보호의무 위반\n      단일로\n      기타단일로\n      승용차\n      보행자\n      1023127.0\n      1982332.0\n      127.762845\n      37.840465\n    \n    \n      2814\n      2021\n      2021-12-31 19:00\n      야\n      금\n      1\n      0\n      0\n      0\n      0\n      경북\n      ...\n      횡단중\n      보행자 보호의무 위반\n      교차로\n      교차로횡단보도내\n      승용차\n      보행자\n      1058805.0\n      1824755.0\n      128.155943\n      36.418521\n    \n    \n      2815\n      2021\n      2021-12-31 21:00\n      야\n      금\n      1\n      0\n      0\n      0\n      0\n      강원\n      ...\n      전복\n      중앙선 침범\n      단일로\n      기타단일로\n      승용차\n      없음\n      1042559.0\n      2010975.0\n      127.985386\n      38.097913\n    \n  \n\n2816 rows × 23 columns"
  },
  {
    "objectID": "posts/est/boostcourse/5. K-beauty.html",
    "href": "posts/est/boostcourse/5. K-beauty.html",
    "title": "6: K-beauty",
    "section": "",
    "text": "e:추정지, p:잠정치, -:자료없음, …:미상자료, x: 비밀번호"
  },
  {
    "objectID": "posts/est/boostcourse/5. K-beauty.html#기간에서-연도를-분리하기",
    "href": "posts/est/boostcourse/5. K-beauty.html#기간에서-연도를-분리하기",
    "title": "6: K-beauty",
    "section": "기간에서 연도를 분리하기",
    "text": "기간에서 연도를 분리하기\n\ndf[\"기간\"]\n# object : string데이터를 의미\n\n0        2014.1/4\n1        2014.1/4\n2        2014.1/4\n3        2014.1/4\n4        2014.1/4\n           ...   \n10795    2019.4/4\n10796    2019.4/4\n10797    2019.4/4\n10798    2019.4/4\n10799    2019.4/4\nName: 기간, Length: 10800, dtype: object\n\n\ndf[“기간”].map?\n\n\ndf[\"연도\"] = list(map(lambda x : int(x.split(\".\")[0]), df[\"기간\"])) \n\n\n# 기간에서 분기만 분리하기\ndf[\"분기\"] = list(map(lambda x : int(x.split(\".\")[1].split()[0].split(\"/\")[0]), df[\"기간\"])) \n\n\n# (1) \".\" 을 기준으로 split하고 ([\"2022\", \"1/4 p\"]) 1번째 인덱스를 취함 -> \"1/4 p\"\n\n# (2) \" \" 을 기준으로 split하고 ([\"1/4\", \"p\"]) 0번째 인덱스를 취함 -> \"1/4\"\n\n# (3) \"/\"을 기준으로 split하고 ([\"1\", \"4\"]) 0번째 인덱스를 취함\n\n# (4) \"1\"에 int() 형변환 함수를 씌워 int64형으로 변환\n\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      국가(대륙)별\n      상품군별\n      판매유형별\n      기간\n      백만원\n      연도\n      분기\n    \n  \n  \n    \n      0\n      합계\n      합계\n      계\n      2014.1/4\n      148272\n      2014\n      1\n    \n    \n      1\n      합계\n      합계\n      면세점\n      2014.1/4\n      -\n      2014\n      1\n    \n    \n      2\n      합계\n      합계\n      면세점 이외\n      2014.1/4\n      -\n      2014\n      1\n    \n    \n      3\n      합계\n      컴퓨터 및 주변기기\n      계\n      2014.1/4\n      4915\n      2014\n      1\n    \n    \n      4\n      합계\n      컴퓨터 및 주변기기\n      면세점\n      2014.1/4\n      -\n      2014\n      1"
  },
  {
    "objectID": "posts/est/boostcourse/5. K-beauty.html#금액을-수치데이터로-표현하기-위해-데이터-타입-변경하기",
    "href": "posts/est/boostcourse/5. K-beauty.html#금액을-수치데이터로-표현하기-위해-데이터-타입-변경하기",
    "title": "6: K-beauty",
    "section": "금액을 수치데이터로 표현하기 위해 데이터 타입 변경하기",
    "text": "금액을 수치데이터로 표현하기 위해 데이터 타입 변경하기\n\n# - 문자를 결측치로 변경하고 float타입으로 변경하기\ndf[\"백만원\"] = df[\"백만원\"].replace(\"-\",pd.np.nan).astype(float)\ndf[\"백만원\"]\n\nC:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_117660\\99655999.py:2: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n  df[\"백만원\"] = df[\"백만원\"].replace(\"-\",pd.np.nan).astype(float)\n\n\n0        148272.0\n1             NaN\n2             NaN\n3          4915.0\n4             NaN\n           ...   \n10795         0.0\n10796       531.0\n10797      1094.0\n10798         1.0\n10799      1093.0\nName: 백만원, Length: 10800, dtype: float64\n\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      국가(대륙)별\n      상품군별\n      판매유형별\n      기간\n      백만원\n      연도\n      분기\n    \n  \n  \n    \n      0\n      합계\n      합계\n      계\n      2014.1/4\n      148272.0\n      2014\n      1\n    \n    \n      1\n      합계\n      합계\n      면세점\n      2014.1/4\n      NaN\n      2014\n      1\n    \n    \n      2\n      합계\n      합계\n      면세점 이외\n      2014.1/4\n      NaN\n      2014\n      1\n    \n    \n      3\n      합계\n      컴퓨터 및 주변기기\n      계\n      2014.1/4\n      4915.0\n      2014\n      1\n    \n    \n      4\n      합계\n      컴퓨터 및 주변기기\n      면세점\n      2014.1/4\n      NaN\n      2014\n      1"
  },
  {
    "objectID": "posts/est/boostcourse/5. K-beauty.html#필요없는-데이터-제거하기",
    "href": "posts/est/boostcourse/5. K-beauty.html#필요없는-데이터-제거하기",
    "title": "6: K-beauty",
    "section": "필요없는 데이터 제거하기",
    "text": "필요없는 데이터 제거하기\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10800 entries, 0 to 10799\nData columns (total 7 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   국가(대륙)별  10800 non-null  object \n 1   상품군별     10800 non-null  object \n 2   판매유형별    10800 non-null  object \n 3   기간       10800 non-null  object \n 4   백만원      7200 non-null   float64\n 5   연도       10800 non-null  int64  \n 6   분기       10800 non-null  int64  \ndtypes: float64(1), int64(2), object(4)\nmemory usage: 590.8+ KB\n\n\n\n# 합계 데이터는 따로 구할 수 있으므로 전체 데이터에서 제거한다.\n\ndf = df[(df[\"국가(대륙)별\"] != \"합계\") & (df[\"상품군별\"] != \"합계\")].copy()\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 9072 entries, 48 to 10799\nData columns (total 7 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   국가(대륙)별  9072 non-null   object \n 1   상품군별     9072 non-null   object \n 2   판매유형별    9072 non-null   object \n 3   기간       9072 non-null   object \n 4   백만원      6048 non-null   float64\n 5   연도       9072 non-null   int64  \n 6   분기       9072 non-null   int64  \ndtypes: float64(1), int64(2), object(4)\nmemory usage: 567.0+ KB\n\n\n\n# 결측치 보기\ndf.isnull().sum()\n\n국가(대륙)별       0\n상품군별          0\n판매유형별         0\n기간            0\n백만원        3024\n연도            0\n분기            0\ndtype: int64"
  },
  {
    "objectID": "posts/est/boostcourse/5. K-beauty.html#전체-상품군-판매액",
    "href": "posts/est/boostcourse/5. K-beauty.html#전체-상품군-판매액",
    "title": "6: K-beauty",
    "section": "전체 상품군 판매액",
    "text": "전체 상품군 판매액\n\n# 판매유형별 데이터는 일부 기간에는 \"계\"만 존재하기 때문에\n# 판매유형별 == \"계\" 데이터만 가져와서 봐야지\n# 평균값을 구하는 그래프에서 올바른 값을 표현할 수 있다.\n\ndf_total=df[df[\"판매유형별\"] == \"계\"].copy()\ndf_total.head()\n\n\n\n\n\n  \n    \n      \n      국가(대륙)별\n      상품군별\n      판매유형별\n      기간\n      백만원\n      연도\n      분기\n    \n  \n  \n    \n      48\n      미국\n      컴퓨터 및 주변기기\n      계\n      2014.1/4\n      2216.0\n      2014\n      1\n    \n    \n      51\n      미국\n      가전·전자·통신기기\n      계\n      2014.1/4\n      2875.0\n      2014\n      1\n    \n    \n      54\n      미국\n      소프트웨어\n      계\n      2014.1/4\n      47.0\n      2014\n      1\n    \n    \n      57\n      미국\n      서 적\n      계\n      2014.1/4\n      962.0\n      2014\n      1\n    \n    \n      60\n      미국\n      사무·문구\n      계\n      2014.1/4\n      25.0\n      2014\n      1\n    \n  \n\n\n\n\n\n# 연도, 판매액 lineplot으로 그리기\n\nsns.lineplot(data=df_total, x=\"연도\", y=\"백만원\")\n\n<AxesSubplot:xlabel='연도', ylabel='백만원'>\n\n\n\n\n\n\n# 연도, 판매액 lineplot으로 그리고 상품군별로 다른 색상으로 표시하기\nsns.lineplot(data=df_total, x=\"연도\", y=\"백만원\", hue=\"상품군별\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n# 오른쪽으로 범례 옮기는거 \n\n<matplotlib.legend.Legend at 0x15f495c10a0>\n\n\n\n\n\n\n# 위에 그린 그래프를 자세히 보기 위해 서브플롯으로 표시하기\nsns.relplot(data=df_total, x=\"연도\", y=\"백만원\", hue=\"상품군별\", kind=\"line\", col=\"상품군별\", col_wrap=4)\n\n<seaborn.axisgrid.FacetGrid at 0x15f48b4bf70>\n\n\n\n\n\n\n# isin을 사용해 화장품만 제외하고 df_sb이라는 변수에 담기\n\ndf_sub=df_total[~df_total[\"상품군별\"].isin([\"화장품\",\"의류 및 패션 관련상품\"])].copy()\n\n# 앞에 ~ 물결 표시해ㅜㅈ면 화장품만 빼고...\n\n\n# 연도별 판매액을 상품군별로 relplot을 활용해 서브플롯으로 그려보기\n\nsns.relplot(data=df_sub, x=\"연도\", y=\"백만원\", hue=\"상품군별\", col=\"상품군별\", col_wrap=4, kind=\"line\")\n#kind기본값은 scatter\n\n<seaborn.axisgrid.FacetGrid at 0x15f4a3a35e0>"
  },
  {
    "objectID": "posts/est/boostcourse/5. K-beauty.html#화장품의-온라인-쇼핑-해외직접판매액",
    "href": "posts/est/boostcourse/5. K-beauty.html#화장품의-온라인-쇼핑-해외직접판매액",
    "title": "6: K-beauty",
    "section": "화장품의 온라인 쇼핑 해외직접판매액",
    "text": "화장품의 온라인 쇼핑 해외직접판매액\n\n# df_cosmetic이라는 변수에 상품군별이 화장품인 데이터만 가져오기\n\ndf_cosmetic = df_total[df_total[\"상품군별\"]==\"화장품\"].copy()\ndf_cosmetic.head()\n\n\n\n\n\n  \n    \n      \n      국가(대륙)별\n      상품군별\n      판매유형별\n      기간\n      백만원\n      연도\n      분기\n    \n  \n  \n    \n      72\n      미국\n      화장품\n      계\n      2014.1/4\n      3740.0\n      2014\n      1\n    \n    \n      117\n      중국\n      화장품\n      계\n      2014.1/4\n      32235.0\n      2014\n      1\n    \n    \n      162\n      일본\n      화장품\n      계\n      2014.1/4\n      1034.0\n      2014\n      1\n    \n    \n      207\n      아세안(ASEAN)\n      화장품\n      계\n      2014.1/4\n      398.0\n      2014\n      1\n    \n    \n      252\n      유럽연합(EU)\n      화장품\n      계\n      2014.1/4\n      937.0\n      2014\n      1\n    \n  \n\n\n\n\n\ndf_cosmetic[\"상품군별\"].unique()\n\narray(['화장품'], dtype=object)\n\n\n\n# 연도와 판매액을 lineplot으로 그리고 분기별로 다른 색상으로 표현해 보기\nplt.figure(figsize=(15,4))\nsns.lineplot(data=df_cosmetic, x=\"연도\", y=\"백만원\", hue=\"분기\")\n\n<AxesSubplot:xlabel='연도', ylabel='백만원'>\n\n\n\n\n\n\ndf_cosmetic.head()\n\n\n\n\n\n  \n    \n      \n      국가(대륙)별\n      상품군별\n      판매유형별\n      기간\n      백만원\n      연도\n      분기\n    \n  \n  \n    \n      72\n      미국\n      화장품\n      계\n      2014.1/4\n      3740.0\n      2014\n      1\n    \n    \n      117\n      중국\n      화장품\n      계\n      2014.1/4\n      32235.0\n      2014\n      1\n    \n    \n      162\n      일본\n      화장품\n      계\n      2014.1/4\n      1034.0\n      2014\n      1\n    \n    \n      207\n      아세안(ASEAN)\n      화장품\n      계\n      2014.1/4\n      398.0\n      2014\n      1\n    \n    \n      252\n      유럽연합(EU)\n      화장품\n      계\n      2014.1/4\n      937.0\n      2014\n      1\n    \n  \n\n\n\n\n\n# 화장품 판매액에 대한 기간별 금액 데이터 시각화 하기\nplt.figure(figsize=(15,4))\nplt.xticks(rotation=30) #x축 기울기\nsns.lineplot(data=df_cosmetic, x=\"기간\", y=\"백만원\")\n\n<AxesSubplot:xlabel='기간', ylabel='백만원'>\n\n\n\n\n\n\n# 화장품 판매액에 대한 기간별 금액 데이터 시각화하고 \"국가(대륙)별\"로 다른 색상으로 표시하기\n\nplt.figure(figsize=(15,4))\nplt.xticks(rotation=30) #x축 기울기\nsns.lineplot(data=df_cosmetic, x=\"기간\", y=\"백만원\", hue=\"국가(대륙)별\")\n\n<AxesSubplot:xlabel='기간', ylabel='백만원'>\n\n\n\n\n\n\n# 중국빼고 보기\n\nplt.figure(figsize=(15,4))\nplt.xticks(rotation=30) #x축 기울기\nsns.lineplot(data=df_cosmetic[df_cosmetic[\"국가(대륙)별\"]!=\"중국\"], x=\"기간\", y=\"백만원\", hue=\"국가(대륙)별\")\n\n<AxesSubplot:xlabel='기간', ylabel='백만원'>\n\n\n\n\n\n\n# 화장품 판매액에 대한 기간별 금액 데이터를 시각화하고 \"판매유형별\"로 다른색상으로 표현하기\nplt.figure(figsize=(15,4))\nplt.xticks(rotation=30) #x축 기울기\ndf_sub = df[df[\"판매유형별\"] != \"계\"].copy()\nsns.lineplot(data=df_sub, x=\"기간\", y=\"백만원\", hue=\"판매유형별\")\n\n<AxesSubplot:xlabel='기간', ylabel='백만원'>\n\n\n\n\n\n\n# 화장품 판매액에 대한 기간별 금액 데이터를 시각화하고 \"판매유형별\"로 다른색상으로 표현하기\nplt.figure(figsize=(15,4))\nplt.xticks(rotation=30) #x축 기울기\ndf_sub = df[(df[\"판매유형별\"] != \"계\") & (df[\"판매유형별\"]!=\"면세점\")].copy()\nsns.lineplot(data=df_sub, x=\"기간\", y=\"백만원\", hue=\"판매유형별\", ci=None)\n\n<AxesSubplot:xlabel='기간', ylabel='백만원'>"
  },
  {
    "objectID": "posts/est/boostcourse/5. K-beauty.html#의류-및-패션관련-상품-온라인쇼핑-해외직접판매액",
    "href": "posts/est/boostcourse/5. K-beauty.html#의류-및-패션관련-상품-온라인쇼핑-해외직접판매액",
    "title": "6: K-beauty",
    "section": "의류 및 패션관련 상품 온라인쇼핑 해외직접판매액",
    "text": "의류 및 패션관련 상품 온라인쇼핑 해외직접판매액\n\n# df_fashion 이라는 변수에 의류 데이터만 가져와 따로 담아두기\n\n\ndf_fashion = df[(df[\"상품군별\"] == \"의류 및 패션 관련상품\") & (df[\"판매유형별\"]==\"계\")].copy()\ndf_fashion.head()\n\n\n\n\n\n  \n    \n      \n      국가(대륙)별\n      상품군별\n      판매유형별\n      기간\n      백만원\n      연도\n      분기\n    \n  \n  \n    \n      66\n      미국\n      의류 및 패션 관련상품\n      계\n      2014.1/4\n      9810.0\n      2014\n      1\n    \n    \n      111\n      중국\n      의류 및 패션 관련상품\n      계\n      2014.1/4\n      12206.0\n      2014\n      1\n    \n    \n      156\n      일본\n      의류 및 패션 관련상품\n      계\n      2014.1/4\n      13534.0\n      2014\n      1\n    \n    \n      201\n      아세안(ASEAN)\n      의류 및 패션 관련상품\n      계\n      2014.1/4\n      3473.0\n      2014\n      1\n    \n    \n      246\n      유럽연합(EU)\n      의류 및 패션 관련상품\n      계\n      2014.1/4\n      1364.0\n      2014\n      1\n    \n  \n\n\n\n\n\n# 의류 및 패션 관련상품 판매액에 대한 기간별 금액 데이터를 시각화하고\n# 국가별로 다른색상으로 표시하기\n\nplt.figure(figsize=(15,4))\nplt.xticks(rotation=30) #x축 기울기\nsns.lineplot(data=df_fashion, x=\"기간\", y=\"백만원\", hue=\"국가(대륙)별\")\n\n<AxesSubplot:xlabel='기간', ylabel='백만원'>\n\n\n\n\n\n\n# 의류 및 패션관련 상품 판매엑에 대한 기간별 금액 데이터 시각화\n# 판매유형별로 다른 색상 표시\n\ndf_fashion2 = df[(df[\"상품군별\"] == \"의류 및 패션 관련상품\") & (df[\"판매유형별\"] != \"계\")].copy()\n\nplt.figure(figsize=(15, 4))\nplt.xticks(rotation=30)\nsns.lineplot(data=df_fashion2, x=\"기간\", y=\"백만원\", hue=\"판매유형별\", ci=None)\n\n<AxesSubplot:xlabel='기간', ylabel='백만원'>"
  },
  {
    "objectID": "posts/est/boostcourse/5. K-beauty.html#데이터-집계하기",
    "href": "posts/est/boostcourse/5. K-beauty.html#데이터-집계하기",
    "title": "6: K-beauty",
    "section": "데이터 집계하기",
    "text": "데이터 집계하기\n\n# 피봇테이블로 \"국가(대륙)별\", \"연도\"별로 합계 금액을 표 형ㅇ태로 구하기\n\ndf_fashion.pivot_table?\n\n\ndf_fashion[\"판매유형별\"].value_counts()\n\n\ndf_fashion.head()\n\n\n\n\n\n  \n    \n      \n      국가(대륙)별\n      상품군별\n      판매유형별\n      기간\n      백만원\n      연도\n      분기\n    \n  \n  \n    \n      66\n      미국\n      의류 및 패션 관련상품\n      계\n      2014.1/4\n      9810.0\n      2014\n      1\n    \n    \n      111\n      중국\n      의류 및 패션 관련상품\n      계\n      2014.1/4\n      12206.0\n      2014\n      1\n    \n    \n      156\n      일본\n      의류 및 패션 관련상품\n      계\n      2014.1/4\n      13534.0\n      2014\n      1\n    \n    \n      201\n      아세안(ASEAN)\n      의류 및 패션 관련상품\n      계\n      2014.1/4\n      3473.0\n      2014\n      1\n    \n    \n      246\n      유럽연합(EU)\n      의류 및 패션 관련상품\n      계\n      2014.1/4\n      1364.0\n      2014\n      1\n    \n  \n\n\n\n\n\nresult = df_fashion.pivot_table(index=\"국가(대륙)별\", columns=\"연도\", values=\"백만원\", aggfunc=\"sum\")\n# 기본은 평균으로 되어있음.. aggfunc\n\nresult\n\n\n\n\n\n  \n    \n      연도\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      국가(대륙)별\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      기타\n      9683.0\n      7248.0\n      5918.0\n      14387.0\n      23901.0\n      6475.0\n    \n    \n      대양주\n      3392.0\n      2349.0\n      3401.0\n      2266.0\n      2725.0\n      2489.0\n    \n    \n      미국\n      33223.0\n      38066.0\n      48451.0\n      50353.0\n      47875.0\n      55536.0\n    \n    \n      아세안(ASEAN)\n      14936.0\n      19639.0\n      24478.0\n      22671.0\n      23068.0\n      31247.0\n    \n    \n      유럽연합(EU)\n      4485.0\n      3374.0\n      4899.0\n      3736.0\n      4114.0\n      3694.0\n    \n    \n      일본\n      48960.0\n      57594.0\n      79905.0\n      90584.0\n      136800.0\n      134637.0\n    \n    \n      중국\n      57531.0\n      142339.0\n      190932.0\n      225407.0\n      288848.0\n      330267.0\n    \n    \n      중남미\n      975.0\n      616.0\n      649.0\n      762.0\n      576.0\n      544.0\n    \n    \n      중동\n      1172.0\n      1018.0\n      968.0\n      772.0\n      879.0\n      951.0"
  },
  {
    "objectID": "posts/est/boostcourse/5. K-beauty.html#연산결과를-시각적으로-보기",
    "href": "posts/est/boostcourse/5. K-beauty.html#연산결과를-시각적으로-보기",
    "title": "6: K-beauty",
    "section": "연산결과를 시각적으로 보기",
    "text": "연산결과를 시각적으로 보기\n\n# 피봇테이블로 구한 결과를 값의 많고 적음에 따라 시각적으로 표현\n\nplt.figure(figsize=(15,4)\nsns.heatmap(result, cmap=\"Blues\", annot= True, fmt=\".0f\")\n# annot=true 숫자값 표시 \n\n<AxesSubplot:xlabel='연도', ylabel='국가(대륙)별'>\n\n\n\n\n\n\nsns.heatmap(result, cmap=\"Blues_r\")\n# _r 하면 위에랑 반대로.. \n\n<AxesSubplot:xlabel='연도', ylabel='국가(대륙)별'>"
  },
  {
    "objectID": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#데이터-미리보기",
    "href": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#데이터-미리보기",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "데이터 미리보기",
    "text": "데이터 미리보기\n\n# sample, head, tail 통해 데이터 미리보기\ndf.head()\n\n\n\n\n\n  \n    \n      \n      기준년도\n      가입자일련번호\n      성별코드\n      연령대코드(5세단위)\n      시도코드\n      신장(5Cm단위)\n      체중(5Kg 단위)\n      허리둘레\n      시력(좌)\n      시력(우)\n      ...\n      감마지티피\n      흡연상태\n      음주여부\n      구강검진 수검여부\n      치아우식증유무\n      결손치유무\n      치아마모증유무\n      제3대구치(사랑니)이상\n      치석\n      데이터공개일자\n    \n  \n  \n    \n      0\n      2017\n      1\n      1\n      13\n      46\n      170.0\n      65.0\n      91.0\n      1.0\n      1.2\n      ...\n      25.0\n      3.0\n      0.0\n      1\n      NaN\n      NaN\n      NaN\n      NaN\n      1.0\n      20181126\n    \n    \n      1\n      2017\n      2\n      2\n      8\n      41\n      150.0\n      45.0\n      73.4\n      1.2\n      1.0\n      ...\n      10.0\n      1.0\n      0.0\n      1\n      NaN\n      NaN\n      NaN\n      NaN\n      1.0\n      20181126\n    \n    \n      2\n      2017\n      3\n      1\n      8\n      45\n      175.0\n      75.0\n      94.0\n      1.0\n      0.8\n      ...\n      136.0\n      1.0\n      0.0\n      1\n      NaN\n      NaN\n      NaN\n      NaN\n      0.0\n      20181126\n    \n    \n      3\n      2017\n      4\n      2\n      12\n      11\n      155.0\n      55.0\n      67.5\n      0.9\n      1.0\n      ...\n      30.0\n      1.0\n      1.0\n      0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      20181126\n    \n    \n      4\n      2017\n      5\n      1\n      8\n      41\n      175.0\n      75.0\n      93.0\n      1.5\n      1.5\n      ...\n      68.0\n      3.0\n      0.0\n      0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      20181126\n    \n  \n\n5 rows × 34 columns\n\n\n\n\ndf.tail()\n\n\n\n\n\n  \n    \n      \n      기준년도\n      가입자일련번호\n      성별코드\n      연령대코드(5세단위)\n      시도코드\n      신장(5Cm단위)\n      체중(5Kg 단위)\n      허리둘레\n      시력(좌)\n      시력(우)\n      ...\n      감마지티피\n      흡연상태\n      음주여부\n      구강검진 수검여부\n      치아우식증유무\n      결손치유무\n      치아마모증유무\n      제3대구치(사랑니)이상\n      치석\n      데이터공개일자\n    \n  \n  \n    \n      999995\n      2017\n      999996\n      2\n      9\n      41\n      165.0\n      55.0\n      70.0\n      1.5\n      1.5\n      ...\n      11.0\n      1.0\n      1.0\n      0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      20181126\n    \n    \n      999996\n      2017\n      999997\n      2\n      9\n      11\n      165.0\n      50.0\n      68.0\n      1.2\n      1.5\n      ...\n      11.0\n      1.0\n      0.0\n      1\n      NaN\n      NaN\n      NaN\n      NaN\n      0.0\n      20181126\n    \n    \n      999997\n      2017\n      999998\n      2\n      12\n      27\n      155.0\n      50.0\n      83.8\n      0.2\n      1.0\n      ...\n      12.0\n      1.0\n      0.0\n      1\n      NaN\n      NaN\n      NaN\n      NaN\n      0.0\n      20181126\n    \n    \n      999998\n      2017\n      999999\n      1\n      11\n      47\n      160.0\n      70.0\n      99.0\n      0.8\n      0.9\n      ...\n      35.0\n      2.0\n      1.0\n      0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      20181126\n    \n    \n      999999\n      2017\n      1000000\n      2\n      9\n      27\n      165.0\n      60.0\n      74.0\n      1.2\n      1.2\n      ...\n      15.0\n      1.0\n      0.0\n      0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      20181126\n    \n  \n\n5 rows × 34 columns\n\n\n\n\ndf.sample()\n\n\n\n\n\n  \n    \n      \n      기준년도\n      가입자일련번호\n      성별코드\n      연령대코드(5세단위)\n      시도코드\n      신장(5Cm단위)\n      체중(5Kg 단위)\n      허리둘레\n      시력(좌)\n      시력(우)\n      ...\n      감마지티피\n      흡연상태\n      음주여부\n      구강검진 수검여부\n      치아우식증유무\n      결손치유무\n      치아마모증유무\n      제3대구치(사랑니)이상\n      치석\n      데이터공개일자\n    \n  \n  \n    \n      1931\n      2017\n      1932\n      1\n      10\n      43\n      165.0\n      80.0\n      92.0\n      1.2\n      1.0\n      ...\n      104.0\n      1.0\n      1.0\n      1\n      NaN\n      NaN\n      NaN\n      NaN\n      0.0\n      20181126\n    \n  \n\n1 rows × 34 columns"
  },
  {
    "objectID": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#groupby",
    "href": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#groupby",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "groupby",
    "text": "groupby\n\n# 성별코드로 그룹화 한 데이터 세어보기\ndf.groupby?\n\n\ndf.groupby([\"성별코드\"])[\"가입자일련번호\"].count()\n\n성별코드\n1    531172\n2    468828\nName: 가입자일련번호, dtype: int64\n\n\n\n# 성별코드와 음주여부로 그룹화 하고 갯수 세기\ndf.groupby([\"성별코드\", \"음주여부\"])[\"가입자일련번호\"].count()\n\n성별코드  음주여부\n1     0.0     175150\n      1.0     355826\n2     0.0     327579\n      1.0     140920\nName: 가입자일련번호, dtype: int64\n\n\n\n# 성별코드와 음주여부로 그룹화 하고 감마지티피의 평균 구하기\n\ndf.groupby([\"성별코드\", \"음주여부\"])[\"감마지티피\"].mean()\n\n성별코드  음주여부\n1     0.0     34.710544\n      1.0     56.707919\n2     0.0     22.660238\n      1.0     25.115149\nName: 감마지티피, dtype: float64\n\n\n\n# 성별코드와 음주여부로 그룹화를 하고 감마지티피의 요약수치를 구한다.\n\ndf.groupby([\"성별코드\", \"음주여부\"])[\"감마지티피\"].describe()\n\n\n\n\n\n  \n    \n      \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      성별코드\n      음주여부\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      0.0\n      175139.0\n      34.710544\n      37.715218\n      1.0\n      18.0\n      25.0\n      38.0\n      999.0\n    \n    \n      1.0\n      355819.0\n      56.707919\n      69.039084\n      1.0\n      24.0\n      37.0\n      63.0\n      999.0\n    \n    \n      2\n      0.0\n      327559.0\n      22.660238\n      25.181300\n      1.0\n      13.0\n      17.0\n      24.0\n      999.0\n    \n    \n      1.0\n      140913.0\n      25.115149\n      35.870812\n      1.0\n      13.0\n      17.0\n      25.0\n      999.0\n    \n  \n\n\n\n\n\n# agg를 사용하면 여러 수치를 함께 구할 수 있다.\ndf.groupby([\"성별코드\", \"음주여부\"])[\"감마지티피\"].agg([\"count\",\"mean\",\"median\"])\n\n\n\n\n\n  \n    \n      \n      \n      count\n      mean\n      median\n    \n    \n      성별코드\n      음주여부\n      \n      \n      \n    \n  \n  \n    \n      1\n      0.0\n      175139\n      34.710544\n      25.0\n    \n    \n      1.0\n      355819\n      56.707919\n      37.0\n    \n    \n      2\n      0.0\n      327559\n      22.660238\n      17.0\n    \n    \n      1.0\n      140913\n      25.115149\n      17.0"
  },
  {
    "objectID": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#pivot_table",
    "href": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#pivot_table",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "pivot_table",
    "text": "pivot_table\n\n\n# 음주여부에 따른 그룹화된 수 피봇테이블 구하기\ndf.pivot_table(index=\"음주여부\")\n\n\n\n\n\n  \n    \n      \n      (혈청지오티)ALT\n      (혈청지오티)AST\n      HDL콜레스테롤\n      LDL콜레스테롤\n      가입자일련번호\n      감마지티피\n      구강검진 수검여부\n      기준년도\n      데이터공개일자\n      성별코드\n      ...\n      청력(우)\n      청력(좌)\n      체중(5Kg 단위)\n      총콜레스테롤\n      치석\n      트리글리세라이드\n      허리둘레\n      혈색소\n      혈청크레아티닌\n      흡연상태\n    \n    \n      음주여부\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0.0\n      24.107862\n      25.094792\n      56.161852\n      114.467632\n      499800.113284\n      26.858541\n      0.358768\n      2017\n      20181126\n      1.651602\n      ...\n      1.041086\n      1.042881\n      60.082827\n      194.699007\n      0.573111\n      122.063887\n      80.269019\n      13.748950\n      0.837132\n      1.320330\n    \n    \n      1.0\n      27.634991\n      27.069879\n      57.606351\n      111.444394\n      500196.825986\n      47.745678\n      0.439257\n      2017\n      20181126\n      1.283686\n      ...\n      1.020745\n      1.021758\n      66.778226\n      196.346568\n      0.626153\n      144.077696\n      82.484576\n      14.704997\n      0.892460\n      1.896158\n    \n  \n\n2 rows × 29 columns\n\n\n\n\ndf.pivot_table(index=\"음주여부\", values=\"가입자일련번호\", aggfunc=\"count\")\n# mean값이 기본 세팅값이므로 aggfunc로 바꿔주기. \n\n\n\n\n\n  \n    \n      \n      가입자일련번호\n    \n    \n      음주여부\n      \n    \n  \n  \n    \n      0.0\n      502729\n    \n    \n      1.0\n      496746\n    \n  \n\n\n\n\n\ndf.pivot_table(index=\"성별코드\", values=\"가입자일련번호\", aggfunc=\"count\")\n# data frame으로 출력된다! \n\n\n\n\n\n  \n    \n      \n      가입자일련번호\n    \n    \n      성별코드\n      \n    \n  \n  \n    \n      1\n      531172\n    \n    \n      2\n      468828\n    \n  \n\n\n\n\n\n# 음주여부에 따른 감마지티피의 평균 구하기\n\npd.pivot_table(df, index=\"음주여부\", values=\"감마지티피\")\n\n\n\n\n\n  \n    \n      \n      감마지티피\n    \n    \n      음주여부\n      \n    \n  \n  \n    \n      0.0\n      26.858541\n    \n    \n      1.0\n      47.745678\n    \n  \n\n\n\n\n\n# 기본값은 평균을 구하지만 aggfunc를 통해 지정이 가능하다.\n\npd.pivot_table(df, index=\"음주여부\", values=\"감마지티피\", aggfunc=[\"mean\", \"median\"])\n\n\n\n\n\n  \n    \n      \n      mean\n      median\n    \n    \n      \n      감마지티피\n      감마지티피\n    \n    \n      음주여부\n      \n      \n    \n  \n  \n    \n      0.0\n      26.858541\n      19.0\n    \n    \n      1.0\n      47.745678\n      30.0\n    \n  \n\n\n\n\n\n# aggfunc에 describe를 사용해 통계요약값을 볼수있다.\n\npd.pivot_table(df, index=[\"성별코드\", \"음주여부\"], values=\"감마지티피\", aggfunc=\"describe\")\n\n\n\n\n\n  \n    \n      \n      \n      25%\n      50%\n      75%\n      count\n      max\n      mean\n      min\n      std\n    \n    \n      성별코드\n      음주여부\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      0.0\n      18.0\n      25.0\n      38.0\n      175139.0\n      999.0\n      34.710544\n      1.0\n      37.715218\n    \n    \n      1.0\n      24.0\n      37.0\n      63.0\n      355819.0\n      999.0\n      56.707919\n      1.0\n      69.039084\n    \n    \n      2\n      0.0\n      13.0\n      17.0\n      24.0\n      327559.0\n      999.0\n      22.660238\n      1.0\n      25.181300\n    \n    \n      1.0\n      13.0\n      17.0\n      25.0\n      140913.0\n      999.0\n      25.115149\n      1.0\n      35.870812"
  },
  {
    "objectID": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#히스토그램",
    "href": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#히스토그램",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "히스토그램",
    "text": "히스토그램\n\ndf.hist(figsize=(12,12))\n\narray([[<AxesSubplot:title={'center':'기준년도'}>,\n        <AxesSubplot:title={'center':'가입자일련번호'}>,\n        <AxesSubplot:title={'center':'성별코드'}>,\n        <AxesSubplot:title={'center':'연령대코드(5세단위)'}>,\n        <AxesSubplot:title={'center':'시도코드'}>,\n        <AxesSubplot:title={'center':'신장(5Cm단위)'}>],\n       [<AxesSubplot:title={'center':'체중(5Kg 단위)'}>,\n        <AxesSubplot:title={'center':'허리둘레'}>,\n        <AxesSubplot:title={'center':'시력(좌)'}>,\n        <AxesSubplot:title={'center':'시력(우)'}>,\n        <AxesSubplot:title={'center':'청력(좌)'}>,\n        <AxesSubplot:title={'center':'청력(우)'}>],\n       [<AxesSubplot:title={'center':'수축기혈압'}>,\n        <AxesSubplot:title={'center':'이완기혈압'}>,\n        <AxesSubplot:title={'center':'식전혈당(공복혈당)'}>,\n        <AxesSubplot:title={'center':'총콜레스테롤'}>,\n        <AxesSubplot:title={'center':'트리글리세라이드'}>,\n        <AxesSubplot:title={'center':'HDL콜레스테롤'}>],\n       [<AxesSubplot:title={'center':'LDL콜레스테롤'}>,\n        <AxesSubplot:title={'center':'혈색소'}>,\n        <AxesSubplot:title={'center':'요단백'}>,\n        <AxesSubplot:title={'center':'혈청크레아티닌'}>,\n        <AxesSubplot:title={'center':'(혈청지오티)AST'}>,\n        <AxesSubplot:title={'center':'(혈청지오티)ALT'}>],\n       [<AxesSubplot:title={'center':'감마지티피'}>,\n        <AxesSubplot:title={'center':'흡연상태'}>,\n        <AxesSubplot:title={'center':'음주여부'}>,\n        <AxesSubplot:title={'center':'구강검진 수검여부'}>,\n        <AxesSubplot:title={'center':'치아우식증유무'}>,\n        <AxesSubplot:title={'center':'결손치유무'}>],\n       [<AxesSubplot:title={'center':'치아마모증유무'}>,\n        <AxesSubplot:title={'center':'제3대구치(사랑니)이상'}>,\n        <AxesSubplot:title={'center':'치석'}>,\n        <AxesSubplot:title={'center':'데이터공개일자'}>, <AxesSubplot:>,\n        <AxesSubplot:>]], dtype=object)\n\n\n\n\n\n\nh = df.hist(figsize=(12,12))"
  },
  {
    "objectID": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#슬라이싱을-사용해-히스토그램-그래기",
    "href": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#슬라이싱을-사용해-히스토그램-그래기",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "슬라이싱을 사용해 히스토그램 그래기",
    "text": "슬라이싱을 사용해 히스토그램 그래기\n\n# [행, 열]\ndf.iloc[:,:] # 몇번째에 있는 행인지, 컬럼인지 지정 가능\n# [:,:] 전체데이터\n\n\n\n\n\n  \n    \n      \n      기준년도\n      가입자일련번호\n      성별코드\n      연령대코드(5세단위)\n      시도코드\n      신장(5Cm단위)\n      체중(5Kg 단위)\n      허리둘레\n      시력(좌)\n      시력(우)\n      ...\n      감마지티피\n      흡연상태\n      음주여부\n      구강검진 수검여부\n      치아우식증유무\n      결손치유무\n      치아마모증유무\n      제3대구치(사랑니)이상\n      치석\n      데이터공개일자\n    \n  \n  \n    \n      0\n      2017\n      1\n      1\n      13\n      46\n      170.0\n      65.0\n      91.0\n      1.0\n      1.2\n      ...\n      25.0\n      3.0\n      0.0\n      1\n      NaN\n      NaN\n      NaN\n      NaN\n      1.0\n      20181126\n    \n    \n      1\n      2017\n      2\n      2\n      8\n      41\n      150.0\n      45.0\n      73.4\n      1.2\n      1.0\n      ...\n      10.0\n      1.0\n      0.0\n      1\n      NaN\n      NaN\n      NaN\n      NaN\n      1.0\n      20181126\n    \n    \n      2\n      2017\n      3\n      1\n      8\n      45\n      175.0\n      75.0\n      94.0\n      1.0\n      0.8\n      ...\n      136.0\n      1.0\n      0.0\n      1\n      NaN\n      NaN\n      NaN\n      NaN\n      0.0\n      20181126\n    \n    \n      3\n      2017\n      4\n      2\n      12\n      11\n      155.0\n      55.0\n      67.5\n      0.9\n      1.0\n      ...\n      30.0\n      1.0\n      1.0\n      0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      20181126\n    \n    \n      4\n      2017\n      5\n      1\n      8\n      41\n      175.0\n      75.0\n      93.0\n      1.5\n      1.5\n      ...\n      68.0\n      3.0\n      0.0\n      0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      20181126\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      999995\n      2017\n      999996\n      2\n      9\n      41\n      165.0\n      55.0\n      70.0\n      1.5\n      1.5\n      ...\n      11.0\n      1.0\n      1.0\n      0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      20181126\n    \n    \n      999996\n      2017\n      999997\n      2\n      9\n      11\n      165.0\n      50.0\n      68.0\n      1.2\n      1.5\n      ...\n      11.0\n      1.0\n      0.0\n      1\n      NaN\n      NaN\n      NaN\n      NaN\n      0.0\n      20181126\n    \n    \n      999997\n      2017\n      999998\n      2\n      12\n      27\n      155.0\n      50.0\n      83.8\n      0.2\n      1.0\n      ...\n      12.0\n      1.0\n      0.0\n      1\n      NaN\n      NaN\n      NaN\n      NaN\n      0.0\n      20181126\n    \n    \n      999998\n      2017\n      999999\n      1\n      11\n      47\n      160.0\n      70.0\n      99.0\n      0.8\n      0.9\n      ...\n      35.0\n      2.0\n      1.0\n      0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      20181126\n    \n    \n      999999\n      2017\n      1000000\n      2\n      9\n      27\n      165.0\n      60.0\n      74.0\n      1.2\n      1.2\n      ...\n      15.0\n      1.0\n      0.0\n      0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      20181126\n    \n  \n\n1000000 rows × 34 columns\n\n\n\n\ndf.iloc[:,:12].hist(figsize=(12,12))\n\narray([[<AxesSubplot:title={'center':'기준년도'}>,\n        <AxesSubplot:title={'center':'가입자일련번호'}>,\n        <AxesSubplot:title={'center':'성별코드'}>],\n       [<AxesSubplot:title={'center':'연령대코드(5세단위)'}>,\n        <AxesSubplot:title={'center':'시도코드'}>,\n        <AxesSubplot:title={'center':'신장(5Cm단위)'}>],\n       [<AxesSubplot:title={'center':'체중(5Kg 단위)'}>,\n        <AxesSubplot:title={'center':'허리둘레'}>,\n        <AxesSubplot:title={'center':'시력(좌)'}>],\n       [<AxesSubplot:title={'center':'시력(우)'}>,\n        <AxesSubplot:title={'center':'청력(좌)'}>,\n        <AxesSubplot:title={'center':'청력(우)'}>]], dtype=object)\n\n\n\n\n\n\n# 슬라이싱을 사용해 앞에서 12번째부터 23번쨰까지 (12:24) \nh = df.iloc[:,12:24].hist(figsize=(12,12), bins=100)   # bins : 막대 개수를 더 ...  연속된 수치데이터를 카테고리 형태로!!"
  },
  {
    "objectID": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#countplot---음주여부",
    "href": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#countplot---음주여부",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "countplot - 음주여부",
    "text": "countplot - 음주여부\n\n# 음주여부에 따른 countplot을 그린다\ndf[\"음주여부\"].value_counts().plot.bar()\n\n<AxesSubplot:>\n\n\n\n\n\n\nsns.countplot(x=\"음주여부\", data=df)\n\n<AxesSubplot:xlabel='음주여부', ylabel='count'>\n\n\n\n\n\n\nsns.countplot(x=\"흡연상태\", data=df)\n\n<AxesSubplot:xlabel='흡연상태', ylabel='count'>"
  },
  {
    "objectID": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#hue-옵션-사용하기",
    "href": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#hue-옵션-사용하기",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "hue 옵션 사용하기",
    "text": "hue 옵션 사용하기\n\n#window\n# sns.set(font_scale=1.5, font=\"Malgun Gothic\") 이렇게도 사용 가능\n\nsns.countplot(data=df, x=\"음주여부\", hue=\"성별코드\")\n\n<AxesSubplot:xlabel='음주여부', ylabel='count'>\n\n\n\n\n\n\n\nsns.countplot(data=df, x=\"연령대코드(5세단위)\", hue=\"음주여부\") \n\n<AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='count'>"
  },
  {
    "objectID": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#countplot---키와-몸무게",
    "href": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#countplot---키와-몸무게",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "countplot - 키와 몸무게",
    "text": "countplot - 키와 몸무게\n\n키와 몸무게는 연속형 데이터이다.\n하지만 데이터는 키는 5cm, 체중은 5kg 단위로 되어 있다.\n이렇게 특정 범위로 묶게 되면 연속형 데이터라기 보다는 범주형 데이터라고 본다.\n\n\nplt.figure(figsize=(15,4))\nsns.countplot(data=df, x=\"신장(5Cm단위)\")\n\n<AxesSubplot:xlabel='신장(5Cm단위)', ylabel='count'>\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.countplot(data=df, x=\"체중(5Kg 단위)\")\n\n<AxesSubplot:xlabel='체중(5Kg 단위)', ylabel='count'>\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.countplot(data=df, x=\"신장(5Cm단위)\", hue=\"성별코드\")\n\n<AxesSubplot:xlabel='신장(5Cm단위)', ylabel='count'>\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.countplot(data=df, x=\"체중(5Kg 단위)\", hue=\"성별코드\")\n\n<AxesSubplot:xlabel='체중(5Kg 단위)', ylabel='count'>\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.countplot(data=df, x=\"체중(5Kg 단위)\", hue=\"음주여부\")\n\n<AxesSubplot:xlabel='체중(5Kg 단위)', ylabel='count'>"
  },
  {
    "objectID": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#braplot---수치형-vs-범주형",
    "href": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#braplot---수치형-vs-범주형",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "14. 4 braplot - 수치형 vs 범주형",
    "text": "14. 4 braplot - 수치형 vs 범주형\n\n# 연령대코드와 총 콜레스테롤 보기\n# hue로 색상 다르게 표현. 음주여부 같이 보기\n\nsns.barplot(data=df, x=\"연령대코드(5세단위)\", y=\"총콜레스테롤\", hue=\"음주여부\")\n\n# 느리다! countplot와 비교했을때 ! 백만개의 데이터가 있는데.. 연령대 코드별로 총 콜레스테롤을 그리긴 했지만. .\n\n<AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='총콜레스테롤'>\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.barplot(data=df, x=\"연령대코드(5세단위)\", y=\"총콜레스테롤\", hue=\"흡연상태\")\n\n<AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='총콜레스테롤'>\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.barplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"총콜레스테롤\", hue=\"흡연상태\")\n\n<AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='총콜레스테롤'>\n\n\n\n\n\n\n#트리글리세라이드(중성지방)에 따른 연령대코드(5세단위)를 음주여부에 따라 barplot로 그리기\nsns.barplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"트리글리세라이드\", hue=\"음주여부\", ci=95)\n\n# 검은색 막대: 신뢰구간을 의미 (ci: 95)-> 95%의 신뢰구간을 표시한다.\n\n<AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='트리글리세라이드'>\n\n\n\n\n\n\nsns.barplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"트리글리세라이드\", hue=\"음주여부\", ci=\"sd\") #sd:표준편차\n# sample로 그려서 편차가 커보인다. \n\n<AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='트리글리세라이드'>\n\n\n\n\n\n\nsns.barplot(data=df, x=\"연령대코드(5세단위)\", y=\"트리글리세라이드\", hue=\"음주여부\", ci=\"sd\") #sd:표준편차\n\n<AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='트리글리세라이드'>\n\n\n\n\n\n\nsns.barplot(data=df, x=\"연령대코드(5세단위)\", y=\"트리글리세라이드\", hue=\"음주여부\", ci=None) #sd:표준편차\n\n<AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='트리글리세라이드'>\n\n\n\n\n\n\n# 연령대코드와 체중(5kg 단위)을 성별에 따라서.\nsns.barplot(data=df, x=\"연령대코드(5세단위)\", y=\"체중(5Kg 단위)\", hue=\"성별코드\", ci=None) \n\n<AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='체중(5Kg 단위)'>\n\n\n\n\n\n\nsns.barplot(data=df, x=\"연령대코드(5세단위)\", y=\"체중(5Kg 단위)\", hue=\"음주여부\", ci=None)\n\n<AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='체중(5Kg 단위)'>"
  },
  {
    "objectID": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#lineplot-and-pointplot",
    "href": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#lineplot-and-pointplot",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "lineplot and pointplot",
    "text": "lineplot and pointplot\n\nsns.lineplot(data=df, x=\"연령대코드(5세단위)\", y=\"체중(5Kg 단위)\", hue=\"성별코드\", ci=None)\n\n<AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='체중(5Kg 단위)'>\n\n\n\n\n\n\nsns.lineplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"체중(5Kg 단위)\", hue=\"성별코드\")\n# 그림자로 표시!! \n\n<AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='체중(5Kg 단위)'>\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.lineplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"체중(5Kg 단위)\", hue=\"성별코드\", ci=\"sd\")\n# 그림자로 표시!! \n\n<AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='체중(5Kg 단위)'>\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.lineplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"신장(5Cm단위)\", hue=\"성별코드\", ci=\"sd\")\n\n<AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='신장(5Cm단위)'>\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.lineplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"신장(5Cm단위)\", hue=\"음주여부\", ci=\"sd\")\n\n<AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='신장(5Cm단위)'>\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.pointplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"신장(5Cm단위)\", hue=\"음주여부\", ci=\"sd\")\n\n<AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='신장(5Cm단위)'>\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.barplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"신장(5Cm단위)\", hue=\"음주여부\", ci=\"sd\")\nsns.pointplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"신장(5Cm단위)\", hue=\"음주여부\", ci=\"sd\")\n# 두개를 겹쳐서 그릴 수도 있다.\n\n<AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='신장(5Cm단위)'>\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.lineplot(data=df, x=\"연령대코드(5세단위)\", y=\"혈색소\", hue=\"음주여부\", ci=None)\n\n<AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='혈색소'>"
  },
  {
    "objectID": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#boxplot",
    "href": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#boxplot",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "boxplot",
    "text": "boxplot\n\n# boxplot으로 신장에 따른 체중을 그리며, 성별코드에 따른 색상으로 표현하기\nplt.figure(figsize=(15,4))\nsns.boxplot(data=df, x=\"신장(5Cm단위)\", y=\"체중(5Kg 단위)\", hue=\"성별코드\")\n\n<AxesSubplot:xlabel='신장(5Cm단위)', ylabel='체중(5Kg 단위)'>\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.boxplot(data=df, x=\"신장(5Cm단위)\", y=\"체중(5Kg 단위)\", hue=\"음주여부\")\n\n<AxesSubplot:xlabel='신장(5Cm단위)', ylabel='체중(5Kg 단위)'>"
  },
  {
    "objectID": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#violinplot",
    "href": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#violinplot",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "violinplot",
    "text": "violinplot\n\nplt.figure(figsize=(15,4))\nsns.violinplot(data=df, x=\"신장(5Cm단위)\", y=\"체중(5Kg 단위)\")\n\n<AxesSubplot:xlabel='신장(5Cm단위)', ylabel='체중(5Kg 단위)'>\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.violinplot(data=df_sample, x=\"신장(5Cm단위)\", y=\"체중(5Kg 단위)\", hue=\"음주여부\")\n\n<AxesSubplot:xlabel='신장(5Cm단위)', ylabel='체중(5Kg 단위)'>\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.violinplot(data=df_sample, x=\"신장(5Cm단위)\", y=\"체중(5Kg 단위)\", hue=\"음주여부\", split=True)\n# split : 두개의 값을 합쳐서 그림\n\n<AxesSubplot:xlabel='신장(5Cm단위)', ylabel='체중(5Kg 단위)'>\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.violinplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"혈색소\", hue=\"음주여부\", split=True)\n\n<AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='혈색소'>"
  },
  {
    "objectID": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#swarm-plot",
    "href": "posts/est/boostcourse/4. 건강검진 데이터로 가설검정.html#swarm-plot",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "swarm plot",
    "text": "swarm plot\n\n범주형 데이터를 산점도로 시각화하고자 할 대 사용한다.\n\n\nplt.figure(figsize=(15,4))\nsns.swarmplot(data=df_sample, x=\"신장(5Cm단위)\", y=\"체중(5Kg 단위)\", hue=\"음주여부\")\n\nC:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 37.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\nC:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 52.7% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\nC:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 51.9% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\nC:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 55.4% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\nC:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 48.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\nC:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 26.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n<AxesSubplot:xlabel='신장(5Cm단위)', ylabel='체중(5Kg 단위)'>\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.swarmplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"혈색소\", hue=\"음주여부\")\n\nC:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 7.1% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\nC:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 12.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\nC:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 9.3% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\nC:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 11.2% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n<AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='혈색소'>\n\n\n\n\n\n\n# lmplot 으로 그리기\nsns.lmplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"혈색소\", hue=\"음주여부\")\n# 회귀선을 그려서 상관관계를 보여준다.\n\n<seaborn.axisgrid.FacetGrid at 0x294282b0790>\n\n\n\n\n\n\n# lmplot 으로 그리기\nsns.lmplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"혈색소\", hue=\"음주여부\", col=\"성별코드\")\n# 회귀선을 그려서 상관관계를 보여준다.\n# col통해서 여러게 나오게 한다. \n\n<seaborn.axisgrid.FacetGrid at 0x294289fddf0>"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html",
    "href": "posts/est/데이터 개념 공부.html",
    "title": "데이터 개념공부",
    "section": "",
    "text": "자격증 공부하면서 헷갈렸던 개념 공부"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html#k-익명성",
    "href": "posts/est/데이터 개념 공부.html#k-익명성",
    "title": "데이터 개념공부",
    "section": "k-익명성",
    "text": "k-익명성\n\n주어진 데이터 집합에서 같은 값이 적어도 k개 이상 존재\n연결 공격 취약점 방어"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html#l-다양성",
    "href": "posts/est/데이터 개념 공부.html#l-다양성",
    "title": "데이터 개념공부",
    "section": "l-다양성",
    "text": "l-다양성\n\n주어진 데이터 집합에서 비식별 되는 레코드들은 적어도 l개의 서로 다른 민감한 정보를 가짐\nk-익명성에 두가지 취약점인 동질성 공격, 배경 지식에 의한 공격 방어"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html#t-근접성",
    "href": "posts/est/데이터 개념 공부.html#t-근접성",
    "title": "데이터 개념공부",
    "section": "t-근접성",
    "text": "t-근접성\n\n정보의 분포가 t이하의 차이를 보여야 함\nl-다양성의 쏠림 공격, 유사성 공격 보완"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html#m-유일성",
    "href": "posts/est/데이터 개념 공부.html#m-유일성",
    "title": "데이터 개념공부",
    "section": "m-유일성",
    "text": "m-유일성\n\n원본 데이터와 동일한 속성 값의 조합이 비식별 결과 데이터에 최소 m개 이상 존재하여 재식별 가능성 낮춤"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html#분류-모델",
    "href": "posts/est/데이터 개념 공부.html#분류-모델",
    "title": "데이터 개념공부",
    "section": "분류 모델",
    "text": "분류 모델"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html#회귀모델",
    "href": "posts/est/데이터 개념 공부.html#회귀모델",
    "title": "데이터 개념공부",
    "section": "회귀모델",
    "text": "회귀모델"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html#svm",
    "href": "posts/est/데이터 개념 공부.html#svm",
    "title": "데이터 개념공부",
    "section": "SVM",
    "text": "SVM\n\nMargin : 서포트 벡터를 지나는 초평면 사이 거리"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html#군집-분석",
    "href": "posts/est/데이터 개념 공부.html#군집-분석",
    "title": "데이터 개념공부",
    "section": "군집 분석",
    "text": "군집 분석\nex) 미국 주별 강력 범죄율 군집분석: 가까운 거리에 있는 유사 특징을 가진 도시들을 묶어서 보여줌\n\n군집내 유사성, 군집간 상이성"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html#연관성분석",
    "href": "posts/est/데이터 개념 공부.html#연관성분석",
    "title": "데이터 개념공부",
    "section": "연관성분석",
    "text": "연관성분석\n\n유사 개체들을 그룹화하여 각 집단의 특성을 파악하고 사건의 연관규칙을 찾는다.\n\n- 지지도\n- 신뢰도\n- 향상도"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html#인공신경망",
    "href": "posts/est/데이터 개념 공부.html#인공신경망",
    "title": "데이터 개념공부",
    "section": "인공신경망",
    "text": "인공신경망"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html#오토인코더",
    "href": "posts/est/데이터 개념 공부.html#오토인코더",
    "title": "데이터 개념공부",
    "section": "오토인코더",
    "text": "오토인코더\n\n다차원데이터를 저차원, 고차원 데이터로 바꾸면서 특징을 찾는다.\n다차원 데이터를 입력하면 encoder 통해 차원을 줄이는 은닉충으로 이동하고 decoder 통해 차원을 늘리는 출력층으로 내보낸 뒤 출력값을 입력값과 비슷하게 만드는 가중치를 찾는다.\n데이터를 압축하고 배경의 잡음을 억제"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html#gan",
    "href": "posts/est/데이터 개념 공부.html#gan",
    "title": "데이터 개념공부",
    "section": "GAN",
    "text": "GAN\n\n학습 데이터 패턴과 유사한 것을 만드는 생성자(generator) 네트워크와 패턴 진위 여부를 판별하는 판별자(discriminator) 네트워크로 구성\n기계학습에서 교수님이 설명해주신 내용중에.. 그 경찰이랑 범죄자 관련 내용 참고하자"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html#독립변수x연속형-종속변수y연속형",
    "href": "posts/est/데이터 개념 공부.html#독립변수x연속형-종속변수y연속형",
    "title": "데이터 개념공부",
    "section": "독립변수(X:연속형)-종속변수(Y:연속형)",
    "text": "독립변수(X:연속형)-종속변수(Y:연속형)\n\n회귀 분석\n인공신경망 모델\nk-최근접 이웃기법\n의사결정나무(회귀 나무)"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html#독립변수x연속형-종속변수y이산형범주형",
    "href": "posts/est/데이터 개념 공부.html#독립변수x연속형-종속변수y이산형범주형",
    "title": "데이터 개념공부",
    "section": "독립변수(X:연속형)-종속변수(Y:이산형/범주형)",
    "text": "독립변수(X:연속형)-종속변수(Y:이산형/범주형)\n\n로지스틱 회귀 분석\n판별 분석\nK-최근접 이웃기법\n의사결정나무(분류나무)"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html#독립변수x이산형범주형-종속변수y연속형",
    "href": "posts/est/데이터 개념 공부.html#독립변수x이산형범주형-종속변수y연속형",
    "title": "데이터 개념공부",
    "section": "독립변수(X:이산형/범주형)-종속변수(Y:연속형)",
    "text": "독립변수(X:이산형/범주형)-종속변수(Y:연속형)\n\n회귀 분석\n인공신경망 모델\n의사결정나무(회귀 나무)"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html#독립변수x이산형범주형-종속변수y이산형범주형",
    "href": "posts/est/데이터 개념 공부.html#독립변수x이산형범주형-종속변수y이산형범주형",
    "title": "데이터 개념공부",
    "section": "독립변수(X:이산형/범주형)-종속변수(Y:이산형/범주형)",
    "text": "독립변수(X:이산형/범주형)-종속변수(Y:이산형/범주형)\n\n인공신경망 모델\n의사결정나무(분류 나무)\n로지스틱 회귀 분석"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html#필터-기법",
    "href": "posts/est/데이터 개념 공부.html#필터-기법",
    "title": "데이터 개념공부",
    "section": "필터 기법",
    "text": "필터 기법"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html#래퍼-기법",
    "href": "posts/est/데이터 개념 공부.html#래퍼-기법",
    "title": "데이터 개념공부",
    "section": "래퍼 기법",
    "text": "래퍼 기법\n\n예측 정확도 측면에서 가장 좋은 성능을 보이는 하위집합 선택\n그리디 알고리즘(문제를 해결하는 과정에서 그 순간 최적이라고 생각하는 결정)\n전진 선택법, 후진 소거법, 단계적 방법\n변수의 일부를 모델링에 사용하고 그 결과를 확인하는 작업 반복"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html#임베디드-기법",
    "href": "posts/est/데이터 개념 공부.html#임베디드-기법",
    "title": "데이터 개념공부",
    "section": "임베디드 기법",
    "text": "임베디드 기법\n\n모델의 정확도에 기여 하는 변수 학습\n좀 더 적은 계수를 가지는 회귀식 찾기\n\n- 사례\n\n라쏘(LASSO): 가중치의 절댓값의 합을 최소화, L1-norm을 통해 제약을 줌\n릿지(Ridge): 가중치들의 제곱합을 최소화, L2-norm\n엘라스틱넷: 라쏘 + 릿지"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html#자기회귀모형ar",
    "href": "posts/est/데이터 개념 공부.html#자기회귀모형ar",
    "title": "데이터 개념공부",
    "section": "자기회귀모형(AR)",
    "text": "자기회귀모형(AR)\n\n현 시점의 자료가 p시점 전의 유한개의 과거 자료로 설명될 수 있는 모형"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html#이동평균모형ma",
    "href": "posts/est/데이터 개념 공부.html#이동평균모형ma",
    "title": "데이터 개념공부",
    "section": "이동평균모형(MA)",
    "text": "이동평균모형(MA)\n\n시간이 지날수록 관측치의 평균값이 지속적으로 증가하거나 감소하는 시계열 모형\n유한개의 백색잡음의 선형결합으로 정상성 만족"
  },
  {
    "objectID": "posts/est/데이터 개념 공부.html#자기-회귀-누적-이동평균-모형arima",
    "href": "posts/est/데이터 개념 공부.html#자기-회귀-누적-이동평균-모형arima",
    "title": "데이터 개념공부",
    "section": "자기 회귀 누적 이동평균 모형(ARIMA)",
    "text": "자기 회귀 누적 이동평균 모형(ARIMA)\n\n분기/반기/연간 단위로 다음 지표를 예측하거나 주간/월간 단위로 지표 리뷰\n비정상 시계열 모형. 차분이나 변환으로 AR,MA,ARMA모형으로 정상화\nARIMA(p,d,q)\np: AR과 관련, q: MA와 관련, d:ARIMA에서 ARMA로 정상화 할때 차분 횟수\nARIMA(0,0,0): 백색잡음 모형\nARIMA(p,0,0): AR모형\nARIMA(0,0,q): MA모형"
  },
  {
    "objectID": "posts/est/sklearn.html",
    "href": "posts/est/sklearn.html",
    "title": "파이썬:사이킷런",
    "section": "",
    "text": "ref:https://losskatsu.github.io/machine-learning/sklearn/#train-test-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%84%ED%95%A0%ED%95%98%EA%B8%B0\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\n# 참고: 분류용 가상 데이터 만들기\nfrom sklearn.datasets import make_classification\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB #나이브 베이즈\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport matplotlib.pyplot as plt\n\n\n# 분류용 가상 데이터 만들기\nfrom sklearn.datasets import make_classification\n\n\nX, Y = make_classification(n_samples=1000, n_features=4,\n                        n_informative=2, n_redundant=0,\n                        random_state=0, shuffle=False)\n\n\n# n_informative: 종속변수와 상관관계가 존재하는 독립변수 수(default=2)\n# n_redundant: 독립변수끼리 종속관계에 있는 독립변수 수\n\n\nraw = datasets.load_breast_cancer()         ## sklearn에 내장된 원본 데이터 불러오기\nprint(raw.feature_names)                    ## 열(column) 이름 확인\n['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n 'mean smoothness' 'mean compactness' 'mean concavity'\n 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n 'radius error' 'texture error' 'perimeter error' 'area error'\n 'smoothness error' 'compactness error' 'concavity error'\n 'concave points error' 'symmetry error' 'fractal dimension error'\n 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n 'worst smoothness' 'worst compactness' 'worst concavity'\n 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n\ndata = pd.DataFrame(raw.data)               ## 독립변수 데이터 모음  \ntarget = pd.DataFrame(raw.target)           ## 종속변수 데이터 모음\nrawData = pd.concat([data,target], axis=1)  ## 독립변수 + 종속변수 열 결합\n\n## 열(column)이름 설정\nrawData.columns=['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n 'mean smoothness', 'mean compactness', 'mean concavity',\n 'mean concave points', 'mean symmetry', 'mean fractal dimension',\n 'radius error', 'texture error', 'perimeter error', 'area error',\n 'smoothness error', 'compactness error', 'concavity error',\n 'concave points error', 'symmetry error', 'fractal dimension error',\n 'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n 'worst smoothness', 'worst compactness', 'worst concavity',\n 'worst concave points', 'worst symmetry', 'worst fractal dimension'\n , 'cancer']\n\nrawData.head(10)                                ## 데이터 확인 \n\n['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n 'mean smoothness' 'mean compactness' 'mean concavity'\n 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n 'radius error' 'texture error' 'perimeter error' 'area error'\n 'smoothness error' 'compactness error' 'concavity error'\n 'concave points error' 'symmetry error' 'fractal dimension error'\n 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n 'worst smoothness' 'worst compactness' 'worst concavity'\n 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n\n\n\n\n\n\n  \n    \n      \n      mean radius\n      mean texture\n      mean perimeter\n      mean area\n      mean smoothness\n      mean compactness\n      mean concavity\n      mean concave points\n      mean symmetry\n      mean fractal dimension\n      ...\n      worst texture\n      worst perimeter\n      worst area\n      worst smoothness\n      worst compactness\n      worst concavity\n      worst concave points\n      worst symmetry\n      worst fractal dimension\n      cancer\n    \n  \n  \n    \n      0\n      17.99\n      10.38\n      122.80\n      1001.0\n      0.11840\n      0.27760\n      0.30010\n      0.14710\n      0.2419\n      0.07871\n      ...\n      17.33\n      184.60\n      2019.0\n      0.1622\n      0.6656\n      0.7119\n      0.2654\n      0.4601\n      0.11890\n      0\n    \n    \n      1\n      20.57\n      17.77\n      132.90\n      1326.0\n      0.08474\n      0.07864\n      0.08690\n      0.07017\n      0.1812\n      0.05667\n      ...\n      23.41\n      158.80\n      1956.0\n      0.1238\n      0.1866\n      0.2416\n      0.1860\n      0.2750\n      0.08902\n      0\n    \n    \n      2\n      19.69\n      21.25\n      130.00\n      1203.0\n      0.10960\n      0.15990\n      0.19740\n      0.12790\n      0.2069\n      0.05999\n      ...\n      25.53\n      152.50\n      1709.0\n      0.1444\n      0.4245\n      0.4504\n      0.2430\n      0.3613\n      0.08758\n      0\n    \n    \n      3\n      11.42\n      20.38\n      77.58\n      386.1\n      0.14250\n      0.28390\n      0.24140\n      0.10520\n      0.2597\n      0.09744\n      ...\n      26.50\n      98.87\n      567.7\n      0.2098\n      0.8663\n      0.6869\n      0.2575\n      0.6638\n      0.17300\n      0\n    \n    \n      4\n      20.29\n      14.34\n      135.10\n      1297.0\n      0.10030\n      0.13280\n      0.19800\n      0.10430\n      0.1809\n      0.05883\n      ...\n      16.67\n      152.20\n      1575.0\n      0.1374\n      0.2050\n      0.4000\n      0.1625\n      0.2364\n      0.07678\n      0\n    \n    \n      5\n      12.45\n      15.70\n      82.57\n      477.1\n      0.12780\n      0.17000\n      0.15780\n      0.08089\n      0.2087\n      0.07613\n      ...\n      23.75\n      103.40\n      741.6\n      0.1791\n      0.5249\n      0.5355\n      0.1741\n      0.3985\n      0.12440\n      0\n    \n    \n      6\n      18.25\n      19.98\n      119.60\n      1040.0\n      0.09463\n      0.10900\n      0.11270\n      0.07400\n      0.1794\n      0.05742\n      ...\n      27.66\n      153.20\n      1606.0\n      0.1442\n      0.2576\n      0.3784\n      0.1932\n      0.3063\n      0.08368\n      0\n    \n    \n      7\n      13.71\n      20.83\n      90.20\n      577.9\n      0.11890\n      0.16450\n      0.09366\n      0.05985\n      0.2196\n      0.07451\n      ...\n      28.14\n      110.60\n      897.0\n      0.1654\n      0.3682\n      0.2678\n      0.1556\n      0.3196\n      0.11510\n      0\n    \n    \n      8\n      13.00\n      21.82\n      87.50\n      519.8\n      0.12730\n      0.19320\n      0.18590\n      0.09353\n      0.2350\n      0.07389\n      ...\n      30.73\n      106.20\n      739.3\n      0.1703\n      0.5401\n      0.5390\n      0.2060\n      0.4378\n      0.10720\n      0\n    \n    \n      9\n      12.46\n      24.04\n      83.97\n      475.9\n      0.11860\n      0.23960\n      0.22730\n      0.08543\n      0.2030\n      0.08243\n      ...\n      40.68\n      97.65\n      711.4\n      0.1853\n      1.0580\n      1.1050\n      0.2210\n      0.4366\n      0.20750\n      0\n    \n  \n\n10 rows × 31 columns\n\n\n\n\nx = rawData[['mean radius', 'mean texture']]\ny = rawData['cancer']\n\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25, random_state=0)\n\nfit(x_train, y_train)     ## 모수 추정(estimat)\nget_params()              ## 추정된 모수 확인\npredict(x_test)           ## x_test로부터 라벨 예측\npredict_log_proba(x_test) ## 로그 취한 확률 예측\npredict_proba(x_test)     ## 각 라벨로 예측될 확률\nscore(x_test, y_test)     ## 모델 정확도 평가를 위한 mean accuracy\n- 선형회귀\n\nclf = LinearRegression()\nclf.fit(x_train,y_train)  # 모수 추정\nclf.coef_                 # 추정 된 모수 확인(상수항 제외)\nclf.intercept_            # 추정 된 상수항 확인\nclf.predict(x_test)\n#clf.predic(x_test)        # 예측\nclf.score(x_test, y_test) # 모형 성능 평가\n\n0.6092200214592733\n\n\n- 로지스틱\n\nclf = LogisticRegression(solver='lbfgs').fit(x_train,y_train)\nclf.predict(x_test)\nclf.predict_proba(x_test)\nclf.score(x_test,y_test)\n\n0.9020979020979021\n\n\n- 나이브베이즈\n\ngnb = GaussianNB()\ngnb.fit(x_train, y_train)\ngnb.predict(x_test)\ngnb.score(x_test, y_test)\n\n0.8951048951048951\n\n\n- 의사결정나무\n\nclf = tree.DecisionTreeClassifier()\nclf.fit(x_train, y_train)\nclf.predict(x_test)\nclf.predict_proba(x_test)\nclf.score(x_test, y_test)\n\n0.8601398601398601\n\n\n- svm\n\nclf = svm.SVC(kernel='linear')\nclf.fit(x_train, y_train)\nclf.predict(x_test)\nclf.score(x_test, y_test)\n\n0.9020979020979021\n\n\n- 랜덤포레스트\n\nclf = RandomForestClassifier(max_depth=2, random_state=0)\nclf.fit(x_train, y_train)\nclf.feature_importances_\nclf.predict(x_test)\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n       1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n       1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n       0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n       0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n       1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n       1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0])"
  },
  {
    "objectID": "posts/ref/Ref.html",
    "href": "posts/ref/Ref.html",
    "title": "Ref",
    "section": "",
    "text": "- ref: https://3months.tistory.com/392\n\n주피터 랩 단축키\n\n- ref: Markdown 문법\n\n- Quarto 블로그\n\nref: Quarto, Quarto\n블로그 만들기\n\n\n- 라텍스\n\nMathcha(수식편집)\nTables generator(표편집)\nMyscript(손글씨인식)"
  },
  {
    "objectID": "posts/ref/Ref.html#essi-alizadeh",
    "href": "posts/ref/Ref.html#essi-alizadeh",
    "title": "Ref",
    "section": "Essi Alizadeh",
    "text": "Essi Alizadeh"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph3-2.html",
    "href": "posts/Graph Machine Learning/graph3-2.html",
    "title": "CH3. 비지도 그래프 학습(오토인코더)",
    "section": "",
    "text": "그래프 머신러닝\ngithub"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph3-2.html#load-dataset",
    "href": "posts/Graph Machine Learning/graph3-2.html#load-dataset",
    "title": "CH3. 비지도 그래프 학습(오토인코더)",
    "section": "Load Dataset",
    "text": "Load Dataset\n\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import fashion_mnist\n\n2023-04-06 18:27:51.222436: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\n(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n29515/29515 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26421880/26421880 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n5148/5148 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4422102/4422102 [==============================] - 0s 0us/step\n\n\n\nx_train = x_train.astype('float32') / 255. #0~1사이로 정규화 하기 위하여\nx_test = x_test.astype('float32') / 255.\n\nprint (x_train.shape)\nprint (x_test.shape)\n\n(60000, 28, 28)\n(10000, 28, 28)\n\n\n\nfrom matplotlib import pyplot as plt\n\n\nclasses = {\n    0:\"T-shirt/top\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle boot\", \n}\n\n\nn = 10\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # display original\n    ax = plt.subplot(1, n, i + 1)\n    plt.imshow(x_test[i])\n    plt.title(classes[y_test[i]])\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\nplt.show()\n# plt.savefig(\"TrainingSet.png\")"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph3-2.html#create-autoencoder",
    "href": "posts/Graph Machine Learning/graph3-2.html#create-autoencoder",
    "title": "CH3. 비지도 그래프 학습(오토인코더)",
    "section": "Create Autoencoder",
    "text": "Create Autoencoder\n\nfrom tensorflow.keras.layers import Flatten, Conv2D, Dropout, MaxPooling2D, UpSampling2D, Input\n\n\nfrom tensorflow.keras import Model\n\n\n인코더/디코더\n\ninput_img = Input(shape=(28, 28, 1))\n\n#인코딩\nx = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)      # 출력값 28x28x16\nx = MaxPooling2D((2, 2), padding='same')(x)                               # 출력값 14x14x16\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)               # 출력값 14x14x8\nx = MaxPooling2D((2, 2), padding='same')(x)                               # 출력값 7x7x8\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)               # 출력값 7x7x8\nencoded = MaxPooling2D((2, 2), padding='same')(x)                         # 출력값 4x4x8\n\n# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n\n#디코딩\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)         # 출력값 4x4x8\nx = UpSampling2D((2, 2))(x)                                               # 출력값 8x8x8\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(16, (3, 3), activation='relu')(x)\nx = UpSampling2D((2, 2))(x)\ndecoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n\nautoencoder = Model(input_img, decoded)\n\n\nModel(input_img, encoded).summary()\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 28, 28, 1)]       0         \n                                                                 \n conv2d_7 (Conv2D)           (None, 28, 28, 16)        160       \n                                                                 \n max_pooling2d_3 (MaxPooling  (None, 14, 14, 16)       0         \n 2D)                                                             \n                                                                 \n conv2d_8 (Conv2D)           (None, 14, 14, 8)         1160      \n                                                                 \n max_pooling2d_4 (MaxPooling  (None, 7, 7, 8)          0         \n 2D)                                                             \n                                                                 \n conv2d_9 (Conv2D)           (None, 7, 7, 8)           584       \n                                                                 \n max_pooling2d_5 (MaxPooling  (None, 4, 4, 8)          0         \n 2D)                                                             \n                                                                 \n=================================================================\nTotal params: 1,904\nTrainable params: 1,904\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\n손실함수/옵티마이저\n\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n\n\n\n학습\n\nfrom tensorflow.keras.callbacks import TensorBoard\n\n\nautoencoder.fit(x_train, x_train,\n                epochs=50,\n                batch_size=128,\n                shuffle=True,\n                validation_data=(x_test, x_test),\n                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n\nEpoch 1/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0399 - val_loss: 0.0086\nEpoch 2/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0085 - val_loss: 0.0085\nEpoch 3/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0085 - val_loss: 0.0085\nEpoch 4/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0084\nEpoch 5/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0084\nEpoch 6/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0083\nEpoch 7/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0083 - val_loss: 0.0083\nEpoch 8/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0082 - val_loss: 0.0083\nEpoch 9/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0082 - val_loss: 0.0082\nEpoch 10/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0081 - val_loss: 0.0081\nEpoch 11/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0081 - val_loss: 0.0081\nEpoch 12/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0081 - val_loss: 0.0081\nEpoch 13/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0081 - val_loss: 0.0081\nEpoch 14/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 15/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 16/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 17/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 18/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 19/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 20/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 21/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 22/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 23/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 24/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 25/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 26/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 27/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 28/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 29/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 30/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 31/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 32/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 33/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 34/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 35/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 36/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 37/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 38/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 39/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 40/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 41/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 42/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 43/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 44/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 45/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 46/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 47/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 48/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 49/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 50/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\n\n\n<keras.callbacks.History at 0x7f27e4110970>\n\n\n\nautoencoder.save(\"./data/Batch50.p\")\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 8). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ./data/Batch50.p/assets\n\n\nINFO:tensorflow:Assets written to: ./data/Batch50.p/assets\n\n\n\n\n예측\n\n예시1\n\n\nfrom tensorflow.keras.models import load_model\n\n\nautoencoder_first = load_model(\"./data/Batch50.p\")\n\n\ndecoded_imgs = autoencoder_first.predict(x_test)\n\nn = 6\nplt.figure(figsize=(20, 7))\nfor i in range(1, n + 1):\n    # Display original\n    ax = plt.subplot(2, n, i)\n    plt.imshow(x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # Display reconstruction\n    ax = plt.subplot(2, n, i + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)  #X축 숨기기\n    ax.get_yaxis().set_visible(False)\nplt.show()\n\n313/313 [==============================] - 0s 1ms/step\n\n\n\n\n\n\n첫 줄 우너본 이미지, 두번째 줄 재구성된 이미지\n\n\n\n예시2\n\nfrom tensorflow.keras.optimizers import Adam\n\n\nautoencoder.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy')\n\n\nautoencoder.fit(x_train, x_train,\n                epochs=10,\n                batch_size=128,\n                shuffle=True,\n                validation_data=(x_test, x_test),\n                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n\nEpoch 1/10\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0079\nEpoch 2/10\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0079\nEpoch 3/10\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 4/10\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0079\nEpoch 5/10\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0079\nEpoch 6/10\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0079\nEpoch 7/10\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0079\nEpoch 8/10\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0079\nEpoch 9/10\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0079\nEpoch 10/10\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0079\n\n\n<keras.callbacks.History at 0x7f272ba11550>\n\n\nepochs 위에건 50.. 넘 오래걸려서 10개로 줄여보자\n\nautoencoder.save(\"./data/Batch100.p\")\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 8). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ./data/Batch100.p/assets\n\n\nINFO:tensorflow:Assets written to: ./data/Batch100.p/assets\n\n\n\ndecoded_imgs = autoencoder.predict(x_test)\n\nn = 10\nplt.figure(figsize=(20, 4))\nfor i in range(1, n + 1):\n    # Display original\n    ax = plt.subplot(2, n, i)\n    plt.imshow(x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # Display reconstruction\n    ax = plt.subplot(2, n, i + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\nplt.show()\n\n313/313 [==============================] - 1s 2ms/step\n\n\n\n\n\n아담으로 5번 한거랑 우에서 50번한거랑 비슷하게 나왔땅"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph3-2.html#embeddingst-sne사용",
    "href": "posts/Graph Machine Learning/graph3-2.html#embeddingst-sne사용",
    "title": "CH3. 비지도 그래프 학습(오토인코더)",
    "section": "Embeddings(T-SNE사용)",
    "text": "Embeddings(T-SNE사용)\n- T-SNE: 2차원 평면에서 이미지 인코딩된 버전 표현\n\nembeddings = Model(input_img, Flatten()(encoded)).predict(x_test)\n\n313/313 [==============================] - 0s 815us/step\n\n\n\nfrom sklearn.manifold import TSNE\nimport numpy as np\n\n\ntsne = TSNE(n_components=2)\n\n\nemb2d = tsne.fit_transform(embeddings)\n\n\nx,y = np.squeeze(emb2d[:, 0]), np.squeeze(emb2d[:, 1])\n\n\nnp.squeeze(emb2d[:, 0])는 emb2d의 모든 행에서 첫 번째 열만 선택하여 1차원 배열로 만드는 함수. 이를 통해 2차원 배열의 차원을 축소시키고, x좌표 값을 얻을 수 있다.\nnp.squeeze(emb2d[:, 1])는 emb2d의 모든 행에서 두 번째 열만 선택하여 1차원 배열로 만드는 함수. 이를 통해 2차원 배열의 차원을 축소시키고, y좌표 값을 얻을 수 있다.\n\n\nimport pandas as pd\n\n\nfrom matplotlib.cm import tab10\n\n\nsummary =  pd.DataFrame({\"x\": x, \"y\": y, \"target\": y_test, \"size\": 10})\n\nplt.figure(figsize=(10,8))\n\nfor key, sel in summary.groupby(\"target\"):\n    plt.scatter(sel[\"x\"], sel[\"y\"], s=10, color=tab10.colors[key], label=classes[key])\n    \nplt.legend()\nplt.axis(\"off\")\n\n(-99.49505577087402, 96.12302360534667, -95.63964920043945, 102.19923477172851)\n\n\n\n\n\n\n샘플이 속한 클래스에 따라서 색상이 지정\n서로 다른 의류의 클러스터링 명확\n\n\n단점: 학습된 이미지를 정확히 재생성하고 일반화하지 않는 경향"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph3-2.html#denoising",
    "href": "posts/Graph Machine Learning/graph3-2.html#denoising",
    "title": "CH3. 비지도 그래프 학습(오토인코더)",
    "section": "Denoising",
    "text": "Denoising\n- 노이즈 제거 오토인코더 : 다양한 강도의 노이즈를 사용해 손상된 입력에 대해 노이즈 없는 답안 사용\n\nfrom tensorflow.keras.layers import GaussianNoise\n\n\ninput_img = Input(shape=(28, 28, 1))\n\nnoisy_input = GaussianNoise(0.1)(input_img)\n\nx = Conv2D(16, (3, 3), activation='relu', padding='same')(noisy_input)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nencoded = MaxPooling2D((2, 2), padding='same')(x)\n\n# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(16, (3, 3), activation='relu')(x)\nx = UpSampling2D((2, 2))(x)\ndecoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n\nnoisy_autoencoder = Model(input_img, decoded)\n\n\n손상된 입력을 사용해 네트워크 훈련\n출력에는 노이즈 없는 이미지 사용\nGaussianNoise 학습 중에 확률적 노이즈 추가\n\n\nModel(input_img, decoded).summary()\n\nModel: \"model_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 28, 28, 1)]       0         \n                                                                 \n gaussian_noise (GaussianNoi  (None, 28, 28, 1)        0         \n se)                                                             \n                                                                 \n conv2d_14 (Conv2D)          (None, 28, 28, 16)        160       \n                                                                 \n max_pooling2d_6 (MaxPooling  (None, 14, 14, 16)       0         \n 2D)                                                             \n                                                                 \n conv2d_15 (Conv2D)          (None, 14, 14, 8)         1160      \n                                                                 \n max_pooling2d_7 (MaxPooling  (None, 7, 7, 8)          0         \n 2D)                                                             \n                                                                 \n conv2d_16 (Conv2D)          (None, 7, 7, 8)           584       \n                                                                 \n max_pooling2d_8 (MaxPooling  (None, 4, 4, 8)          0         \n 2D)                                                             \n                                                                 \n conv2d_17 (Conv2D)          (None, 4, 4, 8)           584       \n                                                                 \n up_sampling2d_6 (UpSampling  (None, 8, 8, 8)          0         \n 2D)                                                             \n                                                                 \n conv2d_18 (Conv2D)          (None, 8, 8, 8)           584       \n                                                                 \n up_sampling2d_7 (UpSampling  (None, 16, 16, 8)        0         \n 2D)                                                             \n                                                                 \n conv2d_19 (Conv2D)          (None, 14, 14, 16)        1168      \n                                                                 \n up_sampling2d_8 (UpSampling  (None, 28, 28, 16)       0         \n 2D)                                                             \n                                                                 \n conv2d_20 (Conv2D)          (None, 28, 28, 1)         145       \n                                                                 \n=================================================================\nTotal params: 4,385\nTrainable params: 4,385\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nnoisy_autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n\n\nnoisy_autoencoder.fit(x_train, x_train,\n                epochs=50,\n                batch_size=128,\n                shuffle=True,\n                validation_data=(x_test, x_test),\n                callbacks=[TensorBoard(log_dir='/tmp/noisy_autoencoder')])\n\nEpoch 1/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0201\nEpoch 2/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0193\nEpoch 3/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0180\nEpoch 4/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0162\nEpoch 5/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0157\nEpoch 6/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0150\nEpoch 7/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0135\nEpoch 8/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0129\nEpoch 9/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0123\nEpoch 10/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0111\nEpoch 11/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0109\nEpoch 12/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0103\nEpoch 13/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0098\nEpoch 14/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0096\nEpoch 15/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0094\nEpoch 16/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0094\nEpoch 17/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0090\nEpoch 18/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0088\nEpoch 19/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0086\nEpoch 20/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0088\nEpoch 21/50\n469/469 [==============================] - 11s 24ms/step - loss: 0.0084 - val_loss: 0.0087\nEpoch 22/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0087\nEpoch 23/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0086\nEpoch 24/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0086\nEpoch 25/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0086\nEpoch 26/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0086\nEpoch 27/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 28/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 29/50\n469/469 [==============================] - 11s 24ms/step - loss: 0.0084 - val_loss: 0.0086\nEpoch 30/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 31/50\n469/469 [==============================] - 11s 24ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 32/50\n469/469 [==============================] - 11s 24ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 33/50\n469/469 [==============================] - 11s 24ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 34/50\n469/469 [==============================] - 11s 24ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 35/50\n469/469 [==============================] - 11s 24ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 36/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 37/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 38/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 39/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 40/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 41/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 42/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 43/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0084\nEpoch 44/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 45/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0084\nEpoch 46/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 47/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0084\nEpoch 48/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 49/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0084\nEpoch 50/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\n\n\n<keras.callbacks.History at 0x7f27900e2670>\n\n\n\nautoencoder.save(\"./data/DenoisingAutoencoder.p\")\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 8). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ./data/DenoisingAutoencoder.p/assets\n\n\nINFO:tensorflow:Assets written to: ./data/DenoisingAutoencoder.p/assets\n\n\n\nnoise_factor = 0.1\nx_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \nx_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)\n\n\nnp.clip 데이터 0과 1사이의 값으로 클리핑\n\n\ndecoded_imgs = autoencoder.predict(x_test_noisy)\n\ndecoded_imgs_denoised = noisy_autoencoder.predict(x_test_noisy)\n\nn = 6\nplt.figure(figsize=(20, 10))\nfor i in range(1, n + 1):\n    # Display original\n    ax = plt.subplot(3, n, i)\n    plt.imshow(x_test_noisy[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    if i==0:\n        plt.ylabel(\"Original\")\n    else:\n        ax.get_yaxis().set_visible(False)\n        \n    # Display reconstruction\n    ax = plt.subplot(3, n, i + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    if i==0:\n        plt.ylabel(\"Vanilla Autoencoder\")\n    else:\n        ax.get_yaxis().set_visible(False)\n     \n    ax = plt.subplot(3, n, i + 2*n)\n    plt.imshow(decoded_imgs_denoised[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    if i==0:\n        plt.ylabel(\"Denoising Autoencoder\")\n    else:\n        ax.get_yaxis().set_visible(False)\n    \n        \nplt.show()\n\n313/313 [==============================] - 0s 1ms/step\n313/313 [==============================] - 0s 1ms/step\n\n\n\n\n\n\ndecoded_imgs = noisy_autoencoder.predict(x_test_noisy)\n\nn = 10\nplt.figure(figsize=(20, 4))\nfor i in range(1, n + 1):\n    # Display original\n    ax = plt.subplot(2, n, i)\n    plt.imshow(x_test_noisy[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # Display reconstruction\n    ax = plt.subplot(2, n, i + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()\n\n313/313 [==============================] - 0s 1ms/step"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph4-1.html",
    "href": "posts/Graph Machine Learning/graph4-1.html",
    "title": "CH4. 지도 그래프 학습(특징기반방법)",
    "section": "",
    "text": "ref\n\n그래프 머신러닝\ngithub\n\n\n\n특징기반방법(Feature based methods)\n\n설명적인 특징 집합을 특정 출력에 매핑하는 함수 찾기.\n해당 개념을 학습할 만큼 전체를 충분히 대표하도록 주의 깊게 설계.\n평균 차수, 전체 효율성, 특징정인 경로 길이에 의존\n\n\nsellargraph 통해서 데이터셋 로드\n\n\nfrom stellargraph import datasets\nfrom IPython.display import display, HTML\n\ndataset = datasets.PROTEINS()\ndisplay(HTML(dataset.description))\ngraphs, graph_labels = dataset.load()\n\n2023-04-06 21:04:18.948281: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\nEach graph represents a protein and graph labels represent whether they are are enzymes or non-enzymes. The dataset includes 1113 graphs with 39 nodes and 73 edges on average for each graph. Graph nodes have 4 attributes (including a one-hot encoding of their label), and each graph is labelled as belonging to 1 of 2 classes.\n\n\n\nstellargraph형식에서 networkx 형식으로 그래프 변환\n\n\nstellargraph 표현에서 numpy 인접행렬로 그래프 변환\n인접행렬을 사용해 networkx 표현으로 돌리기\n\n\n# tellargraph 형태에서 numpy인접행렬로 변환\nadjs = [graph.to_adjacency_matrix().A for graph in graphs]\n\n# Pandas.Series로 구성된 라벨을 numpy array로 변환\nlabels = graph_labels.to_numpy(dtype=int)\n\n\n각 그래프에 대해 설명하기 위해 전역 측정 지표 계산\n\n\n간선수, 평균 클러스터 계수, 전역 효율성 선택\n\n\nimport numpy as np\nimport networkx as nx\n\nmetrics = []\nfor adj in adjs:\n  G = nx.from_numpy_matrix(adj)\n\n  # 기본 속성\n  num_edges = G.number_of_edges()\n\n  # 클러스터링 방법\n  cc = nx.average_clustering(G)\n\n  # 효율성 측정\n  eff = nx.global_efficiency(G)\n\n  metrics.append([num_edges, cc, eff])\n\n\nscikit-learn 유틸리티를 활용해 훈련 및 테스트 세트를 생성\n\n데이터셋의 70% 훈련, 30% 테스트\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(metrics, labels, test_size=0.3, random_state=42)\n\n\n머신러닝 알고리즘 학습 시작\n\nscikit-learn의 SVC모듈 사용\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nclf = svm.SVC()\nclf.fit(X_train_scaled, y_train)\n\ny_pred = clf.predict(X_test_scaled)\n\nprint('Accuracy', accuracy_score(y_test,y_pred))\nprint('Precision', precision_score(y_test,y_pred))\nprint('Recall', recall_score(y_test,y_pred))\nprint('F1-score', f1_score(y_test,y_pred))\n\nAccuracy 0.7455089820359282\nPrecision 0.7709251101321586\nRecall 0.8413461538461539\nF1-score 0.8045977011494253"
  },
  {
    "objectID": "posts/Graph Machine Learning/9999.html",
    "href": "posts/Graph Machine Learning/9999.html",
    "title": "coco",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport networkx as nx\nimport sklearn\n\n# split \nfrom sklearn.model_selection import train_test_split\n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n# models \nfrom sklearn.ensemble import RandomForestClassifier \n\n# 평가 \nfrom sklearn import metrics \n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x>0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\ndef down_sample_textbook(df):\n    df = df[df[\"is_fraud\"]==0].sample(frac=0.20, random_state=42).append(df[df[\"is_fraud\"] == 1])\n    df_majority = df[df.is_fraud==0]\n    df_minority = df[df.is_fraud==1]\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef split(Graph,test_size=0.20,random_state=42):\n    edg = list(range(len(Graph.edges))) \n    edg_att = list(nx.get_edge_attributes(Graph, \"label\").values())\n    return train_test_split(edg,edg_att,test_size=test_size,random_state=random_state) \n\ndef embedding(Graph):\n    _edgs = list(Graph.edges)\n    _train_edges, _test_edges, y, yy = split(Graph)\n    _train_graph = Graph.edge_subgraph([_edgs[x] for x in _train_edges]).copy()\n    _train_graph.add_nodes_from(list(set(Graph.nodes) - set(_train_graph.nodes)))\n    _embedded = AverageEmbedder(Node2Vec(_train_graph, weight_key='weight').fit(window=10).wv)\n    X = [_embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in _train_edges]\n    XX = [_embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in _test_edges]\n    return X,XX,y,yy \n\ndef evaluate(lrnr,XX,yy):\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n                  'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n                  'f1':[sklearn.metrics.f1_score(yy,yyhat)]})\n    return df \n\ndef anal(df,n_estimators=10):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=n_estimators, random_state=42) \n    lrnr.fit(X,y)\n    return lrnr, XX,yy, evaluate(lrnr,XX,yy)\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")"
  },
  {
    "objectID": "posts/Graph Machine Learning/9999.html#read-and-define-data",
    "href": "posts/Graph Machine Learning/9999.html#read-and-define-data",
    "title": "coco",
    "section": "read and define data",
    "text": "read and define data\n\ndf = pd.read_csv(\"~/Desktop/fraudTrain.csv\")\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      trans_date_trans_time\n      cc_num\n      merchant\n      category\n      amt\n      first\n      last\n      gender\n      street\n      ...\n      lat\n      long\n      city_pop\n      job\n      dob\n      trans_num\n      unix_time\n      merch_lat\n      merch_long\n      is_fraud\n    \n  \n  \n    \n      669418\n      669418\n      2019-10-12 18:21\n      4.089100e+18\n      fraud_Haley, Jewess and Bechtelar\n      shopping_pos\n      7.53\n      Debra\n      Stark\n      F\n      686 Linda Rest\n      ...\n      32.3836\n      -94.8653\n      24536\n      Multimedia programmer\n      1983-10-14\n      d313353fa30233e5fab5468e852d22fc\n      1350066071\n      32.202008\n      -94.371865\n      0\n    \n    \n      32567\n      32567\n      2019-01-20 13:06\n      4.247920e+12\n      fraud_Turner LLC\n      travel\n      3.79\n      Judith\n      Moss\n      F\n      46297 Benjamin Plains Suite 703\n      ...\n      39.5370\n      -83.4550\n      22305\n      Television floor manager\n      1939-03-09\n      88c65b4e1585934d578511e627fe3589\n      1327064760\n      39.156673\n      -82.930503\n      0\n    \n    \n      156587\n      156587\n      2019-03-24 18:09\n      4.026220e+12\n      fraud_Klein Group\n      entertainment\n      59.07\n      Debbie\n      Payne\n      F\n      204 Ashley Neck Apt. 169\n      ...\n      41.5224\n      -71.9934\n      4720\n      Broadcast presenter\n      1977-05-18\n      3bd9ede04b5c093143d5e5292940b670\n      1332612553\n      41.657152\n      -72.595751\n      0\n    \n    \n      1020243\n      1020243\n      2020-02-25 15:12\n      4.957920e+12\n      fraud_Monahan-Morar\n      personal_care\n      25.58\n      Alan\n      Parsons\n      M\n      0547 Russell Ford Suite 574\n      ...\n      39.6171\n      -102.4776\n      207\n      Network engineer\n      1955-12-04\n      19e16ee7a01d229e750359098365e321\n      1361805120\n      39.080346\n      -103.213452\n      0\n    \n    \n      116272\n      116272\n      2019-03-06 23:19\n      4.178100e+15\n      fraud_Kozey-Kuhlman\n      personal_care\n      84.96\n      Jill\n      Flores\n      F\n      639 Cruz Islands\n      ...\n      41.9488\n      -86.4913\n      3104\n      Horticulturist, commercial\n      1981-03-29\n      a0c8641ca1f5d6e243ed5a2246e66176\n      1331075954\n      42.502065\n      -86.732664\n      0\n    \n  \n\n5 rows × 23 columns\n\n\n\n\n# df_downsampled = down_sample_textbook(df)"
  },
  {
    "objectID": "posts/Graph Machine Learning/9999.html#embedding",
    "href": "posts/Graph Machine Learning/9999.html#embedding",
    "title": "coco",
    "section": "embedding",
    "text": "embedding\n\n#G_down = build_graph_bipartite(df_downsampled)\n\n\n# X,XX,y,yy = embedding(G_down)"
  },
  {
    "objectID": "posts/Graph Machine Learning/9999.html#learn",
    "href": "posts/Graph Machine Learning/9999.html#learn",
    "title": "coco",
    "section": "learn",
    "text": "learn\n\n# lrnr = RandomForestClassifier(n_estimators=10, random_state=42) \n# lrnr.fit(X,y)"
  },
  {
    "objectID": "posts/Graph Machine Learning/9999.html#evaluate",
    "href": "posts/Graph Machine Learning/9999.html#evaluate",
    "title": "coco",
    "section": "evaluate",
    "text": "evaluate\n\n# evaluate(lrnr,XX,yy)"
  },
  {
    "objectID": "posts/Graph Machine Learning/9999.html#read-and-define-data-1",
    "href": "posts/Graph Machine Learning/9999.html#read-and-define-data-1",
    "title": "coco",
    "section": "read and define data",
    "text": "read and define data\n\ndf = pd.read_csv(\"~/Desktop/fraudTrain.csv\")\nlrnr2, _,_,_ = anal(down_sample_textbook(our_sampling1(df)),n_estimators=100)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00<?, ?it/s]\n\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:03<00:00,  3.01it/s]"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph basic.html",
    "href": "posts/Graph Machine Learning/graph basic.html",
    "title": "CH1. graph basic",
    "section": "",
    "text": "그래프 머신러닝\ngithub"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph basic.html#간선가중그래프-gvew",
    "href": "posts/Graph Machine Learning/graph basic.html#간선가중그래프-gvew",
    "title": "CH1. graph basic",
    "section": "간선가중그래프 \\(G=(V,E,w)\\)",
    "text": "간선가중그래프 \\(G=(V,E,w)\\)\n\n\\(V\\): 노드의 집합, \\(E\\): 간선의 집합, \\(w: E \\to \\mathbb{R}\\): 각 간선을 실수 \\(e \\in E\\) 가중값으로 대중시키는 가중함수"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph basic.html#노드가중그래프-gvew",
    "href": "posts/Graph Machine Learning/graph basic.html#노드가중그래프-gvew",
    "title": "CH1. graph basic",
    "section": "노드가중그래프 \\(G=(V,E,w)\\)",
    "text": "노드가중그래프 \\(G=(V,E,w)\\)\n\n\\(V\\): 노드의 집합, \\(E\\): 간선의 집합, \\(w: V \\to \\mathbb{R}\\): 각 간선을 실수 \\(v \\in V\\) 가중값으로 대중시키는 가중함수"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph basic.html#예제k-means",
    "href": "posts/Graph Machine Learning/graph basic.html#예제k-means",
    "title": "CH1. graph basic",
    "section": "예제(k-means)",
    "text": "예제(k-means)\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_iris\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# 데이터셋 로드\niris = load_iris()\ndf = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n\n\n# 모델 생성 및 학습\nkmeans = KMeans(n_clusters=3, random_state=0)\nkmeans.fit(df)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\nKMeans(n_clusters=3, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=3, random_state=0)\n\n\n\n# 클러스터링 결과 시각화\ndf['cluster'] = kmeans.labels_\nplt.scatter(df['petal length (cm)'], df['petal width (cm)'], c=df['cluster'])\nplt.xlabel('petal length (cm)')\nplt.ylabel('petal width (cm)')\nplt.show()\n\n\n\n\n- ref: [출처] 클러스터링(Clustering)이란? 클러스터링 특징, 종류, 예제 실습|작성자 리르시"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph3-3.html",
    "href": "posts/Graph Machine Learning/graph3-3.html",
    "title": "CH3. 비지도 그래프 학습(그래프신경망)",
    "section": "",
    "text": "그래프 머신러닝\ngithub"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph3-3.html#unsupervised-graph-representation-learning-using-graph-convnet",
    "href": "posts/Graph Machine Learning/graph3-3.html#unsupervised-graph-representation-learning-using-graph-convnet",
    "title": "CH3. 비지도 그래프 학습(그래프신경망)",
    "section": "Unsupervised graph representation learning using Graph ConvNet",
    "text": "Unsupervised graph representation learning using Graph ConvNet\n\n스펙트럼 그래프 합성곱\n\n- 키프와 웰링이 제안한 정규화\n\\[H^t=\\sigma(\\hat D^{-\\dfrac{1}{2}} \\hat A \\hat D^{-\\dfrac{1}{2}}XW)\\]\n\n\\(\\hat D\\)는 \\(\\hat A\\)의 대각 노드 차수 행렬\n\n\n#from networkx import karate_club_graph, to_numpy_matrix\nimport numpy as np\nimport networkx as nx\nfrom scipy.linalg import sqrtm\nimport matplotlib.pyplot as plt\n\nG = nx.barbell_graph(m1=10, m2=4)\n\norder = np.arange(G.number_of_nodes())\nA = nx.to_numpy_matrix(G, nodelist=order)\nI = np.eye(G.number_of_nodes())\n\n- 자체 루프 추가, 대각 노드 차수 행렬 준비\n\nnp.random.seed(7)\n\nA_hat = A + np.eye(G.number_of_nodes()) # add self-connections  # G의 노드 수만큼 단위 행렬 만들기\n\nD_hat = np.array(np.sum(A_hat, axis=0))[0]\nD_hat = np.array(np.diag(D_hat))\nD_hat = np.linalg.inv(sqrtm(D_hat))\n# D_hat은 A_hat의 각 열의 합으로 이루어진 대각행렬의 역행렬\n# 그래프 각 노드의 연결 강도를 정규화\n\nA_hat = D_hat @ A_hat @ D_hat\n#A_hat을 정규화된 그래프 연결 행렬로 \n\n- 2개의 레이어로 구성된 GCN(그래프합성곱신경망)만들기\n\n가중치 \\(W\\)\n\n\ndef glorot_init(nin, nout):  # 각  GCN의 가중치 행렬 난수로 초기화\n  sd = np.sqrt(6.0 / (nin + nout))\n  return np.random.uniform(-sd, sd, size=(nin, nout))\n\nclass GCNLayer():\n  def __init__(self, n_inputs, n_outputs):\n      self.n_inputs = n_inputs\n      self.n_outputs = n_outputs\n      self.W = glorot_init(self.n_outputs, self.n_inputs)\n      self.activation = np.tanh\n      \n  def forward(self, A, X): #인접행렬과 피처행렬을 이용해 그래프 신경망을 순방향으로 계산. 결과 반환\n      self._X = (A @ X).T # (N,N)*(N,n_outputs) ==> (n_outputs,N)\n      H = self.W @ self._X # (N, D)*(D, n_outputs) => (N, n_outputs)\n      H = self.activation(H)\n      return H.T # (n_outputs, N)\n\n- 네트워크 만들고 순방향 패스 계산. 네트워크를 통해 신호 전파\n\n\ngcn1 = GCNLayer(G.number_of_nodes(), 8)\ngcn2 = GCNLayer(8, 4)\ngcn3 = GCNLayer(4, 2)\n\nH1 = gcn1.forward(A_hat, I)   #중간 출력 \nH2 = gcn2.forward(A_hat, H1)  #중간 출력\nH3 = gcn3.forward(A_hat, H2)  #최종 결과\n\nembeddings = H3  # 최종 결과\n\n\nembeddings\n\nmatrix([[-0.01476752, -0.05053743],\n        [-0.01476752, -0.05053743],\n        [-0.01476752, -0.05053743],\n        [-0.01476752, -0.05053743],\n        [-0.01476752, -0.05053743],\n        [-0.01476752, -0.05053743],\n        [-0.01476752, -0.05053743],\n        [-0.01476752, -0.05053743],\n        [-0.01476752, -0.05053743],\n        [-0.03171266, -0.05642689],\n        [-0.08356075, -0.03146726],\n        [-0.11696591, -0.00297877],\n        [-0.10436451,  0.0226648 ],\n        [-0.06008126,  0.01901998],\n        [-0.00104536, -0.05409599],\n        [ 0.01041858, -0.06209606],\n        [ 0.01041858, -0.06209606],\n        [ 0.01041858, -0.06209606],\n        [ 0.01041858, -0.06209606],\n        [ 0.01041858, -0.06209606],\n        [ 0.01041858, -0.06209606],\n        [ 0.01041858, -0.06209606],\n        [ 0.01041858, -0.06209606],\n        [ 0.01041858, -0.06209606]])\n\n\n\nfrom gem.embedding.gf import GraphFactorization\n\n\ndef draw_graph(G, filename=None, node_size=50):\n  pos_nodes = nx.spring_layout(G)\n  nx.draw_networkx(G, pos_nodes, with_labels=False, node_size=node_size, edge_color='gray')\n  \n  pos_attrs = {}\n  for node, coords in pos_nodes.items():\n    pos_attrs[node] = (coords[0], coords[1] + 0.08)\n\n  plt.axis('off')\n  axis = plt.gca()\n  axis.set_xlim([1.2*x for x in axis.get_xlim()])\n  axis.set_ylim([1.2*y for y in axis.get_ylim()])\n\nembeddings = np.array(embeddings)\ndraw_graph(G)\n\n\n\n\n\nplt.scatter(embeddings[:, 0], embeddings[:, 1])\nplt.savefig('embedding_gcn.png',dpi=300)"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph3-3.html#unsupervised-gcn-training-using-similarity-graph-distance",
    "href": "posts/Graph Machine Learning/graph3-3.html#unsupervised-gcn-training-using-similarity-graph-distance",
    "title": "CH3. 비지도 그래프 학습(그래프신경망)",
    "section": "Unsupervised GCN training using similarity graph distance",
    "text": "Unsupervised GCN training using similarity graph distance\n\nStellarGraph 사용\n대상 변수 없이 비지도 방식으로 벡터 삽입하는 방법\n\n\n!pip install -q stellargraph[demos]==1.2.1\n\n\npip install chardet\n\nCollecting chardet\n  Downloading chardet-5.1.0-py3-none-any.whl (199 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.1/199.1 kB 10.0 MB/s eta 0:00:00\nInstalling collected packages: chardet\nSuccessfully installed chardet-5.1.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport pandas as pd\nimport numpy as np\nimport networkx as nx\nimport os\n\nimport stellargraph as sg\nfrom stellargraph.mapper import FullBatchNodeGenerator\nfrom stellargraph.layer import GCN\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, optimizers, losses, metrics, Model\nfrom sklearn import preprocessing, model_selection\nfrom IPython.display import display, HTML\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n- 데이터셋: PROTENIS\n\ndataset = sg.datasets.PROTEINS()\ndisplay(HTML(dataset.description))\ngraphs, graph_labels = dataset.load()\n\nEach graph represents a protein and graph labels represent whether they are are enzymes or non-enzymes. The dataset includes 1113 graphs with 39 nodes and 73 edges on average for each graph. Graph nodes have 4 attributes (including a one-hot encoding of their label), and each graph is labelled as belonging to 1 of 2 classes.\n\n\n\n39개 노드, 73개 간선이 있는 1,114개 그래프로 구성\n각 노드는 4개의 속성으로 설명되며 두 클래스 중 하나에 속한다.\n\n\n# let's print some info to better understand the dataset\nprint(graphs[0].info())\ngraph_labels.value_counts().to_frame()\n\nStellarGraph: Undirected multigraph\n Nodes: 42, Edges: 162\n\n Node types:\n  default: [42]\n    Features: float32 vector, length 4\n    Edge types: default-default->default\n\n Edge types:\n    default-default->default: [162]\n        Weights: all 1 (default)\n        Features: none\n\n\n\n\n\n\n  \n    \n      \n      label\n    \n  \n  \n    \n      1\n      663\n    \n    \n      2\n      450\n    \n  \n\n\n\n\n- 모델 만들기\n\n# TODO\ngenerator = sg.mapper.PaddedGraphGenerator(graphs)\n\n\n# 64,32 사이즈의 레이어 2개를 포함한 GCN model 정의\n# ReLU 활성화 함수는 레이어 간 비선형을 추가하고자 사용\n\ngc_model = sg.layer.GCNSupervisedGraphClassification(\n    [64, 32], [\"relu\", \"relu\"], generator, pool_all_layers=True\n)\n\n\n# 다음 레이어에 연결할 수 있도록 GC 레이어의 입력 및 출력 텐서 확인\n\n\ninp1, out1 = gc_model.in_out_tensors()\ninp2, out2 = gc_model.in_out_tensors()\n\nvec_distance = tf.norm(out1 - out2, axis=1)\n\n\n# 모델 생성, 임베딩을 찾기 쉽게 반사 모델 생성\npair_model = Model(inp1 + inp2, vec_distance)\nembedding_model = Model(inp1, out1)\n\n- 훈련\n\n입력 그래프의 각 쌍에 유사성 점수 할당\n단순화를 위해 그래프의 라플라시안 스펙트럼 사이의 거리 사용\n\n\ndef graph_distance(graph1, graph2):\n    spec1 = nx.laplacian_spectrum(graph1.to_networkx(feature_attr=None))\n    spec2 = nx.laplacian_spectrum(graph2.to_networkx(feature_attr=None))\n    k = min(len(spec1), len(spec2))\n    return np.linalg.norm(spec1[:k] - spec2[:k])\n\n\ngraph_idx = np.random.RandomState(0).randint(len(graphs), size=(100, 2))\ntargets = [graph_distance(graphs[left], graphs[right]) for left, right in graph_idx]\ntrain_gen = generator.flow(graph_idx, batch_size=10, targets=targets)\n\n<class 'networkx.utils.decorators.argmap'> compilation 20:4: FutureWarning: laplacian_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n\n\n- 모델 학습\n\npair_model.compile(optimizers.Adam(1e-2), loss=\"mse\")\n\n\nhistory = pair_model.fit(train_gen, epochs=500, verbose=0)\nsg.utils.plot_history(history)\n\n\n\n\n- 시각화\n\n# 임베딩 검색\n\nembeddings = embedding_model.predict(generator.flow(graphs))\n\n1113/1113 [==============================] - 1s 531us/step\n\n\n\n# 차원 축소를 위해 TSNE 사용\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(2)\ntwo_d = tsne.fit_transform(embeddings)\n\n\n출력이 32차원이므로 임베딩을 2차원 공간에 플로팅해 임베딩 정상적 평가\n\n\nplt.scatter(two_d[:, 0], two_d[:, 1], c=graph_labels.cat.codes, cmap=\"jet\", alpha=0.4)\nplt.savefig('embedding_TSNE.png',dpi=300)\n\n\n\n\n\n빨간색=1, 파란색=0"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph8(logistic-amt+time+citypop+lat+merchlat).html",
    "href": "posts/Graph Machine Learning/graph8(logistic-amt+time+citypop+lat+merchlat).html",
    "title": "CH8. 신용카드 거래 분석(로지스틱amt+time+city_pop+lat+merch_lat-f1:0.985323)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n_df = pd.read_csv(\"fraudTrain.csv\")\n\n\ncus_list = set(_df.query('is_fraud==1').cc_num.tolist())\n_df2 = _df.query(\"cc_num in @ cus_list\")\n_df2 = _df2.assign(time= list(map(lambda x: int(x.split(' ')[-1].split(':')[0]), _df2['trans_date_trans_time'])))\n\n\n_df2.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 651430 entries, 3 to 1048574\nData columns (total 24 columns):\n #   Column                 Non-Null Count   Dtype  \n---  ------                 --------------   -----  \n 0   Unnamed: 0             651430 non-null  int64  \n 1   trans_date_trans_time  651430 non-null  object \n 2   cc_num                 651430 non-null  float64\n 3   merchant               651430 non-null  object \n 4   category               651430 non-null  object \n 5   amt                    651430 non-null  float64\n 6   first                  651430 non-null  object \n 7   last                   651430 non-null  object \n 8   gender                 651430 non-null  object \n 9   street                 651430 non-null  object \n 10  city                   651430 non-null  object \n 11  state                  651430 non-null  object \n 12  zip                    651430 non-null  int64  \n 13  lat                    651430 non-null  float64\n 14  long                   651430 non-null  float64\n 15  city_pop               651430 non-null  int64  \n 16  job                    651430 non-null  object \n 17  dob                    651430 non-null  object \n 18  trans_num              651430 non-null  object \n 19  unix_time              651430 non-null  int64  \n 20  merch_lat              651430 non-null  float64\n 21  merch_long             651430 non-null  float64\n 22  is_fraud               651430 non-null  int64  \n 23  time                   651430 non-null  int64  \ndtypes: float64(6), int64(6), object(12)\nmemory usage: 124.3+ MB\n\n\nmerch_lat과 merch_long 은 상점의 위도 경도, 위의 lat과 long은 고객의 ??\ndob는 생년월일(date of birth)을 나타내는 변수\nunix_time 1970년 1월 1일 0시 0분 0초(UTC)부터 경과된 시간을 초(second) 단위로 표현하는 방법\nzip 우편번호\n`\n\n_df2[\"is_fraud\"].value_counts()\n\n0    645424\n1      6006\nName: is_fraud, dtype: int64\n\n\n\n_df2[\"is_fraud\"].value_counts()/len(_df2)\n\n0    0.99078\n1    0.00922\nName: is_fraud, dtype: float64\n\n\n\ntype(_df2)\n\npandas.core.frame.DataFrame\n\n\n\ndiff = _df2['lat'] - _df2['merch_lat']\nlatabs=abs(diff)\nprint(\"lat:\",abs(diff).mean())\ndiff2 = _df2['long'] - _df2['merch_long']\nlongabs=abs(diff2)\nprint(\"long:\",abs(diff2).mean())\n\nlat: 0.5002190204058765\nlong: 0.5004574515650185\n\n\n\n_df2 = _df2.assign(latabs=abs(_df2['lat'] - _df2['merch_lat']))\n\n\n_df2 = _df2.assign(longabs=abs(_df2['long'] - _df2['merch_long']))\n\n\n_df2.groupby(by=['is_fraud']).agg({'city_pop':np.mean,'amt':np.mean,'time':np.mean,'latabs':np.mean, 'longabs':np.mean})\n\n\n\n\n\n  \n    \n      \n      city_pop\n      amt\n      time\n      latabs\n      longabs\n    \n    \n      is_fraud\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      83870.443845\n      67.743047\n      12.813152\n      0.500202\n      0.500468\n    \n    \n      1\n      96323.951715\n      530.573492\n      13.915917\n      0.502055\n      0.499343\n    \n  \n\n\n\n\n\n_df3=_df2[['amt','time','city_pop','lat','merch_lat','is_fraud']]\n\n\ndata=np.hstack([_df3.values[:,:]])\n\n\ndata\n\narray([[4.5000000e+01, 0.0000000e+00, 1.9390000e+03, 4.6230600e+01,\n        4.7034331e+01, 0.0000000e+00],\n       [9.4630000e+01, 0.0000000e+00, 2.1580000e+03, 4.0375000e+01,\n        4.0653382e+01, 0.0000000e+00],\n       [4.4540000e+01, 0.0000000e+00, 2.6910000e+03, 3.7993100e+01,\n        3.7162705e+01, 0.0000000e+00],\n       ...,\n       [6.0300000e+00, 1.6000000e+01, 5.2000000e+02, 4.2193900e+01,\n        4.2633354e+01, 0.0000000e+00],\n       [1.1694000e+02, 1.6000000e+01, 1.5830000e+03, 4.1182600e+01,\n        4.1400318e+01, 0.0000000e+00],\n       [6.8100000e+00, 1.6000000e+01, 1.6555600e+05, 3.4077000e+01,\n        3.3601468e+01, 0.0000000e+00]])\n\n\n\nX = data[:,:-1]\ny = data[:,-1]\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n\n\nlr = LogisticRegression()\n\n\nlr.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\ny_pred=lr.predict(X_test)\n\n\nacc= accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1score = f1_score(y_test, y_pred, average='weighted')\nprint(\"Accuracy: {}\".format(acc))\nprint(\"Precision: {}\".format(precision))\nprint(\"Recall: {}\".format(recall))\nprint(\"F1 score: {}\".format(f1score))\n\nAccuracy: 0.9893541900127412\nPrecision: 0.9815505737898247\nRecall: 0.9893541900127412\nF1 score: 0.9853239587931848\n\n\n\nacc= accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nf1score = f1_score(y_test, y_pred, average='macro')\nprint(\"Accuracy: {}\".format(acc))\nprint(\"Precision:{}\".format(precision))\nprint(\"Recall: {}\".format(recall))\nprint(\"F1 score: {}\".format(f1score))\n\nAccuracy: 0.9893541900127412\nPrecision:0.5132529996838914\nRecall: 0.5018112950402922\nF1 score: 0.5016129588437686\n\n\n\n간단하게 생각해보면, 고객의 lat과 상점의 lat의 차이가 크다.. 그러면 사기거래일 가능성이 클 거 같은 느낌.?"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph8(logistic, amt+time).html",
    "href": "posts/Graph Machine Learning/graph8(logistic, amt+time).html",
    "title": "CH8. 신용카드 거래 분석(로지스틱amt+time-f1:0.009370)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n_df = pd.read_csv(\"fraudTrain.csv\")\n\n\ncus_list = set(_df.query('is_fraud==1').cc_num.tolist())\n_df2 = _df.query(\"cc_num in @ cus_list\")\n_df2 = _df2.assign(time= list(map(lambda x: int(x.split(' ')[-1].split(':')[0]), _df2['trans_date_trans_time'])))\n\n\n_df2.shape\n\n(651430, 24)\n\n\n\n_df2.columns\n\nIndex(['Unnamed: 0', 'trans_date_trans_time', 'cc_num', 'merchant', 'category',\n       'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip',\n       'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time',\n       'merch_lat', 'merch_long', 'is_fraud', 'time'],\n      dtype='object')\n\n\n\n_df2.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 651430 entries, 3 to 1048574\nData columns (total 24 columns):\n #   Column                 Non-Null Count   Dtype  \n---  ------                 --------------   -----  \n 0   Unnamed: 0             651430 non-null  int64  \n 1   trans_date_trans_time  651430 non-null  object \n 2   cc_num                 651430 non-null  float64\n 3   merchant               651430 non-null  object \n 4   category               651430 non-null  object \n 5   amt                    651430 non-null  float64\n 6   first                  651430 non-null  object \n 7   last                   651430 non-null  object \n 8   gender                 651430 non-null  object \n 9   street                 651430 non-null  object \n 10  city                   651430 non-null  object \n 11  state                  651430 non-null  object \n 12  zip                    651430 non-null  int64  \n 13  lat                    651430 non-null  float64\n 14  long                   651430 non-null  float64\n 15  city_pop               651430 non-null  int64  \n 16  job                    651430 non-null  object \n 17  dob                    651430 non-null  object \n 18  trans_num              651430 non-null  object \n 19  unix_time              651430 non-null  int64  \n 20  merch_lat              651430 non-null  float64\n 21  merch_long             651430 non-null  float64\n 22  is_fraud               651430 non-null  int64  \n 23  time                   651430 non-null  int64  \ndtypes: float64(6), int64(6), object(12)\nmemory usage: 124.3+ MB\n\n\n\n_df2[\"is_fraud\"].value_counts()\n\n0    645424\n1      6006\nName: is_fraud, dtype: int64\n\n\n\n_df2[\"is_fraud\"].value_counts()/len(_df2)\n\n0    0.99078\n1    0.00922\nName: is_fraud, dtype: float64\n\n\n\n_df2.groupby(by=['is_fraud']).agg({'city_pop':np.mean,'amt':np.mean,'time':np.mean})\n\n\n\n\n\n  \n    \n      \n      city_pop\n      amt\n      time\n    \n    \n      is_fraud\n      \n      \n      \n    \n  \n  \n    \n      0\n      83870.443845\n      67.743047\n      12.813152\n    \n    \n      1\n      96323.951715\n      530.573492\n      13.915917\n    \n  \n\n\n\n\n\n_df2.groupby(by=['category']).agg({'is_fraud':np.mean})\n\n\n\n\n\n  \n    \n      \n      is_fraud\n    \n    \n      category\n      \n    \n  \n  \n    \n      entertainment\n      0.003907\n    \n    \n      food_dining\n      0.002628\n    \n    \n      gas_transport\n      0.007570\n    \n    \n      grocery_net\n      0.004802\n    \n    \n      grocery_pos\n      0.022539\n    \n    \n      health_fitness\n      0.002408\n    \n    \n      home\n      0.002488\n    \n    \n      kids_pets\n      0.003440\n    \n    \n      misc_net\n      0.023023\n    \n    \n      misc_pos\n      0.004859\n    \n    \n      personal_care\n      0.003774\n    \n    \n      shopping_net\n      0.027628\n    \n    \n      shopping_pos\n      0.011342\n    \n    \n      travel\n      0.004886\n    \n  \n\n\n\n\n\n_df2.groupby(by=['time']).agg({'is_fraud':np.mean}).plot()\n\n<Axes: xlabel='time'>\n\n\n\n\n\n\n그래프상 시간을 3등분 하거나 2등분 해서 적합시키면 좋을 거 같다.\n\n3등분: 20 ~ 04, 04 ~ 12, 12 ~ 20\n\n\n2등분: 06 ~ 18, 18 ~ 06\n\n사기거래와 사기거래가 아닌 그룹에서 데이터 범주가 차이가 나는걸 보면\n금액, 시간..\n\n\n_df3=_df2[['amt','time','category','is_fraud']]\n_df3\n\n\n\n\n\n  \n    \n      \n      amt\n      time\n      category\n      is_fraud\n    \n  \n  \n    \n      3\n      45.00\n      0\n      gas_transport\n      0\n    \n    \n      5\n      94.63\n      0\n      gas_transport\n      0\n    \n    \n      6\n      44.54\n      0\n      grocery_net\n      0\n    \n    \n      7\n      71.65\n      0\n      gas_transport\n      0\n    \n    \n      8\n      4.27\n      0\n      misc_pos\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1048567\n      39.96\n      16\n      kids_pets\n      0\n    \n    \n      1048568\n      20.67\n      16\n      entertainment\n      0\n    \n    \n      1048569\n      6.03\n      16\n      food_dining\n      0\n    \n    \n      1048571\n      116.94\n      16\n      misc_pos\n      0\n    \n    \n      1048574\n      6.81\n      16\n      misc_pos\n      0\n    \n  \n\n651430 rows × 4 columns\n\n\n\n\n_df3.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 651430 entries, 3 to 1048574\nData columns (total 4 columns):\n #   Column    Non-Null Count   Dtype  \n---  ------    --------------   -----  \n 0   amt       651430 non-null  float64\n 1   time      651430 non-null  int64  \n 2   category  651430 non-null  object \n 3   is_fraud  651430 non-null  int64  \ndtypes: float64(1), int64(2), object(1)\nmemory usage: 24.9+ MB\n\n\n\n_df4=_df2[['amt','time','is_fraud']]\n_df4\n\n\n\n\n\n  \n    \n      \n      amt\n      time\n      is_fraud\n    \n  \n  \n    \n      3\n      45.00\n      0\n      0\n    \n    \n      5\n      94.63\n      0\n      0\n    \n    \n      6\n      44.54\n      0\n      0\n    \n    \n      7\n      71.65\n      0\n      0\n    \n    \n      8\n      4.27\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      1048567\n      39.96\n      16\n      0\n    \n    \n      1048568\n      20.67\n      16\n      0\n    \n    \n      1048569\n      6.03\n      16\n      0\n    \n    \n      1048571\n      116.94\n      16\n      0\n    \n    \n      1048574\n      6.81\n      16\n      0\n    \n  \n\n651430 rows × 3 columns\n\n\n\n\ndata=np.hstack([_df4.values[:,:]])\n\n\ndata\n\narray([[ 45.  ,   0.  ,   0.  ],\n       [ 94.63,   0.  ,   0.  ],\n       [ 44.54,   0.  ,   0.  ],\n       ...,\n       [  6.03,  16.  ,   0.  ],\n       [116.94,  16.  ,   0.  ],\n       [  6.81,  16.  ,   0.  ]])\n\n\n\nX = data[:,:-1]\ny = data[:,-1]\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n\n\nlr = LogisticRegression()\n\n\nlr.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\ny_pred=lr.predict(X_test)\n\n\nacc= accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1score = f1_score(y_test, y_pred, average='weighted')\nprint(\"Accuracy: {:.6f}\".format(acc))\nprint(\"Precision: {:.6f}\".format(precision))\nprint(\"Recall: {:.6f}\".format(recall))\nprint(\"F1 score: {:.6f}\".format(f1score))\n\nAccuracy: 0.051349\nPrecision: 0.005162\nRecall: 0.051349\nF1 score: 0.009370\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\nacc= accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nf1score = f1_score(y_test, y_pred, average='macro')\nprint(\"Accuracy: {}\".format(acc))\nprint(\"Precision:{}\".format(precision))\nprint(\"Recall: {}\".format(recall))\nprint(\"F1 score: {}\".format(f1score))\n\nAccuracy: 0.051348571604009643\nPrecision:0.004232713983377072\nRecall: 0.04223025149824091\nF1 score: 0.007685871263604649\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\nacc= accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='micro')\nrecall = recall_score(y_test, y_pred, average='micro')\nf1score = f1_score(y_test, y_pred, average='micro')\nprint(\"Accuracy: {}\".format(acc))\nprint(\"Precision:{}\".format(precision))\nprint(\"Recall: {}\".format(recall))\nprint(\"F1 score: {}\".format(f1score))\n\nAccuracy: 0.051348571604009643\nPrecision:0.051348571604009643\nRecall: 0.051348571604009643\nF1 score: 0.051348571604009643\n\n\n- average 매개 변수\n\nNone: 클래스별 metric 값을 계산\nmicro: 모든 샘플을 하나의 그룹으로 취급하여 metric 값을 계산\nmacro: 클래스별 metric 값을 동일한 가중치로 더하여 산술평균을 계산\nweighted: 클래스별 metric 값을 라벨 수로 가중 평균하여 계산\nsamples: 샘플마다 metric 값을 계산"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph8(logistic-amt+time+lat+merchlat).html",
    "href": "posts/Graph Machine Learning/graph8(logistic-amt+time+lat+merchlat).html",
    "title": "CH8. 신용카드 거래 분석(로지스틱amt+time+lat+merch_lat-f1:0.98538)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n_df = pd.read_csv(\"fraudTrain.csv\")\n\n\ncus_list = set(_df.query('is_fraud==1').cc_num.tolist())\n_df2 = _df.query(\"cc_num in @ cus_list\")\n_df2 = _df2.assign(time= list(map(lambda x: int(x.split(' ')[-1].split(':')[0]), _df2['trans_date_trans_time'])))\n\n\n_df2.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 651430 entries, 3 to 1048574\nData columns (total 24 columns):\n #   Column                 Non-Null Count   Dtype  \n---  ------                 --------------   -----  \n 0   Unnamed: 0             651430 non-null  int64  \n 1   trans_date_trans_time  651430 non-null  object \n 2   cc_num                 651430 non-null  float64\n 3   merchant               651430 non-null  object \n 4   category               651430 non-null  object \n 5   amt                    651430 non-null  float64\n 6   first                  651430 non-null  object \n 7   last                   651430 non-null  object \n 8   gender                 651430 non-null  object \n 9   street                 651430 non-null  object \n 10  city                   651430 non-null  object \n 11  state                  651430 non-null  object \n 12  zip                    651430 non-null  int64  \n 13  lat                    651430 non-null  float64\n 14  long                   651430 non-null  float64\n 15  city_pop               651430 non-null  int64  \n 16  job                    651430 non-null  object \n 17  dob                    651430 non-null  object \n 18  trans_num              651430 non-null  object \n 19  unix_time              651430 non-null  int64  \n 20  merch_lat              651430 non-null  float64\n 21  merch_long             651430 non-null  float64\n 22  is_fraud               651430 non-null  int64  \n 23  time                   651430 non-null  int64  \ndtypes: float64(6), int64(6), object(12)\nmemory usage: 124.3+ MB\n\n\nmerch_lat과 merch_long 은 상점의 위도 경도, 위의 lat과 long은 고객의 ??\ndob는 생년월일(date of birth)을 나타내는 변수\nunix_time 1970년 1월 1일 0시 0분 0초(UTC)부터 경과된 시간을 초(second) 단위로 표현하는 방법\nzip 우편번호\n`\n\n_df2[\"is_fraud\"].value_counts()\n\n0    645424\n1      6006\nName: is_fraud, dtype: int64\n\n\n\n_df2[\"is_fraud\"].value_counts()/len(_df2)\n\n0    0.99078\n1    0.00922\nName: is_fraud, dtype: float64\n\n\n\ntype(_df2)\n\npandas.core.frame.DataFrame\n\n\n\ndiff = _df2['lat'] - _df2['merch_lat']\nlatabs=abs(diff)\nprint(\"lat:\",abs(diff).mean())\ndiff2 = _df2['long'] - _df2['merch_long']\nlongabs=abs(diff2)\nprint(\"long:\",abs(diff2).mean())\n\nlat: 0.5002190204058765\nlong: 0.5004574515650185\n\n\n\n_df2 = _df2.assign(latabs=abs(_df2['lat'] - _df2['merch_lat']))\n\n\n_df2 = _df2.assign(longabs=abs(_df2['long'] - _df2['merch_long']))\n\n\n_df2.groupby(by=['is_fraud']).agg({'city_pop':np.mean,'amt':np.mean,'time':np.mean,'latabs':np.mean, 'longabs':np.mean})\n\n\n\n\n\n  \n    \n      \n      city_pop\n      amt\n      time\n      latabs\n      longabs\n    \n    \n      is_fraud\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      83870.443845\n      67.743047\n      12.813152\n      0.500202\n      0.500468\n    \n    \n      1\n      96323.951715\n      530.573492\n      13.915917\n      0.502055\n      0.499343\n    \n  \n\n\n\n\n\n_df3=_df2[['amt','time','lat','merch_lat','is_fraud']]\n\n\ndata=np.hstack([_df3.values[:,:]])\n\n\ndata\n\narray([[ 45.      ,   0.      ,  46.2306  ,  47.034331,   0.      ],\n       [ 94.63    ,   0.      ,  40.375   ,  40.653382,   0.      ],\n       [ 44.54    ,   0.      ,  37.9931  ,  37.162705,   0.      ],\n       ...,\n       [  6.03    ,  16.      ,  42.1939  ,  42.633354,   0.      ],\n       [116.94    ,  16.      ,  41.1826  ,  41.400318,   0.      ],\n       [  6.81    ,  16.      ,  34.077   ,  33.601468,   0.      ]])\n\n\n\nX = data[:,:-1]\ny = data[:,-1]\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n\n\nlr = LogisticRegression()\n\n\nlr.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\ny_pred=lr.predict(X_test)\n\n\nacc= accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1score = f1_score(y_test, y_pred, average='weighted')\nprint(\"Accuracy: {}\".format(acc))\nprint(\"Precision: {}\".format(precision))\nprint(\"Recall: {}\".format(recall))\nprint(\"F1 score: {}\".format(f1score))\n\nAccuracy: 0.9891776553121594\nPrecision: 0.9810129371194015\nRecall: 0.9891776553121594\nF1 score: 0.9850783784050309\n\n\n\nacc= accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nf1score = f1_score(y_test, y_pred, average='macro')\nprint(\"Accuracy: {}\".format(acc))\nprint(\"Precision:{}\".format(precision))\nprint(\"Recall: {}\".format(recall))\nprint(\"F1 score: {}\".format(f1score))\n\nAccuracy: 0.9891776553121594\nPrecision:0.4952274089672451\nRecall: 0.49934905923560957\nF1 score: 0.4972796937822675\n\n\n\n간단하게 생각해보면, 고객의 lat과 상점의 lat의 차이가 크다.. 그러면 사기거래일 가능성이 클 거 같은 느낌.?"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph3-1.html",
    "href": "posts/Graph Machine Learning/graph3-1.html",
    "title": "CH3. 비지도 그래프 학습(얕은 임베딩 방법)",
    "section": "",
    "text": "그래프 머신러닝\ngithub\n비지도 알고리즘은 스스로 클러스터를 찾고, 패턴 발견, 이상치 감지"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph3-1.html#그래프-분해graph-factorization",
    "href": "posts/Graph Machine Learning/graph3-1.html#그래프-분해graph-factorization",
    "title": "CH3. 비지도 그래프 학습(얕은 임베딩 방법)",
    "section": "그래프 분해(Graph Factorization)",
    "text": "그래프 분해(Graph Factorization)\n\n그래프의 인접 행렬을 분해하여 노드 및 엣지에 대한 임베딩을 생성\n행렬 분해 알고리즘을 사용하며, 높은 차원의 임베딩 생성 가능\n단점으로는 계산 복잡도가 높고, 대규모 그래프에서는 적용이 어려울 수 있음\n\n\nimport networkx as nx\n\nG = nx.barbell_graph(m1=3, m2=2)\ndraw_graph(G)\n\n\n\n\n\nfrom pathlib import Path\nPath(\"gem/intermediate\").mkdir(parents=True, exist_ok=True)\n\n\nfrom gem.embedding.gf import GraphFactorization\n\nG = nx.barbell_graph(m1=10, m2=4)\ndraw_graph(G)\n\ngf = GraphFactorization(d=2,  data_set=None,max_iter=10000, eta=1*10**-4, regu=1.0)  #2차원 임베딩 공간 생성\ngf.learn_embedding(G)\n\n\n\n\n./gf not found. Reverting to Python implementation. Please compile gf, place node2vec in the path and grant executable permission\n\n\narray([[-0.00023347,  0.00423037],\n       [-0.00023539,  0.00422975],\n       [-0.00022958,  0.00424174],\n       [-0.00024031,  0.0042126 ],\n       [-0.00021981,  0.00425632],\n       [-0.0002757 ,  0.00433198],\n       [-0.00043729,  0.00388277],\n       [-0.00120009,  0.00441975],\n       [ 0.00012455,  0.00342726],\n       [ 0.00703928,  0.00360693],\n       [ 0.00545482, -0.00073236],\n       [-0.00191172, -0.00167922],\n       [-0.00762759, -0.00011384],\n       [-0.001844  ,  0.00466985],\n       [ 0.00523278,  0.01142738],\n       [ 0.0052346 ,  0.01142813],\n       [ 0.00523662,  0.0114319 ],\n       [ 0.00523082,  0.01144355],\n       [ 0.00519996,  0.01138897],\n       [ 0.00519592,  0.01136481],\n       [ 0.00511132,  0.01157818],\n       [ 0.0049727 ,  0.01154795],\n       [ 0.00689695,  0.01202013],\n       [ 0.00920799,  0.0153358 ]])\n\n\n\nmax_iter: 최대 반복 횟수\neta: 학습률\nregu: 정규화 계수\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor x in G.nodes():\n    \n    v = gf.get_embedding()[x]\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=12)"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph3-1.html#고차-근접-보존-임베딩hope",
    "href": "posts/Graph Machine Learning/graph3-1.html#고차-근접-보존-임베딩hope",
    "title": "CH3. 비지도 그래프 학습(얕은 임베딩 방법)",
    "section": "고차 근접 보존 임베딩(HOPE)",
    "text": "고차 근접 보존 임베딩(HOPE)\n\n고차 근접성 유지, 임베딩 대칭 속성 강제X\n그래프의 2차원 행렬을 생성하여 노드 및 엣지에 대한 임베딩을 생성\n그래프의 고차원 구조를 보존하기 위해 그래프의 라플라시안 행렬을 이용하여 행렬 생성\n계산이 간단하고, 다양한 유형의 그래프에 적용 가능\n\n\nimport networkx as nx\nfrom gem.embedding.hope import HOPE\n\nG = nx.barbell_graph(m1=10, m2=4)  #바벨 그래프 생성\ndraw_graph(G)\n\nhp = HOPE(d=4, beta=0.01)\nhp.learn_embedding(G)\n\n\n\n\nSVD error (low rank): 0.052092\n\n\narray([[-0.07024409, -0.07024348, -0.07024409, -0.07024348],\n       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],\n       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],\n       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],\n       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],\n       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],\n       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],\n       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],\n       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],\n       [-0.07104037, -0.07104201, -0.07104037, -0.07104201],\n       [-0.00797181, -0.00799433, -0.00797181, -0.00799433],\n       [-0.00079628, -0.00099787, -0.00079628, -0.00099787],\n       [ 0.00079628, -0.00099787,  0.00079628, -0.00099787],\n       [ 0.00797181, -0.00799433,  0.00797181, -0.00799433],\n       [ 0.07104037, -0.07104201,  0.07104037, -0.07104201],\n       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],\n       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],\n       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],\n       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],\n       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],\n       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],\n       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],\n       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],\n       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348]])\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor x in G.nodes():\n    \n    v = hp.get_embedding()[x,2:]\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=20)\n\n\n\n\n\n방향이 없으므로 원천 노드와 대상 노드 간에 차이가 없다."
  },
  {
    "objectID": "posts/Graph Machine Learning/graph3-1.html#전역-구조-정보를-통한-그래프-표현graphrep",
    "href": "posts/Graph Machine Learning/graph3-1.html#전역-구조-정보를-통한-그래프-표현graphrep",
    "title": "CH3. 비지도 그래프 학습(얕은 임베딩 방법)",
    "section": "전역 구조 정보를 통한 그래프 표현(GraphRep)",
    "text": "전역 구조 정보를 통한 그래프 표현(GraphRep)\n\n그래프를 분해하여 노드 및 엣지의 임베딩을 생성\n그래프의 구조 정보와 노드의 속성 정보를 동시에 고려하여 임베딩 생성\n다양한 유형의 그래프에 적용 가능하며, 다른 방법들에 비해 높은 임베딩 품질을 보장\n\n\nimport networkx as nx\nfrom karateclub.node_embedding.neighbourhood.grarep import GraRep\n\nG = nx.barbell_graph(m1=10, m2=4)\ndraw_graph(G)\n\ngr = GraRep(dimensions=2,order=3)  #dimension:임베딩 공간 차수, order:노드사이의 최대 근접 차수\ngr.fit(G)\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nida = 4 #4번째와 5번째\nidb = 5\nfor x in G.nodes():\n    \n    v = gr.get_embedding()[x]\n    ax.scatter(v[ida],v[idb], s=1000)\n    ax.annotate(str(x), (v[ida],v[idb]), fontsize=12)"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph3-1.html#deepwalk",
    "href": "posts/Graph Machine Learning/graph3-1.html#deepwalk",
    "title": "CH3. 비지도 그래프 학습(얕은 임베딩 방법)",
    "section": "DeepWalk",
    "text": "DeepWalk\n\n노드 간의 구조적 유사성을 학습하여 노드를 저차원 벡터로 표현\n노드가 깊이 우선 탐색(DFS) 방식으로 샘플링된 문맥을 윈도우로 사용해 노드의 임베딩 학습\n무방향성 그래프에서 랜덤 워크를 수행(무작위 경로 생성)하여, 노드의 이웃 노드를 샘플링하고 이를 바탕으로 임베딩 생성\n이웃 노드를 샘플링할 때, 유사한 패턴의 노드를 더 많이 샘플링하여 군집 구조 정보를 잘 반영\n계산이 간단하고, 다양한 유형의 그래프에 적용 가능\n\n- 그래프 생성 알고리즘\n1 랜덤 워크 생성: 입력 그래프 \\(G\\)의 각 노드에 대해 고정된 최대 길이(\\(t\\))를 갖는 랜덤 워크 세트 계산\n2 skip-gram학습\n3 임베딩 생성\n\nimport networkx as nx\nfrom karateclub.node_embedding.neighbourhood.deepwalk import DeepWalk\n\nG = nx.barbell_graph(m1=10, m2=4)\ndraw_graph(G)\n\ndw = DeepWalk(dimensions=2)\ndw.fit(G)\n\n\n\n\n\nDeepWalkr가 영역 1을 영역 3과 분리\n영역 2에 속하는 노드에 의해서 오염됬고 임베딩 공간에서 명확한 구분이 보이지 않는다.\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor x in G.nodes():\n    \n    v = dw.get_embedding()[x]\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=12)"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph3-1.html#node2vec",
    "href": "posts/Graph Machine Learning/graph3-1.html#node2vec",
    "title": "CH3. 비지도 그래프 학습(얕은 임베딩 방법)",
    "section": "Node2Vec",
    "text": "Node2Vec\n\n랜덤워크를 그래프에 편향된 무작위경로 생성\n랜덤 워크를 수행하여 노드에 대한 임베딩을 생성\n노드 간의 구조적 유사성과 동시에 노드의 속성 정보를 고려하여 임베딩을 생성\n랜덤 워크 수행 시, 노드의 탐색 패턴을 조절하는 파라미터를 추가하여 다양한 유형의 그래프에서 임베딩 품질을 조정할 수 있음\n다양한 그래프 분석 및 예측에 적용 가능\n너비 우선 탐색(BFS), 깊이 우선 탐색(DFS)을 병합해 그래프 탐색\np 랜덤 워크가 이전 노드로 돌아갈 확률\nq 랜덤 워크가 새로운 노드로 통과할 확률\n\n\nimport networkx as nx\nfrom node2vec import Node2Vec\n\nG = nx.barbell_graph(m1=10, m2=4)\ndraw_graph(G)\n\nnode2vec = Node2Vec(G, dimensions=2)\nmodel = node2vec.fit(window=10)\nembeddings = model.wv\n\n\n\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:00<00:00, 231.58it/s]\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor x in G.nodes():\n    \n    v = model.wv[str(x)]\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=16)\n\nplt.show()\n\n\n\n\n\nDeepWalk에 비해 임베딩 공간에서 노드 간 더 나은 분리도"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph3-1.html#edge2vec",
    "href": "posts/Graph Machine Learning/graph3-1.html#edge2vec",
    "title": "CH3. 비지도 그래프 학습(얕은 임베딩 방법)",
    "section": "Edge2Vec",
    "text": "Edge2Vec\n\n랜덤 워크를 수행하여 엣지에 대한 임베딩을 생성\n노드-엣지-노드 패턴을 이용하여 엣지의 구조 정보를 고려하여 임베딩 생성\n노드 및 엣지 분석에 적용 가능\n\\(v_i, v_j\\) : 인접한 노드\n\\(f(v_i), f(v_j)\\) : Node2Vec으로 계산된 임베딩\n\n\n\n\n연산자\n방정식\n클래스이름\n\n\n\n\n평균\n\\(\\dfrac{f(v_i)+f(v_j)}{2}\\)\nAverageEmbedder\n\n\n아다마르\n\\(f(v_i)*f(v_j)\\)\nHamamarEmbedder\n\n\nL1가중\n\\(|f(v_i)-f(v_j)|\\)\nWeightedL1Embedder\n\n\nL2가중\n\\(|f(v_i)-f(v_j)|^2\\)\nWeightedL2Embedder\n\n\n\n- HadamardEmbedder\n\nfrom node2vec.edges import HadamardEmbedder\nedges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor x in G.edges():\n    \n    v = edges_embs[(str(x[0]), str(x[1]))]\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=16)\n\nplt.show()\n\n\n\n\n- WeightedL1Embedder\n\nfrom node2vec.edges import WeightedL1Embedder\n\nedges_embs2 = WeightedL1Embedder(keyed_vectors=model.wv)\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor x in G.edges():\n    \n    v = edges_embs2[(str(x[0]), str(x[1]))]\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=16)\n\nplt.show()\n\n\n\n\n- WeightedL2Embedder\n\nfrom node2vec.edges import WeightedL2Embedder\n\nedges_embs3 = WeightedL2Embedder(keyed_vectors=model.wv)\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor x in G.edges():\n    \n    v = edges_embs3[(str(x[0]), str(x[1]))]\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=16)\n\nplt.show()"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph3-1.html#graph2vec",
    "href": "posts/Graph Machine Learning/graph3-1.html#graph2vec",
    "title": "CH3. 비지도 그래프 학습(얕은 임베딩 방법)",
    "section": "Graph2Vec",
    "text": "Graph2Vec\n\n주어진 그래프 세트에서 각 점이 그래프를 나타내는 임베딩 공간 생성\n그래프 자체를 벡터화하여 그래프에 대한 임베딩을 생성\n전체그래프의 구조 정보와 그래프 내 노드 및 엣지의 속성 정보를 고려하여 임베딩 생성\n그래프 분류 및 유사도 측정에 적용 가능\n\n\nimport random\nimport matplotlib.pyplot as plt\nfrom karateclub import Graph2Vec\n\nn_graphs = 20\n\ndef generate_radom():\n    n = random.randint(6, 20)\n    k = random.randint(5, n)\n    p = random.uniform(0, 1)\n    return nx.watts_strogatz_graph(n,k,p), [n,k,p]   #20개의 watts_strogatz 그래프 생성\n\nGs = [generate_radom() for x in range(n_graphs)]\n\nmodel = Graph2Vec(dimensions=2, wl_iterations=10) #2차원으로 초기화 \n\n# 학습\nmodel.fit([x[0] for x in Gs])\nembeddings = model.get_embedding()\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor i,vec in enumerate(embeddings):\n    \n    ax.scatter(vec[0],vec[1], s=1000)\n    ax.annotate(str(i), (vec[0],vec[1]), fontsize=40)"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph4-2.html",
    "href": "posts/Graph Machine Learning/graph4-2.html",
    "title": "CH4. 지도 그래프 학습(얕은 임베딩 방법)",
    "section": "",
    "text": "그래프 머신러닝\ngithub"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph4-2.html#utility-graph-plot-matrix",
    "href": "posts/Graph Machine Learning/graph4-2.html#utility-graph-plot-matrix",
    "title": "CH4. 지도 그래프 학습(얕은 임베딩 방법)",
    "section": "Utility graph plot matrix",
    "text": "Utility graph plot matrix\n\nimport matplotlib.pyplot as plt\n\ndef draw_graph(G, node_names={}, nodes_label=[], node_size=900):\n    pos_nodes = nx.spring_layout(G)\n    \n    col = {0:\"steelblue\",1:\"red\",2:\"green\"}\n    \n    colors = [col[x] for x in nodes_label]\n    \n    nx.draw_networkx(G, pos_nodes, with_labels=True, node_color=colors, node_size=node_size, edge_color='gray', \n            arrowsize=30)\n    \n    \n    \n    pos_attrs = {}\n    for node, coords in pos_nodes.items():\n        pos_attrs[node] = (coords[0], coords[1] + 0.08)\n        \n    \n    plt.axis('off')\n    axis = plt.gca()\n    axis.set_xlim([1.2*x for x in axis.get_xlim()])\n    axis.set_ylim([1.2*y for y in axis.get_ylim()])\n    plt.show()"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph4-2.html#라벨-전파-알고리즘",
    "href": "posts/Graph Machine Learning/graph4-2.html#라벨-전파-알고리즘",
    "title": "CH4. 지도 그래프 학습(얕은 임베딩 방법)",
    "section": "라벨 전파 알고리즘",
    "text": "라벨 전파 알고리즘\n\n주어진 노드의 라벨을 인접 노드 또는 해당 노드에서 도달할 가능성이 높은 노드로 전파\n\n\nimport networkx as nx\n\nG = nx.barbell_graph(m1=3, m2=2)\nnodes_label = [0 for x in range(len(G.nodes()))]\nnodes_label[0] = 1\nnodes_label[6] = 2\ndraw_graph(G, nodes_label=nodes_label, node_size=1200)\n\n\n\n\n\n0과 6에만 라벨이 지정됨\n라벨이 지정된 노드의 정보를 이용하여 다른 노드로 이동할 확률 계싼\n\n\n그림의 그래프에 대한 대각 차수 행렬\n\nimport numpy as np\nfrom numpy.linalg import inv\n\nD = [G.degree(n) for n in G.nodes()]\nD = np.diag(D)\nD\n\narray([[2, 0, 0, 0, 0, 0, 0, 0],\n       [0, 2, 0, 0, 0, 0, 0, 0],\n       [0, 0, 3, 0, 0, 0, 0, 0],\n       [0, 0, 0, 2, 0, 0, 0, 0],\n       [0, 0, 0, 0, 2, 0, 0, 0],\n       [0, 0, 0, 0, 0, 3, 0, 0],\n       [0, 0, 0, 0, 0, 0, 2, 0],\n       [0, 0, 0, 0, 0, 0, 0, 2]])\n\n\n\n\n그림의 그래프에 대한 전이 행렬\n\nA = inv(D)*nx.to_numpy_matrix(G)\nA\n\nmatrix([[0.        , 0.5       , 0.5       , 0.        , 0.        ,\n         0.        , 0.        , 0.        ],\n        [0.5       , 0.        , 0.5       , 0.        , 0.        ,\n         0.        , 0.        , 0.        ],\n        [0.33333333, 0.33333333, 0.        , 0.33333333, 0.        ,\n         0.        , 0.        , 0.        ],\n        [0.        , 0.        , 0.5       , 0.        , 0.5       ,\n         0.        , 0.        , 0.        ],\n        [0.        , 0.        , 0.        , 0.5       , 0.        ,\n         0.5       , 0.        , 0.        ],\n        [0.        , 0.        , 0.        , 0.        , 0.33333333,\n         0.        , 0.33333333, 0.33333333],\n        [0.        , 0.        , 0.        , 0.        , 0.        ,\n         0.5       , 0.        , 0.5       ],\n        [0.        , 0.        , 0.        , 0.        , 0.        ,\n         0.5       , 0.5       , 0.        ]])\n\n\n\n\nLabel propagation implemenation\n\n행렬의 각 행은 샘플, 각 열은 특징을 나타냄\n\n\nimport numpy as np\nimport networkx as nx\nfrom numpy.linalg import inv\nfrom abc import ABCMeta, abstractmethod\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.multiclass import check_classification_targets\nfrom sklearn.utils.validation import check_is_fitted, _deprecate_positional_args\n\nclass GraphLabelPropagation(ClassifierMixin, BaseEstimator, metaclass=ABCMeta):\n    \"\"\"Graph label propagation module.\n    Parameters\n    ----------\n    max_iter : int, default=30\n        Change maximum number of iterations allowed.\n    tol : float, default=1e-3\n        Convergence tolerance: threshold to consider the system at steady\n        state.\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self, max_iter=30, tol=1e-3):\n\n        self.max_iter = max_iter\n        self.tol = tol\n\n    def predict(self, X):\n        \"\"\"Performs inductive inference across the model.\n        Parameters\n        ----------\n        X : A networkx array.\n            The data matrix.\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            Predictions for input data.\n        \"\"\"\n        probas = self.predict_proba(X)\n        return self.classes_[np.argmax(probas, axis=1)].ravel()\n\n    def predict_proba(self, X):\n        \"\"\"Predict probability for each possible outcome.\n        Compute the probability estimates for each single node in X\n        and each possible outcome seen during training (categorical\n        distribution).\n        Parameters\n        ----------\n        X : A networkx array.\n        Returns\n        -------\n        probabilities : ndarray of shape (n_samples, n_classes)\n            Normalized probability distributions across\n            class labels.\n        \"\"\"\n        check_is_fitted(self)\n        \n        return self.label_distributions_\n    \n    def _validate_data(self, X, y):\n        if not isinstance(X, nx.Graph):\n            raise ValueError(\"Input should be a networkX graph\")\n        if not len(y) == len(X.nodes()):\n            raise ValueError(\"Label data input shape should be equal to the number of nodes in the graph\")\n        return X, y\n    \n    @staticmethod\n    def build_label(x,classes):\n        tmp = np.zeros((classes))\n        tmp[x] = 1\n        return tmp\n    \n    def fit(self, X, y):\n        \"\"\"Fit a semi-supervised label propagation model based\n        on the input graph G and corresponding label matrix y with a dedicated marker value for\n        unlabeled samples.\n        Parameters\n        ----------\n        X : A networkX array.\n        y : array-like of shape (n_samples,)\n            `n_labeled_samples` (unlabeled points are marked as -1)\n            All unlabeled samples will be transductively assigned labels.\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X, y = self._validate_data(X, y)\n        self.X_ = X\n        check_classification_targets(y)\n\n        D = [X.degree(n) for n in X.nodes()]\n        D = np.diag(D)\n        \n        # label construction\n        # construct a categorical distribution for classification only\n        unlabeled_index = np.where(y==-1)[0]\n        labeled_index = np.where(y!=-1)[0]\n        unique_classes = np.unique(y[labeled_index])\n        \n        self.classes_ = unique_classes\n        \n        Y0 = np.array([self.build_label(y[x], len(unique_classes)) \n                                 if x in labeled_index else np.zeros(len(unique_classes)) for x in range(len(y))])\n        \n        A = inv(D)*nx.to_numpy_matrix(G)\n        Y_prev = Y0\n        it = 0\n        c_tool = 10\n        \n        while it < self.max_iter & c_tool > self.tol:\n            Y = A*Y_prev\n            #force labeled nodes\n            Y[labeled_index] = Y0[labeled_index]\n            \n            it +=1\n            c_tol = np.sum(np.abs(Y-Y_prev))\n            \n            Y_prev = Y\n            \n        self.label_distributions_ = Y\n        return self\n\n\n이게 뭐시여\n\n\n\nLabel propagation execution\n\nglp = GraphLabelPropagation()\ny = np.array([-1 for x in range(len(G.nodes()))])\ny[0] = 1\ny[6] = 0\nglp.fit(G,y)\ntmp = glp.predict(G)\nprint(glp.predict_proba(G))\n\ndraw_graph(G, nodes_label=tmp+1, node_size=1200)\n\n[[0.         1.        ]\n [0.05338542 0.90006109]\n [0.11845743 0.8081115 ]\n [0.31951678 0.553297  ]\n [0.553297   0.31951678]\n [0.8081115  0.11845743]\n [1.         0.        ]\n [0.90006109 0.05338542]]"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph4-2.html#라벨-확산-알고리즘",
    "href": "posts/Graph Machine Learning/graph4-2.html#라벨-확산-알고리즘",
    "title": "CH4. 지도 그래프 학습(얕은 임베딩 방법)",
    "section": "라벨 확산 알고리즘",
    "text": "라벨 확산 알고리즘\n\n라벨 전파 방식의 한계인 초기 라벨링 극복하고자 만들어짐\n초기 라벨은 학습 과정엥서 수정이 불가. 각 반복에서 얻은 원래 값과 같아야함\n라벨이 지정된 제약을 완화해 라벨이 지정된 입력 노드가 교육 프로세스 중에 라벨 변경할 수 있도록 한다.\n\n\nimport networkx as nx\n\nG = nx.barbell_graph(m1=3, m2=2)\nnodes_label = [0 for x in range(len(G.nodes()))]\nnodes_label[0] = 1\nnodes_label[6] = 2\ndraw_graph(G, nodes_label=nodes_label, node_size=1200)\n\n\n\n\n\nDegree matrix\n\nimport numpy as np\nfrom numpy.linalg import inv\n\nD = [G.degree(n) for n in G.nodes()]\nD = np.diag(D)\nD\n\narray([[2, 0, 0, 0, 0, 0, 0, 0],\n       [0, 2, 0, 0, 0, 0, 0, 0],\n       [0, 0, 3, 0, 0, 0, 0, 0],\n       [0, 0, 0, 2, 0, 0, 0, 0],\n       [0, 0, 0, 0, 2, 0, 0, 0],\n       [0, 0, 0, 0, 0, 3, 0, 0],\n       [0, 0, 0, 0, 0, 0, 2, 0],\n       [0, 0, 0, 0, 0, 0, 0, 2]])\n\n\n\n\nNormalized graph Laplacian matrix\n\n라플라시안 행렬 위키백과\n\n\n\nfrom scipy.linalg import fractional_matrix_power\nD_inv = fractional_matrix_power(D, -0.5)\nL = D_inv*nx.to_numpy_matrix(G)*D_inv\nL\n\nmatrix([[0.        , 0.5       , 0.40824829, 0.        , 0.        ,\n         0.        , 0.        , 0.        ],\n        [0.5       , 0.        , 0.40824829, 0.        , 0.        ,\n         0.        , 0.        , 0.        ],\n        [0.40824829, 0.40824829, 0.        , 0.40824829, 0.        ,\n         0.        , 0.        , 0.        ],\n        [0.        , 0.        , 0.40824829, 0.        , 0.5       ,\n         0.        , 0.        , 0.        ],\n        [0.        , 0.        , 0.        , 0.5       , 0.        ,\n         0.40824829, 0.        , 0.        ],\n        [0.        , 0.        , 0.        , 0.        , 0.40824829,\n         0.        , 0.40824829, 0.40824829],\n        [0.        , 0.        , 0.        , 0.        , 0.        ,\n         0.40824829, 0.        , 0.5       ],\n        [0.        , 0.        , 0.        , 0.        , 0.        ,\n         0.40824829, 0.5       , 0.        ]])\n\n\n\n\nLabel spreading implementation\n\nimport numpy as np\nimport networkx as nx\nfrom sklearn.preprocessing import normalize\nfrom scipy.linalg import fractional_matrix_power\nfrom sklearn.utils.multiclass import check_classification_targets\n\nclass GraphLabelSpreading(GraphLabelPropagation):\n    \"\"\"Graph label propagation module.\n    Parameters\n    ----------\n    max_iter : int, default=30\n        Change maximum number of iterations allowed.\n    tol : float, default=1e-3\n        Convergence tolerance: threshold to consider the system at steady\n        state.\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self, max_iter=30, tol=1e-3, alpha=0.6):\n\n        self.alpha = alpha\n        super().__init__(max_iter, tol)\n    \n    def fit(self, X, y):\n        \"\"\"Fit a semi-supervised label propagation model based\n        on the input graph G and corresponding label matrix y with a dedicated marker value for\n        unlabeled samples.\n        Parameters\n        ----------\n        X : A networkX array.\n        y : array-like of shape (n_samples,)\n            `n_labeled_samples` (unlabeled points are marked as -1)\n            All unlabeled samples will be transductively assigned labels.\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X, y = self._validate_data(X, y)\n        self.X_ = X\n        check_classification_targets(y)\n\n        D = [X.degree(n) for n in X.nodes()]\n        D = np.diag(D)\n        D_inv = np.matrix(fractional_matrix_power(D,-0.5))\n        L = D_inv*nx.to_numpy_matrix(G)*D_inv\n        \n        # label construction\n        # construct a categorical distribution for classification only\n        unlabeled_index = np.where(y==-1)[0]\n        labeled_index = np.where(y!=-1)[0]\n        unique_classes = np.unique(y[labeled_index])\n        \n        self.classes_ = unique_classes\n        \n        Y0 = np.array([self.build_label(y[x], len(unique_classes)) \n                                 if x in labeled_index else np.zeros(len(unique_classes)) for x in range(len(y))])\n        \n        Y_prev = Y0\n        it = 0\n        c_tool = 10\n        \n        while it < self.max_iter & c_tool > self.tol:\n            Y = self.alpha*(L*Y_prev)+((1-self.alpha)*Y0)\n\n            it +=1\n            c_tol = np.sum(np.abs(Y-Y_prev))\n            Y_prev = Y\n        self.label_distributions_ = Y\n        return self\n\n\n\nLabel Spreading\n\ngls = GraphLabelSpreading(max_iter=1000)\ny = np.array([-1 for x in range(len(G.nodes()))])\ny[0] = 1\ny[6] = 0\ngls.fit(G,y)\ntmp = gls.predict(G)\nprint(gls.predict_proba(G))\ndraw_graph(G, nodes_label=tmp+1, node_size=1200)\n\n[[0.00148824 0.50403871]\n [0.00148824 0.19630098]\n [0.00471728 0.18369265]\n [0.01591722 0.05001252]\n [0.05001252 0.01591722]\n [0.18369265 0.00471728]\n [0.50403871 0.00148824]\n [0.19630098 0.00148824]]\n\n\n\n\n\n\n라벨 전파 할당보다 라벨 확산 할당이 확률이 더 낮다.\n초기 라벨 할당의 영향이 정규화 매개 변수 \\(\\alpha\\)에 의해 가중되기 때문에"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph5-1.html",
    "href": "posts/Graph Machine Learning/graph5-1.html",
    "title": "CH5. 그래프에서의 머신러닝 문제(링크예측)",
    "section": "",
    "text": "그래프 머신러닝\ngithub"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph5-1.html#유사성-기반-방법",
    "href": "posts/Graph Machine Learning/graph5-1.html#유사성-기반-방법",
    "title": "CH5. 그래프에서의 머신러닝 문제(링크예측)",
    "section": "유사성 기반 방법",
    "text": "유사성 기반 방법\n\n지수 기반 방법\n\n주어진 두 노드의 이웃을 기반으로 간단한 지수 계싼을 통한 방법\n\n\n자원 할당 지수\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nedges = [[1,3],[2,3],[2,4],[4,5],[5,6],[5,7]]\nG = nx.from_edgelist(edges)\npreds = nx.resource_allocation_index(G,[(1,2),(2,5),(3,4)])\nprint(list(preds))\n\n[(1, 2, 0.5), (2, 5, 0.5), (3, 4, 0.5)]\n\n\n\n노드 쌍의 지원 할당 지수 목록\n노드 쌍 사이에 간선이 있을 확률 0.5\n\n- 오류나넹.. 아래와 같이 코드 나와야함\n\n\ndraw_graph(G)\n\n\n\n자카드 계수\n\\[jaccard Coefficient(u,v) = \\dfrac{|N(u) \\cap N(v)|}{|N(u) \\cup N(v)|}\\]\n\n\\(N(v)\\):노드의 이웃 계산\n\n\nimport networkx as nx\nedges = [[1,3],[2,3],[2,4],[4,5],[5,6],[5,7]]\nG = nx.from_edgelist(edges)\npreds = nx.jaccard_coefficient(G,[(1,2),(2,5),(3,4)])\nprint(list(preds))\ndraw_graph(G)\n\n[(1, 2, 0.5), (2, 5, 0.25), (3, 4, 0.3333333333333333)]\n\n\nTypeError: '_AxesStack' object is not callable\n\n\n<Figure size 640x480 with 0 Axes>\n\n\n- 위 함수는 자꾸 오류가 나서 밑에 처럼 바꿔서 진행\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\ndef draw_graph(G):\n    nx.draw_networkx(G, with_labels=True)\n    plt.show()\n\nedges = [[1,3],[2,3],[2,4],[4,5],[5,6],[5,7]]\nG = nx.from_edgelist(edges)\npreds = nx.jaccard_coefficient(G,[(1,2),(2,5),(3,4)])\nprint(list(preds))\ndraw_graph(G)\n\n[(1, 2, 0.5), (2, 5, 0.25), (3, 4, 0.3333333333333333)]\n\n\n\n\n\n[(1, 2, 0.5), (2, 5, 0.25), (3, 4, 0.3333333333333333)]\n\n노드 (1,2)사이 간선 확률 0.5, (2,5)사이에 간선 확률 0.25, (3,4)사이에 간선 확률 0.3\n\n\n\n\n커뮤니티 기반 방법\n\n주어진 두 노드가 속한 커뮤니티에 대한 정보를 사용해 지수 계산\n\n\n커뮤니티 공통 이웃\n\n공통 이웃 수를 계산하고 이 값에 동일한 커뮤니티에 속한 공통 이웃 수를 더한다.\n\n\\[Community Common Neighbor(u,v)=|N(v) \\cup N(u)| + \\sum_{w in N(v) \\cap N(u)} f(w)\\]\n\nw가 u,v와 동일한 커뮤니티에 속하면 \\(f(w)=1\\) 그렇지 않으면 \\(0\\)\n\n\nimport networkx as nx\nedges = [[1,3],[2,3],[2,4],[4,5],[5,6],[5,7]]\nG = nx.from_edgelist(edges)\n\n\n# 1,2,3은 0이라는 커뮤니티\nG.nodes[1][\"community\"] = 0\nG.nodes[2][\"community\"] = 0\nG.nodes[3][\"community\"] = 0\n\n\n\n# 4,5,6,7은 1이라는 커뮤니티\nG.nodes[4][\"community\"] = 1\nG.nodes[5][\"community\"] = 1\nG.nodes[6][\"community\"] = 1\nG.nodes[7][\"community\"] = 1\n\n\npreds = nx.cn_soundarajan_hopcroft(G,[(1,2),(2,5),(3,4)])\nprint(list(preds))\nnx.draw_networkx(G)\n\n[(1, 2, 2), (2, 5, 1), (3, 4, 1)]\n\n\n\n\n\n\nnx.degree_pearson_correlation_coefficient?\n\n\nSignature:\nnx.degree_pearson_correlation_coefficient(\n    G,\n    x='out',\n    y='in',\n    weight=None,\n    nodes=None,\n)\nDocstring:\nCompute degree assortativity of graph.\nAssortativity measures the similarity of connections\nin the graph with respect to the node degree.\nThis is the same as degree_assortativity_coefficient but uses the\npotentially faster scipy.stats.pearsonr function.\nParameters\n----------\nG : NetworkX graph\nx: string ('in','out')\n   The degree type for source node (directed graphs only).\ny: string ('in','out')\n   The degree type for target node (directed graphs only).\nweight: string or None, optional (default=None)\n   The edge attribute that holds the numerical value used\n   as a weight.  If None, then each edge has weight 1.\n   The degree is the sum of the edge weights adjacent to the node.\nnodes: list or iterable (optional)\n    Compute pearson correlation of degrees only for specified nodes.\n    The default is all nodes.\nReturns\n-------\nr : float\n   Assortativity of graph by degree.\nExamples\n--------\n>>> G = nx.path_graph(4)\n>>> r = nx.degree_pearson_correlation_coefficient(G)\n>>> print(f\"{r:3.1f}\")\n-0.5\nNotes\n-----\nThis calls scipy.stats.pearsonr.\nReferences\n----------\n.. [1] M. E. J. Newman, Mixing patterns in networks\n       Physical Review E, 67 026126, 2003\n.. [2] Foster, J.G., Foster, D.V., Grassberger, P. & Paczuski, M.\n   Edge direction and the structure of networks, PNAS 107, 10815-20 (2010).\nFile:      ~/anaconda3/envs/py38/lib/python3.8/site-packages/networkx/algorithms/assortativity/correlation.py\nType:      function\n\n\n\n\nnx.draw_networkx\n\n\n결과값 해석.. 어케 함..\n\n\n\n커뮤니티 자원 할당\n\\[Community Common Neighbor(u,v) = \\sum_{w in N(v) \\cap N(u)} \\dfrac{f(w)}{|N(w)|}\\]\n\nimport networkx as nx\nedges = [[1,3],[2,3],[2,4],[4,5],[5,6],[5,7]]\nG = nx.from_edgelist(edges)\n\nG.nodes[1][\"community\"] = 0\nG.nodes[2][\"community\"] = 0\nG.nodes[3][\"community\"] = 0\n\nG.nodes[4][\"community\"] = 1\nG.nodes[5][\"community\"] = 1\nG.nodes[6][\"community\"] = 1\nG.nodes[7][\"community\"] = 1\npreds = nx.ra_index_soundarajan_hopcroft(G,[(1,2),(2,5),(3,4)])\nprint(list(preds))\ndraw_graph(G)\n\n[(1, 2, 0.5), (2, 5, 0), (3, 4, 0)]"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph5-1.html#임베딩-기반-방법",
    "href": "posts/Graph Machine Learning/graph5-1.html#임베딩-기반-방법",
    "title": "CH5. 그래프에서의 머신러닝 문제(링크예측)",
    "section": "임베딩 기반 방법",
    "text": "임베딩 기반 방법\n\n주어진 그래프에 대한 각 노드 쌍을 특징 벡터(x)로 표현\n클래스 라벨(y)를 해당 노드 쌍 각각에 할당\n\n- 전체 프로세스\n1 그래프 G의 각 노드에 대해 해당 임베딩 벡터를 node2vec 알고리즘 사용해 계산\n2 그래프 가능한 모든 노드 쌍에 대해 edge2vec알고리즘 사용해 임베딩 계산\n- 데이터셋: cora\n\n인용 데이터셋을 사용해 다음과 같이 networkx 그래프 작성\n\n\nimport networkx as nx\nimport pandas as pd\n\nedgelist = pd.read_csv(\"cora.cites\", sep='\\t', header=None, names=[\"target\", \"source\"])\nG = nx.from_pandas_edgelist(edgelist)\ndraw_graph(G)\n\n\n\n\n\n그래프 G에서 훈련 및 테스트 데이터셋 생성\n\n\n훈련 및 테스트 데이터셋에서 그래프 G의 실제 노드 쌍을 나타내는 집합+ G 실제 노드를 나타내지 않는 노드 쌍도 포함\n양의 인스턴스(클래스 라벨1) : 실제 간선 나타내느 ㄴ쌍\n음의 인스턴스(클래스 라벨0) : 실제 간선을 나타내지 않는 쌍\n\n\nfrom stellargraph.data import EdgeSplitter\n\nedgeSplitter = EdgeSplitter(G)\ngraph_test, samples_test, labels_test = edgeSplitter.train_test_split(\n    p=0.1, method=\"global\"\n)\n\n2023-04-06 22:36:33.732989: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n** Sampled 527 positive and 527 negative edges. **\n\n\n\ngraph_test 모든 노드 포함, 간선의 부분 집합만 포함하는 원본 그래프의 부분 집합\nsample_test 각각의 노드 쌍 포함, 실제 간선을 나태나는 노드쌍과 실제 모서리를 나타내지 않는 노드 쌍이 포함\nlabel_test sample_test와 같은 길이의 벡터. 0(샘플테스트 백터에서 음의인스터스를 나타내는 위치)또는 1(양의 인스턴스 위치)만 포함\n\n\n훈련 세트 생성\n\n\nedgeSplitter = EdgeSplitter(graph_test, G)\ngraph_train, samples_train, labels_train = edgeSplitter.train_test_split(\n    p=0.1, method=\"global\"\n)\n\n** Sampled 475 positive and 475 negative edges. **\n\n\n\n훈련 세트 특징 벡터 생성\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder\n\nnode2vec = Node2Vec(graph_train)  # 각 노드에 대한 임베딩 생성\nmodel = node2vec.fit()\nedges_embs = HadamardEmbedder(keyed_vectors=model.wv) #훈련세트에 포함된 각 노드 쌍의 임베딩 생성 -> 모델 학습 위한 특징 벡터로 사용\ntrain_embeddings = [edges_embs[str(x[0]),str(x[1])] for x in samples_train]\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04<00:00,  2.05it/s]\n\n\n\n테스트 세트 특징 벡터 생성\n\n\ntest_embeddings = [edges_embs[str(x[0]),str(x[1])] for x in samples_test]\n\n\ntrain_embedding특징 공간과 train_labels 라벨 할당을 이용해 머신러닝 알고리즘 학습\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=1000)\nrf.fit(train_embeddings, labels_train);\n\n\nfrom sklearn import metrics\n\ny_pred = rf.predict(test_embeddings)\n\nprint('Precision:', metrics.precision_score(labels_test, y_pred))\nprint('Recall:', metrics.recall_score(labels_test, y_pred))\nprint('F1-Score:', metrics.f1_score(labels_test, y_pred))\n\nPrecision: 0.8952380952380953\nRecall: 0.713472485768501\nF1-Score: 0.7940865892291447"
  },
  {
    "objectID": "posts/Graph Machine Learning/9999_graph8_guebin.html",
    "href": "posts/Graph Machine Learning/9999_graph8_guebin.html",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(basic)",
    "section": "",
    "text": "그래프 머신러닝\ngithub\nCredit Card Transactions Fraud Detection Dataset\n컬리이미지"
  },
  {
    "objectID": "posts/Graph Machine Learning/9999_graph8_guebin.html#네트워크-토폴로지",
    "href": "posts/Graph Machine Learning/9999_graph8_guebin.html#네트워크-토폴로지",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(basic)",
    "section": "네트워크 토폴로지",
    "text": "네트워크 토폴로지\n\n각 그래프별 차수 분포 살펴보기\n\n\n# for G in [G_bu, G_tu]:\n#     plt.figure(figsize=(10,10))\n#     degrees = pd.Series({k:v for k, v in nx.degree(G)})\n#     degrees.plot.hist()\n#     plt.yscale(\"log\")\n\n- 각 그래프 간선 가중치 분포\n\n# for G in [G_bu, G_tu]:\n#     allEdgeWeights = pd.Series({\n#         (d[0],d[1]):d[2][\"weight\"]\n#         for d in G.edges(data=True)})\n#     np.quantile(allEdgeWeights.values,\n#                [0.10, 0.50, 0.70, 0.9])\n    \n\n\n# np.quantile(allEdgeWeights.values,[0.10, 0.50, 0.70, 0.9])\n\n- 매게 중심성 측정 지표\n\n# for G in [G_bu, G_tu]:\n#     plt.figure(figsize=(10,10))\n#     bc_distr = pd.Series(nx.betweenness_centrality(G))\n#     bc_distr.plot.hist()\n#     plt.yscale(\"log\")\n\n\n그래프 내에서 노드가 얼마나 중심적인 역할을 하는지 나타내는 지표\n해당 노드가 얼마나 많은 최단경로에 포함되는지 살피기\n노드가 많은 최단경로를 포함하면 해당노드의 매개중심성은 커진다.\n\n- 상관계수\n\n# for G in [G_bu, G_tu]:\n#     print(nx.degree_pearson_correlation_coefficient(G))\n\n\n음의 동류성\n연결도 높은 개인이 연골도 낮은 개인과 연관돼 있다.\n이분그래프: 낮은 차수의 고객은 들어오는 트랜잭션 수가 많은 높은 차수의 판매자와만 연결되어 상관계수가 낮다.\n삼분그래프:동류성이 훨씬 더 낮다. 트랜잭션 노드가 있기 댸문에?"
  },
  {
    "objectID": "posts/Graph Machine Learning/9999_graph8_guebin.html#커뮤니티-감지",
    "href": "posts/Graph Machine Learning/9999_graph8_guebin.html#커뮤니티-감지",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(basic)",
    "section": "커뮤니티 감지",
    "text": "커뮤니티 감지\n\n# pip install python-louvain\n\n\n# import networkx as nx\n# import community\n\n\n# import community\n# for G in [G_bu, G_tu]:\n#     parts = community.best_partition(G, random_state=42, weight='weight')\n\n\n# communities = pd.Series(parts)\n\n\n# print(communities.value_counts().sort_values(ascending=False))\n\n\n커뮤니티 감지를 통해 특정 사기 패턴 식별\n커뮤니티 추출 후 포함된 노드 수에 따라 정렬\n\n\n# communities.value_counts().plot.hist(bins=20)\n\n\n2500부근에 형성되었고 ..\n\n\n# graphs = [] # 부분그래프 저장\n# d = {}  # 부정 거래 비율 저장 \n# for x in communities.unique():\n#     tmp = nx.subgraph(G, communities[communities==x].index)\n#     fraud_edges = sum(nx.get_edge_attributes(tmp, \"label\").values())\n#     ratio = 0 if fraud_edges == 0 else (fraud_edges/tmp.number_of_edges())*100\n#     d[x] = ratio\n#     graphs += [tmp]\n\n# pd.Series(d).sort_values(ascending=False)\n\n\n사기 거래 비율 계산. 사기 거래가 집중된 특정 하위 그래프 식별\n특정 커뮤니티에 포함된 노드를 사용하여 노드 유도 하위 그래프 생성\n하위 그래프: 모든 간선 수에 대한 사기 거래 간선 수의 비율로 사기 거래 백분율 계싼\n\n\n# gId = 10\n# plt.figure(figsize=(10,10))\n# spring_pos = nx.spring_layout(graphs[gId])\n# plt.axis(\"off\")\n# edge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\n# nx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n#                  edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n커뮤니티 감지 알고리즘에 의해 감지된 노드 유도 하위 그래프 그리기\n특정 커뮤니티 인덱스 gId가 주어지면 해당 커뮤니티에서 사용 가능한 노드로 유도 하위 그래프 추출하고 얻는다.\n\n\n# gId = 6\n# plt.figure(figsize=(10,10))\n# spring_pos = nx.spring_layout(graphs[gId])\n# plt.axis(\"off\")\n# edge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\n# nx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n#                  edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n# pd.Series(d).plot.hist(bins=20)"
  },
  {
    "objectID": "posts/Graph Machine Learning/9999_graph8_guebin.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "href": "posts/Graph Machine Learning/9999_graph8_guebin.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(basic)",
    "section": "사기 탐지를 위한 지도 및 비지도 임베딩",
    "text": "사기 탐지를 위한 지도 및 비지도 임베딩\n\n트랜잭션 간선으로 표기\n각 간선을 올바른 클래스(사기 또는 정상)으로 분류\n\n\n지도학습\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\ndf_downsampled\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      trans_date_trans_time\n      cc_num\n      merchant\n      category\n      amt\n      first\n      last\n      gender\n      street\n      ...\n      lat\n      long\n      city_pop\n      job\n      dob\n      trans_num\n      unix_time\n      merch_lat\n      merch_long\n      is_fraud\n    \n  \n  \n    \n      2449\n      2449\n      2019-01-02 1:06\n      4.613310e+12\n      fraud_Rutherford-Mertz\n      grocery_pos\n      281.06\n      Jason\n      Murphy\n      M\n      542 Steve Curve Suite 011\n      ...\n      35.9946\n      -81.7266\n      885\n      Soil scientist\n      1988-09-15\n      e8a81877ae9a0a7f883e15cb39dc4022\n      1325466397\n      36.430124\n      -81.179483\n      1\n    \n    \n      2472\n      2472\n      2019-01-02 1:47\n      3.401870e+14\n      fraud_Jenkins, Hauck and Friesen\n      gas_transport\n      11.52\n      Misty\n      Hart\n      F\n      27954 Hall Mill Suite 575\n      ...\n      29.4400\n      -98.4590\n      1595797\n      Horticultural consultant\n      1960-10-28\n      bc7d41c41103877b03232f03f1f8d3f5\n      1325468849\n      29.819364\n      -99.142791\n      1\n    \n    \n      2523\n      2523\n      2019-01-02 3:05\n      3.401870e+14\n      fraud_Goodwin-Nitzsche\n      grocery_pos\n      276.31\n      Misty\n      Hart\n      F\n      27954 Hall Mill Suite 575\n      ...\n      29.4400\n      -98.4590\n      1595797\n      Horticultural consultant\n      1960-10-28\n      b98f12f4168391b2203238813df5aa8c\n      1325473523\n      29.273085\n      -98.836360\n      1\n    \n    \n      2546\n      2546\n      2019-01-02 3:38\n      4.613310e+12\n      fraud_Erdman-Kertzmann\n      gas_transport\n      7.03\n      Jason\n      Murphy\n      M\n      542 Steve Curve Suite 011\n      ...\n      35.9946\n      -81.7266\n      885\n      Soil scientist\n      1988-09-15\n      397894a5c4c02e3c61c784001f0f14e4\n      1325475483\n      35.909292\n      -82.091010\n      1\n    \n    \n      2553\n      2553\n      2019-01-02 3:55\n      3.401870e+14\n      fraud_Koepp-Parker\n      grocery_pos\n      275.73\n      Misty\n      Hart\n      F\n      27954 Hall Mill Suite 575\n      ...\n      29.4400\n      -98.4590\n      1595797\n      Horticultural consultant\n      1960-10-28\n      7863235a750d73a244c07f1fb7f0185a\n      1325476547\n      29.786426\n      -98.683410\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      725847\n      725847\n      2019-11-06 1:26\n      3.040770e+13\n      fraud_Hackett-Lueilwitz\n      grocery_pos\n      78.05\n      Danielle\n      Evans\n      F\n      76752 David Lodge Apt. 064\n      ...\n      42.1939\n      -76.7361\n      520\n      Psychotherapist\n      1991-10-13\n      2aa8214261dd0db70698227a1d32c5bd\n      1352165216\n      42.703030\n      -76.806033\n      0\n    \n    \n      459398\n      459398\n      2019-07-22 3:02\n      3.014350e+13\n      fraud_Schaefer, McGlynn and Bosco\n      gas_transport\n      58.00\n      Lisa\n      Collins\n      F\n      44197 Jeffrey Port Suite 050\n      ...\n      39.8016\n      -75.3478\n      504\n      Engineer, control and instrumentation\n      1980-08-17\n      5ee3245696a1c6d9b05fe24b2a4b0d9f\n      1342926151\n      39.410811\n      -75.758650\n      0\n    \n    \n      757268\n      757268\n      2019-11-19 23:35\n      3.525590e+15\n      fraud_Jewess LLC\n      shopping_pos\n      3.87\n      Scott\n      Fuller\n      M\n      861 Karen Common\n      ...\n      36.0424\n      -79.3242\n      6006\n      Paramedic\n      1984-07-20\n      a54289bca789f924f76758aa39330473\n      1353368126\n      35.365866\n      -78.437476\n      0\n    \n    \n      42530\n      42530\n      2019-01-26 10:32\n      5.501080e+15\n      fraud_Torp-Labadie\n      gas_transport\n      91.58\n      Wayne\n      Payne\n      M\n      587 Bradley Inlet Suite 281\n      ...\n      41.1464\n      -81.5107\n      47772\n      Equities trader\n      1966-01-04\n      b834d64418609bbbc79656203425e47b\n      1327573922\n      41.563620\n      -82.136101\n      0\n    \n    \n      806990\n      806990\n      2019-12-06 6:37\n      2.297450e+15\n      fraud_Torphy-Goyette\n      shopping_pos\n      8.58\n      Laura\n      Walker\n      F\n      611 Michael Rue\n      ...\n      39.9972\n      -88.6962\n      478\n      Landscape architect\n      1960-01-13\n      a5046e10941bcea5e3dedc55a4b8b8e0\n      1354775833\n      40.571912\n      -88.132946\n      0\n    \n  \n\n12012 rows × 23 columns\n\n\n\n\n무작위 언더샘플링 사용\n소수 클래스(사기거래)이 샘플 수 와 일치시키려고 다수 클래스(정상거래)의 하위 샘플을 가져옴\n데이터 불균형을 처리하기 위해서\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\n데이터 8:2 비율로 학습 검증\n\n\npip install node2vec\n\nCollecting node2vec\n  Downloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\nRequirement already satisfied: joblib<2.0.0,>=1.1.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.2.0)\nCollecting gensim<5.0.0,>=4.1.2\n  Downloading gensim-4.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/26.5 MB 71.8 MB/s eta 0:00:0000:0100:01\nCollecting tqdm<5.0.0,>=4.55.1\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 18.6 MB/s eta 0:00:00\nCollecting networkx<3.0,>=2.5\n  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 90.4 MB/s eta 0:00:00\nRequirement already satisfied: numpy<2.0.0,>=1.19.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.24.2)\nRequirement already satisfied: scipy>=1.7.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (1.10.1)\nCollecting smart-open>=1.8.1\n  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 13.6 MB/s eta 0:00:00\nInstalling collected packages: tqdm, smart-open, networkx, gensim, node2vec\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.0\n    Uninstalling networkx-3.0:\n      Successfully uninstalled networkx-3.0\nSuccessfully installed gensim-4.3.1 networkx-2.8.8 node2vec-0.4.6 smart-open-6.3.0 tqdm-4.65.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04<00:00,  2.44it/s]\n\n\n\nNode2Vec 알고리즘 사용해 특징 공간 구축\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Prekcision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n<class 'node2vec.edges.HadamardEmbedder'>\nPrecision: 0.6953125\nRecall: 0.156140350877193\nF1-Score: 0.2550143266475645\n<class 'node2vec.edges.AverageEmbedder'>\nPrecision: 0.6813353566009105\nRecall: 0.787719298245614\nF1-Score: 0.7306753458096015\n<class 'node2vec.edges.WeightedL1Embedder'>\nPrecision: 0.5925925925925926\nRecall: 0.028070175438596492\nF1-Score: 0.05360134003350084\n<class 'node2vec.edges.WeightedL2Embedder'>\nPrecision: 0.5833333333333334\nRecall: 0.02456140350877193\nF1-Score: 0.04713804713804714\n\n\n\nNode2Vec 알고리즘 사용해 각 Edge2Vec 알고리즘으로 특징 공간 생성\nsklearn 파이썬 라이브러리의 RandomForestClassifier은 이전 단계에서 생성한 특징에 대해 학습\n검증 테스트 위해 정밀도, 재현율, F1-score 성능 지표 측정\n\n\n\n비지도학습\n\nk-means 알고리즘 사용\n지도학습과의 차이점은 특징 공간이 학습-검증 분할을 안함.\n\n\nnod2vec_unsup = Node2Vec(G_down, weight_key='weight')\nunsup_vals = nod2vec_unsup.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04<00:00,  2.25it/s]\n\n\n\n다운샘플링 절차에 전체 그래프 알고리즘 계산\n\n\nfrom sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_down, \"label\").values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv) \n\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    \n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.HadamardEmbedder'>\nNMI: 0.0429862559854\nHomogeneity: 0.03813140300201337\nCompleteness: 0.049433212382250756\nV-Measure: 0.0430529554606017\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.AverageEmbedder'>\nNMI: 0.09395128496638593\nHomogeneity: 0.08960753766432715\nCompleteness: 0.09886731281849871\nV-Measure: 0.09400995872350308\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.WeightedL1Embedder'>\nNMI: 0.17593048106009063\nHomogeneity: 0.17598531397290276\nCompleteness: 0.17597737533152563\nV-Measure: 0.17598134456268477\n<class 'node2vec.edges.WeightedL2Embedder'>\nNMI: 0.1362053730791375\nHomogeneity: 0.1349991253997398\nCompleteness: 0.1375429939044335\nV-Measure: 0.13625918760275774\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn("
  },
  {
    "objectID": "posts/Graph Machine Learning/graph5-2.html",
    "href": "posts/Graph Machine Learning/graph5-2.html",
    "title": "CH5. 그래프에서의 머신러닝 문제(커뮤니티와 같은 의미 있는 구조 감지)",
    "section": "",
    "text": "그래프 머신러닝\ngithub"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph5-2.html#임베딩-기반-커뮤니티-감지",
    "href": "posts/Graph Machine Learning/graph5-2.html#임베딩-기반-커뮤니티-감지",
    "title": "CH5. 그래프에서의 머신러닝 문제(커뮤니티와 같은 의미 있는 구조 감지)",
    "section": "임베딩 기반 커뮤니티 감지",
    "text": "임베딩 기반 커뮤니티 감지\n\n노드 임베딩에 얕은 클러스터링 기술 적용\n임베딩 방법 사용시 노드 간의 유사성을 나타내는 거리를 정의하는 벡터 공간에 노드를 투영\n\n\n바벨 그래프 생성\n\n\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\nimport networkx as nx \nG = nx.barbell_graph(m1=10, m2=4) \n\n\n임베딩 알고리즘(HOPE)를 사용해 감소된 밀집 노드 얻기\n\n\nfrom gem.embedding.hope import HOPE \ngf = HOPE(d=4, beta=0.01) \ngf.learn_embedding(G) \nembeddings = gf.get_embedding() \n\nSVD error (low rank): 0.052092\n\n\n\n클러스터링 알고리즘 실행\n\n\nfrom sklearn.mixture import GaussianMixture\ngm = GaussianMixture(n_components=3, random_state=0) #.(embeddings)\nlabels = gm.fit_predict(embeddings)\n\n\n다양한 색상으로 강조 표시된 계산된 커뮤니티로 네트워크 그리기\n\ncolors = [\"blue\", \"green\", \"red\"]\n\nnx.draw_spring(G, node_color=[colors[label] for label in labels])\n\n위 코드 자꾸 '_AxesStack' object is not callable 이렇게 오류가 난다. ㅇ\n아래와 같이 그림이 표시되어야 함"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph5-2.html#스펙트럼-방법-및-행렬-분해",
    "href": "posts/Graph Machine Learning/graph5-2.html#스펙트럼-방법-및-행렬-분해",
    "title": "CH5. 그래프에서의 머신러닝 문제(커뮤니티와 같은 의미 있는 구조 감지)",
    "section": "스펙트럼 방법 및 행렬 분해",
    "text": "스펙트럼 방법 및 행렬 분해\n\n스펙트럼 클러스터링: 라플라시안 행렬의 고유 벡터에 표준 클러스터링 알고리즘 적용\n임베딩 기술이 라플라시안 행렬의 첫 번쨰 k-고유 벡터롤 고려해 얻은 스펙트럼 임베딩을 이용한 임베딩 기반 커뮤니티 탐지 알고리즘의 특별한 경우 (?)\n\n\n#pip install communities\n\n\nfrom communities.algorithms import spectral_clustering\n\nadj=np.array(nx.adjacency_matrix(G).todense())\n\ncommunities = spectral_clustering(adj, k=3)\n\n/tmp/ipykernel_1863787/1958540878.py:3: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n  adj=np.array(nx.adjacency_matrix(G).todense())\n\n\n\nplt.figure(figsize=(20, 5))\n\nfor ith, community in enumerate(communities):\n    cols = [\"red\" if node in community else \"blue\" for node in G.nodes]\n    plt.subplot(1,3,ith+1)\n    plt.title(f\"Community {ith}\")\n    nx.draw_spring(G, node_color=cols)\n\nTypeError: '_AxesStack' object is not callable\n\n\n\n\n\n\n그전에 nx.draw가 오류 날때는 nx.draw_networkx로 바꿔서 하니까 되긴 됬는데.. nx.darw_spinrg은 어떻게 해야하누.."
  },
  {
    "objectID": "posts/Graph Machine Learning/graph8.html",
    "href": "posts/Graph Machine Learning/graph8.html",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(basic)",
    "section": "",
    "text": "그래프 머신러닝\ngithub\nCredit Card Transactions Fraud Detection Dataset\n컬리이미지"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph8.html#네트워크-토폴로지",
    "href": "posts/Graph Machine Learning/graph8.html#네트워크-토폴로지",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(basic)",
    "section": "네트워크 토폴로지",
    "text": "네트워크 토폴로지\n\n각 그래프별 차수 분포 살펴보기\n\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    degrees = pd.Series({k:v for k, v in nx.degree(G)})\n    degrees.plot.hist()\n    plt.yscale(\"log\")\n\n\n\n\n\n\n\n- 각 그래프 간선 가중치 분포\n\nfor G in [G_bu, G_tu]:\n    allEdgeWeights = pd.Series({\n        (d[0],d[1]):d[2][\"weight\"]\n        for d in G.edges(data=True)})\n    np.quantile(allEdgeWeights.values,\n               [0.10, 0.50, 0.70, 0.9])\n    \n\n\nnp.quantile(allEdgeWeights.values,[0.10, 0.50, 0.70, 0.9])\n\narray([  4.17,  48.31,  76.29, 146.7 ])\n\n\n- 매게 중심성 측정 지표\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    bc_distr = pd.Series(nx.betweenness_centrality(G))\n    bc_distr.plot.hist()\n    plt.yscale(\"log\")\n\nKeyboardInterrupt: \n\n\n\n\n\n<Figure size 1000x1000 with 0 Axes>\n\n\n\n그래프 내에서 노드가 얼마나 중심적인 역할을 하는지 나타내는 지표\n해당 노드가 얼마나 많은 최단경로에 포함되는지 살피기\n노드가 많은 최단경로를 포함하면 해당노드의 매개중심성은 커진다.\n\n- 상관계수\n\nfor G in [G_bu, G_tu]:\n    print(nx.degree_pearson_correlation_coefficient(G))\n\n-0.10159189882353903\n-0.8017506210033467\n\n\n\n음의 동류성\n연결도 높은 개인이 연골도 낮은 개인과 연관돼 있다.\n이분그래프: 낮은 차수의 고객은 들어오는 트랜잭션 수가 많은 높은 차수의 판매자와만 연결되어 상관계수가 낮다.\n삼분그래프:동류성이 훨씬 더 낮다. 트랜잭션 노드가 있기 댸문에?"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph8.html#커뮤니티-감지",
    "href": "posts/Graph Machine Learning/graph8.html#커뮤니티-감지",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(basic)",
    "section": "커뮤니티 감지",
    "text": "커뮤니티 감지\n\n# pip install python-louvain\n\n\nimport networkx as nx\nimport community\n\n\nimport community\nfor G in [G_bu, G_tu]:\n    parts = community.best_partition(G, random_state=42, weight='weight')\n\n\ncommunities = pd.Series(parts)\n\n\nprint(communities.value_counts().sort_values(ascending=False))\n\n12    4019\n71    3999\n27    3743\n52    3739\n43    3679\n      ... \n32    1110\n93    1097\n49    1060\n26    1003\n33     892\nLength: 96, dtype: int64\n\n\n\n커뮤니티 감지를 통해 특정 사기 패턴 식별\n커뮤니티 추출 후 포함된 노드 수에 따라 정렬\n\n\ncommunities.value_counts().plot.hist(bins=20)\n\n<Axes: ylabel='Frequency'>\n\n\n\n\n\n\n2500부근에 형성되었고 ..\n\n\ngraphs = [] # 부분그래프 저장\nd = {}  # 부정 거래 비율 저장 \nfor x in communities.unique():\n    tmp = nx.subgraph(G, communities[communities==x].index)\n    fraud_edges = sum(nx.get_edge_attributes(tmp, \"label\").values())\n    ratio = 0 if fraud_edges == 0 else (fraud_edges/tmp.number_of_edges())*100\n    d[x] = ratio\n    graphs += [tmp]\n\npd.Series(d).sort_values(ascending=False)\n\n48    8.684864\n13    6.956522\n55    6.781235\n45    6.743257\n88    6.338616\n        ...   \n93    0.996377\n75    0.952381\n51    0.765957\n82    0.737265\n33    0.335946\nLength: 96, dtype: float64\n\n\n\n사기 거래 비율 계산. 사기 거래가 집중된 특정 하위 그래프 식별\n특정 커뮤니티에 포함된 노드를 사용하여 노드 유도 하위 그래프 생성\n하위 그래프: 모든 간선 수에 대한 사기 거래 간선 수의 비율로 사기 거래 백분율 계싼\n\n\ngId = 10\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\n커뮤니티 감지 알고리즘에 의해 감지된 노드 유도 하위 그래프 그리기\n특정 커뮤니티 인덱스 gId가 주어지면 해당 커뮤니티에서 사용 가능한 노드로 유도 하위 그래프 추출하고 얻는다.\n\n\ngId = 6\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\npd.Series(d).plot.hist(bins=20)\n\n<Axes: ylabel='Frequency'>"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph8.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "href": "posts/Graph Machine Learning/graph8.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(basic)",
    "section": "사기 탐지를 위한 지도 및 비지도 임베딩",
    "text": "사기 탐지를 위한 지도 및 비지도 임베딩\n\n트랜잭션 간선으로 표기\n각 간선을 올바른 클래스(사기 또는 정상)으로 분류\n\n\n지도학습\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\n무작위 언더샘플링 사용\n소수 클래스(사기거래)이 샘플 수 와 일치시키려고 다수 클래스(정상거래)의 하위 샘플을 가져옴\n데이터 불균형을 처리하기 위해서\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\n데이터 8:2 비율로 학습 검증\n\n\npip install node2vec\n\nCollecting node2vec\n  Downloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\nRequirement already satisfied: joblib<2.0.0,>=1.1.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.2.0)\nCollecting gensim<5.0.0,>=4.1.2\n  Downloading gensim-4.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/26.5 MB 71.8 MB/s eta 0:00:0000:0100:01\nCollecting tqdm<5.0.0,>=4.55.1\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 18.6 MB/s eta 0:00:00\nCollecting networkx<3.0,>=2.5\n  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 90.4 MB/s eta 0:00:00\nRequirement already satisfied: numpy<2.0.0,>=1.19.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.24.2)\nRequirement already satisfied: scipy>=1.7.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (1.10.1)\nCollecting smart-open>=1.8.1\n  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 13.6 MB/s eta 0:00:00\nInstalling collected packages: tqdm, smart-open, networkx, gensim, node2vec\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.0\n    Uninstalling networkx-3.0:\n      Successfully uninstalled networkx-3.0\nSuccessfully installed gensim-4.3.1 networkx-2.8.8 node2vec-0.4.6 smart-open-6.3.0 tqdm-4.65.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04<00:00,  2.44it/s]\n\n\n\nNode2Vec 알고리즘 사용해 특징 공간 구축\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n<class 'node2vec.edges.HadamardEmbedder'>\nPrecision: 0.6953125\nRecall: 0.156140350877193\nF1-Score: 0.2550143266475645\n<class 'node2vec.edges.AverageEmbedder'>\nPrecision: 0.6813353566009105\nRecall: 0.787719298245614\nF1-Score: 0.7306753458096015\n<class 'node2vec.edges.WeightedL1Embedder'>\nPrecision: 0.5925925925925926\nRecall: 0.028070175438596492\nF1-Score: 0.05360134003350084\n<class 'node2vec.edges.WeightedL2Embedder'>\nPrecision: 0.5833333333333334\nRecall: 0.02456140350877193\nF1-Score: 0.04713804713804714\n\n\n\nNode2Vec 알고리즘 사용해 각 Edge2Vec 알고리즘으로 특징 공간 생성\nsklearn 파이썬 라이브러리의 RandomForestClassifier은 이전 단계에서 생성한 특징에 대해 학습\n검증 테스트 위해 정밀도, 재현율, F1-score 성능 지표 측정\n\n\n\n비지도학습\n\nk-means 알고리즘 사용\n지도학습과의 차이점은 특징 공간이 학습-검증 분할을 안함.\n\n\nnod2vec_unsup = Node2Vec(G_down, weight_key='weight')\nunsup_vals = nod2vec_unsup.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04<00:00,  2.25it/s]\n\n\n\n다운샘플링 절차에 전체 그래프 알고리즘 계산\n\n\nfrom sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_down, \"label\").values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv) \n\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    \n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.HadamardEmbedder'>\nNMI: 0.0429862559854\nHomogeneity: 0.03813140300201337\nCompleteness: 0.049433212382250756\nV-Measure: 0.0430529554606017\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.AverageEmbedder'>\nNMI: 0.09395128496638593\nHomogeneity: 0.08960753766432715\nCompleteness: 0.09886731281849871\nV-Measure: 0.09400995872350308\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.WeightedL1Embedder'>\nNMI: 0.17593048106009063\nHomogeneity: 0.17598531397290276\nCompleteness: 0.17597737533152563\nV-Measure: 0.17598134456268477\n<class 'node2vec.edges.WeightedL2Embedder'>\nNMI: 0.1362053730791375\nHomogeneity: 0.1349991253997398\nCompleteness: 0.1375429939044335\nV-Measure: 0.13625918760275774\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn("
  },
  {
    "objectID": "posts/Graph Machine Learning/graph2.html",
    "href": "posts/Graph Machine Learning/graph2.html",
    "title": "CH2. 그래프 머신러닝",
    "section": "",
    "text": "그래프 머신러닝\ngithub\nMachine Learning on Graphs: A Model and Comprehensive Taxonomy\n\n\n\nimport matplotlib.pyplot as plt\ndef draw_graph(G, pos_nodes, node_names={}, node_size=50, plot_weight=False):\n    nx.draw(G, pos_nodes, with_labels=False, node_size=node_size, edge_color='gray', arrowsize=30)\n    \n    pos_attrs = {}\n    for node, coords in pos_nodes.items():\n        pos_attrs[node] = (coords[0], coords[1] + 0.08)\n        \n    nx.draw_networkx_labels(G, pos_attrs, font_family='serif', font_size=20)\n    \n    \n    if plot_weight:\n        pos_attrs = {}\n        for node, coords in pos_nodes.items():\n            pos_attrs[node] = (coords[0], coords[1] + 0.08)\n        \n        nx.draw_networkx_labels(G, pos_attrs, font_family='serif', font_size=20)\n        edge_labels=dict([((a,b,),d[\"weight\"]) for a,b,d in G.edges(data=True)])\n        nx.draw_networkx_edge_labels(G, pos_nodes, edge_labels=edge_labels)\n    \n    plt.axis('off')\n    axis = plt.gca()\n    axis.set_xlim([1.2*x for x in axis.get_xlim()])\n    axis.set_ylim([1.2*y for y in axis.get_ylim()])"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph2.html#node2vec-알고리즘-구현-방법",
    "href": "posts/Graph Machine Learning/graph2.html#node2vec-알고리즘-구현-방법",
    "title": "CH2. 그래프 머신러닝",
    "section": "Node2Vec 알고리즘 구현 방법",
    "text": "Node2Vec 알고리즘 구현 방법\n1 그래프 생성\n2 Node2Vec 객체 생성\nnode2vec = Node2Vec(G, dimensions=, walk_length=, num_walks=, p=, q=, workers=)\n\ndimensions : 임베딩할 벡터의 차원 수\nwalk_length : 랜덤 워크에서 한 번에 이동할 노드 수\nnum_walks : 랜덤 워크를 몇 번 반복할 것인지\np, q : 노드 탐색을 위한 확률값을 조정하는 매개변수\n\n3 랜덤 워크를 수행하여 노드 시퀀스 생성\n4 생성된 노드 시퀀스(그래프 내의 노드들이 순서) 임베딩\n\n랜덤 워크(Random Walk)\n\n그래프 내의 노드를 방문하며 노드 간의 관계를 탐색하는 과정\n시작 노드에서 출발하여 현재 노드와 연결된 노드들 중 하나를 무작위로 선택하여 이동\n만약 선택된 노드가 이전에 방문한 노드이면 return parameter에 따라 이전 노드로 다시 돌아갈 확률이 높아짐\n반면, 이전 노드와 멀리 떨어진 노드를 샘플링하는 경우에는 in-out parameter에 따라 더 멀리 떨어진 노드를 샘플링하는 비율이 조절\n랜덤 워크 시퀀스는 각 노드의 방문 횟수, 이웃 노드의 구성 등 그래프의 구조적 특성을 반영"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph2.html#node2vec-example",
    "href": "posts/Graph Machine Learning/graph2.html#node2vec-example",
    "title": "CH2. 그래프 머신러닝",
    "section": "Node2Vec example",
    "text": "Node2Vec example\n\n\nimport networkx as nx\nfrom node2vec import Node2Vec\n\nG = nx.barbell_graph(m1=7, m2=4)  # 바벨 그래프 생성\ndraw_graph(G, nx.spring_layout(G))\n\nnode2vec = Node2Vec(G, dimensions=2) # 그래프의 각 노드를 2차원 벡터로 생성\nmodel = node2vec.fit(window=10) # 원본 그래프 노드에 임베딩 알고리즘 적용해 생성된 2차원 벡터\n     \n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:00<00:00, 346.40it/s]"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph2.html#edge2vec-알고리즘-구현-방법",
    "href": "posts/Graph Machine Learning/graph2.html#edge2vec-알고리즘-구현-방법",
    "title": "CH2. 그래프 머신러닝",
    "section": "Edge2Vec 알고리즘 구현 방법",
    "text": "Edge2Vec 알고리즘 구현 방법\n1 그래프 데이터 로딩\n2 모델 학습\n3 엣지 임베딩: 학습된 Edge2Vec모델을 이용해 각 엣지를 임베딩 벡터로 변환하고 임베딩 벡터의 차원 수는 모델 하이퍼파라미터 값에 따른다.\n4 분류 또는 유사도 측정"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph2.html#edge2vec-example",
    "href": "posts/Graph Machine Learning/graph2.html#edge2vec-example",
    "title": "CH2. 그래프 머신러닝",
    "section": "Edge2Vec example",
    "text": "Edge2Vec example\n\nfrom node2vec.edges import HadamardEmbedder # Node2Vec 모델을 사용하여 학습된 그래프 임베딩 모델에서 HadamardEmbedder를 이용하여 엣지 임베딩 벡터를 생성하고, 그것을 시각화하는 코드\n\nedges_embs = HadamardEmbedder(keyed_vectors=model.wv) # model.wv : 학습된 노드 입베딩 벡터를 가지고있는 객체\nfig, ax=plt.subplots(figsize=(10,10))\n\nfor x in G.edges():\n    v = edges_embs[(str(x[0]), str(x[1]))]\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=12)  # 엣지를 해당하는 점위에 텍스트로 나타냄, 즉 해당 점의 x,y좌표를 설정\n\n\n\n\n\nHadamardEmbedder: 두 노드의 임베딩 벡터를 element-wise 곱한 결과를 사용하여 엣지의 임베딩 벡터를 생성"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph2.html#graph2vec-알고리즘-구현-방법",
    "href": "posts/Graph Machine Learning/graph2.html#graph2vec-알고리즘-구현-방법",
    "title": "CH2. 그래프 머신러닝",
    "section": "Graph2Vec 알고리즘 구현 방법",
    "text": "Graph2Vec 알고리즘 구현 방법\n1 그래프 분할: 서로 겹치지 않게 입력 그래프를 여러 개의 부분 그래프로 분할\n2 순서 부여: 분할된 부분 그래프에 순서를 부여\n3 부분 그래프 임베딩\n4 전체 그래프 임베딩\n5 분류 또는 유사도 측정"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph2.html#graph2vec-example",
    "href": "posts/Graph Machine Learning/graph2.html#graph2vec-example",
    "title": "CH2. 그래프 머신러닝",
    "section": "Graph2Vec Example",
    "text": "Graph2Vec Example\n\npip install karateclub\n\nCollecting karateclub\n  Downloading karateclub-1.3.3.tar.gz (64 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.5/64.5 kB 5.2 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\nCollecting numpy<1.23.0\n  Downloading numpy-1.22.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.9/16.9 MB 81.5 MB/s eta 0:00:0000:0100:01\nCollecting networkx<2.7\n  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/1.9 MB 88.6 MB/s eta 0:00:00\nCollecting decorator==4.4.2\n  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\nRequirement already satisfied: tqdm in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from karateclub) (4.65.0)\nRequirement already satisfied: python-louvain in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from karateclub) (0.16)\nRequirement already satisfied: scikit-learn in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from karateclub) (1.2.2)\nRequirement already satisfied: scipy in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from karateclub) (1.10.1)\nCollecting pygsp\n  Downloading PyGSP-0.5.1-py2.py3-none-any.whl (1.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 78.9 MB/s eta 0:00:00\nRequirement already satisfied: gensim>=4.0.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from karateclub) (4.3.1)\nCollecting pandas<=1.3.5\n  Downloading pandas-1.3.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.5/11.5 MB 94.0 MB/s eta 0:00:0000:0100:01\nRequirement already satisfied: six in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from karateclub) (1.16.0)\nCollecting python-Levenshtein\n  Downloading python_Levenshtein-0.20.9-py3-none-any.whl (9.4 kB)\nRequirement already satisfied: smart-open>=1.8.1 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from gensim>=4.0.0->karateclub) (6.3.0)\nRequirement already satisfied: pytz>=2017.3 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from pandas<=1.3.5->karateclub) (2022.7.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from pandas<=1.3.5->karateclub) (2.8.2)\nCollecting Levenshtein==0.20.9\n  Downloading Levenshtein-0.20.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (174 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 174.0/174.0 kB 36.6 MB/s eta 0:00:00\nCollecting rapidfuzz<3.0.0,>=2.3.0\n  Downloading rapidfuzz-2.15.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 102.2 MB/s eta 0:00:00\nRequirement already satisfied: threadpoolctl>=2.0.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from scikit-learn->karateclub) (3.1.0)\nRequirement already satisfied: joblib>=1.1.1 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from scikit-learn->karateclub) (1.2.0)\nBuilding wheels for collected packages: karateclub\n  Building wheel for karateclub (setup.py) ... done\n  Created wheel for karateclub: filename=karateclub-1.3.3-py3-none-any.whl size=101987 sha256=9b0452b9dfbfaa045ccbf70aeee54c5ac34fdb4bf2d2af2d6facab840d6d499d\n  Stored in directory: /home/coco/.cache/pip/wheels/2b/93/72/8e0b3ec687bea23bd34bbb723a82fcb6b074cb756a29441f0c\nSuccessfully built karateclub\nInstalling collected packages: rapidfuzz, numpy, networkx, decorator, pandas, Levenshtein, python-Levenshtein, pygsp, karateclub\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.24.2\n    Uninstalling numpy-1.24.2:\n      Successfully uninstalled numpy-1.24.2\n  Attempting uninstall: networkx\n    Found existing installation: networkx 2.8.8\n    Uninstalling networkx-2.8.8:\n      Successfully uninstalled networkx-2.8.8\n  Attempting uninstall: decorator\n    Found existing installation: decorator 5.1.1\n    Uninstalling decorator-5.1.1:\n      Successfully uninstalled decorator-5.1.1\n  Attempting uninstall: pandas\n    Found existing installation: pandas 1.5.3\n    Uninstalling pandas-1.5.3:\n      Successfully uninstalled pandas-1.5.3\nSuccessfully installed Levenshtein-0.20.9 decorator-4.4.2 karateclub-1.3.3 networkx-2.6.3 numpy-1.23.5 pandas-1.3.5 pygsp-0.5.1 python-Levenshtein-0.20.9 rapidfuzz-2.15.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport random\nimport matplotlib.pyplot as plt\n\nfrom karateclub import Graph2Vec\n\nn_graphs = 20\n\ndef generate_radom():\n    n = random.randint(6, 20)  # 노드 갯수\n    k = random.randint(5, n)   # k-최근접 이웃 개수\n    p = random.uniform(0, 1)   # 연결 확률 파라미터\n    return nx.watts_strogatz_graph(n,k,p), [n,k,p]   # 함수를 사용하여 20개의 무작위 그래프 생성\n\nGs = [generate_radom() for x in range(n_graphs)]\n\nmodel = Graph2Vec(dimensions=2, wl_iterations=10)  # 임베딩 벡터 차원수: 2차원\nmodel.fit([x[0] for x in Gs])\nembeddings = model.get_embedding()\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor i,vec in enumerate(embeddings):\n    \n    ax.scatter(vec[0],vec[1], s=1000)\n    ax.annotate(str(i), (vec[0],vec[1]), fontsize=40)"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph2.html#얕은-임베딩",
    "href": "posts/Graph Machine Learning/graph2.html#얕은-임베딩",
    "title": "CH2. 그래프 머신러닝",
    "section": "얕은 임베딩",
    "text": "얕은 임베딩\n\n학습된 입력 데이터에 대한 임베딩 값만 학습하고 반환\nNode2Vec, Edge2Vec, Graph2Vec\n학습한 데이터의 벡터 표현만 반환\n보이지 않는 데이터에 대한 임베딩 벡터는 못얻는다.\n지도학습/비지도학습 각각 정의 가능\n\n\nmodel.fit(graphs_list) #graphs_list 학습\nembedding=model.get_embedding()[i]"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph2.html#그래프-자동-인코딩",
    "href": "posts/Graph Machine Learning/graph2.html#그래프-자동-인코딩",
    "title": "CH2. 그래프 머신러닝",
    "section": "그래프 자동 인코딩",
    "text": "그래프 자동 인코딩\n\n보이지 않는 인스턴스에 대한 임베딩 벡터도 생성\n비지도 학습에 적합\n\n\nmodel.fit(graphs_list) #graphs_list 학습\nembedding=model.get_embedding(G) #보이지 않는 새로운 그래프 G의 임베딩 벡터 생성"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph2.html#근방-집계",
    "href": "posts/Graph Machine Learning/graph2.html#근방-집계",
    "title": "CH2. 그래프 머신러닝",
    "section": "근방 집계",
    "text": "근방 집계\n\n그래프 수준에서 임베딩 추출\n노드는 일부 소석승 기반으로 라벨 지정\n일반 사상 함수 \\(f(G)\\)를 학습 할 수 있고 보이지 않는 인스턴스에 대한 임베딩 벡터도 생성 가능"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph2.html#그래프-정규화",
    "href": "posts/Graph Machine Learning/graph2.html#그래프-정규화",
    "title": "CH2. 그래프 머신러닝",
    "section": "그래프 정규화",
    "text": "그래프 정규화\n\n그래프를 입력으로 받지 않는다.\n프로세스를 정규화하고자 상호작용을 나타내는 특정집합 학습\n준지도학습/지도 학습에 사용"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph8(logistic+graph).html",
    "href": "posts/Graph Machine Learning/graph8(logistic+graph).html",
    "title": "CH8. 신용카드 거래 분석(로지스틱+그래프)",
    "section": "",
    "text": "import os\nimport math\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndefault_edge_color = 'gray'\ndefault_node_color = '#407cc9'\nenhanced_node_color = '#f5b042'\nenhanced_edge_color = '#cc2f04'\n\n\ndf\n\nimport pandas as pd\ndf = pd.read_csv(\"fraudTrain.csv\")\ndf = df[df[\"is_fraud\"]==0].sample(frac=0.20, random_state=42).append(df[df[\"is_fraud\"] == 1])\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      trans_date_trans_time\n      cc_num\n      merchant\n      category\n      amt\n      first\n      last\n      gender\n      street\n      ...\n      lat\n      long\n      city_pop\n      job\n      dob\n      trans_num\n      unix_time\n      merch_lat\n      merch_long\n      is_fraud\n    \n  \n  \n    \n      669418\n      669418\n      2019-10-12 18:21\n      4.089100e+18\n      fraud_Haley, Jewess and Bechtelar\n      shopping_pos\n      7.53\n      Debra\n      Stark\n      F\n      686 Linda Rest\n      ...\n      32.3836\n      -94.8653\n      24536\n      Multimedia programmer\n      1983-10-14\n      d313353fa30233e5fab5468e852d22fc\n      1350066071\n      32.202008\n      -94.371865\n      0\n    \n    \n      32567\n      32567\n      2019-01-20 13:06\n      4.247920e+12\n      fraud_Turner LLC\n      travel\n      3.79\n      Judith\n      Moss\n      F\n      46297 Benjamin Plains Suite 703\n      ...\n      39.5370\n      -83.4550\n      22305\n      Television floor manager\n      1939-03-09\n      88c65b4e1585934d578511e627fe3589\n      1327064760\n      39.156673\n      -82.930503\n      0\n    \n    \n      156587\n      156587\n      2019-03-24 18:09\n      4.026220e+12\n      fraud_Klein Group\n      entertainment\n      59.07\n      Debbie\n      Payne\n      F\n      204 Ashley Neck Apt. 169\n      ...\n      41.5224\n      -71.9934\n      4720\n      Broadcast presenter\n      1977-05-18\n      3bd9ede04b5c093143d5e5292940b670\n      1332612553\n      41.657152\n      -72.595751\n      0\n    \n    \n      1020243\n      1020243\n      2020-02-25 15:12\n      4.957920e+12\n      fraud_Monahan-Morar\n      personal_care\n      25.58\n      Alan\n      Parsons\n      M\n      0547 Russell Ford Suite 574\n      ...\n      39.6171\n      -102.4776\n      207\n      Network engineer\n      1955-12-04\n      19e16ee7a01d229e750359098365e321\n      1361805120\n      39.080346\n      -103.213452\n      0\n    \n    \n      116272\n      116272\n      2019-03-06 23:19\n      4.178100e+15\n      fraud_Kozey-Kuhlman\n      personal_care\n      84.96\n      Jill\n      Flores\n      F\n      639 Cruz Islands\n      ...\n      41.9488\n      -86.4913\n      3104\n      Horticulturist, commercial\n      1981-03-29\n      a0c8641ca1f5d6e243ed5a2246e66176\n      1331075954\n      42.502065\n      -86.732664\n      0\n    \n  \n\n5 rows × 23 columns\n\n\n\n\ndf[\"is_fraud\"].value_counts()\n\n0    208514\n1      6006\nName: is_fraud, dtype: int64\n\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x>0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.30, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00<?, ?it/s]\n\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04<00:00,  2.29it/s]\n\n\n\n\n_df2\n\ncus_list = set(_df.query('is_fraud==1').cc_num.tolist())\n_df2 = _df.query(\"cc_num in @ cus_list\")\n_df2 = _df2.assign(time= list(map(lambda x: int(x.split(' ')[-1].split(':')[0]), _df2['trans_date_trans_time'])))\n\n\n_df2[\"is_fraud\"].value_counts()\n\n0    645424\n1      6006\nName: is_fraud, dtype: int64\n\n\n\ndf = _df2 \n\n\ndef build_graph_bipartite2(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x>0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled2 = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled2 = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled2.is_fraud.value_counts())\nG_down2 = build_graph_bipartite(df_downsampled2)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges2, test_edges2, train_labels2, test_labels2 = train_test_split(list(range(len(G_down2.edges))), \n                                                                      list(nx.get_edge_attributes(G_down2, \"label\").values()), \n                                                                      test_size=0.30, \n                                                                      random_state=42)\n\n\nedgs2 = list(G_down2.edges)\ntrain_graph2 = G_down2.edge_subgraph([edgs[x] for x in train_edges2]).copy()\ntrain_graph2.add_nodes_from(list(set(G_down2.nodes) - set(train_graph2.nodes)))\n\n\nnode2vec_train2 = Node2Vec(train_graph2, weight_key='weight')\nmodel_train2 = node2vec_train2.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00<?, ?it/s]\n\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04<00:00,  2.40it/s]\n\n\n\n\ntraing(graph), test(logistic)\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges2]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred2 = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred2)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred2)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred2)) \n\n<class 'node2vec.edges.HadamardEmbedder'>\nPrecision: 0.6554307116104869\nRecall: 0.09599561162918267\nF1-Score: 0.16746411483253587\n<class 'node2vec.edges.AverageEmbedder'>\nPrecision: 0.7195710455764075\nRecall: 0.7361492046077893\nF1-Score: 0.7277657266811279\n<class 'node2vec.edges.WeightedL1Embedder'>\nPrecision: 0.4666666666666667\nRecall: 0.01151947339550192\nF1-Score: 0.022483940042826552\n<class 'node2vec.edges.WeightedL2Embedder'>\nPrecision: 0.5319148936170213\nRecall: 0.013713658804168952\nF1-Score: 0.026737967914438502"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph8(logistic-amt+time+citypop).html",
    "href": "posts/Graph Machine Learning/graph8(logistic-amt+time+citypop).html",
    "title": "CH8. 신용카드 거래 분석(로지스틱amt+time+city_pop-f1:0.986655)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n_df = pd.read_csv(\"fraudTrain.csv\")\n\n\ncus_list = set(_df.query('is_fraud==1').cc_num.tolist())\n_df2 = _df.query(\"cc_num in @ cus_list\")\n_df2 = _df2.assign(time= list(map(lambda x: int(x.split(' ')[-1].split(':')[0]), _df2['trans_date_trans_time'])))\n\n\n_df2.shape\n\n(651430, 24)\n\n\n\n_df2.columns\n\nIndex(['Unnamed: 0', 'trans_date_trans_time', 'cc_num', 'merchant', 'category',\n       'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip',\n       'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time',\n       'merch_lat', 'merch_long', 'is_fraud', 'time'],\n      dtype='object')\n\n\n\n_df2.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 651430 entries, 3 to 1048574\nData columns (total 24 columns):\n #   Column                 Non-Null Count   Dtype  \n---  ------                 --------------   -----  \n 0   Unnamed: 0             651430 non-null  int64  \n 1   trans_date_trans_time  651430 non-null  object \n 2   cc_num                 651430 non-null  float64\n 3   merchant               651430 non-null  object \n 4   category               651430 non-null  object \n 5   amt                    651430 non-null  float64\n 6   first                  651430 non-null  object \n 7   last                   651430 non-null  object \n 8   gender                 651430 non-null  object \n 9   street                 651430 non-null  object \n 10  city                   651430 non-null  object \n 11  state                  651430 non-null  object \n 12  zip                    651430 non-null  int64  \n 13  lat                    651430 non-null  float64\n 14  long                   651430 non-null  float64\n 15  city_pop               651430 non-null  int64  \n 16  job                    651430 non-null  object \n 17  dob                    651430 non-null  object \n 18  trans_num              651430 non-null  object \n 19  unix_time              651430 non-null  int64  \n 20  merch_lat              651430 non-null  float64\n 21  merch_long             651430 non-null  float64\n 22  is_fraud               651430 non-null  int64  \n 23  time                   651430 non-null  int64  \ndtypes: float64(6), int64(6), object(12)\nmemory usage: 124.3+ MB\n\n\nmerch_lat과 merch_long 은 상점의 위도 경도, 위의 lat과 long은 고객의 ??\ndob는 생년월일(date of birth)을 나타내는 변수\nunix_time 1970년 1월 1일 0시 0분 0초(UTC)부터 경과된 시간을 초(second) 단위로 표현하는 방법\nzip 우편번호\n`\n\nlen(set(_df2['city']))\n\n576\n\n\n\n_df2[\"is_fraud\"].value_counts()\n\n0    645424\n1      6006\nName: is_fraud, dtype: int64\n\n\n\n_df2[\"is_fraud\"].value_counts()/len(_df2)\n\n0    0.99078\n1    0.00922\nName: is_fraud, dtype: float64\n\n\n\n_df2.groupby(by=['is_fraud']).agg({'city_pop':np.mean,'amt':np.mean,'time':np.mean})\n\n\n\n\n\n  \n    \n      \n      city_pop\n      amt\n      time\n    \n    \n      is_fraud\n      \n      \n      \n    \n  \n  \n    \n      0\n      83870.443845\n      67.743047\n      12.813152\n    \n    \n      1\n      96323.951715\n      530.573492\n      13.915917\n    \n  \n\n\n\n\n\n_df3=_df2[['amt','time','city_pop','is_fraud']]\n\n\n_df3.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 651430 entries, 3 to 1048574\nData columns (total 4 columns):\n #   Column    Non-Null Count   Dtype  \n---  ------    --------------   -----  \n 0   amt       651430 non-null  float64\n 1   time      651430 non-null  int64  \n 2   city_pop  651430 non-null  int64  \n 3   is_fraud  651430 non-null  int64  \ndtypes: float64(1), int64(3)\nmemory usage: 24.9 MB\n\n\n\ndata=np.hstack([_df3.values[:,:]])\n\n\ndata\n\narray([[4.50000e+01, 0.00000e+00, 1.93900e+03, 0.00000e+00],\n       [9.46300e+01, 0.00000e+00, 2.15800e+03, 0.00000e+00],\n       [4.45400e+01, 0.00000e+00, 2.69100e+03, 0.00000e+00],\n       ...,\n       [6.03000e+00, 1.60000e+01, 5.20000e+02, 0.00000e+00],\n       [1.16940e+02, 1.60000e+01, 1.58300e+03, 0.00000e+00],\n       [6.81000e+00, 1.60000e+01, 1.65556e+05, 0.00000e+00]])\n\n\n\nX = data[:,:-1]\ny = data[:,-1]\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n\n\nlr = LogisticRegression()\n\n\nlr.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\ny_pred=lr.predict(X_test)\n\n\nacc= accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1score = f1_score(y_test, y_pred, average='weighted')\nprint(\"Accuracy: {}\".format(acc))\nprint(\"Precision: {}\".format(precision))\nprint(\"Recall: {}\".format(recall))\nprint(\"F1 score: {}\".format(f1score))\n\nAccuracy: 0.9902215126721213\nPrecision: 0.9831134944972946\nRecall: 0.9902215126721213\nF1 score: 0.9866547019260462\n\n\n\nacc= accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nf1score = f1_score(y_test, y_pred, average='macro')\nprint(\"Accuracy: {}\".format(acc))\nprint(\"Precision:{}\".format(precision))\nprint(\"Recall: {}\".format(recall))\nprint(\"F1 score: {}\".format(f1score))\n\nAccuracy: 0.9902215126721213\nPrecision:0.4957576316517569\nRecall: 0.49934201359322505\nF1 score: 0.49754336709114605\n\n\n\nf1 score가 엄청 커졌다. 이유가 뭘까? 처음에 city_pop에 대한 걸 생각했을때는 사기거래=0과 사기거래=1의 큰 차이가 없어보였는데 갑자기…"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph8.사기거래=0필터.html",
    "href": "posts/Graph Machine Learning/graph8.사기거래=0필터.html",
    "title": "CH8. 신용카드 거래 분석(사기거래=0제외:_df2)",
    "section": "",
    "text": "df에서 사기거래가 한 번이라도 있었던 사람의 데이터만 모아서 학습시켜봄. 즉, 사기거래가 한번이라도 없던 사람의 데이터는 제거했다.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n_df = pd.read_csv(\"fraudTrain.csv\")\n\n\ncus_list = set(_df.query('is_fraud==1').cc_num.tolist())\n_df2 = _df.query(\"cc_num in @ cus_list\")\n_df2 = _df2.assign(time= list(map(lambda x: int(x.split(' ')[-1].split(':')[0]), _df2['trans_date_trans_time'])))\n\n\n_df2.shape\n\n(651430, 24)\n\n\n\n_df2.columns\n\nIndex(['Unnamed: 0', 'trans_date_trans_time', 'cc_num', 'merchant', 'category',\n       'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip',\n       'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time',\n       'merch_lat', 'merch_long', 'is_fraud', 'time'],\n      dtype='object')\n\n\n\n_df2.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 651430 entries, 3 to 1048574\nData columns (total 24 columns):\n #   Column                 Non-Null Count   Dtype  \n---  ------                 --------------   -----  \n 0   Unnamed: 0             651430 non-null  int64  \n 1   trans_date_trans_time  651430 non-null  object \n 2   cc_num                 651430 non-null  float64\n 3   merchant               651430 non-null  object \n 4   category               651430 non-null  object \n 5   amt                    651430 non-null  float64\n 6   first                  651430 non-null  object \n 7   last                   651430 non-null  object \n 8   gender                 651430 non-null  object \n 9   street                 651430 non-null  object \n 10  city                   651430 non-null  object \n 11  state                  651430 non-null  object \n 12  zip                    651430 non-null  int64  \n 13  lat                    651430 non-null  float64\n 14  long                   651430 non-null  float64\n 15  city_pop               651430 non-null  int64  \n 16  job                    651430 non-null  object \n 17  dob                    651430 non-null  object \n 18  trans_num              651430 non-null  object \n 19  unix_time              651430 non-null  int64  \n 20  merch_lat              651430 non-null  float64\n 21  merch_long             651430 non-null  float64\n 22  is_fraud               651430 non-null  int64  \n 23  time                   651430 non-null  int64  \ndtypes: float64(6), int64(6), object(12)\nmemory usage: 124.3+ MB\n\n\n\n_df2[\"is_fraud\"].value_counts()\n\n0    645424\n1      6006\nName: is_fraud, dtype: int64\n\n\n\n_df2[\"is_fraud\"].value_counts()/len(_df2)\n\n0    0.99078\n1    0.00922\nName: is_fraud, dtype: float64\n\n\n\n_df2.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      trans_date_trans_time\n      cc_num\n      merchant\n      category\n      amt\n      first\n      last\n      gender\n      street\n      ...\n      long\n      city_pop\n      job\n      dob\n      trans_num\n      unix_time\n      merch_lat\n      merch_long\n      is_fraud\n      time\n    \n  \n  \n    \n      3\n      3\n      2019-01-01 0:01\n      3.534090e+15\n      fraud_Kutch, Hermiston and Farrell\n      gas_transport\n      45.00\n      Jeremy\n      White\n      M\n      9443 Cynthia Court Apt. 038\n      ...\n      -112.1138\n      1939\n      Patent attorney\n      1967-01-12\n      6b849c168bdad6f867558c3793159a81\n      1325376076\n      47.034331\n      -112.561071\n      0\n      0\n    \n    \n      5\n      5\n      2019-01-01 0:04\n      4.767270e+15\n      fraud_Stroman, Hudson and Erdman\n      gas_transport\n      94.63\n      Jennifer\n      Conner\n      F\n      4655 David Island\n      ...\n      -75.2045\n      2158\n      Transport planner\n      1961-06-19\n      189a841a0a8ba03058526bcfe566aab5\n      1325376248\n      40.653382\n      -76.152667\n      0\n      0\n    \n    \n      6\n      6\n      2019-01-01 0:04\n      3.007470e+13\n      fraud_Rowe-Vandervort\n      grocery_net\n      44.54\n      Kelsey\n      Richards\n      F\n      889 Sarah Station Suite 624\n      ...\n      -100.9893\n      2691\n      Arboriculturist\n      1993-08-16\n      83ec1cc84142af6e2acf10c44949e720\n      1325376282\n      37.162705\n      -100.153370\n      0\n      0\n    \n    \n      7\n      7\n      2019-01-01 0:05\n      6.011360e+15\n      fraud_Corwin-Collins\n      gas_transport\n      71.65\n      Steven\n      Williams\n      M\n      231 Flores Pass Suite 720\n      ...\n      -78.6003\n      6018\n      Designer, multimedia\n      1947-08-21\n      6d294ed2cc447d2c71c7171a3d54967c\n      1325376308\n      38.948089\n      -78.540296\n      0\n      0\n    \n    \n      8\n      8\n      2019-01-01 0:05\n      4.922710e+15\n      fraud_Herzog Ltd\n      misc_pos\n      4.27\n      Heather\n      Chase\n      F\n      6888 Hicks Stream Suite 954\n      ...\n      -79.6607\n      1472\n      Public affairs consultant\n      1941-03-07\n      fc28024ce480f8ef21a32d64c93a29f5\n      1325376318\n      40.351813\n      -79.958146\n      0\n      0\n    \n  \n\n5 rows × 24 columns\n\n\n\n\ndf = _df2\n\n\nimport os\nimport math\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndefault_edge_color = 'gray'\ndefault_node_color = '#407cc9'\nenhanced_node_color = '#f5b042'\nenhanced_edge_color = '#cc2f04'\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x>0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n    \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n    \n    \n    return G\n    \n\n\nG_bu = build_graph_bipartite(df, nx.Graph(name=\"Bipartite Undirect\"))\n\n\nG_tu = build_graph_tripartite(df, nx.Graph())\n\n\nfor G in [G_bu, G_tu]:\n    print(\"nede:\", nx.number_of_nodes(G))\nfor G in [G_bu, G_tu]:\n    print(\"edge:\",nx.number_of_edges(G))\n\nnede: 1289\nnede: 652719\nedge: 268964\nedge: 1302860\n\n\n\nfor G in [G_bu, G_tu]:\n    print(nx.degree_pearson_correlation_coefficient(G))\n\n-0.37874509971252246\n-0.7222260615980979\n\n\n\nimport networkx as nx\nimport community\n\n\nimport community\nfor G in [G_bu, G_tu]:\n    parts = community.best_partition(G, random_state=42, weight='weight')\n\n\ncommunities = pd.Series(parts)\n\n\nprint(communities.value_counts().sort_values(ascending=False))\n\n2      21225\n34      9652\n117     9362\n3       8477\n78      8384\n       ...  \n127      812\n110      793\n121      785\n103      449\n95       439\nLength: 142, dtype: int64\n\n\n\ncommunities.value_counts().plot.hist(bins=20)\n\n<Axes: ylabel='Frequency'>\n\n\n\n\n\n\ngraphs = [] # 부분그래프 저장\nd = {}  # 부정 거래 비율 저장 \nfor x in communities.unique():\n    tmp = nx.subgraph(G, communities[communities==x].index)\n    fraud_edges = sum(nx.get_edge_attributes(tmp, \"label\").values())\n    ratio = 0 if fraud_edges == 0 else (fraud_edges/tmp.number_of_edges())*100\n    d[x] = ratio\n    graphs += [tmp]\n\npd.Series(d).sort_values(ascending=False)\n\n99     2.549511\n69     2.080000\n4      1.996198\n43     1.969697\n133    1.871491\n         ...   \n127    0.246609\n91     0.120192\n104    0.115207\n103    0.000000\n121    0.000000\nLength: 142, dtype: float64\n\n\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00<?, ?it/s]\n\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:03<00:00,  3.09it/s]\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n<class 'node2vec.edges.HadamardEmbedder'>\nPrecision: 0.5428571428571428\nRecall: 0.016725352112676055\nF1-Score: 0.032450896669513236\n<class 'node2vec.edges.AverageEmbedder'>\nPrecision: 0.6840981856990395\nRecall: 0.5642605633802817\nF1-Score: 0.6184273999035215\n<class 'node2vec.edges.WeightedL1Embedder'>\nPrecision: 0.45454545454545453\nRecall: 0.022007042253521125\nF1-Score: 0.041981528127623846\n<class 'node2vec.edges.WeightedL2Embedder'>\nPrecision: 0.46296296296296297\nRecall: 0.022007042253521125\nF1-Score: 0.04201680672268907\n\n\n오히려 f1score가 낮아졌다…"
  },
  {
    "objectID": "posts/Graph Machine Learning/graph8.df원본에서진행.html",
    "href": "posts/Graph Machine Learning/graph8.df원본에서진행.html",
    "title": "CH8. 신용카드 거래 분석(df원본데이터)",
    "section": "",
    "text": "df에서 그냥 처음부터 샘플뽑지 않고 전체 데이터를 한번 돌려보자. f1-score가 어떻게 나오는지 기존 샘플한거랑 비교해보기\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n_df = pd.read_csv(\"fraudTrain.csv\")\n\n\n_df[\"is_fraud\"].value_counts()\n\n0    1042569\n1       6006\nName: is_fraud, dtype: int64\n\n\n\n_df[\"is_fraud\"].value_counts()/len(_df)\n\n0    0.994272\n1    0.005728\nName: is_fraud, dtype: float64\n\n\n\ndf = _df\n\n\nimport os\nimport math\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndefault_edge_color = 'gray'\ndefault_node_color = '#407cc9'\nenhanced_node_color = '#f5b042'\nenhanced_edge_color = '#cc2f04'\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x>0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00<?, ?it/s]\n\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:03<00:00,  2.53it/s]\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n<class 'node2vec.edges.HadamardEmbedder'>\nPrecision: 0.7345132743362832\nRecall: 0.1424892703862661\nF1-Score: 0.23867721063982747\n<class 'node2vec.edges.AverageEmbedder'>\nPrecision: 0.6886792452830188\nRecall: 0.751931330472103\nF1-Score: 0.7189167008617152\n<class 'node2vec.edges.WeightedL1Embedder'>\nPrecision: 0.6136363636363636\nRecall: 0.02317596566523605\nF1-Score: 0.04466501240694789\n<class 'node2vec.edges.WeightedL2Embedder'>\nPrecision: 0.66\nRecall: 0.02832618025751073\nF1-Score: 0.05432098765432099\n\n\n\n음 어차피 나중에 downsampled를 하기 때문에 … f1 score값은 샘플을 무얼 하든 큰 차이가 없다. 오히려 더 낮은듯"
  },
  {
    "objectID": "posts/Machine Learning/MachineLearning_midterm(202250926).html",
    "href": "posts/Machine Learning/MachineLearning_midterm(202250926).html",
    "title": "기계학습 midterm",
    "section": "",
    "text": "import torch \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom fastai.vision.all import *\n\n\n1. 크롤링을 통한 이미지 분석 및 CAM\n\n#\n# 크롤링에 필요한 준비작업들\n!pip install -Uqq duckduckgo_search\nfrom duckduckgo_search import ddg_images\nfrom fastdownload import download_url\nfrom fastcore.all import *\ndef search_images(term, max_images=200): return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nflask 1.1.4 requires click<8.0,>=5.1, but you have click 8.1.3 which is incompatible.\n\n\n\n# \n# 폴더만드는코드 -- 사실 손으로 만들어도 무방함.. \n!mkdir images\n!mkdir images/train\n!mkdir images/test \n!mkdir images/train/Harry Potter\n!mkdir images/train/Ronald Bilius Weasley\n!mkdir images/test/Harry Potter\n!mkdir images/test/Ronald Bilius Weasley\n\nmkdir: cannot create directory ‘images’: File exists\nmkdir: cannot create directory ‘images/train’: File exists\nmkdir: cannot create directory ‘images/test’: File exists\nmkdir: cannot create directory ‘Potter’: File exists\nmkdir: cannot create directory ‘Bilius’: File exists\nmkdir: cannot create directory ‘Weasley’: File exists\n\n\n\ndownload_images(dest='./images/train/Harry Potter',urls=search_images('Harry Potter',max_images=200))\ntime.sleep(10) # 서버과부하를 위한 휴식코드 \ndownload_images(dest='./images/train/Ronald Bilius Weasley',urls=search_images('Ronald Bilius Weasley',max_images=200)) \ntime.sleep(10) # 서버과부하를 위한 휴식코드 \ndownload_images(dest='./images/train/Harry Potter',urls=search_images('Harry Potter movie',max_images=200))  \ntime.sleep(10) # 서버과부하를 위한 휴식코드 \ndownload_images(dest='./images/train/Ronald Bilius Weasley',urls=search_images('Ronald Weasley',max_images=200))\ntime.sleep(10) # 서버과부하를 위한 휴식코드 \n\n\ndownload_images(dest='./images/test/Harry Potter',urls=search_images('Harry Potter photo',max_images=50)) \ntime.sleep(10) # 서버과부하를 위한 휴식코드 \ndownload_images(dest='./images/test/Ronald Bilius Weasley',urls=search_images('Ronald Bilius Weasley photo',max_images=50))  \ntime.sleep(10) # 서버과부하를 위한 휴식코드 \n\n\nbad_images = verify_images(get_image_files('./images'))\nbad_images\n\n(#62) [Path('images/train/Harry Potter/d75f9d42-7db5-418a-bc65-8a10c4dd6df3.jpg'),Path('images/train/Harry Potter/fbdce828-967e-4ee0-99dc-74b065cec931.jpg'),Path('images/train/Harry Potter/f2216c3c-9d40-4551-8dbc-0812b61be308.jpg'),Path('images/train/Harry Potter/72319413-8aed-4ff8-bd15-9247c9e6b9fc.jpg'),Path('images/train/Harry Potter/5a5656d6-b298-43f1-88dd-526eb597258a.jpg'),Path('images/train/Harry Potter/cfa5a057-4bbe-43a1-9bdd-7e6214fef746.jpg'),Path('images/train/Harry Potter/6902097c-8e55-4fc7-b5c8-aa6cb88a287e.jpg'),Path('images/train/Harry Potter/e1af5803-a0f0-4311-9c54-95b1506ee512.jpg'),Path('images/train/Harry Potter/1a6e2dd5-d8d0-4220-969f-05aa234af09d.jpg'),Path('images/train/Harry Potter/506feddc-f171-4808-9643-66183bc85d6d.jpg')...]\n\n\n\nbad_images.map(Path.unlink)\n\n(#62) [None,None,None,None,None,None,None,None,None,None...]\n\n\n\ndls = ImageDataLoaders.from_folder(path = './images', train='train',valid='test',item_tfms=Resize(512),bs=8) \n\n\ndls.show_batch()\n\n\n\n\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.944444\n      1.663112\n      0.458763\n      01:01\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.539615\n      1.090526\n      0.567010\n      01:03\n    \n  \n\n\n\n\nnet1= lrnr.model[0]\nnet2= lrnr.model[1]\n\n\n_X, _y = dls.one_batch() \n\n\nnet1.to(\"cpu\")\nnet2.to(\"cpu\") \n_X = _X.to(\"cpu\")\n\n\nnet2= torch.nn.Sequential(\n    torch.nn.AdaptiveAvgPool2d(output_size=1), \n    torch.nn.Flatten(), \n    torch.nn.Linear(512,2,bias=False) \n)\n\n\nnet = torch.nn.Sequential(\n    net1,\n    net2\n)\n\n\nlrnr2= Learner(dls,net,metrics=accuracy)\n\n\nlrnr2.fine_tune(5) \n\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00<?]\n    \n    \n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n  \n\n\n    \n      \n      0.00% [0/140 00:00<?]\n    \n    \n\n\nRuntimeError: ignored\n\n\n\ndls.vocab\n# net(x)에서 뒤쪽의 값이 클수록 \"yunakim\" 을 의미한다.\n\n['yeonkyungkim', 'yunakim']\n\n\n\nximg = PILImage.create('/content/images/test/yeonkyungkim/5f00874f-87e0-47ea-b299-411238c078f3.jpg')\nx = first(dls.test_dl([ximg]))[0]\n\n\nwhy = torch.einsum('cb,abij->acij',net2[2].weight,net1(x))\n\n\nnet2[0](why)\n\nTensorImage([[[[0.2771]],\n\n              [[0.0351]]]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\nnet(x)\n\nTensorImage([[0.2771, 0.0351]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\nximg\n\n\n\n\n\nnet2(net1(x))\n\nTensorImage([[0.2771, 0.0351]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\n0.2771>0.0421 이므로 ximg는 yenkyungkim일 확률이 더 높다.\n\n\n(why[0,0,:,:]).mean(), (why[0,1,:,:]).mean()\n\n(TensorImage(0.2771, device='cuda:0', grad_fn=<AliasBackward0>),\n TensorImage(0.0351, device='cuda:0', grad_fn=<AliasBackward0>))\n\n\n\n(why[0,0,:,:]).to(torch.int64)\n\nTensorImage([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  0,  0],\n             [ 0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  0,  0],\n             [ 0,  0, -2, -3, -2, -1,  0,  0,  0,  0,  1,  1,  1,  1,  0,  0],\n             [ 0, -1, -3, -3, -1,  0,  0,  0,  0,  0,  1,  1,  1,  1,  0,  0],\n             [ 0, -3, -3, -2,  0,  1,  0,  0,  0,  0,  1,  1,  1,  1,  0,  0],\n             [-1, -4, -3,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  0,  0],\n             [-2, -3, -1,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  0,  1],\n             [-1, -2, -1,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  3],\n             [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  2,  2],\n             [ 0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0],\n             [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 0,  0,  1,  1,  1,  1,  1,  1,  1,  0,  0,  0,  0,  0,  0,  0],\n             [ 0,  1,  2,  2,  1,  2,  2,  2,  1,  0,  0,  0,  0,  0,  0,  0]],\n            device='cuda:0')\n\n\n\nwhy_yeonkyungkim = why[0,0,:,:]\nwhy_yunakim = why[0,1,:,:]\n\n\nfig, ax = plt.subplots(1,3,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_yeonkyungkim.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear')\nax[2].imshow(why_yunakim.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear')\n\n<matplotlib.image.AxesImage at 0x7f48ffaa2ad0>\n\n\n\n\n\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_yeonkyungkim.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_yunakim.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\n\n<matplotlib.image.AxesImage at 0x7f4844ce5d10>\n\n\n\n\n\n\nsftmax = torch.nn.Softmax(dim=1)\n\n\npath = './images'\n\n\nfig, ax = plt.subplots(5,5) \nk=0 \nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -> acij', net2[2].weight, net1(x))\n        why_yeonkyungkim = why[0,0,:,:] \n        why_yunakim = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob>dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_yeonkyungkim.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"yeonkyungkim(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_yunakim.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"yunakim(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()\n\n\n\n\n\nfig, ax = plt.subplots(5,5) \nk=25\nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -> acij', net2[2].weight, net1(x))\n        why_yeonkyungkim = why[0,0,:,:] \n        why_yunakim = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob>dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_yeonkyungkim.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"yeonkyungkim(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_yunakim.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"yunakim(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()\n\n\n\n\n\nfig, ax = plt.subplots(5,5) \nk=50\nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -> acij', net2[2].weight, net1(x))\n        why_yeonkyungkim = why[0,0,:,:] \n        why_yunakim = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob>dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_yeonkyungkim.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"yeonkyungkim(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_yunakim.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"yunakim(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()\n\n\n\n\n\nfig, ax = plt.subplots(5,5) \nk=100\nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -> acij', net2[2].weight, net1(x))\n        why_yeonkyungkim = why[0,0,:,:] \n        why_yunakim = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob>dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_yeonkyungkim.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"yeonkyungkim(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_yunakim.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"yunakim(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()\n\n\n\n\n\n\n2. Overparameterized Model\n\nx = torch.rand([1000,1])*2-1\ny = 3.14 + 6.28*x + torch.randn([1000,1]) \n\n\nplt.plot(x,y,'o',alpha=0.1)\n\n\n\n\n\nx[:5]\n\ntensor([[ 0.5621],\n        [-0.7204],\n        [ 0.2043],\n        [-0.9416],\n        [ 0.7436]])\n\n\n\n# (1) y= beta0 + beta1*x1 + e \nnet = torch.nn.Linear(1,1) \noptimizr = torch.optim.SGD(net.parameters(),lr=0.1)\n\n\nplt.plot(x,y,'o',alpha=0.1)\nplt.plot(x,net(x).data,'-')\n\n\n\n\n\nfor epoc in range(100):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = torch.mean((yhat-y)**2)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.1)\nplt.plot(x,net(x).data,'-')\n\n\n\n\n\nnet.weight.data, net.bias.data\n\n(tensor([[6.3128]]), tensor([3.1519]))\n\n\nbeta0 = 3.1519, beta1=6.3128\n\n# (2) y= beta0 + e\nw0hat = torch.tensor([0.00],requires_grad=True) \n\n\n# 학습전\nplt.plot(x,y,'o',alpha=0.1)\nplt.plot(x,(x*0+w0hat).data,'-')\n\n\n\n\n\nfor epoc in range(100):\n    ## 1 \n    yhat = x*0 + w0hat \n    ## 2 \n    loss = torch.mean((yhat-y)**2)\n    ## 3 \n    loss.backward()\n    ## 4 \n    w0hat.data = w0hat.data - 0.1 * w0hat.grad\n    w0hat.grad = None\n\n\n# 학습 후\nplt.plot(x,y,'o',alpha=0.1)\nplt.plot(x,(x*0+w0hat).data,'-')\n\n\n\n\n\nw0hat\n\ntensor([3.1982], requires_grad=True)\n\n\n\nbeta0 = 3.1982\n\n\n# (3) y=beta1*x + e \n\n\nnet = torch.nn.Linear(in_features=1,out_features=1,bias=False) \noptimizr = torch.optim.SGD(net.parameters(),lr=0.1) \n\n\n# 학습전\nplt.plot(x,y,'o',alpha=0.1)\nplt.plot(x,net(x).data,'-')\n\n\n\n\n\nfor epoc in range(100):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = torch.mean((y-yhat)**2)\n    ## 3 \n    loss.backward() \n    ## 4\n    optimizr.step()\n    optimizr.zero_grad() \n\n\n# 학습후\nplt.plot(x,y,'o',alpha=0.05)\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\nnet.weight\n\nParameter containing:\ntensor([[6.3822]], requires_grad=True)\n\n\n\nbeta1 = 6.3822\n\n\n# (4) y= alpha0 + beta0 + beta1*x + e\n_1 = torch.ones([1000,1])\nX = torch.concat([_1,x],axis=1)\n\n\nnet = torch.nn.Linear(in_features=2,out_features=1) \noptimizr = torch.optim.SGD(net.parameters(),lr=0.1) \n\n\n# 학습전\nplt.plot(x,y,'o',alpha=0.1)\nplt.plot(x,net(X).data,'-')\n\n\n\n\n\nfor epoc in range(100):\n    ## 1 \n    yhat = net(X) \n    ## 2 \n    loss = torch.mean((y-yhat)**2)\n    ## 3 \n    loss.backward() \n    ## 4\n    optimizr.step()\n    optimizr.zero_grad() \n\n\n# 학습후\nplt.plot(x,y,'o',alpha=0.05)\nplt.plot(x,net(X).data,'--')\n\n\n\n\n\nnet.bias,net.weight\n\n(Parameter containing:\n tensor([1.3951], requires_grad=True), Parameter containing:\n tensor([[1.7568, 6.3125]], requires_grad=True))\n\n\n\n# alpha0 + beta0\n1.3951+1.7568\n\n3.1519\n\n\n\nalpha0 + beat0 = 3.1519 이며 (1)에서 구한 beta0 값인 3.1519와 동일하다.\n\n\n# (5) y=alpha0 + beta0 + beta1*x + alpha1*x + e\nX = torch.concat([_1,_1,x,x],axis=1) \nX\n\ntensor([[ 1.0000,  1.0000,  0.5621,  0.5621],\n        [ 1.0000,  1.0000, -0.7204, -0.7204],\n        [ 1.0000,  1.0000,  0.2043,  0.2043],\n        ...,\n        [ 1.0000,  1.0000, -0.6327, -0.6327],\n        [ 1.0000,  1.0000,  0.8546,  0.8546],\n        [ 1.0000,  1.0000,  0.3835,  0.3835]])\n\n\n\nnet = torch.nn.Linear(in_features=4,out_features=1,bias=False) \noptimizr = torch.optim.SGD(net.parameters(),lr=0.1) \n\n\n# 학습전\nplt.plot(x,y,'o',alpha=0.1)\nplt.plot(x,net(X).data,'-')\n\n\n\n\n\nfor epoc in range(100):\n    ## 1 \n    yhat = net(X) \n    ## 2 \n    loss = torch.mean((y-yhat)**2)\n    ## 3 \n    loss.backward() \n    ## 4\n    optimizr.step()\n    optimizr.zero_grad() \n\n\n# 학습후\nplt.plot(x,y,'o',alpha=0.1)\nplt.plot(x,net(X).data,'-')\n\n\n\n\n\nnet.weight, net.bias\n\n(Parameter containing:\n tensor([[1.7991, 1.3527, 3.3789, 2.9408]], requires_grad=True), None)\n\n\n\n# alpha0 + beta0\n1.7991+1.3527\n\n3.1517999999999997\n\n\n\n# alpha1 + beta1\n3.3789+2.9408\n\n6.319699999999999\n\n\n\nalpha0+beta0 = 3.1518, alpha1+beta1 = 6.3197로 (1)에서 구한 값과 비슷하다.\n\n\n민정, 슬기, 성재, 세민, 구환 모두 옳은 설명\n\n\n\n3. 차원축소기법과 표현학습\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/STML2022/master/posts/iris.csv\")\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Sepal Length\n      Sepal Width\n      Petal Length\n      Petal Width\n      Species\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      145\n      6.7\n      3.0\n      5.2\n      2.3\n      virginica\n    \n    \n      146\n      6.3\n      2.5\n      5.0\n      1.9\n      virginica\n    \n    \n      147\n      6.5\n      3.0\n      5.2\n      2.0\n      virginica\n    \n    \n      148\n      6.2\n      3.4\n      5.4\n      2.3\n      virginica\n    \n    \n      149\n      5.9\n      3.0\n      5.1\n      1.8\n      virginica\n    \n  \n\n150 rows × 5 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nX = torch.tensor(df.drop(columns=['Species']).to_numpy(), dtype=torch.float32)\n\n\nX.shape\n\ntorch.Size([150, 4])\n\n\n\nl1 = torch.nn.Linear(in_features=4, out_features=2, bias=False)\nprint(l1(X).shape)\n\ntorch.Size([150, 2])\n\n\n\n# k=3이상인 경우 softmax와 crossentropyloss사용\n\na1=torch.nn.Softmax(dim=1)\nZ=a1(l1(X))\n\n\nl2 = torch.nn.Linear(in_features=2, out_features=4, bias=False)\n\n\nXhat = l2(l1(X))\n\n\nfig,ax = plt.subplots(figsize=(10,10)) \nax.imshow(torch.concat([X,Z.data,Xhat.data],axis=1)[:10])\nax.set_xticks(np.arange(0,10)) \nax.set_xticklabels([r'$X_1$',r'$X_2$',r'$X_3$',r'$X_4$',r'$Z_1$',r'$Z_2$',r'$\\hat{X}_1$',r'$\\hat{X}_2$',r'$\\hat{X}_3$',r'$\\hat{X}_4$'])\nax.vlines([3.5,5.5],ymin=-0.5,ymax=9.5,lw=2,color='red',linestyle='dashed')\nax.set_title(r'First 10 obs of $\\bf [X, Z, \\hat{X}]$ // before learning',size=25);\n\n\n\n\n\nnet = torch.nn.Sequential(\n    l1,   # 선형변환 \n    l2,\n)\n\nloss_fn= torch.nn.MSELoss()\noptimizr= torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(1000): \n    ## 1 \n    Z = net[0](X)\n    Xhat = net[1](Z) \n    ## 2 \n    loss=loss_fn(Xhat,X) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n\nfig,ax = plt.subplots(figsize=(10,10)) \nax.imshow(torch.concat([X,Z.data,Xhat.data],axis=1)[:10])\nax.set_xticks(np.arange(0,10)) \nax.set_xticklabels([r'$X_1$',r'$X_2$',r'$X_3$',r'$X_4$',r'$Z_1$',r'$Z_2$',r'$\\hat{X}_1$',r'$\\hat{X}_2$',r'$\\hat{X}_3$',r'$\\hat{X}_4$'])\nax.vlines([3.5,5.5],ymin=-0.5,ymax=9.5,lw=2,color='red',linestyle='dashed')\nax.set_title(r'First 10 obs of $\\bf [X, Z, \\hat{X}]$ // after learning',size=25);\n\n\n\n\n\n# (4) Z -> y 로 가는 네트워크 설계\n\nnet=torch.nn.Linear(2,4)\n\n\nnet(Z)[:5]\n\ntensor([[ 1.9397, -2.4390,  4.0082, -4.5496],\n        [ 1.8009, -2.3025,  3.7815, -4.3085],\n        [ 1.8454, -2.2254,  3.7054, -4.1847],\n        [ 1.8578, -2.2069,  3.6879, -4.1551],\n        [ 1.9715, -2.4088,  3.9845, -4.5026]], grad_fn=<SliceBackward0>)\n\n\n\nloss_fn= torch.nn.BCEWithLogitsLoss()\noptimizr= torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(1000): \n    ## 1 \n    Xhat = net(Z)\n    ## 2 \n    loss=loss_fn(Xhat,X) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n    # 오류가 나는데 밑에 글을 봐도 잘 모르겠어용.. ㅠㅠ \n\nRuntimeError: ignored\n\n\n\n규빈, 민정, 성재, 슬기 모두 옳은 설명이다."
  },
  {
    "objectID": "posts/Machine Learning/2022_11_29_13wk_2_final_checkpoint_ipynb의_사본.html",
    "href": "posts/Machine Learning/2022_11_29_13wk_2_final_checkpoint_ipynb의_사본.html",
    "title": "기계학습 final(교수님)",
    "section": "",
    "text": "기말고사"
  },
  {
    "objectID": "posts/Machine Learning/2022_11_29_13wk_2_final_checkpoint_ipynb의_사본.html#hihello-90점",
    "href": "posts/Machine Learning/2022_11_29_13wk_2_final_checkpoint_ipynb의_사본.html#hihello-90점",
    "title": "기계학습 final(교수님)",
    "section": "1. hi?hello!! (90점)",
    "text": "1. hi?hello!! (90점)\n아래와 같은 데이터가 있다고 하자.\n\ntxt = list('hi?hello!!')*100 \ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5], txt_y[:5]\n\n(['h', 'i', '?', 'h', 'e'], ['i', '?', 'h', 'e', 'l'])\n\n\ntxt_x와 txt_y를 이용하여 아래와 같은 순서로 다음문자를 예측하고 싶은 신경망을 설계하고 싶다.\nh \\(\\to\\) i \\(\\to\\) ? \\(\\to\\) h \\(\\to\\) e \\(\\to\\) l \\(\\to\\) l \\(\\to\\) o \\(\\to\\) ! \\(\\to\\) ! \\(\\to\\) h \\(\\to\\) i \\(\\to\\) ? \\(\\to\\) h \\(\\to\\) e \\(\\to\\) \\(\\dots\\)\n(1)-(6) 의 풀이에 공통적으로 필요한 과정 정리\n\ndef f(txt,mapping):\n    return [mapping[key] for key in txt] \nsig = torch.nn.Sigmoid()\nsoft = torch.nn.Softmax(dim=1)\ntanh = torch.nn.Tanh()\nmapping = {'!':0, '?':1,'h':2,'i':3,'e':4,'l':5,'o':6} \nx= torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny= torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")\n\n(1) torch.nn.RNN()을 이용하여 다음문자를 예측하는 신경망을 설계하고 학습하라.\n(풀이)\n\nrnn = torch.nn.RNN(7,8).to(\"cuda:0\")\nlinr = torch.nn.Linear(8,7).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1\n    hidden, hT = rnn(x) # _water 사실 생략할 수 있어요..\n    output = linr(hidden)\n    ## 2\n    loss = loss_fn(output,y)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nyhat=soft(output)    \nplt.matshow(yhat.to(\"cpu\").data[:10],cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(7),labels=['!','?','h','i','e','l','o']);\n\n\n\n\n(2) torch.nn.RNNCell()을 이용하여 다음문자를 예측하는 신경망을 설계하고 학습하라.\n(풀이)\n\ntorch.manual_seed(43052)\nrnncell = torch.nn.RNNCell(7,8).to(\"cuda:0\")\nlinr = torch.nn.Linear(8,7).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1\n    hidden = [] \n    ht = torch.zeros(8).to(\"cuda:0\")\n    for xt,yt in zip(x,y): \n        ht = rnncell(xt,ht) \n        hidden.append(ht) \n    hidden = torch.stack(hidden)\n    output = linr(hidden)\n    ## 2 \n    loss = loss_fn(output,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\nyhat[:10].to(\"cpu\").detach().numpy().round(3)\n\narray([[0.   , 0.005, 0.008, 0.972, 0.014, 0.001, 0.   ],\n       [0.   , 0.997, 0.002, 0.   , 0.   , 0.001, 0.   ],\n       [0.   , 0.001, 0.999, 0.   , 0.001, 0.   , 0.   ],\n       [0.   , 0.   , 0.   , 0.   , 0.999, 0.   , 0.   ],\n       [0.   , 0.   , 0.   , 0.   , 0.   , 1.   , 0.   ],\n       [0.   , 0.001, 0.   , 0.   , 0.   , 0.999, 0.   ],\n       [0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 1.   ],\n       [0.999, 0.   , 0.   , 0.   , 0.   , 0.   , 0.001],\n       [0.999, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.001, 0.998, 0.   , 0.   , 0.   , 0.   ]], dtype=float32)\n\n\n\nplt.matshow(yhat.to(\"cpu\").data[:10],cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(7),labels=['!','?','h','i','e','l','o']);\n\n\n\n\n(3) torch.nn.Module을 상속받은 클래스를 정의하고 (2)의 결과와 동일한 적합값이 나오는 신경망을 설계한 뒤 학습하라. (초기값을 적절하게 설정할 것)\n\nclass를 이용하지 않으면 점수없음.\ntorch.nn.RNN(), torch.nn.RNNCell() 을 이용한 네트워크를 학습시킬시 점수 없음. (초기값을 셋팅하는 용도로는 torch.nn.RNN(), torch.nn.RNNCell()을 코드에 포함시키는 것이 가능)\n\n(풀이)\n\nclass rNNCell(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.i2h = torch.nn.Linear(7,8)\n        self.h2h = torch.nn.Linear(8,8) \n        self.tanh = torch.nn.Tanh()\n    def forward(self,xt,ht):\n        ht = self.tanh(self.i2h(xt)+self.h2h(ht))\n        return ht\n\n\nrnncell = rNNCell().to(\"cuda:0\")\nlinr = torch.nn.Linear(8,7).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(linr.parameters()),lr=0.1)\n\n\n## 초기화의 설정을 위한 코드\ntorch.manual_seed(43052)\n_rnncell = torch.nn.RNNCell(7,8).to(\"cuda:0\")\n_linr = torch.nn.Linear(8,7).to(\"cuda:0\")\nrnncell.i2h.weight.data = _rnncell.weight_ih.data \nrnncell.h2h.weight.data = _rnncell.weight_hh.data \nrnncell.h2h.bias.data = _rnncell.bias_hh.data\nrnncell.i2h.bias.data = _rnncell.bias_ih.data\nlinr.weight.data = _linr.weight.data \nlinr.bias.data = _linr.bias.data \n\n\nfor epoc in range(100):\n    ## 1\n    hidden = [] \n    ht = torch.zeros(8).to(\"cuda:0\")\n    for xt,yt in zip(x,y): \n        ht = rnncell(xt,ht)\n        ot = linr(ht) \n        hidden.append(ht) \n    hidden = torch.stack(hidden)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\nyhat[:10].to(\"cpu\").detach().numpy().round(3)\n\narray([[0.   , 0.005, 0.008, 0.972, 0.014, 0.001, 0.   ],\n       [0.   , 0.997, 0.002, 0.   , 0.   , 0.001, 0.   ],\n       [0.   , 0.001, 0.999, 0.   , 0.001, 0.   , 0.   ],\n       [0.   , 0.   , 0.   , 0.   , 0.999, 0.   , 0.   ],\n       [0.   , 0.   , 0.   , 0.   , 0.   , 1.   , 0.   ],\n       [0.   , 0.001, 0.   , 0.   , 0.   , 0.999, 0.   ],\n       [0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 1.   ],\n       [0.999, 0.   , 0.   , 0.   , 0.   , 0.   , 0.001],\n       [0.999, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.001, 0.998, 0.   , 0.   , 0.   , 0.   ]], dtype=float32)\n\n\n\nplt.matshow(yhat.to(\"cpu\").data[:10],cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(7),labels=['!','?','h','i','e','l','o']);\n\n\n\n\n(4) torch.nn.LSTM()을 이용하여 다음문자를 예측하는 신경망을 설계하고 학습하라.\n(풀이)\n\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\nlinr = torch.nn.Linear(4,7).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1\n    hidden, (hT,cT) = lstm(x)\n    output = linr(hidden)\n    ## 2\n    loss = loss_fn(output,y)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nyhat=soft(output)    \nplt.matshow(yhat.to(\"cpu\").data[:10],cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(7),labels=['!','?','h','i','e','l','o']);\n\n\n\n\n(5) torch.nn.LSTMCell()을 이용하여 다음문자를 예측하는 신경망을 설계하고 학습하라.\n(풀이)\n\ntorch.manual_seed(43052) \nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\")\nlinr = torch.nn.Linear(4,7).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstmcell.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1\n    hidden = []\n    ht = torch.zeros(4).to(\"cuda:0\")\n    ct = torch.zeros(4).to(\"cuda:0\")\n    for xt,yt in zip(x,y): \n        ht,ct = lstmcell(xt,(ht,ct))\n        hidden.append(ht) \n    hidden = torch.stack(hidden)\n    output = linr(hidden)\n    ## 2 \n    loss = loss_fn(output,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\nyhat[:10].to(\"cpu\").detach().numpy().round(3)\n\narray([[0.   , 0.014, 0.084, 0.081, 0.822, 0.   , 0.   ],\n       [0.002, 0.91 , 0.   , 0.083, 0.003, 0.   , 0.001],\n       [0.001, 0.   , 0.999, 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.001, 0.005, 0.072, 0.917, 0.004, 0.   ],\n       [0.   , 0.   , 0.004, 0.   , 0.001, 0.995, 0.   ],\n       [0.   , 0.   , 0.   , 0.   , 0.   , 0.999, 0.001],\n       [0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.999],\n       [0.998, 0.001, 0.   , 0.   , 0.   , 0.   , 0.   ],\n       [0.99 , 0.   , 0.006, 0.001, 0.   , 0.003, 0.   ],\n       [0.007, 0.   , 0.992, 0.   , 0.   , 0.001, 0.   ]], dtype=float32)\n\n\n\nplt.matshow(yhat.to(\"cpu\").data[:10],cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(7),labels=['!','?','h','i','e','l','o']);\n\n\n\n\n(6) (5)의 결과와 동일한 적합값을 출력하는 신경망을 직접설계한 뒤 학습시켜라. (초기값을 적절하게 설정할 것)\n\nclass를 이용하지 않아도 무방함.\ntorch.nn.LSTM(), torch.nn.LSTMCell() 을 이용한 네트워크를 학습시킬시 점수 없음. (초기값을 셋팅하는 용도로는 torch.nn.LSTM(), torch.nn.LSTMCell()을 코드에 포함시키는 것은 가능)\n\n(풀이)\n\nclass lSTMCell(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.i2h = torch.nn.Linear(7,16)\n        self.h2h = torch.nn.Linear(4,16) \n        self.tanh = torch.nn.Tanh()\n    def forward(self,xt,past):\n        ht,ct = past \n        ifgo = self.i2h(xt) + self.h2h(ht) \n        it = sig(ifgo[0:4])\n        ft = sig(ifgo[4:8])\n        gt = tanh(ifgo[8:12])\n        ot = sig(ifgo[12:16])\n        ct = ft*ct + it*gt\n        ht = ot*self.tanh(ct) \n        return ht,ct\n\n\nlstmcell = lSTMCell().to(\"cuda:0\")\nlinr = torch.nn.Linear(4,7).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstmcell.parameters())+list(linr.parameters()),lr=0.1)\n\n\n# 초기값셋팅\ntorch.manual_seed(43052) \n_lstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\")\n_linr = torch.nn.Linear(4,7).to(\"cuda:0\")\nlstmcell.i2h.weight.data = _lstmcell.weight_ih.data \nlstmcell.h2h.weight.data = _lstmcell.weight_hh.data \nlstmcell.i2h.bias.data = _lstmcell.bias_ih.data\nlstmcell.h2h.bias.data = _lstmcell.bias_hh.data\nlinr.weight.data = _linr.weight.data \nlinr.bias.data = _linr.bias.data \n\n\nfor epoc in range(100):\n    ## 1\n    hidden = []     \n    ht = torch.zeros(4).to(\"cuda:0\")\n    ct = torch.zeros(4).to(\"cuda:0\")\n    for xt,yt in zip(x,y): \n        ht,ct = lstmcell(xt,(ht,ct))\n        hidden.append(ht) \n    hidden = torch.stack(hidden)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\nyhat[:10].to(\"cpu\").detach().numpy().round(3)\n\n\nplt.matshow(yhat.to(\"cpu\").data[:10],cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(7),labels=['!','?','h','i','e','l','o']);"
  },
  {
    "objectID": "posts/Machine Learning/2022_11_29_13wk_2_final_checkpoint_ipynb의_사본.html#다음을-읽고-참-거짓을-판단하여라.-10점",
    "href": "posts/Machine Learning/2022_11_29_13wk_2_final_checkpoint_ipynb의_사본.html#다음을-읽고-참-거짓을-판단하여라.-10점",
    "title": "기계학습 final(교수님)",
    "section": "2. 다음을 읽고 참 거짓을 판단하여라. (10점)",
    "text": "2. 다음을 읽고 참 거짓을 판단하여라. (10점)\n(1) LSTM은 RNN보다 장기기억에 유리하다. (True)\n(2) torch.nn.Embedding(num_embeddings=2,embedding_dim=1)와 torch.nn.Linear(in_features=1,out_features=1)의 학습가능한 파라메터수는 같다. (True)\n(3) 아래와 같은 네트워크를 고려하자.\nnet = torch.nn.Linear(1,1)\n차원이 (n,1) 인 임의의 텐서에 대하여 net(x)와 net.forward(x)의 출력결과는 같다. (True)\n(4) 아래와 같이 a,b,c,d 가 반복되는 문자열이 반복되는 자료에서 다음문자열을 맞추는 과업을 수행하기 위해서는 반드시 순환신경망의 형태로 설계해야만 한다. (False) –> 오타로인하여 문제삭제\na,b,c,d,a,b,c,d,...\n(5) RNN 혹은 LSTM 으로 신경망을 설계할 시 손실함수는 항상 torch.nn.CrossEntropyLoss 를 사용해야 한다. (False)"
  },
  {
    "objectID": "posts/Machine Learning/2. CNN/2022_10_26_(8주차)_10월26일(2)_ipynb의_사본.html",
    "href": "posts/Machine Learning/2. CNN/2022_10_26_(8주차)_10월26일(2)_ipynb의_사본.html",
    "title": "기계학습 (1026) 8주차",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-xkHPJ1DiPKfseoBl9yUY4P"
  },
  {
    "objectID": "posts/Machine Learning/2. CNN/2022_10_26_(8주차)_10월26일(2)_ipynb의_사본.html#imports",
    "href": "posts/Machine Learning/2. CNN/2022_10_26_(8주차)_10월26일(2)_ipynb의_사본.html#imports",
    "title": "기계학습 (1026) 8주차",
    "section": "imports",
    "text": "imports\n\nimport torch \nimport torchvision\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "posts/Machine Learning/2. CNN/2022_10_26_(8주차)_10월26일(2)_ipynb의_사본.html#transfer-learning",
    "href": "posts/Machine Learning/2. CNN/2022_10_26_(8주차)_10월26일(2)_ipynb의_사본.html#transfer-learning",
    "title": "기계학습 (1026) 8주차",
    "section": "Transfer Learning",
    "text": "Transfer Learning\n\npath = untar_data(URLs.CIFAR)\n\n\n\n\n\n\n    \n      \n      100.00% [168173568/168168549 00:04<00:00]\n    \n    \n\n\n\npath.ls()\n\n(#3) [Path('/root/.fastai/data/cifar10/train'),Path('/root/.fastai/data/cifar10/test'),Path('/root/.fastai/data/cifar10/labels.txt')]\n\n\n\n수제네트워크\n\ndls\n\n\ndls = ImageDataLoaders.from_folder(path,train='train',valid='test') \n\n\n_X,_y = dls.one_batch()\n_X.shape, _y.shape\n\n(torch.Size([64, 3, 32, 32]), torch.Size([64]))\n\n\n\n!ls /home/cgb4/.fastai/data/cifar10/train # 10개의 클래스\n\nairplane  automobile  bird  cat  deer  dog  frog  horse  ship  truck\n\n\n\ndls.show_batch() #어떠한 이미지가 어떠한 라벨로 되어있는지 확인이 가능하다.\n\n\n\n\n\nlrnr 생성\n\n\nnet1 = torch.nn.Sequential( #수제네트워크: 임의로 지정한 네트워크\n    torch.nn.Conv2d(3,128,(5,5)), #conv.:선형변환 \n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten()\n)\n\n\nnet1(_X.to(\"cpu\")).shape\n\ntorch.Size([64, 25088])\n\n\n\nnet = torch.nn.Sequential(\n    net1, \n    torch.nn.Linear(25088,10)\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=accuracy) \n\n\n학습\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.104867\n      1.105857\n      0.620600\n      00:04\n    \n    \n      1\n      0.961969\n      1.050836\n      0.640000\n      00:04\n    \n    \n      2\n      0.902597\n      1.058793\n      0.637600\n      00:04\n    \n    \n      3\n      0.854093\n      1.036581\n      0.657200\n      00:04\n    \n    \n      4\n      0.779191\n      1.013788\n      0.663400\n      00:04\n    \n    \n      5\n      0.723487\n      1.091586\n      0.642500\n      00:04\n    \n    \n      6\n      0.694052\n      1.064836\n      0.655700\n      00:04\n    \n    \n      7\n      0.629718\n      1.044633\n      0.668900\n      00:04\n    \n    \n      8\n      0.589516\n      1.168362\n      0.645100\n      00:04\n    \n    \n      9\n      0.572035\n      1.117689\n      0.654800\n      00:04\n    \n  \n\n\n\n\n이게 생각보다 잘 안맞아요.. 70넘기 힘듬\n\n\n\n전이학습 (남이 만든 네트워크)\n\nlrnr 생성\n\n\n#collapse_output\nnet = torchvision.models.resnet18(weights=torchvision.models.resnet.ResNet18_Weights.IMAGENET1K_V1)\nnet\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n\n\n\n\\(k=1000\\) 즉 1000개의 물체를 구분하는 모형임\n\n\nnet.fc = torch.nn.Linear(in_features=512, out_features=10) \n\n\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=accuracy)\n\n\n학습\n\n\nlrnr.fit(10) \n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.813139\n      0.803660\n      0.735300\n      00:10\n    \n    \n      1\n      0.667533\n      0.742656\n      0.756300\n      00:11\n    \n    \n      2\n      0.544296\n      0.735011\n      0.755900\n      00:10\n    \n    \n      3\n      0.449801\n      0.671868\n      0.784000\n      00:10\n    \n    \n      4\n      0.390996\n      0.657825\n      0.780100\n      00:11\n    \n    \n      5\n      0.310046\n      0.690071\n      0.788700\n      00:10\n    \n    \n      6\n      0.259605\n      0.671683\n      0.802500\n      00:10\n    \n    \n      7\n      0.199240\n      0.715251\n      0.796800\n      00:10\n    \n    \n      8\n      0.195551\n      0.772891\n      0.795100\n      00:10\n    \n    \n      9\n      0.150421\n      0.764864\n      0.801600\n      00:10\n    \n  \n\n\n\n\nCIFAR10을 맞추기 위한 네트워크가 아님에도 불구하고 상당히 잘맞음\n일반인이 거의 밑바닥에서 설계하는것보다 전이학습을 이용하는 것이 효율적일 경우가 많다.\n\n\n\n전이학습 다른 구현: 순수 fastai 이용\n- 예전코드 복습\n\npath = untar_data(URLs.PETS)/'images'\n\n\nfiles= get_image_files(path)\n\n\ndef label_func(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\n\n\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512)) \n\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.191067\n      0.027880\n      0.991881\n      00:29\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.039166\n      0.012174\n      0.996617\n      00:37\n    \n  \n\n\n\n- 사실 위의 코드가 transfer learning 이었음.\n\n#collapse_output\nlrnr.model\n\nSequential(\n  (0): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (4): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (5): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (1): Sequential(\n    (0): AdaptiveConcatPool2d(\n      (ap): AdaptiveAvgPool2d(output_size=1)\n      (mp): AdaptiveMaxPool2d(output_size=1)\n    )\n    (1): fastai.layers.Flatten(full=False)\n    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.25, inplace=False)\n    (4): Linear(in_features=1024, out_features=512, bias=False)\n    (5): ReLU(inplace=True)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.5, inplace=False)\n    (8): Linear(in_features=512, out_features=2, bias=False)\n  )\n)"
  },
  {
    "objectID": "posts/Machine Learning/2. CNN/2022_10_26_(8주차)_10월26일(2)_ipynb의_사본.html#cam",
    "href": "posts/Machine Learning/2. CNN/2022_10_26_(8주차)_10월26일(2)_ipynb의_사본.html#cam",
    "title": "기계학습 (1026) 8주차",
    "section": "CAM",
    "text": "CAM\n\nCAM이란?\n\nref: http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf\n\n- Class Activation Mapping (CAM)은 설명가능한 인공지능모형 (eXplainable Artificial Intelligence, XAI) 중 하나로 CNN의 판단근거를 시각화하는 기술\n\n\n학습에 사용할 데이터 Load\n\npath = untar_data(URLs.PETS)/'images'\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 00:18<00:00]\n    \n    \n\n\n\npath.ls()\n\n(#7393) [Path('/root/.fastai/data/oxford-iiit-pet/images/scottish_terrier_46.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Maine_Coon_64.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/pomeranian_82.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Sphynx_131.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/japanese_chin_38.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/British_Shorthair_18.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Sphynx_84.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/samoyed_122.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Russian_Blue_253.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/basset_hound_25.jpg')...]\n\n\n\nfiles= get_image_files(path)\ndef label_func(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512)) \n\n\n\n구현0단계– 예비학습\n\n# 하나의 이미지 선택\n\nximg = PILImage.create('/home/cgb4/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_106.jpg')\nximg\n\n\n\n\n\nx = first(dls.test_dl([ximg]))[0] #이미지를 숫자 형태로 가져오고 싶음 이미지에 대응하는 숫자들.. \nx\n\nTensorImage([[[[0.9059, 0.9059, 0.9098,  ..., 0.9059, 0.9059, 0.9059],\n               [0.9059, 0.9059, 0.9098,  ..., 0.9059, 0.9059, 0.9059],\n               [0.9059, 0.9059, 0.9098,  ..., 0.9059, 0.9059, 0.9059],\n               ...,\n               [0.8745, 0.8784, 0.8824,  ..., 0.8902, 0.8863, 0.8824],\n               [0.9059, 0.8980, 0.8902,  ..., 0.8824, 0.8863, 0.8824],\n               [0.8863, 0.8863, 0.8824,  ..., 0.8784, 0.8863, 0.8863]],\n\n              [[0.9137, 0.9137, 0.9176,  ..., 0.9059, 0.9059, 0.9059],\n               [0.9137, 0.9137, 0.9176,  ..., 0.9059, 0.9059, 0.9059],\n               [0.9137, 0.9137, 0.9176,  ..., 0.9059, 0.9059, 0.9059],\n               ...,\n               [0.8784, 0.8824, 0.8863,  ..., 0.8745, 0.8667, 0.8588],\n               [0.9098, 0.9020, 0.8902,  ..., 0.8745, 0.8706, 0.8627],\n               [0.8902, 0.8902, 0.8784,  ..., 0.8784, 0.8745, 0.8706]],\n\n              [[0.9098, 0.9098, 0.9137,  ..., 0.9137, 0.9137, 0.9137],\n               [0.9098, 0.9098, 0.9137,  ..., 0.9137, 0.9137, 0.9137],\n               [0.9098, 0.9098, 0.9137,  ..., 0.9137, 0.9137, 0.9137],\n               ...,\n               [0.8863, 0.8902, 0.8980,  ..., 0.8784, 0.8706, 0.8667],\n               [0.9176, 0.9137, 0.9059,  ..., 0.8745, 0.8706, 0.8667],\n               [0.8980, 0.9020, 0.8980,  ..., 0.8745, 0.8706, 0.8667]]]],\n            device='cuda:0')\n\n\n\n\n# AP layer\n\nap = torch.nn.AdaptiveAvgPool2d(output_size=1) # 데이터를 요약하는 방식 중 평균을 내는 방법 \n\n\nX = torch.arange(48).reshape(1,3,4,4)*1.0  #X는 아무 값이나 4*4의 컬러 이미지\nX\n\ntensor([[[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.],\n          [ 8.,  9., 10., 11.],\n          [12., 13., 14., 15.]],\n\n         [[16., 17., 18., 19.],\n          [20., 21., 22., 23.],\n          [24., 25., 26., 27.],\n          [28., 29., 30., 31.]],\n\n         [[32., 33., 34., 35.],\n          [36., 37., 38., 39.],\n          [40., 41., 42., 43.],\n          [44., 45., 46., 47.]]]])\n\n\n\nap(X) #각 채널들의 평균값 \n\ntensor([[[[ 7.5000]],\n\n         [[23.5000]],\n\n         [[39.5000]]]])\n\n\n\nX[0,0,...].mean(),X[0,1,...].mean(),X[0,2,...].mean()\n\n(tensor(7.5000), tensor(23.5000), tensor(39.5000))\n\n\n\n\n# torch.einsum\n(예시1)\n\ntsr = torch.arange(12).reshape(4,3)\ntsr\n\ntensor([[ 0,  1,  2],\n        [ 3,  4,  5],\n        [ 6,  7,  8],\n        [ 9, 10, 11]])\n\n\n\ntorch.einsum('ij->ji',tsr) #tsr.T 해도 됨. \n\ntensor([[ 0,  3,  6,  9],\n        [ 1,  4,  7, 10],\n        [ 2,  5,  8, 11]])\n\n\n(예시2)\n\ntsr1 = torch.arange(12).reshape(4,3).float()\ntsr2 = torch.arange(15).reshape(3,5).float()\n\n\ntsr1 @ tsr2 #4X5행렬\n\ntensor([[ 25.,  28.,  31.,  34.,  37.],\n        [ 70.,  82.,  94., 106., 118.],\n        [115., 136., 157., 178., 199.],\n        [160., 190., 220., 250., 280.]])\n\n\n\ntorch.einsum('ij,jk -> ik',tsr1,tsr2) \n\ntensor([[ 25.,  28.,  31.,  34.,  37.],\n        [ 70.,  82.,  94., 106., 118.],\n        [115., 136., 157., 178., 199.],\n        [160., 190., 220., 250., 280.]])\n\n\n(예시3)\n\nx.to(\"cpu\").shape #(1,3,512,512) -> (512,512,3)으로 바꾸고 싶다.\n\ntorch.Size([1, 3, 512, 512])\n\n\n\ntorch.einsum('ocij -> ijc',x.to(\"cpu\")).shape\n\ntorch.Size([512, 512, 3])\n\n\n\nplt.imshow(torch.einsum('ocij -> ijc',x.to(\"cpu\")))\n\n<matplotlib.image.AxesImage at 0x7f5fc4136290>\n\n\n\n\n\n\n\n\n구현1단계– 이미지분류 잘하는 네트워크 선택\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.169602\n      0.011903\n      0.996617\n      00:29\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.052203\n      0.012352\n      0.998647\n      00:38\n    \n  \n\n\n\n\n\n구현2단계– 네트워크의 끝 부분 수정\n- 모형의 분해\n\nnet1= lrnr.model[0]\nnet2= lrnr.model[1]\n\n- net2를 좀더 살펴보자.\n\nnet2\n\nSequential(\n  (0): AdaptiveConcatPool2d(\n    (ap): AdaptiveAvgPool2d(output_size=1)\n    (mp): AdaptiveMaxPool2d(output_size=1)\n  )\n  (1): fastai.layers.Flatten(full=False)\n  (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Dropout(p=0.25, inplace=False)\n  (4): Linear(in_features=1024, out_features=512, bias=False)\n  (5): ReLU(inplace=True)\n  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (7): Dropout(p=0.5, inplace=False)\n  (8): Linear(in_features=512, out_features=2, bias=False)\n)\n\n\n\n_X, _y = dls.one_batch() \n\n\nnet1.to(\"cpu\")\nnet2.to(\"cpu\") \n_X = _X.to(\"cpu\")\n\n\nprint(net1(_X).shape)\nprint(net2[0](net1(_X)).shape)\nprint(net2[1](net2[0](net1(_X))).shape)\nprint(net2[2](net2[1](net2[0](net1(_X)))).shape)\n\n# (64,512,16,16) \n# ↓AP:mean\n# (64,512,1,1) \n# ↓MP(Maxpooling):max\n# (64,512,1,1) \n\ntorch.Size([64, 512, 16, 16])\ntorch.Size([64, 1024, 1, 1])\ntorch.Size([64, 1024])\ntorch.Size([64, 1024])\n\n\n- net2를 아래와 같이 수정하고 재학습하자 (왜?)\n\nnet2= torch.nn.Sequential(\n    torch.nn.AdaptiveAvgPool2d(output_size=1), # (64,512,16,16) -> (64,512,1,1) \n    torch.nn.Flatten(), # (64,512,1,1) -> (64,512) \n    torch.nn.Linear(512,2,bias=False) # (64,512) -> (64,2) \n)\n\n\nnet = torch.nn.Sequential(\n    net1,\n    net2\n)\n\n\nlrnr2= Learner(dls,net,metrics=accuracy) # loss_fn??\n\n\nlrnr2.loss_func, lrnr.loss_func ## 알아서 기존의 loss function으로 잘 들어가 있음. \n\n(FlattenedLoss of CrossEntropyLoss(), FlattenedLoss of CrossEntropyLoss())\n\n\n\nlrnr2.fine_tune(5) # net2를 수정해서 accuracy가 안좋아지긴 했는데 그래도 쓸만함 \n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.252908\n      0.741022\n      0.755751\n      00:38\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.130946\n      0.126084\n      0.957375\n      00:38\n    \n    \n      1\n      0.143405\n      0.229703\n      0.905954\n      00:38\n    \n    \n      2\n      0.092800\n      0.104366\n      0.962788\n      00:38\n    \n    \n      3\n      0.046969\n      0.043439\n      0.983762\n      00:38\n    \n    \n      4\n      0.024211\n      0.038318\n      0.983762\n      00:38\n    \n  \n\n\n\n\n\n구현3단계– 수정된 net2에서 Linear와 AP의 순서를 바꿈\n- 1개의 observation을 고정하였을 경우 출력과정 상상\n\nximg = PILImage.create('/home/cgb4/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_106.jpg')\nx = first(dls.test_dl([ximg]))[0]\n\n\nnet2\n\nSequential(\n  (0): AdaptiveAvgPool2d(output_size=1)\n  (1): Flatten(start_dim=1, end_dim=-1)\n  (2): Linear(in_features=512, out_features=2, bias=False)\n)\n\n\n\nprint(net1(x).shape)\nprint(net2[0](net1(x)).shape)\nprint(net2[1](net2[0](net1(x))).shape)\nprint(net2[2](net2[1](net2[0](net1(x)))).shape)\n\ntorch.Size([1, 512, 16, 16])\ntorch.Size([1, 512, 1, 1])\ntorch.Size([1, 512])\ntorch.Size([1, 2])\n\n\n- 최종결과 확인\n\nnet(x)\n\nTensorImage([[-9.0358,  9.0926]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\ndls.vocab\n\n['cat', 'dog']\n\n\n\nnet(x)에서 뒤쪽의 값이 클수록 ’dog’를 의미한다.\n\n- net2의 순서 바꾸기 전 전체 네트워크:\n\\[\\underset{(1,3,512,512)}{\\boldsymbol x} \\overset{net_1}{\\to} \\left( \\underset{(1,512,16,16)}{\\tilde{\\boldsymbol x}} \\overset{ap}{\\to} \\underset{(1,512,1,1)}{{\\boldsymbol \\sharp}}\\overset{flatten}{\\to} \\underset{(1,512)}{{\\boldsymbol \\sharp}}\\overset{linear}{\\to} \\underset{(1,2)}{\\hat{\\boldsymbol y}}\\right) = [-9.0358,  9.0926]\\]\n- 아래와 같이 순서를 바꿔서 한번 계산해보고 싶다. (왜???..)\n\\[\\underset{(1,3,224,224)}{\\boldsymbol x} \\overset{net_1}{\\to} \\left( \\underset{(1,512,16,16)}{\\tilde{\\boldsymbol x}} \\overset{linear}{\\to} \\underset{(1,2,16,16)}{{\\bf why}}\\overset{ap}{\\to} \\underset{(1,2,1,1)}{{\\boldsymbol \\sharp}}\\overset{flatten}{\\to} \\underset{(1,2)}{\\hat{\\boldsymbol y}}\\right) = [−9.0358,9.0926]\\]\n\n여기에서 (1,512,16,16) -> (1,2,16,16) 로 가는 선형변환을 적용하는 방법? (16,16) each pixel에 대하여 (512 \\(\\to\\) 2)로 가는 변환을 수행\n\n- 통찰: 이 경우 특이하게도 레이어의 순서를 바꿨을때 출력이 동일함 (선형변환하고 평균내거나 평균내고 선형변환하는건 같으니까)\n\n_x =torch.tensor([1,2,3.14,4]).reshape(4,1)\n_x \n\ntensor([[1.0000],\n        [2.0000],\n        [3.1400],\n        [4.0000]])\n\n\n\n_l1 = torch.nn.Linear(1,1,bias=False)\n_l1(_x).mean() # _x -> 선형변환 -> 평균 \n\ntensor(-1.4045, grad_fn=<MeanBackward0>)\n\n\n\n_l1(_x.mean().reshape(1,1)) # _x -> 평균 -> 선형변환\n\ntensor([[-1.4045]], grad_fn=<MmBackward0>)\n\n\n- 구현해보자.\n\nwhy = torch.einsum('cb,abij->acij',net2[2].weight,net1(x))\n\n\nnet2[0](why)\n\nTensorImage([[[[-9.0358]],\n\n              [[ 9.0926]]]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\nnet(x)\n\nTensorImage([[-9.0358,  9.0926]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\n\n잠깐 멈추고 생각\n- 이미지\n\nximg\n\n\n\n\n- 네트워크의 결과\n\nnet2(net1(x))\n\nTensorImage([[-9.0358,  9.0926]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\n-9.0358 << 9.0926 이므로 ’ximg’는 높은 확률로 개라는 뜻이다.\n\n- 아래의네트워크를 관찰\n\\[\\underset{(1,2,16,16)}{{\\bf why}}\\overset{ap}{\\to} \\underset{(1,2,1,1)}{{\\boldsymbol \\sharp}}\\overset{flatten}{\\to} \\underset{(1,2)}{\\hat{\\boldsymbol y}} = [-9.0358,9.0926]\\]\n\nnet2[0](why)\n\nTensorImage([[[[-9.0358]],\n\n              [[ 9.0926]]]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n더 파고들어서 분석해보자.\n\nwhy.shape\n\ntorch.Size([1, 2, 16, 16])\n\n\n\n(why[0,0,:,:]).mean(), (why[0,1,:,:]).mean()\n\n(TensorImage(-9.0358, device='cuda:0', grad_fn=<AliasBackward0>),\n TensorImage(9.0926, device='cuda:0', grad_fn=<AliasBackward0>))\n\n\nwhy[0,0,:,:]\n\n#collapse_output\n(why[0,0,:,:]).to(torch.int64)\n\nTensorImage([[   0,    0,    0,    0,    0,    0,   -1,   -4,   -5,   -4,   -1,\n                 0,    0,    0,    0,    0],\n             [   0,    0,    0,    0,    0,   -1,  -12,  -26,  -33,  -28,  -14,\n                -3,    0,    0,    0,    0],\n             [   0,    0,    1,    1,    0,   -2,  -22,  -60,  -75,  -73,  -41,\n               -10,    0,    0,    0,    0],\n             [   0,    0,    0,    0,    0,   -2,  -25,  -75, -116, -110,  -64,\n               -18,    0,    0,    0,    0],\n             [   0,    0,    0,    0,    0,   -1,  -22,  -76, -147, -132,  -69,\n               -21,   -1,    0,    0,    0],\n             [   0,    0,    0,    0,    0,   -1,  -16,  -60, -112, -110,  -59,\n               -18,   -3,    0,    0,   -7],\n             [   0,    0,    0,    0,    0,    0,   -9,  -38,  -66,  -66,  -37,\n               -12,   -2,    0,    0,   -2],\n             [   0,    1,    1,    0,    0,    0,   -4,  -25,  -34,  -27,  -18,\n                -6,   -1,    0,    0,    0],\n             [   1,    1,    1,    0,    0,    0,   -2,  -11,  -15,  -10,   -5,\n                -2,    0,    0,    0,    0],\n             [   1,    1,    0,    0,    0,    0,   -1,   -2,   -4,   -3,    0,\n                 0,    0,    0,   -1,    0],\n             [   0,    0,    0,   -1,   -3,   -1,   -1,   -1,   -2,   -2,   -2,\n                -1,   -2,   -2,    0,    0],\n             [   0,    0,    0,   -1,   -1,   -1,   -1,   -2,   -5,   -4,   -3,\n                -1,    0,    0,   -1,   -1],\n             [  -1,    0,    0,    0,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n                -2,    0,   -1,   -1,   -1],\n             [  -1,   -2,   -1,    0,   -1,   -3,   -2,    0,    2,    0,    0,\n                -1,    0,   -1,   -2,   -3],\n             [  -3,   -4,   -3,   -3,   -3,   -5,   -3,   -1,   -1,   -3,   -2,\n                -2,   -1,   -2,   -4,   -4],\n             [  -3,   -4,   -4,   -4,   -4,   -3,   -3,   -2,   -3,   -4,   -4,\n                -3,   -2,   -3,   -4,   -4]], device='cuda:0')\n\n\n\n이 값들의 평균은 -9.0358 이다. (이 값이 클수록 이 그림이 고양이라는 의미 = 이 값이 작을수록 이 그림이 고양이가 아니라는 의미)\n그런데 살펴보니 대부분의 위치에서 0에 가까운 값을 가짐. 다만 특정위치에서 엄청 큰 작은값이 있어서 -9.0358이라는 평균값이 나옴 \\(\\to\\) 특정위치에 존재하는 엄청 작은 값들은 ximg가 고양이가 아니라고 판단하는 근거가 된다.\n\nwhy[0,1,:,:]\n\n#collapse_output\n(why[0,1,:,:]).to(torch.int64)\n\nTensorImage([[  0,   0,   0,   0,   0,   0,   1,   4,   5,   4,   1,   0,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,   1,  12,  27,  34,  29,  15,   3,   0,\n                0,   0,   0],\n             [  0,   0,  -1,  -1,   0,   2,  23,  62,  79,  76,  43,  11,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,   2,  26,  79, 122, 116,  66,  18,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,   1,  24,  81, 152, 136,  72,  21,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,   0,  18,  64, 116, 113,  61,  19,   3,\n                0,   0,   6],\n             [  0,   0,   0,   0,   0,   0,  10,  40,  69,  68,  38,  12,   1,\n                0,   0,   2],\n             [  0,  -1,  -1,   0,   0,   0,   4,  25,  35,  28,  18,   6,   1,\n                0,   0,   0],\n             [ -1,  -1,  -1,   0,   0,   0,   2,  10,  14,  10,   5,   1,   0,\n                0,   0,   0],\n             [  0,  -1,   0,   0,   0,   0,   0,   2,   4,   3,   0,   0,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   3,   1,   0,   1,   2,   2,   2,   0,   1,\n                2,   0,   0],\n             [  0,   0,   0,   0,   0,   1,   1,   2,   5,   3,   3,   1,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,   1,   1,   0,   0,   0,   1,   1,   0,\n                0,   0,   1],\n             [  1,   1,   1,   0,   0,   2,   1,   0,  -2,   0,   0,   1,   0,\n                0,   1,   1],\n             [  2,   2,   2,   2,   2,   4,   2,   1,   1,   2,   1,   1,   1,\n                1,   3,   3],\n             [  2,   3,   3,   3,   3,   2,   2,   2,   2,   3,   2,   2,   2,\n                2,   3,   3]], device='cuda:0')\n\n\n\n이 값들의 평균은 9.0926 이다. (이 값이 클수록 이 그림이 강아지라는 의미)\n그런데 살펴보니 대부분의 위치에서 0에 가까운 값을 가짐. 다만 특정위치에서 엄청 큰 값들이 있어서 9.0926이라는 평균값이 나옴 \\(\\to\\) 특정위치에 존재하는 엄청 큰 값들은 결국 ximg를 강아지라고 판단하는 근거가 된다.\n\n- 시각화\n\nwhy_cat = why[0,0,:,:]\nwhy_dog = why[0,1,:,:]\n\n\nfig, ax = plt.subplots(1,3,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_cat.to(\"cpu\").detach(),cmap='magma')\nax[2].imshow(why_dog.to(\"cpu\").detach(),cmap='magma')\n\n<matplotlib.image.AxesImage at 0x7f5fce6d7b90>\n\n\n\n\n\n\nmagma = 검은색 < 보라색 < 빨간색 < 노란색\n왼쪽그림의 검은 부분은 고양이가 아니라는 근거, 오른쪽그림의 노란부분은 강아지라는 근거\n\n- why_cat, why_dog를 (16,16) \\(\\to\\) (512,512) 로 resize\n\nfig, ax = plt.subplots(1,3,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear')\nax[2].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear')\n\n<matplotlib.image.AxesImage at 0x7f5fbd81c890>\n\n\n\n\n\n- 겹쳐그리기\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\n\n<matplotlib.image.AxesImage at 0x7f5fd48cd7d0>\n\n\n\n\n\n- 하니이미지 시각화\n\n#\n#!wget https://github.com/guebin/DL2022/blob/master/_notebooks/2022-09-06-hani01.jpeg?raw=true\nximg= PILImage.create('2022-09-06-hani01.jpeg')\nx= first(dls.test_dl([ximg]))[0]\n\n\nwhy = torch.einsum('cb,abij->acij',net2[2].weight,net1(x))\nwhy_cat = why[0,0,:,:]\nwhy_dog = why[0,1,:,:]\n\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\n\n<matplotlib.image.AxesImage at 0x7f5fbca0ad10>\n\n\n\n\n\n- 하니이미지 시각화 with prob\n\nsftmax=torch.nn.Softmax(dim=1)\n\n\nsftmax(net(x))\n\nTensorImage([[1.5489e-05, 9.9998e-01]], device='cuda:0',\n            grad_fn=<AliasBackward0>)\n\n\n\ncatprob, dogprob = sftmax(net(x))[0,0].item(), sftmax(net(x))[0,1].item()\n\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[0].set_title('catprob= %f' % catprob) \nax[1].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].set_title('dogprob=%f' % dogprob)\n\nText(0.5, 1.0, 'dogprob=0.999985')\n\n\n\n\n\n\n\n구현4단계– CAM 시각화\n\nsftmax = torch.nn.Softmax(dim=1)\n\n\nfig, ax = plt.subplots(5,5) \nk=0 \nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -> acij', net2[2].weight, net1(x))\n        why_cat = why[0,0,:,:] \n        why_dog = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob>dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_cat.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"cat(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_dog.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"dog(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()\n\n\n\n\n\nfig, ax = plt.subplots(5,5) \nk=25\nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -> acij', net2[2].weight, net1(x))\n        why_cat = why[0,0,:,:] \n        why_dog = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob>dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_cat.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"cat(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_dog.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"dog(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()\n\n\n\n\n\nfig, ax = plt.subplots(5,5) \nk=50\nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -> acij', net2[2].weight, net1(x))\n        why_cat = why[0,0,:,:] \n        why_dog = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob>dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_cat.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"cat(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_dog.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"dog(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()\n\n\n\n\n\nfig, ax = plt.subplots(5,5) \nk=75\nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -> acij', net2[2].weight, net1(x))\n        why_cat = why[0,0,:,:] \n        why_dog = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob>dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_cat.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"cat(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_dog.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"dog(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/Machine Learning/2. CNN/기계학습특강2022_10_19_ipynb의_사본.html",
    "href": "posts/Machine Learning/2. CNN/기계학습특강2022_10_19_ipynb의_사본.html",
    "title": "기계학습 (1019) 7주차",
    "section": "",
    "text": "import torch\nfrom fastai.vision.all import *\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Machine Learning/2. CNN/기계학습특강2022_10_19_ipynb의_사본.html#깊은신경망-오버피팅",
    "href": "posts/Machine Learning/2. CNN/기계학습특강2022_10_19_ipynb의_사본.html#깊은신경망-오버피팅",
    "title": "기계학습 (1019) 7주차",
    "section": "깊은신경망– 오버피팅",
    "text": "깊은신경망– 오버피팅\n\n데이터\n- model: \\(y_i = (0\\times x_i) + \\epsilon_i\\)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(100,1)\ny=torch.randn(100).reshape(100,1)*0.01\nplt.plot(x,y)\n\n\n\n\n\n\n모든 데이터를 사용하여 적합 (512, relu, 1000 epochs)\n\ntorch.manual_seed(1) \nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=512,out_features=1)\n)\nloss_fn = torch.nn.MSELoss() #y가 연속형일때는 MSE, 0또는 1일땐 BCE\noptimizr = torch.optim.Adam(net.parameters())\n\nfor epoc in range(1000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y)\nplt.plot(x,net(x).data, '--')\n\n\n\n\n\n\n전체데이터를 8:2로 나누어서 8만을 학습\n- 데이터를 8:2로 나눈다 8:training 훈련 셋 2: 검증셋\n\nxtr = x[:80]\nytr = y[:80] \nxtest = x[80:] \nytest = y[80:] \n\n\nplt.plot(x,y,alpha=0.5)\nplt.plot(xtr,ytr)\nplt.plot(xtest,ytest)\n\n\n\n\n\nx.shape, xtr.shape, xtest.shape\n\n(torch.Size([100, 1]), torch.Size([80, 1]), torch.Size([20, 1]))\n\n\n\ny.shape, ytr.shape, ytest.shape\n\n(torch.Size([100, 1]), torch.Size([80, 1]), torch.Size([20, 1]))\n\n\n\nplt.plot(xtr,ytr,'o')\nplt.plot(xtest,ytest,'o')\n\n\n\n\n\n# 처음 80개만 가지고 net를 학습시키면, \n\n- (xtr,ytr) 만 가지고 net를 학습시킨다.\n\ntorch.manual_seed(1) \nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=512,out_features=1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nfor epoc in range(1000):\n    ## 1 \n    #               원래 yhat=net(X)  -> 80개하니까 net(xtr)  -> 변수가 많아져서.. 귀찮아져.. 그냥 loss에 바로 넣자!! \n    ## 2 \n    loss = loss_fn(net(xtr),ytr)    # 원래 loss_fn(yhat,y)  \n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(xtr,ytr,'o')\nplt.plot(xtest,ytest,'o')\n#plt.plot(xtr,net(xtr).data,'--')\n#plt.plot(xtest,net(xtest).data,'--')\nplt.plot(x,net(x).data,'--k') \n\n# 보여준 파란색 데이터는 잘맞는데.. 노란색 데이터는 잘 안맞는거 같아.\n# 이런 상황을 오버피팅 이라고 한다! -> 파악하지 않아야 할 것까지 파악해 버린것.\n\n\n\n\n\n# 데이터 수에 비해 노드 수(feature수)가 많으면 오버피팅\n# 차원의 저주\n# 언더라인 외 오차항 따라가는거.. 오버피팅\n\n\n# 예시\n# y = 0,1,1,1,0,1,0,0,1\n# x1 = 0,0,0,1,0,0,0\n# x2 = \n# x3 = \n# 변수가 많으면 결정계수 값이 올라가서"
  },
  {
    "objectID": "posts/Machine Learning/2. CNN/기계학습특강2022_10_19_ipynb의_사본.html#깊은신경망-드랍아웃",
    "href": "posts/Machine Learning/2. CNN/기계학습특강2022_10_19_ipynb의_사본.html#깊은신경망-드랍아웃",
    "title": "기계학습 (1019) 7주차",
    "section": "깊은신경망– 드랍아웃",
    "text": "깊은신경망– 드랍아웃\n\n오버피팅의 해결\n\n# 오버피팅을 해결할 방법은 제대로 된 건 없지만.. 그 중 하나인 드랍아웃\n\n- 오버피팅의 해결책: 드랍아웃\n\ntorch.manual_seed(1) \nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=512),\n    torch.nn.ReLU(),\n    torch.nn.Dropout(0.8),   # 0.8은 0으로 0.2만 살아남음\n    torch.nn.Linear(in_features=512,out_features=1)  # 보통 linear해서 다 더하고 -> sigmoid로 \n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nfor epoc in range(1000):\n    ## 1 \n    #\n    ## 2 \n    loss = loss_fn(net(xtr),ytr) \n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(xtr,ytr,'o')\nplt.plot(xtest,ytest,'o')\nplt.plot(x,net(x).data,'--k') \nplt.title(r\"network is in training mode\",fontsize=15)\n\nText(0.5, 1.0, 'network is in training mode')\n\n\n\n\n\n\n# 더 오차항을 따라가는 거 같어 \n# 오잉 다시 돌려보면 그래프가 바껴 net 넣었는데 바뀌는게 이상해! \n\n- 올바른 사용법\n\nnet.training\n\nTrue\n\n\n\nnet.eval()\nnet.training\n\nFalse\n\n\n\nplt.plot(xtr,ytr,'o')\nplt.plot(xtest,ytest,'o')\nplt.plot(x,net(x).data,'--k') \nplt.title(r\"network is in evaluation mode\",fontsize=15)\n\nText(0.5, 1.0, 'network is in evaluation mode')\n\n\n\n\n\n\n\n드랍아웃 레이어\n\n# 드랍아웃 레이어는 뭔가? 왜 결과가 랜덤으로 나왔는가?\n\n\n_x = torch.linspace(0,1,101) \n_x \n\ntensor([0.0000, 0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800,\n        0.0900, 0.1000, 0.1100, 0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700,\n        0.1800, 0.1900, 0.2000, 0.2100, 0.2200, 0.2300, 0.2400, 0.2500, 0.2600,\n        0.2700, 0.2800, 0.2900, 0.3000, 0.3100, 0.3200, 0.3300, 0.3400, 0.3500,\n        0.3600, 0.3700, 0.3800, 0.3900, 0.4000, 0.4100, 0.4200, 0.4300, 0.4400,\n        0.4500, 0.4600, 0.4700, 0.4800, 0.4900, 0.5000, 0.5100, 0.5200, 0.5300,\n        0.5400, 0.5500, 0.5600, 0.5700, 0.5800, 0.5900, 0.6000, 0.6100, 0.6200,\n        0.6300, 0.6400, 0.6500, 0.6600, 0.6700, 0.6800, 0.6900, 0.7000, 0.7100,\n        0.7200, 0.7300, 0.7400, 0.7500, 0.7600, 0.7700, 0.7800, 0.7900, 0.8000,\n        0.8100, 0.8200, 0.8300, 0.8400, 0.8500, 0.8600, 0.8700, 0.8800, 0.8900,\n        0.9000, 0.9100, 0.9200, 0.9300, 0.9400, 0.9500, 0.9600, 0.9700, 0.9800,\n        0.9900, 1.0000])\n\n\n\ndout = torch.nn.Dropout(0.9)\ndout(_x)\n\ntensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.6000,  0.0000,\n         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         0.0000,  2.5000,  0.0000,  2.7000,  0.0000,  0.0000,  0.0000,  0.0000,\n         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  3.7000,  0.0000,  3.9000,\n         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         0.0000,  0.0000,  0.0000,  5.9000,  0.0000,  6.1000,  0.0000,  0.0000,\n         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         7.2000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         0.0000,  0.0000,  0.0000,  8.3000,  8.4000,  0.0000,  0.0000,  0.0000,\n         0.0000,  8.9000,  0.0000,  0.0000,  9.2000,  0.0000,  0.0000,  0.0000,\n         0.0000,  0.0000,  0.0000,  0.0000, 10.0000])\n\n\n\n90%의 드랍아웃: 드랍아웃층의 입력 중 임의로 90%를 골라서 결과를 랜덤으로 0으로 만든다. + 그리고 0이 되지않고 살아남은 값들은 10배 만큼 값이 커진다.\n\n- 드랍아웃레이어 정리 - 구조: 입력 -> 드랍아웃레이어 -> 출력 - 역할: (1) 입력의 일부를 임의로 0으로 만드는 역할 (2) 0이 안된것들은 스칼라배하여 드랍아웃을 통과한 모든 숫자들의 총합이 일정하게 되도록 조정 - 효과: 오버피팅을 억제하는 효과가 있음 (왜??) - 의미: each iteration (each epoch x) 마다 학습에 참여하는 노드가 로테이션으로 랜덤으로 결정됨. - 느낌: 모든 노드가 골고루 학습가능 + 한 두개의 특화된 능력치가 개발되기 보다 평균적인 능력치가 전반적으로 개선됨"
  },
  {
    "objectID": "posts/Machine Learning/2. CNN/기계학습특강2022_10_19_ipynb의_사본.html#이미지자료분석-data",
    "href": "posts/Machine Learning/2. CNN/기계학습특강2022_10_19_ipynb의_사본.html#이미지자료분석-data",
    "title": "기계학습 (1019) 7주차",
    "section": "이미지자료분석– data",
    "text": "이미지자료분석– data\n- download data\n\nimport torch\nimport torchvision\n\n\npath = untar_data(URLs.MNIST)\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:00<00:00]\n    \n    \n\n\n- training set\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX = torch.concat([X0,X1])/255\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\n- test set\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nXX = torch.concat([X0,X1])/255\nyy = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\n\nX.shape,XX.shape,y.shape,yy.shape\n\n(torch.Size([12665, 1, 28, 28]),\n torch.Size([2115, 1, 28, 28]),\n torch.Size([12665, 1]),\n torch.Size([2115, 1]))\n\n\n\n# training 12,656개.. 2,115개는.. test set?"
  },
  {
    "objectID": "posts/Machine Learning/2. CNN/기계학습특강2022_10_19_ipynb의_사본.html#이미지자료분석-cnn-예비학습",
    "href": "posts/Machine Learning/2. CNN/기계학습특강2022_10_19_ipynb의_사본.html#이미지자료분석-cnn-예비학습",
    "title": "기계학습 (1019) 7주차",
    "section": "이미지자료분석– CNN 예비학습",
    "text": "이미지자료분석– CNN 예비학습\n\n기존의 MLP 모형\n- 교재의 모형 (fastai.book)\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -> \"node1\"\n    \"x2\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \n    \"x784\" -> \"node1\"\n    \"x1\" -> \"node2\"\n    \"x2\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"x784\" -> \"node2\"\n    \n    \"x1\" -> \"...\"\n    \"x2\" -> \"...\"\n    \"..\" -> \"...\"\n    \"x784\" -> \"...\"\n\n    \"x1\" -> \"node30\"\n    \"x2\" -> \"node30\"\n    \"..\" -> \"node30\"\n    \"x784\" -> \"node30\"\n\n\n    label = \"Layer 1: ReLU\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"y\"\n    \"node2\" -> \"y\"\n    \"...\" -> \"y\"\n    \"node30\" -> \"y\"\n    label = \"Layer 2: Sigmoid\"\n}\n''')\n\n\n\n\n- 왜 28$$28 이미지를 784개의 벡터로 만든 다음에 모형을 돌려야 하는가?\n\n# nxp 매트릭스 꼴로.. 정리해서 넣으려고\n\n- 기존에 개발된 모형이 회귀분석 기반으로 되어있어서 결국 회귀분석 틀에 짜 맞추어서 이미지자료를 분석하는 느낌\n- observation의 차원은 \\(784\\)가 아니라 \\(1\\times (28\\times 28)\\)이 되어야 맞다.\n\n\n새로운 아키텍처의 제시\n- 예전\n\\(\\underset{(n,784)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,30)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,30)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n\\(l_1\\): 선형변환, feature를 뻥튀기하는 역할\n\\(relu\\): 뻥튀기된 feature에 비선형을 추가하여 표현력 극대화\n\\(l_2\\): 선형변환, 뻥튀기된 feature를 요약 하는 역할 (=데이터를 요약하는 역할)\n\n- 새로운 아키텍처 - \\(conv\\): feature를 뻥튀기하는 역할 (2d ver \\(l_1\\) 느낌) - \\(relu\\): 뻥튀기된 feature에 비선형을 추가하여 표현력 극대화 - \\(pooling\\): 데이터를 요약하는 역할\n\n\nCONV 레이어 (선형변환의 2D 버전)\n- 우선 연산하는 방법만 살펴보자.\n(예시1)\n\ntorch.nn.Conv2d?\n\n\ntorch.manual_seed(43052)\n_conv = torch.nn.Conv2d(1,1,(2,2)) # 입력1, 출력1, (2,2) window size\n_conv.weight.data, _conv.bias.data\n\n(tensor([[[[-0.1733, -0.4235],\n           [ 0.1802,  0.4668]]]]),\n tensor([0.2037]))\n\n\n\n_X = torch.arange(0,4).reshape(1,2,2).float()  #(1,2,2):흑백이미지, 2x2\n_X\n\ntensor([[[0., 1.],\n         [2., 3.]]])\n\n\n\n(-0.1733)*0 + (-0.4235)*1 +\\\n(0.1802)*2 + (0.4668)*3 + 0.2037\n\n1.541\n\n\n\n_conv(_X)\n\ntensor([[[1.5410]]], grad_fn=<SqueezeBackward1>)\n\n\n(예시2) 잘하면 평균도 계산하겠다?\n\n_conv.weight.data = torch.tensor([[[[1/4, 1/4],[1/4,1/4]]]])\n_conv.bias.data = torch.tensor([0.0])\n\n\n_conv(_X) , (0+1+2+3)/4\n\n(tensor([[[1.5000]]], grad_fn=<SqueezeBackward1>), 1.5)\n\n\n(예시3) 이동평균?\n\n_X = torch.arange(0,25).float().reshape(1,5,5) \n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_conv(_X)\n\ntensor([[[ 3.,  4.,  5.,  6.],\n         [ 8.,  9., 10., 11.],\n         [13., 14., 15., 16.],\n         [18., 19., 20., 21.]]], grad_fn=<SqueezeBackward1>)\n\n\n(예시4) window size가 증가한다면? (2d의 이동평균느낌)\n\n_conv = torch.nn.Conv2d(1,1,(3,3)) # 입력1, 출력1, (3,3) window size    (3,3) 대신 3 넣어도 됨\n_conv.bias.data = torch.tensor([0.0])\n_conv.weight.data = torch.tensor([[[[1/9,1/9,1/9],[1/9,1/9,1/9],[1/9,1/9,1/9]]]])\n\n\n_X,_conv(_X)\n\n(tensor([[[ 0.,  1.,  2.,  3.,  4.],\n          [ 5.,  6.,  7.,  8.,  9.],\n          [10., 11., 12., 13., 14.],\n          [15., 16., 17., 18., 19.],\n          [20., 21., 22., 23., 24.]]]),\n tensor([[[ 6.0000,  7.0000,  8.0000],\n          [11.0000, 12.0000, 13.0000],\n          [16.0000, 17.0000, 18.0000]]], grad_fn=<SqueezeBackward1>))\n\n\n\n(1+2+3+6+7+8+11+12+13)/9\n\n7.0\n\n\n(예시5) 피처뻥튀기\n\n_X = torch.tensor([1.0,1.0,1.0,1.0]).reshape(1,2,2)\n_X\n\ntensor([[[1., 1.],\n         [1., 1.]]])\n\n\n\n_conv = torch.nn.Conv2d(1,8,(2,2))\n_conv.weight.data.shape,_conv.bias.data.shape\n\n(torch.Size([8, 1, 2, 2]), torch.Size([8]))\n\n\n\n_conv(_X).reshape(-1)\n\ntensor([-0.3464,  0.2739,  0.1069,  0.6105,  0.0432,  0.8390,  0.2353,  0.2345],\n       grad_fn=<ReshapeAliasBackward0>)\n\n\n\ntorch.sum(_conv.weight.data[0,...])+_conv.bias.data[0],\\\ntorch.sum(_conv.weight.data[1,...])+_conv.bias.data[1]\n# 여기 이해 안감.. ㅠ \n\n(tensor(-0.3464), tensor(0.2739))\n\n\n결국 아래를 계산한다는 의미\n\ntorch.sum(_conv.weight.data,axis=(2,3)).reshape(-1)+ _conv.bias.data\n\ntensor([-0.3464,  0.2739,  0.1069,  0.6105,  0.0432,  0.8390,  0.2353,  0.2345])\n\n\n\n_conv(_X).reshape(-1)\n\ntensor([-0.3464,  0.2739,  0.1069,  0.6105,  0.0432,  0.8390,  0.2353,  0.2345],\n       grad_fn=<ReshapeAliasBackward0>)\n\n\n(잔소리) axis 사용 익숙하지 않으면 아래 꼭 들으세요..\n\nhttps://guebin.github.io/IP2022/2022/04/11/(6주차)-4월11일.html , numpy공부 4단계: 축\n\n\n\nReLU (2d)\n\n_X = torch.randn(25).reshape(1,5,5)\n_X\n\ntensor([[[ 0.2656,  0.0780,  3.0465,  1.0151, -2.3908],\n         [ 0.4749,  1.6519,  1.5454,  1.0376,  0.9291],\n         [-0.7858,  0.4190,  2.6057, -0.4022,  0.2092],\n         [ 0.9594,  0.6408, -0.0411, -1.0720, -2.0659],\n         [-0.0996,  1.1351,  0.9758,  0.4952, -0.5475]]])\n\n\n\na1=torch.nn.ReLU()\n\n\na1(_X)\n\ntensor([[[0.2656, 0.0780, 3.0465, 1.0151, 0.0000],\n         [0.4749, 1.6519, 1.5454, 1.0376, 0.9291],\n         [0.0000, 0.4190, 2.6057, 0.0000, 0.2092],\n         [0.9594, 0.6408, 0.0000, 0.0000, 0.0000],\n         [0.0000, 1.1351, 0.9758, 0.4952, 0.0000]]])\n\n\n\n\nMaxpooling 레이어\n\n_maxpooling = torch.nn.MaxPool2d((2,2))\n\n\n_X = torch.arange(16).float().reshape(1,4,4) \n\n\n_X, _maxpooling(_X) \n\n(tensor([[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.],\n          [ 8.,  9., 10., 11.],\n          [12., 13., 14., 15.]]]),\n tensor([[[ 5.,  7.],\n          [13., 15.]]]))\n\n\n\n_X = torch.arange(25).float().reshape(1,5,5) \n\n\n_X, _maxpooling(_X) #경계에 있는건 버린당..(데이터를 요약해주는 거니까)\n\n(tensor([[[ 0.,  1.,  2.,  3.,  4.],\n          [ 5.,  6.,  7.,  8.,  9.],\n          [10., 11., 12., 13., 14.],\n          [15., 16., 17., 18., 19.],\n          [20., 21., 22., 23., 24.]]]),\n tensor([[[ 6.,  8.],\n          [16., 18.]]]))\n\n\n\n_X = torch.arange(36).float().reshape(1,6,6) \n\n\n_X, _maxpooling(_X) \n\n(tensor([[[ 0.,  1.,  2.,  3.,  4.,  5.],\n          [ 6.,  7.,  8.,  9., 10., 11.],\n          [12., 13., 14., 15., 16., 17.],\n          [18., 19., 20., 21., 22., 23.],\n          [24., 25., 26., 27., 28., 29.],\n          [30., 31., 32., 33., 34., 35.]]]),\n tensor([[[ 7.,  9., 11.],\n          [19., 21., 23.],\n          [31., 33., 35.]]]))"
  },
  {
    "objectID": "posts/Machine Learning/2. CNN/기계학습특강2022_10_19_ipynb의_사본.html#이미지자료분석-cnn-구현-cpu",
    "href": "posts/Machine Learning/2. CNN/기계학습특강2022_10_19_ipynb의_사본.html#이미지자료분석-cnn-구현-cpu",
    "title": "기계학습 (1019) 7주차",
    "section": "이미지자료분석– CNN 구현 (CPU)",
    "text": "이미지자료분석– CNN 구현 (CPU)\n\nX.shape\n\ntorch.Size([12665, 1, 28, 28])\n\n\n\n(1) Conv2d\n\nc1 = torch.nn.Conv2d(1,16,(5,5)) #16개로뻥튀기하고싶어\nprint(X.shape)\nprint(c1(X).shape)\n\n# 1장의채널이 16장으로 뻥튀기..\n# 28->24 윈도우사이즈만큼.. 한칸씩 이동하면서 나머지 데이터 빠졌엉. 마지막 4개빠짐\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\n\n\n\n\n(2) ReLU\n\na1 = torch.nn.ReLU()\nprint(X.shape)\nprint(c1(X).shape)\nprint(a1(c1(X)).shape)\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 24, 24])\n\n\n\n\n(3) MaxPool2D\n\nm1 =  torch.nn.MaxPool2d((2,2)) \nprint(X.shape)\nprint(c1(X).shape)\nprint(a1(c1(X)).shape)\nprint(m1(a1(c1(X))).shape)\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 12, 12])\n\n\n\n# 16, 12, 12 숫자가 1로 바껴야함...\n\n\n\n(4) 적당히 마무리하고 시그모이드 태우자\n- 펼치자.\n(방법1)\n\nm1(a1(c1(X))).reshape(-1,2304).shape #레이어를 통과하는 느낌이 아닌거같ㅇㅏ요\n\ntorch.Size([12665, 2304])\n\n\n\n16*12*12 \n\n2304\n\n\n(방법2)\n\nflttn = torch.nn.Flatten()\n\n\nprint(X.shape)\nprint(c1(X).shape)\nprint(a1(c1(X)).shape)\nprint(m1(a1(c1(X))).shape)\nprint(flttn(m1(a1(c1(X)))).shape) # 레이어...\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 12, 12])\ntorch.Size([12665, 2304])\n\n\n- 2304 \\(\\to\\) 1 로 차원축소하는 선형레이어를 설계\n\nl1 = torch.nn.Linear(in_features=2304,out_features=1) \nprint(X.shape)\nprint(c1(X).shape)\nprint(a1(c1(X)).shape)\nprint(m1(a1(c1(X))).shape)\nprint(flttn(m1(a1(c1(X)))).shape)\nprint(l1(flttn(m1(a1(c1(X))))).shape)\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 12, 12])\ntorch.Size([12665, 2304])\ntorch.Size([12665, 1])\n\n\n- 시그모이드\n\na2 = torch.nn.Sigmoid()\n\n\nl1 = torch.nn.Linear(in_features=2304,out_features=1) \nprint(X.shape)\nprint(c1(X).shape)\nprint(a1(c1(X)).shape)\nprint(m1(a1(c1(X))).shape)\nprint(flttn(m1(a1(c1(X)))).shape)\nprint(l1(flttn(m1(a1(c1(X))))).shape)\nprint(a1(l1(flttn(m1(a1(c1(X)))))).shape)\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 12, 12])\ntorch.Size([12665, 2304])\ntorch.Size([12665, 1])\ntorch.Size([12665, 1])\n\n\n- 네트워크 설계\n\nnet = torch.nn.Sequential(\n    c1, # 2d: 컨볼루션(선형변환), 피처 뻥튀기 \n    a1, # 2d: 렐루(비선형변환)\n    m1, # 2d: 맥스풀링: 데이터요약\n    flttn, # 2d->1d \n    l1, # 1d: 선형변환\n    a2 # 1d: 시그모이드(비선형변환) \n)\n\n\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nt1= time.time()\nfor epoc in range(100): \n    ## 1\n    yhat = net(X) \n    ## 2\n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward()\n    ## 4\n    optimizr.step()\n    optimizr.zero_grad()\nt2= time.time()\nt2-t1\n\n51.493837118148804\n\n\n\nplt.plot(y)\nplt.plot(net(X).data,'.')\nplt.title('Traning Set',size=15)\n\nText(0.5, 1.0, 'Traning Set')\n\n\n\n\n\n\nplt.plot(yy)\nplt.plot(net(XX).data,'.')\nplt.title('Test Set',size=15)\n\nText(0.5, 1.0, 'Test Set')"
  },
  {
    "objectID": "posts/Machine Learning/2. CNN/기계학습특강2022_10_19_ipynb의_사본.html#이미지자료분석-cnn-구현-gpu",
    "href": "posts/Machine Learning/2. CNN/기계학습특강2022_10_19_ipynb의_사본.html#이미지자료분석-cnn-구현-gpu",
    "title": "기계학습 (1019) 7주차",
    "section": "이미지자료분석– CNN 구현 (GPU)",
    "text": "이미지자료분석– CNN 구현 (GPU)\n\n1. dls\n\nds1=torch.utils.data.TensorDataset(X,y)\nds2=torch.utils.data.TensorDataset(XX,yy)\n\n\nX.shape\n\ntorch.Size([12665, 1, 28, 28])\n\n\n\nlen(X)/10  # batch_size=1266으로 하면 한 epoc을 10번 정도... 위에 epoc100했는데.. 그 세팅 맞춘거...........\n\n1266.5\n\n\n\nlen(XX)\n\n2115\n\n\n\n# 하나는 training, 하나는 test에 대응하는.. dl\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \n\n\ndls = DataLoaders(dl1,dl2) # 이거 fastai 지원함수입니다\n\n\n\n2. lrnr 생성: 아키텍처, 손실함수, 옵티마이저\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\n\n#아키텍처: 여기서는 네트워크...\n\n\nlrnr = Learner(dls,net,loss_fn) #architecture자리에 net\n\n\n\n3. 학습\n\nlrnr.fit(10) # fit (숫자:epoc 숫자를 넣는다.)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.904232\n      0.605049\n      00:01\n    \n    \n      1\n      0.661176\n      0.371011\n      00:00\n    \n    \n      2\n      0.507179\n      0.213586\n      00:00\n    \n    \n      3\n      0.392649\n      0.113123\n      00:00\n    \n    \n      4\n      0.304377\n      0.065496\n      00:00\n    \n    \n      5\n      0.238253\n      0.043172\n      00:00\n    \n    \n      6\n      0.188984\n      0.031475\n      00:00\n    \n    \n      7\n      0.151837\n      0.024563\n      00:00\n    \n    \n      8\n      0.123364\n      0.020047\n      00:00\n    \n    \n      9\n      0.101180\n      0.016816\n      00:00\n    \n  \n\n\n\n\n\n4. 예측 및 시각화\n\n# lrnr.model\n\n\n# net\n\n\n# id(net), id(lrnr.model)\n\n\nnet.to(\"cpu\")  # 네트워크으ㅔ 있는 모든 parameter를 cpu로 옮긴다\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=1, bias=True)\n  (5): Sigmoid()\n)\n\n\n- 결과를 시각화하면 아래와 같다.\n\nplt.plot(net(X).data,'.')\nplt.title(\"Training Set\",size=15)\n\nText(0.5, 1.0, 'Training Set')\n\n\n\n\n\n\nplt.plot(net(XX).data,'.')\nplt.title(\"Test Set\",size=15)\n\nText(0.5, 1.0, 'Test Set')\n\n\n\n\n\n- 빠르고 적합결과도 좋음\n\n# accuracy차이가 별로 없어보여. \n\n\n\nLrnr 오브젝트\n\nlrnr.model\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=1, bias=True)\n  (5): Sigmoid()\n)\n\n\n\nnet\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=1, bias=True)\n  (5): Sigmoid()\n)\n\n\n\nid(lrnr.model), id(net)\n\n(140681387850000, 140681387850000)\n\n\n\nlrnr.model(X)\n\ntensor([[5.4047e-03],\n        [5.1475e-04],\n        [9.8561e-04],\n        ...,\n        [9.9602e-01],\n        [9.9584e-01],\n        [9.9655e-01]], grad_fn=<SigmoidBackward0>)\n\n\n\n\nBCEWithLogitsLoss\n- BCEWithLogitsLoss = Sigmoid + BCELoss - 왜 써요? 수치적으로 더 안정\n- 사용방법\n\ndls 만들기\n\n\nds1=torch.utils.data.TensorDataset(X,y)\nds2=torch.utils.data.TensorDataset(XX,yy)\n\n\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \n\n\ndls = DataLoaders(dl1,dl2) # 이거 fastai 지원함수입니다\n\n\nlrnr생성\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,1),\n    #torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCEWithLogitsLoss()\nlrnr = Learner(dls,net,loss_fn) \n\n\n학습\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.896794\n      0.560268\n      00:00\n    \n    \n      1\n      0.613384\n      0.301413\n      00:00\n    \n    \n      2\n      0.454223\n      0.169741\n      00:00\n    \n    \n      3\n      0.346758\n      0.092166\n      00:00\n    \n    \n      4\n      0.268065\n      0.056573\n      00:00\n    \n    \n      5\n      0.210524\n      0.039757\n      00:00\n    \n    \n      6\n      0.167973\n      0.030431\n      00:00\n    \n    \n      7\n      0.135910\n      0.024560\n      00:00\n    \n    \n      8\n      0.111290\n      0.020503\n      00:00\n    \n    \n      9\n      0.092058\n      0.017516\n      00:00\n    \n  \n\n\n\n\n예측 및 시각화\n\n\nnet.to(\"cpu\")\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=1, bias=True)\n)\n\n\n\nfig,ax = plt.subplots(1,2,figsize=(8,4))\nax[0].plot(net(X).data,',',color=\"C1\")\nax[1].plot(y)\nax[1].plot(a2(net(X)).data,',')\nfig.suptitle(\"Training Set\",size=15)\n\nText(0.5, 0.98, 'Training Set')\n\n\n\n\n\n\nfig,ax = plt.subplots(1,2,figsize=(8,4))\nax[0].plot(net(XX).data,',',color=\"C1\")\nax[1].plot(yy)\nax[1].plot(a2(net(XX)).data,',')\nfig.suptitle(\"Test Set\",size=15)\n\nText(0.5, 0.98, 'Test Set')"
  },
  {
    "objectID": "posts/Machine Learning/2022_12_21_Extra_1_ipynb의_사본.html",
    "href": "posts/Machine Learning/2022_12_21_Extra_1_ipynb의_사본.html",
    "title": "기계학습 (1221)",
    "section": "",
    "text": "import torch\nimport numpy as np \nimport pandas as pd\nfrom fastai.collab import *"
  },
  {
    "objectID": "posts/Machine Learning/2022_12_21_Extra_1_ipynb의_사본.html#주절주절-intro",
    "href": "posts/Machine Learning/2022_12_21_Extra_1_ipynb의_사본.html#주절주절-intro",
    "title": "기계학습 (1221)",
    "section": "주절주절 intro",
    "text": "주절주절 intro\n- Data\n\ndf_view = pd.read_csv('https://raw.githubusercontent.com/guebin/STML2022/main/posts/V.%20RecSys/2022-12-21-rcmdsolo.csv',index_col=0)\ndf_view \n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      영식\n      영철\n      영호\n      광수\n      상철\n      영수\n    \n  \n  \n    \n      옥순\n      3.9\n      4.1\n      NaN\n      0.5\n      0.3\n      NaN\n    \n    \n      영자\n      4.5\n      NaN\n      3.7\n      0.5\n      NaN\n      0.2\n    \n    \n      정숙\n      NaN\n      4.9\n      4.7\n      NaN\n      1.2\n      1.3\n    \n    \n      영숙\n      0.6\n      0.2\n      NaN\n      4.1\n      4.3\n      NaN\n    \n    \n      순자\n      0.7\n      0.9\n      NaN\n      4.2\n      NaN\n      3.9\n    \n    \n      현숙\n      NaN\n      0.2\n      0.3\n      NaN\n      3.5\n      3.4\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- 데이터를 이해할때 필요한 가정들 – 내맘대로 한 설정임.\n\n(옥순,영자,정숙)은 (영식,영철,영호)와 성격이 잘 맞고 (영숙,순자,현숙)은 (광수,상철,영수)와 성격이 잘맞음\n((옥순,영자,정숙),(영식,영철,영호))은 MBTI가 I로 시작하고 ((영숙,순자,현숙),(광수,상철,영수))는 MBTI가 E로 시작한다.\n\n- 목표: NaN 을 추론\n- 수동추론:\n\n(옥순,영호)이 만난다면? \\(\\to\\) 둘다 I성향이니까 잘 맞지 않을까? \\(\\to\\) 4.0 정도?\n(정숙,영식)조합은? \\(\\to\\) 둘다 I성향이니까 잘 맞지 않을까? + 정숙은 다 잘맞던데..? \\(\\to\\) 4.8 정도?\n(현숙,영식)조합은? \\(\\to\\) 현숙은 E성향인데 영식은 I성향이므로 잘 안맞을 것임 + 현숙은 원래 좀 눈이 높음 \\(\\to\\) 0.25 정도?\n\n- 좀 더 체계적인 추론\n사람들이 가지고 있는 성향들을 두 개의 숫자로 표현하자.\n\n옥순의 성향 = (I성향,E성향) = (1.9, 0.0)\n영식의 성향 = (I성향,E성향) = (2.0, 0.1)\n현숙의 성향 = (I성향,E성향) = (0.0, 1.5)\n\n(1) 옥순과 영식의 궁합 \\(\\approx\\) 옥순의I성향\\(\\times\\)영식의I성향 \\(+\\) 옥순의E성향\\(\\times\\)영식의E성향 // 적합\n\na1= np.array([1.9,0.0]).reshape(2,1) # a1은 옥순의 성향, col-vec으로 선언하자. \nb1= np.array([2.0,0.1]).reshape(2,1) # b1은 영식의 성향, col-vec으로 선언하자.\n(a1*b1).sum()\n\n3.8\n\n\n(2) 현숙과 영식의 궁합 \\(\\approx\\) 현숙의I성향\\(\\times\\)영식의I성향 \\(+\\) 현숙의E성향\\(\\times\\)영식의E성향 // 예측\n\na6= np.array([0.0,1.5]).reshape(2,1)\n(a6*b1).sum()\n\n0.15000000000000002\n\n\n\n그럴듯함..\n\n- 모델링\n아래가 같음을 관찰하라. (차원만 다름)\n\n(a1*b1).sum(), a1.T@b1\n\n(3.8, array([[3.8]]))\n\n\n\n(a6*b1).sum(), a6.T@b1\n\n(0.15000000000000002, array([[0.15]]))\n\n\n만약에 여자의성향, 남자의성향을 적당한 매트릭스로 정리할 수 있다면 궁합매트릭스를 만들 수 있음\n\na1= np.array([1.9,0.0]).reshape(2,1)\na2= np.array([2.0,0.1]).reshape(2,1)\na3= np.array([2.5,1.0]).reshape(2,1)\na4= np.array([0.1,1.9]).reshape(2,1)\na5= np.array([0.2,2.1]).reshape(2,1)\na6= np.array([0.0,1.5]).reshape(2,1)\nA = np.concatenate([a1,a2,a3,a4,a5,a6],axis=1)\nA\n\narray([[1.9, 2. , 2.5, 0.1, 0.2, 0. ],\n       [0. , 0.1, 1. , 1.9, 2.1, 1.5]])\n\n\n\nb1= np.array([2.0,0.1]).reshape(2,1)\nb2= np.array([1.9,0.2]).reshape(2,1)\nb3= np.array([1.8,0.3]).reshape(2,1)\nb4= np.array([0.3,2.1]).reshape(2,1)\nb5= np.array([0.2,2.0]).reshape(2,1)\nb6= np.array([0.1,1.9]).reshape(2,1)\nB = np.concatenate([b1,b2,b3,b4,b5,b6],axis=1)\nB\n\narray([[2. , 1.9, 1.8, 0.3, 0.2, 0.1],\n       [0.1, 0.2, 0.3, 2.1, 2. , 1.9]])\n\n\n\nA.T@B\n\narray([[3.8 , 3.61, 3.42, 0.57, 0.38, 0.19],\n       [4.01, 3.82, 3.63, 0.81, 0.6 , 0.39],\n       [5.1 , 4.95, 4.8 , 2.85, 2.5 , 2.15],\n       [0.39, 0.57, 0.75, 4.02, 3.82, 3.62],\n       [0.61, 0.8 , 0.99, 4.47, 4.24, 4.01],\n       [0.15, 0.3 , 0.45, 3.15, 3.  , 2.85]])\n\n\n\na1.T@b1, a2.T@b2, a3.T@b1\n\n(array([[3.8]]), array([[3.82]]), array([[5.1]]))\n\n\n결국 모형은 아래와 같다.\n\\[\\text{궁합매트릭스} = {\\bf A}^\\top {\\bf B} + \\text{오차}\\]\n- 학습전략: 아래의 매트릭스중에서 어떤값은 관측하였고 어떤값은 관측하지 못함 \\(\\to\\) 관측한 값들만 대충 비슷하게 하면 되는거 아니야?\n\nA.T@B \n\narray([[3.8 , 3.61, 3.42, 0.57, 0.38, 0.19],\n       [4.01, 3.82, 3.63, 0.81, 0.6 , 0.39],\n       [5.1 , 4.95, 4.8 , 2.85, 2.5 , 2.15],\n       [0.39, 0.57, 0.75, 4.02, 3.82, 3.62],\n       [0.61, 0.8 , 0.99, 4.47, 4.24, 4.01],\n       [0.15, 0.3 , 0.45, 3.15, 3.  , 2.85]])\n\n\n\ndf_view\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      영식\n      영철\n      영호\n      광수\n      상철\n      영수\n    \n  \n  \n    \n      옥순\n      3.9\n      4.1\n      NaN\n      0.5\n      0.3\n      NaN\n    \n    \n      영자\n      4.5\n      NaN\n      3.7\n      0.5\n      NaN\n      0.2\n    \n    \n      정숙\n      NaN\n      4.9\n      4.7\n      NaN\n      1.2\n      1.3\n    \n    \n      영숙\n      0.6\n      0.2\n      NaN\n      4.1\n      4.3\n      NaN\n    \n    \n      순자\n      0.7\n      0.9\n      NaN\n      4.2\n      NaN\n      3.9\n    \n    \n      현숙\n      NaN\n      0.2\n      0.3\n      NaN\n      3.5\n      3.4\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- 자료를 아래와 같이 정리한다면?\n\ndf = pd.DataFrame([(f,m,df_view.loc[f,m]) for f in df_view.index for m in df_view.columns if not np.isnan(df_view.loc[f,m])])\ndf.columns = ['X1','X2','y']\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      X1\n      X2\n      y\n    \n  \n  \n    \n      0\n      옥순\n      영식\n      3.9\n    \n    \n      1\n      옥순\n      영철\n      4.1\n    \n    \n      2\n      옥순\n      광수\n      0.5\n    \n    \n      3\n      옥순\n      상철\n      0.3\n    \n    \n      4\n      영자\n      영식\n      4.5\n    \n    \n      5\n      영자\n      영호\n      3.7\n    \n    \n      6\n      영자\n      광수\n      0.5\n    \n    \n      7\n      영자\n      영수\n      0.2\n    \n    \n      8\n      정숙\n      영철\n      4.9\n    \n    \n      9\n      정숙\n      영호\n      4.7\n    \n    \n      10\n      정숙\n      상철\n      1.2\n    \n    \n      11\n      정숙\n      영수\n      1.3\n    \n    \n      12\n      영숙\n      영식\n      0.6\n    \n    \n      13\n      영숙\n      영철\n      0.2\n    \n    \n      14\n      영숙\n      광수\n      4.1\n    \n    \n      15\n      영숙\n      상철\n      4.3\n    \n    \n      16\n      순자\n      영식\n      0.7\n    \n    \n      17\n      순자\n      영철\n      0.9\n    \n    \n      18\n      순자\n      광수\n      4.2\n    \n    \n      19\n      순자\n      영수\n      3.9\n    \n    \n      20\n      현숙\n      영철\n      0.2\n    \n    \n      21\n      현숙\n      영호\n      0.3\n    \n    \n      22\n      현숙\n      상철\n      3.5\n    \n    \n      23\n      현숙\n      영수\n      3.4\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nmapp1 = {k[1]:k[0] for k in enumerate(df.X1.unique())}\nmapp2 = {k[1]:k[0] for k in enumerate(df.X2.unique())}\nmapp1,mapp2\n\n({'옥순': 0, '영자': 1, '정숙': 2, '영숙': 3, '순자': 4, '현숙': 5},\n {'영식': 0, '영철': 1, '광수': 2, '상철': 3, '영호': 4, '영수': 5})\n\n\n\nX1 = torch.tensor(list(map(lambda name: mapp1[name], df.X1)))\nX2 = torch.tensor(list(map(lambda name: mapp2[name], df.X2)))\nX1 = torch.nn.functional.one_hot(X1).float()\nX2 = torch.nn.functional.one_hot(X2).float()\ny = torch.tensor(df.y).float()\n\n\nX1\n\ntensor([[1., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0., 1.],\n        [0., 0., 0., 0., 0., 1.],\n        [0., 0., 0., 0., 0., 1.],\n        [0., 0., 0., 0., 0., 1.]])\n\n\n- yhat을 구하는 과정..\n\nl1 = torch.nn.Linear(in_features=6,out_features=2) # I성향 E성향.. #여출 \nl2 = torch.nn.Linear(in_features=6,out_features=2) # 남출\n\n\nl1(X1) # 옥순~현숙의 성향들 \n\ntensor([[-0.1484,  0.1981],\n        [-0.1484,  0.1981],\n        [-0.1484,  0.1981],\n        [-0.1484,  0.1981],\n        [-0.1659,  0.7817],\n        [-0.1659,  0.7817],\n        [-0.1659,  0.7817],\n        [-0.1659,  0.7817],\n        [ 0.1933,  0.7089],\n        [ 0.1933,  0.7089],\n        [ 0.1933,  0.7089],\n        [ 0.1933,  0.7089],\n        [ 0.0649,  0.3645],\n        [ 0.0649,  0.3645],\n        [ 0.0649,  0.3645],\n        [ 0.0649,  0.3645],\n        [ 0.3438,  0.2501],\n        [ 0.3438,  0.2501],\n        [ 0.3438,  0.2501],\n        [ 0.3438,  0.2501],\n        [-0.2503,  0.4676],\n        [-0.2503,  0.4676],\n        [-0.2503,  0.4676],\n        [-0.2503,  0.4676]], grad_fn=<AddmmBackward0>)\n\n\n\nl2(X2) # 영식~영수의 성향들 \n\ntensor([[ 0.2230,  0.3115],\n        [-0.1752, -0.0627],\n        [ 0.2852,  0.4847],\n        [-0.2893,  0.2159],\n        [ 0.2230,  0.3115],\n        [ 0.1466, -0.0453],\n        [ 0.2852,  0.4847],\n        [-0.4553,  0.3573],\n        [-0.1752, -0.0627],\n        [ 0.1466, -0.0453],\n        [-0.2893,  0.2159],\n        [-0.4553,  0.3573],\n        [ 0.2230,  0.3115],\n        [-0.1752, -0.0627],\n        [ 0.2852,  0.4847],\n        [-0.2893,  0.2159],\n        [ 0.2230,  0.3115],\n        [-0.1752, -0.0627],\n        [ 0.2852,  0.4847],\n        [-0.4553,  0.3573],\n        [-0.1752, -0.0627],\n        [ 0.1466, -0.0453],\n        [-0.2893,  0.2159],\n        [-0.4553,  0.3573]], grad_fn=<AddmmBackward0>)\n\n\n- 몇개의 관측치만 생각해보자..\n\ndf.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      X1\n      X2\n      y\n    \n  \n  \n    \n      0\n      옥순\n      영식\n      3.9\n    \n    \n      1\n      옥순\n      영철\n      4.1\n    \n    \n      2\n      옥순\n      광수\n      0.5\n    \n    \n      3\n      옥순\n      상철\n      0.3\n    \n    \n      4\n      영자\n      영식\n      4.5\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n(l1(X1)[0]*l2(X2)[0]).sum() # (옥순의성향 * 영식의성향).sum()\n\ntensor(0.0286, grad_fn=<SumBackward0>)\n\n\n\n이 값이 실제로는 3.9 이어야 한다.\n\n\n(l1(X1)[1]*l2(X2)[1]).sum() # (옥순의성향 * 영철의성향).sum()\n\ntensor(0.0136, grad_fn=<SumBackward0>)\n\n\n\n이 값이 실제로는 4.1 이어야 한다.\n\n- yhat을 구하면!\n\nyhat = (l1(X1) * l2(X2)).sum(axis=1) # (l1(X1) * l2(X2)).sum(1)와 결과가 같음 \nyhat\n\ntensor([ 0.0286,  0.0136,  0.0537,  0.0857,  0.2065, -0.0597,  0.3316,  0.3548,\n        -0.0783, -0.0038,  0.0971,  0.1652,  0.1280, -0.0342,  0.1952,  0.0599,\n         0.1546, -0.0759,  0.2193, -0.0672,  0.0145, -0.0579,  0.1734,  0.2810],\n       grad_fn=<SumBackward1>)\n\n\n\nyhat[:2],y[:2] # 이 값들이 비슷해야 하는데..\n\n(tensor([0.0286, 0.0136], grad_fn=<SliceBackward0>), tensor([3.9000, 4.1000]))\n\n\n- 0~5 까지의 범위로 고정되어 있으니까 아래와 같이 해도 되겠음..\n\nsig = torch.nn.Sigmoid() # range: 0~1\n\n\nyhat = sig((l1(X1) * l2(X2)).sum(axis=1))*5 # (l1(X1) * l2(X2)).sum(1)와 결과가 같음    #range: 0~5\nyhat\n\ntensor([2.5357, 2.5170, 2.5671, 2.6071, 2.7572, 2.4254, 2.9108, 2.9389, 2.4021,\n        2.4953, 2.6213, 2.7061, 2.6598, 2.4572, 2.7432, 2.5749, 2.6928, 2.4052,\n        2.7730, 2.4161, 2.5182, 2.4277, 2.7162, 2.8490],\n       grad_fn=<MulBackward0>)\n\n\n\nloss = torch.mean((y-yhat)**2)\nloss\n\ntensor(3.4368, grad_fn=<MeanBackward0>)"
  },
  {
    "objectID": "posts/Machine Learning/2022_12_21_Extra_1_ipynb의_사본.html#torch를-이용한-학습",
    "href": "posts/Machine Learning/2022_12_21_Extra_1_ipynb의_사본.html#torch를-이용한-학습",
    "title": "기계학습 (1221)",
    "section": "torch를 이용한 학습",
    "text": "torch를 이용한 학습\n\ntorch.manual_seed(43052)\nl1 = torch.nn.Linear(6,2) \nl2 = torch.nn.Linear(6,2)\nsig = torch.nn.Sigmoid() \n\n\nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.Adam(list(l1.parameters())+list(l2.parameters()))\n\n\nfor epoc in range(5000):\n    ## 1 \n    feature1 = l1(X1)\n    feature2 = l2(X2) \n    matching_score = (feature1*feature2).sum(axis=1) \n    yhat = sig(matching_score)*5 # 만약에 1~3점이라면 \"1+sig(matching_score)*2\" 와 같이 하면 되었을듯 \n    ## 2 \n    loss = loss_fn(yhat,y)    \n    ## 3 \n    loss.backward()    \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat\n\ntensor([3.9382, 4.0624, 0.4665, 0.3353, 4.5038, 3.6975, 0.3562, 0.3558, 4.8614,\n        4.7208, 1.1813, 1.3158, 0.4606, 0.3573, 4.1288, 4.2734, 0.8611, 0.7347,\n        4.0493, 4.0464, 0.1810, 0.3124, 3.5031, 3.3948],\n       grad_fn=<MulBackward0>)\n\n\n\ny\n\ntensor([3.9000, 4.1000, 0.5000, 0.3000, 4.5000, 3.7000, 0.5000, 0.2000, 4.9000,\n        4.7000, 1.2000, 1.3000, 0.6000, 0.2000, 4.1000, 4.3000, 0.7000, 0.9000,\n        4.2000, 3.9000, 0.2000, 0.3000, 3.5000, 3.4000])\n\n\n\nl1(X1) # 두번째 칼럼이 I 성향 점수로 \"해석\"된다\n\ntensor([[-1.4663,  0.2938],\n        [-1.4663,  0.2938],\n        [-1.4663,  0.2938],\n        [-1.4663,  0.2938],\n        [-1.7086,  0.6597],\n        [-1.7086,  0.6597],\n        [-1.7086,  0.6597],\n        [-1.7086,  0.6597],\n        [-0.8705,  1.2945],\n        [-0.8705,  1.2945],\n        [-0.8705,  1.2945],\n        [-0.8705,  1.2945],\n        [ 1.1046, -0.8298],\n        [ 1.1046, -0.8298],\n        [ 1.1046, -0.8298],\n        [ 1.1046, -0.8298],\n        [ 0.9880, -0.5193],\n        [ 0.9880, -0.5193],\n        [ 0.9880, -0.5193],\n        [ 0.9880, -0.5193],\n        [ 0.6834, -1.2201],\n        [ 0.6834, -1.2201],\n        [ 0.6834, -1.2201],\n        [ 0.6834, -1.2201]], grad_fn=<AddmmBackward0>)\n\n\n\n포인트: 여성출연자중, 정숙은 대체로 잘 맞춰주고 현숙은 그렇지 않았음.. \\(\\to\\) 그러한 가중치가 잘 드러남!!"
  },
  {
    "objectID": "posts/Machine Learning/2022_12_21_Extra_1_ipynb의_사본.html#fastai를-이용한-학습",
    "href": "posts/Machine Learning/2022_12_21_Extra_1_ipynb의_사본.html#fastai를-이용한-학습",
    "title": "기계학습 (1221)",
    "section": "fastai를 이용한 학습",
    "text": "fastai를 이용한 학습\n(1) dls\n\ndf.head() # 앞단계 전처리의 산물\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      X1\n      X2\n      y\n    \n  \n  \n    \n      0\n      옥순\n      영식\n      3.9\n    \n    \n      1\n      옥순\n      영철\n      4.1\n    \n    \n      2\n      옥순\n      광수\n      0.5\n    \n    \n      3\n      옥순\n      상철\n      0.3\n    \n    \n      4\n      영자\n      영식\n      4.5\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndls = CollabDataLoaders.from_df(df,bs=2,valid_pct=2/24) #bs:배치사이즈\n\n(2) lrnr 생성\n\nlrnr = collab_learner(dls,n_factors=2,y_range=(0,5))\n\n(3) 학습\n\nlrnr.fit(30,lr=0.05)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.005521\n      0.306862\n      00:00\n    \n    \n      1\n      0.006144\n      0.246958\n      00:00\n    \n    \n      2\n      0.006997\n      0.300838\n      00:00\n    \n    \n      3\n      0.009465\n      0.193282\n      00:00\n    \n    \n      4\n      0.011386\n      0.157935\n      00:00\n    \n    \n      5\n      0.011837\n      0.273318\n      00:00\n    \n    \n      6\n      0.011834\n      0.170711\n      00:00\n    \n    \n      7\n      0.011649\n      0.245928\n      00:00\n    \n    \n      8\n      0.012505\n      0.198697\n      00:00\n    \n    \n      9\n      0.014821\n      0.153817\n      00:00\n    \n    \n      10\n      0.012487\n      0.144184\n      00:00\n    \n    \n      11\n      0.011637\n      0.164051\n      00:00\n    \n    \n      12\n      0.011798\n      0.189932\n      00:00\n    \n    \n      13\n      0.012036\n      0.163537\n      00:00\n    \n    \n      14\n      0.012818\n      0.203912\n      00:00\n    \n    \n      15\n      0.017325\n      0.210955\n      00:00\n    \n    \n      16\n      0.024745\n      0.143737\n      00:00\n    \n    \n      17\n      0.025496\n      0.172830\n      00:00\n    \n    \n      18\n      0.025869\n      0.138098\n      00:00\n    \n    \n      19\n      0.025482\n      0.151525\n      00:00\n    \n    \n      20\n      0.027537\n      0.193854\n      00:00\n    \n    \n      21\n      0.024163\n      0.109432\n      00:00\n    \n    \n      22\n      0.020186\n      0.167370\n      00:00\n    \n    \n      23\n      0.017565\n      0.107690\n      00:00\n    \n    \n      24\n      0.015754\n      0.160082\n      00:00\n    \n    \n      25\n      0.013752\n      0.115723\n      00:00\n    \n    \n      26\n      0.012612\n      0.105396\n      00:00\n    \n    \n      27\n      0.011966\n      0.094555\n      00:00\n    \n    \n      28\n      0.014367\n      0.162134\n      00:00\n    \n    \n      29\n      0.013150\n      0.175142\n      00:00\n    \n  \n\n\n\n(4) 예측\n적합값 확인\n\nlrnr.show_results()\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      X1\n      X2\n      y\n      y_pred\n    \n  \n  \n    \n      0\n      1.0\n      3.0\n      3.9\n      3.740652\n    \n    \n      1\n      6.0\n      2.0\n      3.5\n      4.069994\n    \n  \n\n\n\n(옥순의 궁합)\n\ndf_new = pd.DataFrame({'X1':['옥순']*6, 'X2':['영식','영철','영호','광수','상철','영수']})\ndf_new\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      X1\n      X2\n    \n  \n  \n    \n      0\n      옥순\n      영식\n    \n    \n      1\n      옥순\n      영철\n    \n    \n      2\n      옥순\n      영호\n    \n    \n      3\n      옥순\n      광수\n    \n    \n      4\n      옥순\n      상철\n    \n    \n      5\n      옥순\n      영수\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nlrnr.get_preds(dl=dls.test_dl(df_new))\n\n\n\n\n\n\n\n\n(tensor([3.9063, 4.1200, 3.2875, 0.5278, 0.1878, 0.3123]), None)\n\n\n비교를 위해서\n\ndf_view\n\n\n\n\n\n  \n    \n      \n      영식\n      영철\n      영호\n      광수\n      상철\n      영수\n    \n  \n  \n    \n      옥순\n      3.9\n      4.1\n      NaN\n      0.5\n      0.3\n      NaN\n    \n    \n      영자\n      4.5\n      NaN\n      3.7\n      0.5\n      NaN\n      0.2\n    \n    \n      정숙\n      NaN\n      4.9\n      4.7\n      NaN\n      1.2\n      1.3\n    \n    \n      영숙\n      0.6\n      0.2\n      NaN\n      4.1\n      4.3\n      NaN\n    \n    \n      순자\n      0.7\n      0.9\n      NaN\n      4.2\n      NaN\n      3.9\n    \n    \n      현숙\n      NaN\n      0.2\n      0.3\n      NaN\n      3.5\n      3.4\n    \n  \n\n\n\n\n(정숙의 궁합)\n\ndf_new = pd.DataFrame({'X1':['정숙']*6, 'X2':['영식','영철','영호','광수','상철','영수']})\ndf_new\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      X1\n      X2\n    \n  \n  \n    \n      0\n      정숙\n      영식\n    \n    \n      1\n      정숙\n      영철\n    \n    \n      2\n      정숙\n      영호\n    \n    \n      3\n      정숙\n      광수\n    \n    \n      4\n      정숙\n      상철\n    \n    \n      5\n      정숙\n      영수\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nlrnr.get_preds(dl=dls.test_dl(df_new))\n\n\n\n\n\n\n\n\n(tensor([4.7749, 4.8766, 4.7028, 1.7205, 0.5784, 1.1272]), None)\n\n\n비교를 위해서\n\ndf_view\n\n\n\n\n\n  \n    \n      \n      영식\n      영철\n      영호\n      광수\n      상철\n      영수\n    \n  \n  \n    \n      옥순\n      3.9\n      4.1\n      NaN\n      0.5\n      0.3\n      NaN\n    \n    \n      영자\n      4.5\n      NaN\n      3.7\n      0.5\n      NaN\n      0.2\n    \n    \n      정숙\n      NaN\n      4.9\n      4.7\n      NaN\n      1.2\n      1.3\n    \n    \n      영숙\n      0.6\n      0.2\n      NaN\n      4.1\n      4.3\n      NaN\n    \n    \n      순자\n      0.7\n      0.9\n      NaN\n      4.2\n      NaN\n      3.9\n    \n    \n      현숙\n      NaN\n      0.2\n      0.3\n      NaN\n      3.5\n      3.4\n    \n  \n\n\n\n\n- Appedix: fastai 구조공부..\n\nlrnr.model\n\nEmbeddingDotBias(\n  (u_weight): Embedding(7, 2)\n  (i_weight): Embedding(7, 2)\n  (u_bias): Embedding(7, 1)\n  (i_bias): Embedding(7, 1)\n)\n\n\n\nlrnr.model.forward??\n\n\nSignature: lrnr.model.forward(x)\nDocstring:\nDefines the computation performed at every call.\nShould be overridden by all subclasses.\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.\nSource:   \n    def forward(self, x):\n        users,items = x[:,0],x[:,1]\n        dot = self.u_weight(users)* self.i_weight(items)\n        res = dot.sum(1) + self.u_bias(users).squeeze() + self.i_bias(items).squeeze()\n        if self.y_range is None: return res\n        return torch.sigmoid(res) * (self.y_range[1]-self.y_range[0]) + self.y_range[0]\nFile:      ~/anaconda3/envs/py37/lib/python3.7/site-packages/fastai/collab.py\nType:      method\n\n\n\n\n\nbias를 제외하면 우리가 짠 모형과 같음!"
  },
  {
    "objectID": "posts/Machine Learning/2022_12_21_Extra_1_ipynb의_사본.html#data",
    "href": "posts/Machine Learning/2022_12_21_Extra_1_ipynb의_사본.html#data",
    "title": "기계학습 (1221)",
    "section": "data",
    "text": "data\n- 예전에 살펴본 예제\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/main/posts/I.%20Overview/2022-09-08-rcmd_anal.csv')\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      user\n      item\n      rating\n      item_name\n    \n  \n  \n    \n      0\n      1\n      15\n      1.084308\n      홍차5\n    \n    \n      1\n      1\n      1\n      4.149209\n      커피1\n    \n    \n      2\n      1\n      11\n      1.142659\n      홍차1\n    \n    \n      3\n      1\n      5\n      4.033415\n      커피5\n    \n    \n      4\n      1\n      4\n      4.078139\n      커피4\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      995\n      100\n      18\n      4.104276\n      홍차8\n    \n    \n      996\n      100\n      17\n      4.164773\n      홍차7\n    \n    \n      997\n      100\n      14\n      4.026915\n      홍차4\n    \n    \n      998\n      100\n      4\n      0.838720\n      커피4\n    \n    \n      999\n      100\n      7\n      1.094826\n      커피7\n    \n  \n\n1000 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- 기억을 살리기 위해서..\n\ndf_view = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/main/posts/I.%20Overview/2022-09-08-rcmd_view.csv')\ndf_view\n\n\n\n\n\n  \n    \n      \n      커피1\n      커피2\n      커피3\n      커피4\n      커피5\n      커피6\n      커피7\n      커피8\n      커피9\n      커피10\n      홍차1\n      홍차2\n      홍차3\n      홍차4\n      홍차5\n      홍차6\n      홍차7\n      홍차8\n      홍차9\n      홍차10\n    \n  \n  \n    \n      0\n      4.149209\n      NaN\n      NaN\n      4.078139\n      4.033415\n      4.071871\n      NaN\n      NaN\n      NaN\n      NaN\n      1.142659\n      1.109452\n      NaN\n      0.603118\n      1.084308\n      NaN\n      0.906524\n      NaN\n      NaN\n      0.903826\n    \n    \n      1\n      4.031811\n      NaN\n      NaN\n      3.822704\n      NaN\n      NaN\n      NaN\n      4.071410\n      3.996206\n      NaN\n      NaN\n      0.839565\n      1.011315\n      NaN\n      1.120552\n      0.911340\n      NaN\n      0.860954\n      0.871482\n      NaN\n    \n    \n      2\n      4.082178\n      4.196436\n      NaN\n      3.956876\n      NaN\n      NaN\n      NaN\n      4.450931\n      3.972090\n      NaN\n      NaN\n      NaN\n      NaN\n      0.983838\n      NaN\n      0.918576\n      1.206796\n      0.913116\n      NaN\n      0.956194\n    \n    \n      3\n      NaN\n      4.000621\n      3.895570\n      NaN\n      3.838781\n      3.967183\n      NaN\n      NaN\n      NaN\n      4.105741\n      1.147554\n      NaN\n      1.346860\n      NaN\n      0.614099\n      1.297301\n      NaN\n      NaN\n      NaN\n      1.147545\n    \n    \n      4\n      NaN\n      NaN\n      NaN\n      NaN\n      3.888208\n      NaN\n      3.970330\n      3.979490\n      NaN\n      4.010982\n      NaN\n      0.920995\n      1.081111\n      0.999345\n      NaN\n      1.195183\n      NaN\n      0.818332\n      1.236331\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      0.511905\n      1.066144\n      NaN\n      1.315430\n      NaN\n      1.285778\n      NaN\n      0.678400\n      1.023020\n      0.886803\n      NaN\n      4.055996\n      NaN\n      NaN\n      4.156489\n      4.127622\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      96\n      NaN\n      1.035022\n      NaN\n      1.085834\n      NaN\n      0.812558\n      NaN\n      1.074543\n      NaN\n      0.852806\n      3.894772\n      NaN\n      4.071385\n      3.935935\n      NaN\n      NaN\n      3.989815\n      NaN\n      NaN\n      4.267142\n    \n    \n      97\n      NaN\n      1.115511\n      NaN\n      1.101395\n      0.878614\n      NaN\n      NaN\n      NaN\n      1.329319\n      NaN\n      4.125190\n      NaN\n      4.354638\n      3.811209\n      4.144648\n      NaN\n      NaN\n      4.116915\n      3.887823\n      NaN\n    \n    \n      98\n      NaN\n      0.850794\n      NaN\n      NaN\n      0.927884\n      0.669895\n      NaN\n      NaN\n      0.665429\n      1.387329\n      NaN\n      NaN\n      4.329404\n      4.111706\n      3.960197\n      NaN\n      NaN\n      NaN\n      3.725288\n      4.122072\n    \n    \n      99\n      NaN\n      NaN\n      1.413968\n      0.838720\n      NaN\n      NaN\n      1.094826\n      0.987888\n      NaN\n      1.177387\n      3.957383\n      4.136731\n      NaN\n      4.026915\n      NaN\n      NaN\n      4.164773\n      4.104276\n      NaN\n      NaN\n    \n  \n\n100 rows × 20 columns"
  },
  {
    "objectID": "posts/Machine Learning/2022_12_21_Extra_1_ipynb의_사본.html#모형",
    "href": "posts/Machine Learning/2022_12_21_Extra_1_ipynb의_사본.html#모형",
    "title": "기계학습 (1221)",
    "section": "모형",
    "text": "모형\n(편의상 바이어스를 제외하면)\n- 특징벡터:\n\n유저1의 취향 = [커피를 좋아하는 정도, 홍차를 좋아하는 정도]\n아이템1의 특징 = [커피의 특징, 홍차인 특징]\n\n- 평점\n\n유저1이 아이템1을 먹었을경우 평점: 유저1의 취향과 아이템1의 특징의 내적 = (유저1의 취향 \\(\\odot\\) 아이템1의 특징).sum()"
  },
  {
    "objectID": "posts/Machine Learning/2022_12_21_Extra_1_ipynb의_사본.html#학습",
    "href": "posts/Machine Learning/2022_12_21_Extra_1_ipynb의_사본.html#학습",
    "title": "기계학습 (1221)",
    "section": "학습",
    "text": "학습\n(1) dls\n\ndls = CollabDataLoaders.from_df(df)\n\n\ndls.items\n\n\n\n\n\n  \n    \n      \n      user\n      item\n      rating\n      item_name\n    \n  \n  \n    \n      192\n      20\n      1\n      3.933610\n      커피1\n    \n    \n      794\n      80\n      12\n      4.125577\n      홍차2\n    \n    \n      554\n      56\n      17\n      3.826543\n      홍차7\n    \n    \n      524\n      53\n      3\n      1.170372\n      커피3\n    \n    \n      175\n      18\n      10\n      4.170460\n      커피10\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      896\n      90\n      12\n      4.391382\n      홍차2\n    \n    \n      849\n      85\n      3\n      0.693932\n      커피3\n    \n    \n      746\n      75\n      12\n      4.301711\n      홍차2\n    \n    \n      787\n      79\n      14\n      3.930048\n      홍차4\n    \n    \n      100\n      11\n      20\n      1.145191\n      홍차10\n    \n  \n\n800 rows × 4 columns\n\n\n\n(2) lrnr\n\nlrnr = collab_learner(dls,n_factors=2) # 교재에는 y_range 를 설정하도록 되어있지만 설정 안해도 적합에는 크게 상관없음..\n\n(3) fit\n\nlrnr.fit(10,0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      5.409556\n      3.313535\n      00:00\n    \n    \n      1\n      3.724468\n      2.569444\n      00:00\n    \n    \n      2\n      2.855262\n      1.630986\n      00:00\n    \n    \n      3\n      2.051633\n      0.469137\n      00:00\n    \n    \n      4\n      1.483525\n      0.264474\n      00:00\n    \n    \n      5\n      1.096932\n      0.178709\n      00:00\n    \n    \n      6\n      0.824759\n      0.117894\n      00:00\n    \n    \n      7\n      0.630313\n      0.081575\n      00:00\n    \n    \n      8\n      0.487037\n      0.076569\n      00:00\n    \n    \n      9\n      0.380992\n      0.076578\n      00:00\n    \n  \n\n\n\n(4) predict\n(적합된 값 확인)\n\nlrnr.show_results() # 누를때마다 결과다름\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      user\n      item\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      61.0\n      19.0\n      4.160296\n      4.037053\n    \n    \n      1\n      22.0\n      4.0\n      4.192549\n      3.940574\n    \n    \n      2\n      17.0\n      17.0\n      1.096392\n      0.967445\n    \n    \n      3\n      14.0\n      4.0\n      3.826174\n      4.002016\n    \n    \n      4\n      88.0\n      5.0\n      1.197540\n      0.968678\n    \n    \n      5\n      53.0\n      15.0\n      3.859582\n      3.966616\n    \n    \n      6\n      83.0\n      5.0\n      0.752025\n      0.789191\n    \n    \n      7\n      10.0\n      11.0\n      0.676153\n      0.978221\n    \n    \n      8\n      46.0\n      17.0\n      0.833476\n      0.908008\n    \n  \n\n\n\n(예측값)\n\ndf_new = pd.DataFrame({'user':[1,1,1,1], 'item':[9,10,11,12]})\ndf_new\n\n\n\n\n\n  \n    \n      \n      user\n      item\n    \n  \n  \n    \n      0\n      1\n      9\n    \n    \n      1\n      1\n      10\n    \n    \n      2\n      1\n      11\n    \n    \n      3\n      1\n      12\n    \n  \n\n\n\n\n\nlrnr.get_preds(dl=dls.test_dl(df_new))\n\n\n\n\n\n\n\n\n(tensor([4.0201, 4.0401, 0.9940, 0.8291]), None)"
  },
  {
    "objectID": "posts/Machine Learning/2022_09_07_(1주차)_9월7일_ipynb의_사본.html",
    "href": "posts/Machine Learning/2022_09_07_(1주차)_9월7일_ipynb의_사본.html",
    "title": "기계학습 (0907) 1주차",
    "section": "",
    "text": "기계학습특강\n\n# 우리의 1차 목표: 이미지 -> 개/고양이 판단하는 모형을 채용하고, 그 모형에 데이터를 넣어서 학습하고, 그 모형의 결과를 판단하고 싶다. (즉 클래시파이어를 만든다는 소리)\n# 우리의 2차 목표: 그 모형에 \"새로운\" 자료를 전달하여 이미지를 분류할 것이다. (즉 클래시파이어를 쓴다는 소리)\n\n\nfrom fastai.vision.all import *\n\n\npath=untar_data(URLs.PETS)/'images'\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 01:16<00:00]\n    \n    \n\n\n\npath\n\nPath('/root/.fastai/data/oxford-iiit-pet/images')\n\n\n\nPILImage.create('/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg')\n\n\n\n\n\n_lst = '/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg','/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_10.jpg'\n\n\n_lst[0]\n\n'/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg'\n\n\n\nfaaaa = get_image_files(path)\n#교수님은 filenames로 설정함\n\n\nfaaaa[0]\n\nPath('/root/.fastai/data/oxford-iiit-pet/images/leonberger_137.jpg')\n\n\n\nPILImage.create('/root/.fastai/data/oxford-iiit-pet/images/newfoundland_28.jpg')\n\n\n\n\n\nPILImage.create(faaaa[0])\n\n\n\n\n\nprint(faaaa[1])\nPILImage.create(faaaa[1])\n\n/root/.fastai/data/oxford-iiit-pet/images/Birman_139.jpg\n\n\n\n\n\n\nprint(faaaa[3])\nPILImage.create(faaaa[3])\n\n/root/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_148.jpg\n\n\n\n\n\n\nprint(faaaa[6])\nPILImage.create(faaaa[6])\n\n/root/.fastai/data/oxford-iiit-pet/images/Siamese_79.jpg\n\n\n\n\n\n\n'A'.isupper()\n\nTrue\n\n\n\n'abdjlkfwe.jpg'[0]\n\n'a'\n\n\n\ndef f(fname):\n  if fname[0].isupper():\n    return 'cat'\n  else:\n    return 'dog'\n\n\ndls = ImageDataLoaders.from_name_func(path,faaaa,f,item_tfms=Resize(224))\n\n\ndls.show_batch(max_n=25)\n\n\n\n\n\n# 우리의 1차 목표: 이미지 -> 개/고양이 판단하는 모형을 채용하고, 그 모형에 데이터를 넣어서 학습하고, 그 모형의 결과를 판단하고 싶다. (즉 클래시파이어를 만든다는 소리)\n# 우리의 2차 목표: 그 모형에 \"새로운\" 자료를 전달하여 이미지를 분류할 것이다. (즉 클래시파이어를 쓴다는 소리)\n\n\n\n## 오브젝트.. 오브젝트에는 동사와 명사 가 있어요\n\n### 명사\n# (1) 데이터\n# (2) 채용한 모형의 이론\n# (3) 평가기준 matric\n\n\n\n\n### 동사\n# (1) 학습\n# (2) 판단\n\n\nysj = cnn_learner(dls,resnet34,metrics=error_rate)\n\n##저항률 확인(잘 파악하는지 확인하기 위해서 metrics=error_rate 이용)\n\nysj.fine_tune(1)                  \n              ## 3을 쓰면 1보다는 많이 한다는 뜻  \n              ## 학습하다..동사,,\n\n/usr/local/lib/python3.7/dist-packages/fastai/vision/learner.py:284: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.147350\n      0.014042\n      0.004060\n      00:56\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.053051\n      0.012090\n      0.004736\n      00:52\n    \n  \n\n\n\n\n?cnn_learner\n\n\nPILImage.create(faaaa[1])\n\n\n\n\n\nysj.predict(PILImage.create(faaaa[1]))\n\n\n\n\n\n\n\n\n('cat', TensorBase(0), TensorBase([1.0000e+00, 3.0773e-10]))\n\n\nysj.predict(PILImage.create(faaaa[6])) #동사이고.. 뒤에 점찍었으니까 함수다 생각하기 # 입력을 이미지 자체로 넣었는데, 이미지가 저장된 path만 넣어도 되지않을까?\n\nysj.predict(faaaa[6])\n\n\n\n\n\n\n\n\n('cat', TensorBase(0), TensorBase([1.0000e+00, 2.2963e-10]))\n\n\n\nysj.show_results()\n\n\n\n\n\n\n\n\n\n\n\n\n##ysj가 잘구현이 되는지 체크를 해야함\n# 체크를 하는 object를 만들어야함\nchecker = Interpretation.from_learner(ysj)\n\n\n\n\n\n\n\n\n\nchecker.plot_top_losses(16)\n\n# 첫번째 사진에서 5.59는 로스이고 1퍼의 확률로 강아지라고 생각함\n# 로스는 몇퍼의 확률로 잘못생각했느냐에 따라서 달라질 수 있음\n# 맞추는 걸 넘어서 확실해야 로스가 적다. (확신의여부)\n\n# 오버피팅 아냐..? 과대적합..? 자기들이 이미 다학습된 내용 가지고 보여주는거아냐? 생각->새로운 이미지 부여\n\n\n\n\n\n\n\n\n\n\n\n\nimg=PILImage.create(requests.get('https://dimg.donga.com/ugc/CDB/SHINDONGA/Article/5e/0d/9f/01/5e0d9f011a9ad2738de6.jpg').content)\nysj.predict(img)\n\n\n\n\n\n\n\n\n('dog', TensorBase(1), TensorBase([2.8106e-06, 1.0000e+00]))\n\n\n\nimg=PILImage.create(requests.get('https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcnSq1X%2Fbtq4o9AdWTH%2FHTm9TZG4AszSwLPFlVfGW0%2Fimg.jpg').content)\nysj.predict(img)\n\n\n\n\n\n\n\n\n('dog', TensorBase(1), TensorBase([1.0909e-06, 1.0000e+00]))\n\n\n\nimg=PILImage.create(requests.get('https://image.edaily.co.kr/images/photo/files/NP/S/2022/04/PS22042501396.jpg').content)\nysj.predict(img)\n\n\n\n\n\n\n\n\n('cat', TensorBase(0), TensorBase([1.0000e+00, 2.6542e-12]))\n\n\n\nimg=PILImage.create(requests.get('https://blog.kakaocdn.net/dn/zfQQi/btrydI0vGzm/3YY3KrPEwKN558e27H6t0k/img.jpg').content)\nysj.predict(img)\n\n\n\n\n\n\n\n\n('cat', TensorBase(0), TensorBase([0.9805, 0.0195]))\n\n\n\nPILImage.create('/강아지사진1.jpg')"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_09_(10주차)_11월9일_ipynb의_사본.html#import",
    "href": "posts/Machine Learning/3. RNN/2022_11_09_(10주차)_11월9일_ipynb의_사본.html#import",
    "title": "기계학습 (1109) 10주차",
    "section": "import",
    "text": "import\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_09_(10주차)_11월9일_ipynb의_사본.html#예비학습-net.parameters의-의미",
    "href": "posts/Machine Learning/3. RNN/2022_11_09_(10주차)_11월9일_ipynb의_사본.html#예비학습-net.parameters의-의미",
    "title": "기계학습 (1109) 10주차",
    "section": "예비학습: net.parameters()의 의미",
    "text": "예비학습: net.parameters()의 의미\n9월27일 강의노트 중 “net.parameters()의 의미?”를 설명한다.\n- iterator, generator의 개념필요 - https://guebin.github.io/IP2022/2022/06/06/(14주차)-6월6일.html, 클래스공부 8단계 참고\n- 탐구시작: 네트워크 생성\n\nnet = torch.nn.Linear(in_features=1,out_features=1)\nnet.weight\n\nParameter containing:\ntensor([[0.0003]], requires_grad=True)\n\n\n\nnet.bias\n\nParameter containing:\ntensor([0.0782], requires_grad=True)\n\n\n- torch.optim.SGD? 를 확인하면 params에 대한설명에 아래와 같이 되어있음\nparams (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n- 설명을 읽어보면 params에 iterable object를 넣으라고 되어있음 (iterable object는 숨겨진 명령어로 __iter__를 가지고 있는 오브젝트를 의미)\n\nset(dir(net.parameters())) & {'__iter__'}\n\n{'__iter__'}\n\n\n\n# 만약  iter 가 아니면 & 했을때 빈값이 나옴 \n\n\n# for문 뒤에 오는 거.. iterable object\n# for i in [1,2,3] : 이런거.. 리스트, 스트링,, \n# lst.__ 뒤에 iter__ ~ \n\n- 무슨의미?\n\nfor param in net.parameters():\n    print(param)\n\nParameter containing:\ntensor([[0.0003]], requires_grad=True)\nParameter containing:\ntensor([0.0782], requires_grad=True)\n\n\n- 그냥 이건 이런느낌인데?\n\nfor param in [net.weight,net.bias]:\n    print(param)\n\nParameter containing:\ntensor([[0.0003]], requires_grad=True)\nParameter containing:\ntensor([0.0782], requires_grad=True)\n\n\n결론: net.parameters()는 net오브젝트에서 학습할 파라메터를 모두 모아 리스트같은 iterable object로 만드는 함수라 이해할 수 있다.\n- 응용예제1\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-22-regression.csv\") \nx=torch.tensor(df.x).float().reshape(100,1)\ny=torch.tensor(df.y).float().reshape(100,1)\n\n\nb = torch.tensor(-5.0,requires_grad=True)\nw = torch.tensor(10.0,requires_grad=True)\noptimizr = torch.optim.SGD([b,w],lr=1/10) ## 이렇게 전달하면 됩니당!!\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(w*x+b).data,'--')\n\n\n\n\n\nfor epoc in range(30):\n    ## step1\n    yhat = b+ w*x \n    ## step2\n    loss = torch.mean((y-yhat)**2)\n    ## step3\n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(w*x+b).data,'--')\n\n\n\n\n- 응용예제2\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-22-regression.csv\") \nx = torch.tensor(df.x).float().reshape(100,1)\ny = torch.tensor(df.y).float().reshape(100,1)\nX = torch.concat([torch.ones_like(x),x],axis=1)\n\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\noptimizr = torch.optim.SGD([What],lr=1/10) # What은 iterable 하지 않지만 [What]은 iterable 함\n\n\nWhat\n\ntensor([[-5.],\n        [10.]], requires_grad=True)\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(X@What).data,'--')\n\n\n\n\n\nfor epoc in range(30):\n    ## step1\n    yhat = X@What \n    ## step2 \n    loss = torch.mean((y-yhat)**2)\n    ## step3\n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n\nplt.plot(x,y,'o')\nplt.plot(x,(X@What).data,'--')\n\n\n\n\n\n스스로 학습 (중간고사 대비문제)\n아래와 같은 자료가 있다고 가정하자.\n\nx = torch.rand([1000,1])*2-1\ny = 3.14 + 6.28*x + torch.randn([1000,1]) \n\n\nplt.plot(x,y,'o',alpha=0.1)\n\n\n\n\n아래의 모형을 가정하고 \\(\\alpha_0,\\alpha_1,\\beta_0,\\beta_1\\)을 파이토치를 이용하여 추정하고자한다.\n\n\\(y_i = \\alpha_0+\\beta_0+ \\beta_1x_i + \\alpha_1x_i + \\epsilon_i \\quad \\epsilon_i \\sim N(0,\\sigma^2)\\)\n\n아래는 이를 수행하기 위한 코드이다. ???를 적절히 채워서 코드를 완성하라.\n\n항목 추가\n항목 추가\n\n\nalpha0 = torch.tensor([0.5], requires_grad=True)\nalpha1 = torch.tensor([[0.5]], requires_grad=True)\nbeta0 = torch.tensor([0.7], requires_grad=True)\nbeta1 = torch.tensor([[0.7]], requires_grad=True)\n\n\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD([alpha0,alpha1,beta0,beta1], lr=1/10)\n\n\nfor epoc in range(30):\n    ## 1\n    yhat = alpha0 + beta0 + alpha1*x + beta1*x \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nprint(alpha0+beta0)\n\ntensor([3.1593], grad_fn=<AddBackward0>)\n\n\n\n3.14 근처\n\n\nprint(alpha1+beta1)\n\ntensor([[6.0875]], grad_fn=<AddBackward0>)\n\n\n\n6.28 근처"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_09_(10주차)_11월9일_ipynb의_사본.html#define-some-funtions",
    "href": "posts/Machine Learning/3. RNN/2022_11_09_(10주차)_11월9일_ipynb의_사본.html#define-some-funtions",
    "title": "기계학습 (1109) 10주차",
    "section": "Define some funtions",
    "text": "Define some funtions\n\ndef f(txt,mapping):\n    return [mapping[key] for key in txt] \nsoft = torch.nn.Softmax(dim=1)"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_09_(10주차)_11월9일_ipynb의_사본.html#exam2-abc",
    "href": "posts/Machine Learning/3. RNN/2022_11_09_(10주차)_11월9일_ipynb의_사본.html#exam2-abc",
    "title": "기계학습 (1109) 10주차",
    "section": "Exam2: abc",
    "text": "Exam2: abc\n\ndata\n\ntxt = list('abc')*100\ntxt[:10]\n\n['a', 'b', 'c', 'a', 'b', 'c', 'a', 'b', 'c', 'a']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['a', 'b', 'c', 'a', 'b'], ['b', 'c', 'a', 'b', 'c'])\n\n\n\n\n하나의 은닉노드를 이용한 풀이 – 억지로 성공\n- 데이터정리\n\nmapping = {'a':0,'b':1,'c':2}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 0, 1]), tensor([1, 2, 0, 1, 2]))\n\n\n- 학습\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=3,embedding_dim=1), #a,b,c 문자 3개니까 num_embeddings=3 쓰고 \n    torch.nn.Tanh(),\n    #===#\n    torch.nn.Linear(in_features=1,out_features=3)\n    #torch.nn.Softmax() 생략!\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    ## 2 \n    loss = loss_fn(net(x),y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과해석\n\nhidden = net[:-1](x).data\nyhat = soft(net(x)).data \n\n\nplt.plot(hidden[:9],'--o')\n# 가운데:a 맨위:b, 맨아래:c\n\n\n\n\n\nplt.plot(net(x).data[:9],'--o')\n\n\n\n\n\nplt.plot(yhat[:9],'--o')\n\n\n\n\n\n억지로 맞추고있긴한데 파라메터가 부족해보인다.\n\n- 결과시각화1\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n첫 그림 -> 두번째 그림\n\n# 첫번째그림 \nhidden[:9], (net[-1].weight.data).T, net[-1].bias.data\n\n(tensor([[-0.0147],\n         [ 0.9653],\n         [-0.9896],\n         [-0.0147],\n         [ 0.9653],\n         [-0.9896],\n         [-0.0147],\n         [ 0.9653],\n         [-0.9896]]),\n tensor([[-4.6804,  0.3071,  5.2894]]),\n tensor([-1.5440,  0.9143, -1.3970]))\n\n\n\n# hidden : n x 1 형태\n# new[-1] : 3 x 1 형태여서 trans\n\n\nhidden[:9]@(net[-1].weight.data).T + net[-1].bias.data\n\ntensor([[-1.4755,  0.9098, -1.4745],\n        [-6.0618,  1.2108,  3.7086],\n        [ 3.0875,  0.6104, -6.6312],\n        [-1.4755,  0.9098, -1.4745],\n        [-6.0618,  1.2108,  3.7086],\n        [ 3.0875,  0.6104, -6.6312],\n        [-1.4755,  0.9098, -1.4745],\n        [-6.0618,  1.2108,  3.7086],\n        [ 3.0875,  0.6104, -6.6312]])\n\n\n\n(파랑,주황,초록) 순서로 그려짐\n파랑 = hidden * (-4.6804) + (-1.5440)\n주황 = hidden * (0.3071) + (0.9143)\n초록 = hidden * (5.2894) + (-1.3970)\n\n- 내부동작을 잘 뜯어보니까 사실 엉성해. 엄청 위태위태하게 맞추고 있었음. - weight: 파랑과 초록을 구분하는 역할을 함 - weight + bias: 뭔가 교모하게 애매한 주황값을 만들어서 애매하게 ’b’라고 나올 확률을 학습시킨다. \\(\\to\\) 사실 학습하는 것 같지 않고 때려 맞추는 느낌, 쓸수있는 weight가 한정적이라서 생기는 현상 (양수,음수,0)\n\n참고: torch.nn.Linear()의 비밀? - 사실 \\({\\boldsymbol y}={\\boldsymbol x}{\\bf W} + {\\boldsymbol b}\\) 꼴에서의 \\({\\bf W}\\)와 \\({\\boldsymbol b}\\)가 저장되는게 아니다. - \\({\\boldsymbol y}={\\boldsymbol x}{\\bf A}^T + {\\boldsymbol b}\\) 꼴에서의 \\({\\bf A}\\)와 \\({\\boldsymbol b}\\)가 저장된다. - \\({\\bf W} = {\\bf A}^T\\) 인 관계에 있으므로 l1.weight 가 우리가 생각하는 \\({\\bf W}\\) 로 해석하려면 사실 transpose를 취해줘야 한다.\n왜 이렇게..? - 계산의 효율성 때문 (numpy의 구조를 알아야함) - \\({\\boldsymbol x}\\), \\({\\boldsymbol y}\\) 는 수학적으로는 col-vec 이지만 메모리에 저장할시에는 row-vec 로 해석하는 것이 자연스럽다. (사실 메모리는 격자모양으로 되어있지 않음)\n잠깐 딴소리!!\n(예시1)\n\n_arr = np.array(range(4)).reshape(2,2)\n\n\n_arr.strides\n\n(16, 8)\n\n\n\n아래로 한칸 = 16칸 jump\n오른쪽으로 한칸 = 8칸 jump\n\n(예시2)\n\n_arr = np.array(range(6)).reshape(3,2)\n\n\n_arr.strides\n\n(16, 8)\n\n\n\n아래로 한칸 = 16칸 jump\n오른쪽으로 한칸 = 8칸 jump\n\n(예시3)\n\n_arr = np.array(range(6)).reshape(2,3)\n\n\n_arr.strides\n\n(24, 8)\n\n\n\n아래로 한칸 = 24칸 jump\n오른쪽으로 한칸 = 8칸 jump\n\n(예시4)\n\n_arr = np.array(range(4),dtype=np.int8).reshape(2,2)\n\n\n_arr\n\narray([[0, 1],\n       [2, 3]], dtype=int8)\n\n\n\n_arr.strides\n\n(2, 1)\n\n\n\n아래로한칸 = 2칸 (= 2바이트 jump = 16비트 jump)\n오른쪽으로 한칸 = 1칸 jump (= 1바이트 jump = 8비트 jump)\n\n진짜 참고..\n\n1바이트 = 8비트\n1바이트는 2^8=256 의 정보 표현\nnp.int8은 8비트로 정수를 저장한다는 의미\n\n\n2**8\n\n256\n\n\n\nprint(np.array(55,dtype=np.int8))\nprint(np.array(127,dtype=np.int8))\nprint(np.array(300,dtype=np.int8)) # overflow \n\n55\n127\n44\n\n\n딴소리 끝!!\n\n- 결과시각화2\n\ncombined  = torch.concat([hidden,net(x).data,yhat],axis=1)\ncombined.shape\n\ntorch.Size([299, 7])\n\n\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n\nplt.matshow(combined[:15],vmin=-7,vmax=7,cmap='bwr')\nplt.xticks(range(7), labels=[r'$h$',r'$y=a?$',r'$y=b?$',r'$y=c?$',r'$P(y=a)$',r'$P(y=b)$',r'$P(y=c)$'],size=14)\nplt.colorbar()\nplt.gcf().set_figwidth(15)\nplt.gcf().set_figheight(15)\nplt.title(r\"Vis2: $[h | net(x) | \\hat{y}]$\",size=25)\n\nText(0.5, 1.0, 'Vis2: $[h | net(x) | \\\\hat{y}]$')"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_09_(10주차)_11월9일_ipynb의_사본.html#exam3-abcd",
    "href": "posts/Machine Learning/3. RNN/2022_11_09_(10주차)_11월9일_ipynb의_사본.html#exam3-abcd",
    "title": "기계학습 (1109) 10주차",
    "section": "Exam3: abcd",
    "text": "Exam3: abcd\n\ndata\n\ntxt = list('abcd')*100\ntxt[:10]\n\n['a', 'b', 'c', 'd', 'a', 'b', 'c', 'd', 'a', 'b']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['a', 'b', 'c', 'd', 'a'], ['b', 'c', 'd', 'a', 'b'])\n\n\n\n\n하나의 은닉노드를 이용한 풀이 – 억지로 성공\n- 데이터정리\n\nmapping = {'a':0,'b':1,'c':2,'d':3}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 3, 0]), tensor([1, 2, 3, 0, 1]))\n\n\n- 학습\n\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=4,embedding_dim=1),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=1,out_features=4)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nnet[0].weight.data = torch.tensor([[-0.3333],[-2.5000],[5.0000],[0.3333]])\n\nnet[-1].weight.data = torch.tensor([[1.5000],[-6.0000],[-2.0000],[6.0000]])\nnet[-1].bias.data = torch.tensor([0.1500, -2.0000,  0.1500, -2.000])\n\n\nfor epoc in range(5000):\n    ## 1\n    ## 2 \n    loss = loss_fn(net(x),y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과시각화1\n\nhidden = net[:-1](x).data\nyhat = soft(net(x)).data\n\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n- 결과시각화2\n\ncombined  = torch.concat([hidden,net(x).data,yhat],axis=1)\ncombined.shape\n\ntorch.Size([399, 9])\n\n\n\nplt.matshow(combined[:15],vmin=-15,vmax=15,cmap='bwr')\nplt.xticks(range(9), labels=[r'$h$',r'$y=a?$',r'$y=b?$',r'$y=c?$',r'$y=d?$',r'$P(y=a)$',r'$P(y=b)$',r'$P(y=c)$',r'$P(y=d)$'],size=14)\nplt.colorbar()\nplt.gcf().set_figwidth(15)\nplt.gcf().set_figheight(15)\nplt.title(r\"Vis2: $[h | net(x) | \\hat{y}]$\",size=25)\n\nText(0.5, 1.0, 'Vis2: $[h | net(x) | \\\\hat{y}]$')\n\n\n\n\n\n\n\n두개의 은닉노드를 이용한 풀이 – 깔끔한 성공\n- 데이터정리\n\nmapping = {'a':0,'b':1,'c':2,'d':3}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 3, 0]), tensor([1, 2, 3, 0, 1]))\n\n\n- 학습\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=4,embedding_dim=2),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=2,out_features=4)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과시각화1\n\nhidden = net[:-1](x).data\nyhat = soft(net(x)).data\n\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n- 결과시각화2\n\ncombined  = torch.concat([hidden,net(x).data,yhat],axis=1)\ncombined.shape\n\ntorch.Size([399, 10])\n\n\n\nplt.matshow(combined[:15],vmin=-7,vmax=7,cmap='bwr')\nplt.xticks(range(10), labels=[r'$h$',r'$h$',r'$y=a?$',r'$y=b?$',r'$y=c?$',r'$y=d?$',r'$P(y=a)$',r'$P(y=b)$',r'$P(y=c)$',r'$P(y=d)$'],size=14)\nplt.colorbar()\nplt.gcf().set_figwidth(15)\nplt.gcf().set_figheight(15)\nplt.title(r\"Vis2: $[h | net(x) | \\hat{y}]$\",size=25)\n\nText(0.5, 1.0, 'Vis2: $[h | net(x) | \\\\hat{y}]$')"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_09_(10주차)_11월9일_ipynb의_사본.html#exam4-abcde-스스로-공부",
    "href": "posts/Machine Learning/3. RNN/2022_11_09_(10주차)_11월9일_ipynb의_사본.html#exam4-abcde-스스로-공부",
    "title": "기계학습 (1109) 10주차",
    "section": "Exam4: abcde (스스로 공부)",
    "text": "Exam4: abcde (스스로 공부)\n\ndata\n주어진 자료가 다음과 같다고 하자.\n\ntxt = list('abcde')*100\ntxt[:10]\n\n['a', 'b', 'c', 'd', 'e', 'a', 'b', 'c', 'd', 'e']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['a', 'b', 'c', 'd', 'e'], ['b', 'c', 'd', 'e', 'a'])\n\n\n아래 코드를 변형하여 적절한 네트워크를 설계하고 위의 자료를 학습하라. (깔끔한 성공을 위한 최소한의 은닉노드를 설정할 것)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=??,embedding_dim=??),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=??,out_features=??)\n)\n\n\n3개의 은닉노드를 이용한 풀이\na,b,c,d,e 를 표현함에 있어서 3개의 은닉노드면 충분하다. - 1개의 은닉노드 -> 2개의 문자를 표현할 수 있음. - 2개의 은닉노드 -> 4개의 문자를 표현할 수 있음. - 3개의 은닉노드 -> 8개의 문자를 표현할 수 있음.\n\nmapping = {'a':0,'b':1,'c':2,'d':3,'e':4}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 3, 4]), tensor([1, 2, 3, 4, 0]))\n\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=5,embedding_dim=3),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=3,out_features=5)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과시각화1\n\nhidden = net[:-1](x).data\nyhat = soft(net(x)).data\n\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n- 결과시각화2\n\ncombined  = torch.concat([hidden,net(x).data,yhat],axis=1)\ncombined.shape\n\ntorch.Size([499, 13])\n\n\n\nplt.matshow(combined[:15],vmin=-5,vmax=5,cmap='bwr')\nplt.xticks(range(13), labels=[r'$h$',r'$h$',r'$h$',\n                              r'$y=A?$',r'$y=b?$',r'$y=c?$',r'$y=d?$',r'$y=e?$',\n                              r'$P(y=A)$',r'$P(y=b)$',r'$P(y=c)$',r'$P(y=d)$',r'$P(y=e)$'],size=13)\nplt.colorbar()\nplt.gcf().set_figwidth(15)\nplt.gcf().set_figheight(15)\nplt.title(r\"Vis2: $[h | net(x) | \\hat{y}]$\",size=25)\n\nText(0.5, 1.0, 'Vis2: $[h | net(x) | \\\\hat{y}]$')"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_09_(10주차)_11월9일_ipynb의_사본.html#exam5-abacad",
    "href": "posts/Machine Learning/3. RNN/2022_11_09_(10주차)_11월9일_ipynb의_사본.html#exam5-abacad",
    "title": "기계학습 (1109) 10주차",
    "section": "Exam5: AbAcAd",
    "text": "Exam5: AbAcAd\n\ndata\n\ntxt = list('AbAcAd')*100\ntxt[:10]\n\n['A', 'b', 'A', 'c', 'A', 'd', 'A', 'b', 'A', 'c']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['A', 'b', 'A', 'c', 'A'], ['b', 'A', 'c', 'A', 'd'])\n\n\n\n\n두개의 은닉노드를 이용한 풀이 – 실패\n- 데이터정리\n\nmapping = {'A':0,'b':1,'c':2,'d':3}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 0, 2, 0]), tensor([1, 0, 2, 0, 3]))\n\n\n- 학습\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=4,embedding_dim=2),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=2,out_features=4)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과시각화1\n\nhidden = net[:-1](x).data\nyhat = soft(net(x)).data\n\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n- 결과시각화2\n\ncombined  = torch.concat([hidden,net(x).data,yhat],axis=1)\ncombined.shape\n\ntorch.Size([599, 10])\n\n\n\nplt.matshow(combined[:15],vmin=-5,vmax=5,cmap='bwr')\nplt.xticks(range(10), labels=[r'$h$',r'$h$',r'$y=A?$',r'$y=b?$',r'$y=c?$',r'$y=d?$',r'$P(y=A)$',r'$P(y=b)$',r'$P(y=c)$',r'$P(y=d)$'],size=14)\nplt.colorbar()\nplt.gcf().set_figwidth(15)\nplt.gcf().set_figheight(15)\nplt.title(r\"Vis2: $[h | net(x) | \\hat{y}]$\",size=25)\n\nText(0.5, 1.0, 'Vis2: $[h | net(x) | \\\\hat{y}]$')\n\n\n\n\n\n\n실패\n\n- 실패를 해결하는 순진한 접근방식: 위 문제를 해결하기 위해서는 아래와 같은 구조로 데이터를 다시 정리하면 될 것이다.\n\n\n\nX\ny\n\n\n\n\nA,b\nA\n\n\nb,A\nc\n\n\nA,c\nA\n\n\nc,A\nd\n\n\nA,d\nA\n\n\nd,A\nb\n\n\nA,b\nA\n\n\nb,A\nc\n\n\n…\n…\n\n\n\n- 순진한 접근방식의 비판: - 결국 정확하게 직전 2개의 문자를 보고 다음 문제를 예측하는 구조 - 만약에 직전 3개의 문자를 봐야하는 상황이 된다면 또 다시 코드를 수정해야함. - 그리고 실전에서는 직전 몇개의 문자를 봐야하는지 모름.\n\n# \n# x1 -> y1\n# x1, x2 -> y2\n# x1, x2, x3 -> y3\n# x1, x2, x3, x4 -> y4\n# ...\n\n이것에 대한 해결책은 순환신경망이다.\n\n\n순환망을 위하여 data 다시정리\n- 기존의 정리방식\n\ntxt = list('AbAcAd')*100\ntxt[:10]\n\n['A', 'b', 'A', 'c', 'A', 'd', 'A', 'b', 'A', 'c']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['A', 'b', 'A', 'c', 'A'], ['b', 'A', 'c', 'A', 'd'])\n\n\n\nx = torch.tensor(f(txt_x,{'A':0,'b':1,'c':2,'d':3}))\ny = torch.tensor(f(txt_y,{'A':0,'b':1,'c':2,'d':3}))\n\n\nx[:8],y[:8]\n\n(tensor([0, 1, 0, 2, 0, 3, 0, 1]), tensor([1, 0, 2, 0, 3, 0, 1, 0]))\n\n\n- 이번엔 원핫인코딩형태까지 미리 정리하자. (임베딩 레이어 안쓸예정)\n\nx= torch.nn.functional.one_hot(x).float()\ny= torch.nn.functional.one_hot(y).float()\n\n\nx,y\n\n(tensor([[1., 0., 0., 0.],\n         [0., 1., 0., 0.],\n         [1., 0., 0., 0.],\n         ...,\n         [1., 0., 0., 0.],\n         [0., 0., 1., 0.],\n         [1., 0., 0., 0.]]),\n tensor([[0., 1., 0., 0.],\n         [1., 0., 0., 0.],\n         [0., 0., 1., 0.],\n         ...,\n         [0., 0., 1., 0.],\n         [1., 0., 0., 0.],\n         [0., 0., 0., 1.]]))\n\n\n\n\n실패했던 풀이의 재구현1\n- 방금 실패한 풀이\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=4,embedding_dim=2),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=2,out_features=4)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n- Tanh까지만 클래스로 바꾸어서 구현 - 클래스를 이용하는 방법: https://guebin.github.io/DL2022/2022/11/01/(9주차)-11월1일.html#로지스틱-모형을-이용한-풀이\n\nclass Hnet(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 사용할 레이어 정의\n        self.i2h = torch.nn.Linear(in_features=4,out_features=2) #input에 들어가서 hidden을 만들어주는 ?  # 이름만들땐 self를 붙여주기\n        self.tanh = torch.nn.Tanh() # 두레이어 통과시켜서 hidden 출력! \n    def forward(self,x):\n      # yhat을 어떻게 구현할 것인지 정의 \n        hidden = self.tanh(self.i2h(x)) \n        return hidden\n\n- for문돌릴준비\n\ntorch.manual_seed(43052) \nhnet = Hnet()\nlinr = torch.nn.Linear(in_features=2,out_features=4)\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(hnet.parameters())+list(linr.parameters()))\n\n\n# hne.parameters() 안에 뭔 값이 있는데 안보이니까 list화 시키기 \n\n- for문: 20회반복\n\nfor epoc in range(20): \n    ## 1 \n    ## 2 \n    hidden = hnet(x) \n    output = linr(hidden)\n    loss = loss_fn(output,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- linr(hnet(x)) 적합결과 <– 숫자체크\n\nlinr(hnet(x))\n\ntensor([[-0.3589,  0.7921, -0.1970, -0.0302],\n        [-0.2912,  0.8140, -0.2032,  0.0178],\n        [-0.3589,  0.7921, -0.1970, -0.0302],\n        ...,\n        [-0.3589,  0.7921, -0.1970, -0.0302],\n        [-0.1065,  0.6307, -0.0874,  0.1821],\n        [-0.3589,  0.7921, -0.1970, -0.0302]], grad_fn=<AddmmBackward0>)\n\n\n\n\n실패했던 풀이의 재구현2\n- Tanh까지 구현한 클래스\n\n\n# class Hnet(torch.nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         self.i2h = torch.nn.Linear(in_features=4,out_features=2)\n#         self.tanh = torch.nn.Tanh()\n#     def forward(self,x):\n#         hidden = self.tanh(self.i2h(x))\n#         return hidden\n\n- for문돌릴준비\n\ntorch.manual_seed(43052) \nhnet = Hnet()\nlinr = torch.nn.Linear(in_features=2,out_features=4)\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(hnet.parameters())+list(linr.parameters()))\n\n- for문: 20회 반복\n\nT = len(x) \nfor epoc in range(20): \n    ## 1~2\n    loss = 0 \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = hnet(xt) \n        ot = linr(ht) \n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- linr(hnet(x)) 적합결과 <– 숫자체크\n\nlinr(hnet(x))\n\ntensor([[-0.3589,  0.7921, -0.1970, -0.0302],\n        [-0.2912,  0.8140, -0.2032,  0.0178],\n        [-0.3589,  0.7921, -0.1970, -0.0302],\n        ...,\n        [-0.3589,  0.7921, -0.1970, -0.0302],\n        [-0.1065,  0.6307, -0.0874,  0.1821],\n        [-0.3589,  0.7921, -0.1970, -0.0302]], grad_fn=<AddmmBackward0>)\n\n\n\n\n순환신경망의 아이디어\n\n모티브\n(예비생각1) \\({\\boldsymbol h}\\)에 대한 이해\n\\({\\boldsymbol h}\\)는 사실 문자열 ’abcd’들을 숫자로 바꾼 또 다른 형식의 숫자표현이라 해석할 수 있음. 즉 원핫인코딩과 다른 또 다른 형태의 숫자표현이라 해석할 수 있다. (사실 원핫인코딩보다 약간 더 (1) 액기스만 남은 느낌 + (2) 숙성된 느낌을 준다) - (why1) h는 “학습을 용이하게 하기 위해서 x를 적당히 선형적으로 전처리한 상태”라고 이해가능 - (why2) 실제로 예시를 살펴보면 그러했다.\n결론: 사실 \\({\\boldsymbol h}\\)는 잘 숙성되어있는 입력정보 \\({\\bf X}\\) 그 자체로 해석 할 수 있다.\n(예비생각2) 수백년전통을 이어가는 방법\n“1리터에 500만원에 낙찰된 적 있습니다.”\n“2kg에 1억원 정도 추산됩니다.”\n“20여 종 종자장을 블렌딩해 100ml에 5000만원씩 분양 예정입니다.”\n\n모두 씨간장(종자장) 가격에 관한 실제 일화다.\n\n(중략...)\n\n위스키나 와인처럼 블렌딩을 하기도 한다. \n새로 담근 간장에 씨간장을 넣거나, 씨간장독에 햇간장을 넣어 맛을 유지하기도 한다. \n이를 겹장(또는 덧장)이라 한다. \n몇몇 종갓집에선 씨간장 잇기를 몇백 년째 해오고 있다. \n매년 새로 간장을 담가야 이어갈 수 있으니 불씨 꺼트리지 않는 것처럼 굉장히 어려운 일이다.\n이렇게 하는 이유는 집집마다 내려오는 고유 장맛을 잃지 않기 위함이다. \n씨간장이란 그만큼 소중한 주방의 자산이며 정체성이다.\n덧장: 새로운간장을 만들때, 옛날간장을 섞어서 만듬\n* 기존방식 - \\(\\text{콩물} \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}\\)\n* 수백년 전통의 간장맛을 유지하는 방식\n\n\\(\\text{콩물}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_1\\)\n\\(\\text{콩물}_2, \\text{간장}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_2\\)\n\\(\\text{콩물}_3, \\text{간장}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_3\\)\n\n* 수백년 전통의 간장맛을 유지하면서 조리를 한다면?\n\n\\(\\text{콩물}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_1 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_1\\)\n\\(\\text{콩물}_2, \\text{간장}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_2 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_2\\)\n\\(\\text{콩물}_3, \\text{간장}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_3 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_3\\)\n\n점점 맛있는 간장계란밥이 탄생함\n* 알고리즘의 편의상 아래와 같이 생각해도 무방\n\n\\(\\text{콩물}_1, \\text{간장}_0 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_1 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_1\\), \\(\\text{간장}_0=\\text{맹물}\\)\n\\(\\text{콩물}_2, \\text{간장}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_2 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_2\\)\n\\(\\text{콩물}_3, \\text{간장}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_3 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_3\\)\n\n아이디어\n* 수백년 전통의 간장맛을 유지하면서 조리하는 과정을 수식으로?\n\n\\(\\boldsymbol{x}_1, \\boldsymbol{h}_0 \\overset{\\text{숙성}}{\\longrightarrow} \\boldsymbol{h}_1 \\overset{\\text{조리}}{\\longrightarrow} \\hat{\\boldsymbol y}_1\\)\n\\(\\boldsymbol{x}_2, \\boldsymbol{h}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\boldsymbol{h}_2 \\overset{\\text{조리}}{\\longrightarrow} \\hat{\\boldsymbol y}_2\\)\n\\(\\boldsymbol{x}_3, \\boldsymbol{h}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\boldsymbol{h}_3 \\overset{\\text{조리}}{\\longrightarrow} \\hat{\\boldsymbol y}_3\\)\n\n이제 우리가 배울것은 (1) “\\(\\text{콩물}_{t}\\)”와 “\\(\\text{간장}_{t-1}\\)”로 “\\(\\text{간장}_t\\)”를 숙성하는 방법 (2) “\\(\\text{간장}_t\\)”로 “\\(\\text{간장계란밥}_t\\)를 조리하는 방법이다\n즉 숙성담당 네트워크와 조리담당 네트워크를 각각 만들어 학습하면 된다.\n\n\n알고리즘\n세부적인 알고리즘 (\\(t=0,1,2,\\dots\\)에 대하여 한줄 한줄 쓴 알고리즘)\n\n\\(t=0\\)\n\n\\({\\boldsymbol h}_0=[[0,0]]\\) <– \\(\\text{간장}_0\\)은 맹물로 초기화\n\n\\(t=1\\)\n\n\\({\\boldsymbol h}_1= \\tanh({\\boldsymbol x}_1{\\bf W}_{ih}+{\\boldsymbol h}_0{\\bf W}_{hh}+{\\boldsymbol b}_{ih}+{\\boldsymbol b}_{hh})\\) - \\({\\boldsymbol x}_1\\): (1,4) - \\({\\bf W}_{ih}\\): (4,2) - \\({\\boldsymbol h}_0\\): (1,2) - \\({\\bf W}_{hh}\\): (2,2) - \\({\\boldsymbol b}_{ih}\\): (1,2) - \\({\\boldsymbol b}_{hh}\\): (1,2)\n\\({\\boldsymbol o}_1= {\\bf W}_{ho}{\\boldsymbol h}_1+{\\boldsymbol b}_{ho}\\)\n\\(\\hat{\\boldsymbol y}_1 = \\text{soft}({\\boldsymbol o}_1)\\)\n\n\\(t=2\\) <– 여기서부터는 \\(t=2\\)와 비슷\n\n\n좀 더 일반화된 알고리즘\n(ver1)\ninit \\(\\boldsymbol{h}_0\\)\nfor \\(t\\) in \\(1:T\\)\n\n\\({\\boldsymbol h}_t= \\tanh({\\boldsymbol x}_t{\\bf W}_{ih}+{\\boldsymbol h}_{t-1}{\\bf W}_{hh}+{\\boldsymbol b}_{ih}+{\\boldsymbol b}_{hh})\\)\n\\({\\boldsymbol o}_t= {\\bf W}_{ho}{\\boldsymbol h}_1+{\\boldsymbol b}_{ho}\\)\n\\(\\hat{\\boldsymbol y}_t = \\text{soft}({\\boldsymbol o}_t)\\)\n\n(ver2)\ninit hidden\n\nfor t in 1:T \n    hidden = tanh(linr(x)+linr(hidden)) # 더하기 위해서 linr 해준다\n    # t시점 간장              # t-1시점 간장\n    output = linr(hidden)\n    yt_hat = soft(output)\n\n코드상으로는 \\(h_t\\)와 \\(h_{t-1}\\)의 구분이 교모하게 사라진다. (그래서 오히려 좋아)\n\n\n전체알고리즘은 대충 아래와 같은 형식으로 구현될 수 있음\n### \nclass rNNCell(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        linr1 = torch.nn.Linear(?,?) \n        linr2 = torch.nn.Linear(?,?) \n        tanh = torch.nn.Tanh()\n    def forward(self,x,hidden):\n        hidden = tanh(lrnr1(x)+lrnr2(hidden))\n        return hidden\n\ninit ht\nrnncell = rNNCell()\n\nfor t in 1:T \n    xt, yt = x[[t]], y[[t]] \n    ht = rnncell(xt, ht)\n    ot = linr(ht) \n    loss = loss + loss_fn(ot, yt)\n\n\n\n순환신경망 구현1 – 성공\n(1) 숙성담당 네트워크\n\nclass rNNCell(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.i2h = torch.nn.Linear(4,2) # 입력이 h로 간다...i2h\n        self.h2h = torch.nn.Linear(2,2) # h 에서 h 로~ \n        self.tanh = torch.nn.Tanh()\n    def forward(self,x,hidden):\n        hidden = self.tanh(self.i2h(x)+self.h2h(hidden))\n        return hidden\n\n\ntorch.manual_seed(43052)\nrnncell = rNNCell() # 숙성담당 네트워크 \n\n(2) 조리담당 네트워크\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4) \n\n(3) 손실함수, 옵티마이저 설계\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(cook.parameters()))\n\n(4) 학습 (6분정도 걸림)\n\nT = len(x) \nfor epoc in range(5000): \n    ## 1~2\n    loss = 0 \n    ht = torch.zeros(1,2) \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = rnncell(xt,ht) \n        ot = cook(ht) \n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화\n\nT = len(x) \nhidden = torch.zeros(T,2) # 599년치 h를 담을 변수  # 학습만 했기 때문에 어디에 저장해야해~ \n_water = torch.zeros(1,2) # 맹물 \nhidden[[0]] = rnncell(x[[0]],_water) \nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]]) \n\n\nyhat = soft(cook(hidden))\nyhat\n\ntensor([[1.6522e-02, 6.2036e-01, 1.0433e-01, 2.5879e-01],\n        [9.9965e-01, 6.5788e-05, 1.8450e-05, 2.6785e-04],\n        [7.6673e-05, 1.9704e-01, 8.0201e-01, 8.7218e-04],\n        ...,\n        [7.4634e-05, 1.9501e-01, 8.0407e-01, 8.4751e-04],\n        [9.4785e-01, 7.4711e-03, 6.1182e-04, 4.4064e-02],\n        [3.6306e-02, 1.2466e-01, 2.8862e-03, 8.3615e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n\nplt.matshow(yhat.data[-15:])\n\n<matplotlib.image.AxesImage at 0x7f919046aed0>\n\n\n\n\n\n\n아주 특이한 특징: yhat[:15], yhat[:-15] 의 적합결과가 다르다\n왜? 간장계란밥은 간장이 중요한데, 간장은 시간이 갈수록 맛있어지니까..\n\n\n\n순환신경망 구현2 (with RNNCell) – 성공\nref: https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html\n(1) 숙성네트워크\n선언\n\nrnncell = torch.nn.RNNCell(4,2)\n\n가중치초기화 (순환신경망 구현1과 동일하도록)\n\ntorch.manual_seed(43052)\n_rnncell = rNNCell()\n\n\nrnncell.weight_ih.data = _rnncell.i2h.weight.data \nrnncell.weight_hh.data = _rnncell.h2h.weight.data \nrnncell.bias_hh.data = _rnncell.h2h.bias.data \nrnncell.bias_ih.data = _rnncell.i2h.bias.data \n\n(2) 조리담당 네트워크\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4) \n\n(3) 손실함수, 옵티마이저 설계\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(cook.parameters()))\n\n(4) 학습 (6분정도 걸림)\n\nT = len(x) \nfor epoc in range(5000): \n    ## 1~2\n    loss = 0 \n    ht = torch.zeros(1,2) \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = rnncell(xt,ht) \n        ot = cook(ht) \n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화\n\nT = len(x) \nhidden = torch.zeros(T,2) # 599년치 h를 담을 변수 \n_water = torch.zeros(1,2) # 맹물 \nhidden[[0]] = rnncell(x[[0]],_water) \nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]]) \n\n\nyhat = soft(cook(hidden))\nyhat\n\ntensor([[1.6522e-02, 6.2036e-01, 1.0433e-01, 2.5879e-01],\n        [9.9965e-01, 6.5788e-05, 1.8450e-05, 2.6785e-04],\n        [7.6673e-05, 1.9704e-01, 8.0201e-01, 8.7218e-04],\n        ...,\n        [7.4634e-05, 1.9501e-01, 8.0407e-01, 8.4751e-04],\n        [9.4785e-01, 7.4711e-03, 6.1182e-04, 4.4064e-02],\n        [3.6306e-02, 1.2466e-01, 2.8862e-03, 8.3615e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n\nplt.matshow(yhat.data[-15:])\n\n<matplotlib.image.AxesImage at 0x7f917546d210>\n\n\n\n\n\n\n\n순환신경망 구현3 (with RNN) – 성공\n(예비학습)\n- 아무리 생각해도 yhat구하려면 좀 귀찮음\n\nT = len(x) \nhidden = torch.zeros(T,2) # 599년치 h를 담을 변수 \n_water = torch.zeros(1,2) # 맹물 \nhidden[[0]] = rnncell(x[[0]],_water) \nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]])\n\n\nsoft(cook(hidden))\n\ntensor([[1.6522e-02, 6.2036e-01, 1.0433e-01, 2.5879e-01],\n        [9.9965e-01, 6.5788e-05, 1.8450e-05, 2.6785e-04],\n        [7.6673e-05, 1.9704e-01, 8.0201e-01, 8.7218e-04],\n        ...,\n        [7.4634e-05, 1.9501e-01, 8.0407e-01, 8.4751e-04],\n        [9.4785e-01, 7.4711e-03, 6.1182e-04, 4.4064e-02],\n        [3.6306e-02, 1.2466e-01, 2.8862e-03, 8.3615e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n- 이렇게 하면 쉽게(?) 구할 수 있음\n\nrnn = torch.nn.RNN(4,2) \n\n\nrnn.weight_hh_l0.data = rnncell.weight_hh.data \nrnn.bias_hh_l0.data = rnncell.bias_hh.data \nrnn.weight_ih_l0.data = rnncell.weight_ih.data \nrnn.bias_ih_l0.data = rnncell.bias_ih.data \n\n\n_water\n\ntensor([[0., 0.]])\n\n\n\nsoft(cook(rnn(x,_water)[0]))\n\ntensor([[1.6522e-02, 6.2036e-01, 1.0433e-01, 2.5879e-01],\n        [9.9965e-01, 6.5788e-05, 1.8450e-05, 2.6785e-04],\n        [7.6673e-05, 1.9704e-01, 8.0201e-01, 8.7218e-04],\n        ...,\n        [7.4634e-05, 1.9501e-01, 8.0407e-01, 8.4751e-04],\n        [9.4785e-01, 7.4711e-03, 6.1182e-04, 4.4064e-02],\n        [3.6306e-02, 1.2466e-01, 2.8862e-03, 8.3615e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n\n똑같음!\n\n- rnn(x,_water)의 결과는 (1) 599년치 간장 (2) 599번째 간장 이다\n\nrnn(x,_water)\n\n(tensor([[-0.2232,  0.9769],\n         [-0.9999, -0.9742],\n         [ 0.9154,  0.9992],\n         ...,\n         [ 0.9200,  0.9992],\n         [-0.9978, -0.0823],\n         [-0.9154,  0.9965]], grad_fn=<SqueezeBackward1>),\n tensor([[-0.9154,  0.9965]], grad_fn=<SqueezeBackward1>))\n\n\n(예비학습결론) torch.nn.RNN(4,2)는 torch.nn.RNNCell(4,2)의 batch 버전이다. (for문이 포함된 버전이다)\n\ntorch.nn.RNN(4,2)를 이용하여 구현하자.\n(1) 숙성네트워크\n선언\n\ntorch.manual_seed(43052)\nrnn = torch.nn.RNN(4,2)\n\n(2) 조리네트워크\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4)\n\n(3) 손실함수와 옵티마이저\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(cook.parameters())) # 우리가 배울것: 숙성하는 방법 + 요리하는 방법 \n\n(4) 학습\n\nfor epoc in range(5000):\n    ## 1\n    _water = torch.zeros(1,2)\n    hidden, _ = rnn(x,_water)\n    output = cook(hidden)\n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화\n\nyhat = soft(cook(rnn(x,_water)[0]))\nyhat\n\ntensor([[1.9725e-02, 1.5469e-03, 8.2766e-01, 1.5106e-01],\n        [9.1875e-01, 1.6513e-04, 6.7703e-02, 1.3384e-02],\n        [2.0031e-02, 1.0659e-03, 8.5248e-01, 1.2642e-01],\n        ...,\n        [1.9640e-02, 1.3568e-03, 8.3705e-01, 1.4196e-01],\n        [9.9564e-01, 1.3114e-05, 3.5069e-03, 8.4108e-04],\n        [3.5473e-03, 1.5670e-01, 1.4102e-01, 6.9873e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n\nplt.matshow(yhat.data[:15])\n\n<matplotlib.image.AxesImage at 0x7f91754970d0>\n\n\n\n\n\n\n!git add .\n!git commit -m .\n!git push\n\n[master abb1501] .\n 1 file changed, 4132 insertions(+)\n create mode 100644 \"_notebooks/2022-11-09-(10\\354\\243\\274\\354\\260\\250) 11\\354\\233\\2249\\354\\235\\274.ipynb\"\nEnumerating objects: 6, done.\nCounting objects: 100% (6/6), done.\nDelta compression using up to 16 threads\nCompressing objects: 100% (4/4), done.\nWriting objects: 100% (4/4), 814.34 KiB | 23.95 MiB/s, done.\nTotal 4 (delta 2), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (2/2), completed with 2 local objects.\nTo https://github.com/guebin/STML2022.git\n   b77134c..abb1501  master -> master"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_10_31_(9주차)_10월31일_ipynb의_사본.html",
    "href": "posts/Machine Learning/3. RNN/2022_10_31_(9주차)_10월31일_ipynb의_사본.html",
    "title": "기계학습 (1031) 9주차",
    "section": "",
    "text": "import torch\nimport pandas as pd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_10_31_(9주차)_10월31일_ipynb의_사본.html#define-some-funtions",
    "href": "posts/Machine Learning/3. RNN/2022_10_31_(9주차)_10월31일_ipynb의_사본.html#define-some-funtions",
    "title": "기계학습 (1031) 9주차",
    "section": "Define some funtions",
    "text": "Define some funtions\n- 활성화함수들\n\nsig = torch.nn.Sigmoid()\nsoft = torch.nn.Softmax(dim=1)\ntanh = torch.nn.Tanh()\n\n\n_x = torch.linspace(-5,5,100)\nplt.plot(_x,tanh(_x))\nplt.title(\"tanh(x)\", size=15)\n\nText(0.5, 1.0, 'tanh(x)')\n\n\n\n\n\n- 문자열 -> 숫자로 바꾸는 함수\n\ndef f(txt,mapping):\n    return [mapping[key] for key in txt] \n\n(사용예시1)\n\ntxt = ['a','b','a']\nmapping = {'a':33,'b':-22}\nprint('변환전: %s'% txt)\nprint('변환후: %s'% f(txt,mapping))\n\n변환전: ['a', 'b', 'a']\n변환후: [33, -22, 33]\n\n\n(사용예시2)\n\ntxt = ['a','b','a']\nmapping = {'a':[1,0],'b':[0,1]}\nprint('변환전: %s'% txt)\nprint('변환후: %s'% f(txt,mapping))\n\n변환전: ['a', 'b', 'a']\n변환후: [[1, 0], [0, 1], [1, 0]]"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_10_31_(9주차)_10월31일_ipynb의_사본.html#exam1-ab",
    "href": "posts/Machine Learning/3. RNN/2022_10_31_(9주차)_10월31일_ipynb의_사본.html#exam1-ab",
    "title": "기계학습 (1031) 9주차",
    "section": "Exam1: ab",
    "text": "Exam1: ab\n\ndata\n\ntxt = list('ab')*100\ntxt[:10]\n\n['a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b']\n\n\n\nlen(txt)\n\n200\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5] #xa가 입력으로 들어가면 b를 뱉어내고..\n\n(['a', 'b', 'a', 'b', 'a'], ['b', 'a', 'b', 'a', 'b'])\n\n\n\n\n선형모형을 이용한 풀이\n\n(풀이1) 1개의 파라메터 – 실패\n- 데이터정리\n\nx = torch.tensor(f(txt_x,{'a':0,'b':1})).float().reshape(-1,1)\ny = torch.tensor(f(txt_y,{'a':0,'b':1})).float().reshape(-1,1)\n\n\nx[:5],y[:5]\n\n(tensor([[0.],\n         [1.],\n         [0.],\n         [1.],\n         [0.]]),\n tensor([[1.],\n         [0.],\n         [1.],\n         [0.],\n         [1.]]))\n\n\n- 학습 및 결과 시각화\n\nnet = torch.nn.Linear(in_features=1,out_features=1,bias=False)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y[:5],'o')\nplt.plot(net(x).data[:5]) #밑에 노란색줄 what=0\n\n\n\n\n\n잘 학습이 안되었다.\n\n- 학습이 잘 안된 이유\n\npd.DataFrame({'x':x[:5].reshape(-1),'y':y[:5].reshape(-1)})\n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      0.0\n      1.0\n    \n    \n      1\n      1.0\n      0.0\n    \n    \n      2\n      0.0\n      1.0\n    \n    \n      3\n      1.0\n      0.0\n    \n    \n      4\n      0.0\n      1.0\n    \n  \n\n\n\n\n현재 \\(\\hat{y}_i = \\hat{w}x_i\\) 꼴의 아키텍처이고 \\(y_i \\approx \\hat{w}x_i\\) 가 되는 적당한 \\(\\hat{w}\\)를 찾아야 하는 상황 - \\((x_i,y_i)=(0,1)\\) 이면 어떠한 \\(\\hat{w}\\)를 선택해도 \\(y_i \\approx \\hat{w}x_i\\)를 만드는 것이 불가능\n- \\((x_i,y_i)=(1,0)\\) 이면 \\(\\hat{w}=0\\)일 경우 \\(y_i \\approx \\hat{w}x_i\\)로 만드는 것이 가능\n상황을 종합해보니 \\(\\hat{w}=0\\)으로 학습되는 것이 그나마 최선\n\n\n(풀이2) 1개의 파라메터 – 성공, but 확장성이 없는 풀이\n- 0이라는 값이 문제가 되므로 인코딩방식의 변경\n\nx = torch.tensor(f(txt_x,{'a':-1,'b':1})).float().reshape(-1,1) \ny = torch.tensor(f(txt_y,{'a':-1,'b':1})).float().reshape(-1,1)\n\n\nx[:5],y[:5]\n\n(tensor([[-1.],\n         [ 1.],\n         [-1.],\n         [ 1.],\n         [-1.]]),\n tensor([[ 1.],\n         [-1.],\n         [ 1.],\n         [-1.],\n         [ 1.]]))\n\n\n\nnet = torch.nn.Linear(in_features=1,out_features=1,bias=False)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(2000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과는 성공\n\nplt.plot(y[:5],'o')\nplt.plot(net(x).data[:5])\n\n\n\n\n\n딱봐도 클래스가 3개일 경우 확장이 어려워 보인다.\n\n\n\n\n로지스틱 모형을 이용한 풀이\n\n(풀이1) 1개의 파라메터 – 실패\n- 데이터를 다시 a=0, b=1로 정리\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping)).float().reshape(-1,1)\ny = torch.tensor(f(txt_y,mapping)).float().reshape(-1,1)\n\n\nx[:5],y[:5]\n\n(tensor([[0.],\n         [1.],\n         [0.],\n         [1.],\n         [0.]]),\n tensor([[1.],\n         [0.],\n         [1.],\n         [0.],\n         [1.]]))\n\n\n- 학습\n\nnet = torch.nn.Linear(in_features=1,out_features=1,bias=False)\nloss_fn = torch.nn.BCEWithLogitsLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과\n\nplt.plot(y[:10],'o')\nplt.plot(sig(net(x)).data[:10],'--o')\n\n\n\n\n- 결과해석: 예상되었던 실패임 - 아키텍처는 \\(\\hat{y}_i = \\text{sig}(\\hat{w}x_i)\\) 꼴이다. - \\((x_i,y_i)=(0,1)\\) 이라면 어떠한 \\(\\hat{w}\\)을 선택해도 \\(\\hat{w}x_i=0\\) 이다. 이경우 \\(\\hat{y}_i = \\text{sig}(0) = 0.5\\) 가 된다. - \\((x_i,y_i)=(1,0)\\) 이라면 \\(\\hat{w}=-5\\)와 같은 값으로 선택하면 \\(\\text{sig}(-5) \\approx 0 = y_i\\) 와 같이 만들 수 있다. - 상황을 종합하면 net의 weight는 \\(\\text{sig}(\\hat{w}x_i) \\approx 0\\) 이 되도록 적당한 음수로 학습되는 것이 최선임을 알 수 있다.\n\nnet.weight # 적당한 음수값으로 학습되어있음을 확인\n\nParameter containing:\ntensor([[-4.0070]], requires_grad=True)\n\n\n\n\n(풀이2) 2개의 파라메터 + 좋은 초기값 – 성공\n- 동일하게 a=0, b=1로 맵핑\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping)).float().reshape(-1,1)\ny = torch.tensor(f(txt_y,mapping)).float().reshape(-1,1)\n\n\nx[:5],y[:5]\n\n(tensor([[0.],\n         [1.],\n         [0.],\n         [1.],\n         [0.]]),\n tensor([[1.],\n         [0.],\n         [1.],\n         [0.],\n         [1.]]))\n\n\n- 네트워크에서 bias를 넣기로 결정함\n\nnet = torch.nn.Linear(in_features=1,out_features=1,bias=True)\nloss_fn = torch.nn.BCEWithLogitsLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n- net의 초기값을 설정 (이것은 좋은 초기값임)\n\nnet.weight.data = torch.tensor([[-5.00]])\nnet.bias.data = torch.tensor([+2.500])\n\n\nnet(x)[:10]\n\ntensor([[ 2.5000],\n        [-2.5000],\n        [ 2.5000],\n        [-2.5000],\n        [ 2.5000],\n        [-2.5000],\n        [ 2.5000],\n        [-2.5000],\n        [ 2.5000],\n        [-2.5000]], grad_fn=<SliceBackward0>)\n\n\n- 학습전 결과\n\nplt.plot(y[:10],'o')\nplt.plot(sig(net(x)).data[:10],'--o')\n\n\n\n\n- 학습후결과\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y[:10],'o')\nplt.plot(sig(net(x)).data[:10],'--o') #bias가 0값을 뭉개주는..\n\n\n\n\n\n\n(풀이3) 2개의 파라메터 + 나쁜초기값 – 성공\n- a=0, b=1\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping)).float().reshape(-1,1)\ny = torch.tensor(f(txt_y,mapping)).float().reshape(-1,1)\n\n\nx[:5],y[:5]\n\n(tensor([[0.],\n         [1.],\n         [0.],\n         [1.],\n         [0.]]),\n tensor([[1.],\n         [0.],\n         [1.],\n         [0.],\n         [1.]]))\n\n\n- 이전과 동일하게 바이어스가 포함된 네트워크 설정\n\nnet = torch.nn.Linear(in_features=1,out_features=1,bias=True)\nloss_fn = torch.nn.BCEWithLogitsLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n- 초기값설정 (이 초기값은 나쁜 초기값임)\n\nnet.weight.data = torch.tensor([[+5.00]])\nnet.bias.data = torch.tensor([-2.500])\n\n\nnet(x)[:10]\n\ntensor([[-2.5000],\n        [ 2.5000],\n        [-2.5000],\n        [ 2.5000],\n        [-2.5000],\n        [ 2.5000],\n        [-2.5000],\n        [ 2.5000],\n        [-2.5000],\n        [ 2.5000]], grad_fn=<SliceBackward0>)\n\n\n- 학습전상태: 반대모양으로 되어있다.\n\nplt.plot(y[:10],'o')\nplt.plot(sig(net(x)).data[:10],'--o')\n\n\n\n\n- 학습\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y[:10],'o')\nplt.plot(sig(net(x)).data[:10],'--o')\n\n\n\n\n\n결국 수렴하긴 할듯\n\n\n\n(풀이4) 3개의 파라메터를 쓴다면?\n- a=0, b=1로 코딩\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping)).float().reshape(-1,1)\ny = torch.tensor(f(txt_y,mapping)).float().reshape(-1,1)\n\n\nx[:5],y[:5]\n\n(tensor([[0.],\n         [1.],\n         [0.],\n         [1.],\n         [0.]]),\n tensor([[1.],\n         [0.],\n         [1.],\n         [0.],\n         [1.]]))\n\n\n- 3개의 파라메터를 사용하기 위해서 아래와 같은 구조를 생각하자.\ntorch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.ACTIVATION_FUNCTION(),\n    torch.nn.Linear(in_features=1,out_features=1,bias=False)\n)\n위와 같은 네트워크를 설정하면 3개의 파라메터를 사용할 수 있다. 적절한 ACTIVATION_FUNCTION을 골라야 하는데 실험적으로 tanh가 적절하다고 알려져있다. (\\(\\to\\) 그래서 우리도 실험적으로 이해해보자)\n\n(예비학습1) net(x)와 사실 net.forwardx(x)는 같다.\n\nnet(x)[:5] # 풀이3에서 학습한 네트워크임\n\ntensor([[-0.1584],\n        [ 0.1797],\n        [-0.1584],\n        [ 0.1797],\n        [-0.1584]], grad_fn=<SliceBackward0>)\n\n\n\nnet.forward(x)[:5] # 풀이3에서 학습한 네트워크임\n\ntensor([[-0.1584],\n        [ 0.1797],\n        [-0.1584],\n        [ 0.1797],\n        [-0.1584]], grad_fn=<SliceBackward0>)\n\n\n그래서 net.forward를 재정의하면 net(x)의 기능을 재정의 할 수 있다.\n\n# over riding ? \n\n\nnet.forward = lambda x: 1 \n\n\n“lambda x: 1” 은 입력이 x 출력이 1인 함수를 의미 (즉 입력값에 상관없이 항상 1을 출력하는 함수)\n“net.forward = lambda x:1” 이라고 새롭게 선언하였므로 앞으론 net.forward(x), net(x) 도 입력값에 상관없이 항상 1을 출력하게 될 것임\n\n\nnet(x)\n\n1\n\n\n(예비학습2) torch.nn.Module을 상속받아서 네트워크를 만들면 (= “class XXX(torch.nn.Module):” 와 같은 방식으로 클래스를 선언하면) 약속된 아키텍처를 가진 네트워크를 찍어내는 함수를 만들 수 있다.\n(예시1)\n\nclass Mynet1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.a1 = torch.nn.Sigmoid()\n        self.l2 = torch.nn.Linear(in_features=1,out_features=1,bias=False)\n    def forward(self,x):\n        yhat = self.l2(self.a1(self.l1(x)))\n        return yhat\n\n이제\nnet = Mynet1()\n는 아래와 같은 효과를 가진다.\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.Sigmoid(),\n    torch.nn.Linear(in_features=1,out_features=1,bias=False)\n)\n(예시2)\n\nclass Mynet2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.a1 = torch.nn.ReLU()\n        self.l2 = torch.nn.Linear(in_features=1,out_features=1,bias=False)\n    def forward(self,x):\n        yhat = self.l2(self.a1(self.l1(x)))\n        return yhat\n\n이제\nnet = Mynet2()\n는 아래와 같은 효과를 가진다.\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.RuLU(),\n    torch.nn.Linear(in_features=1,out_features=1,bias=False)\n)\n(예시3)\n\nclass Mynet3(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.a1 = torch.nn.Tanh()\n        self.l2 = torch.nn.Linear(in_features=1,out_features=1,bias=False)\n    def forward(self,x):\n        yhat = self.l2(self.a1(self.l1(x)))\n        return yhat\n\n이제\nnet = Mynet3()\n는 아래와 같은 효과를 가진다.\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=1,out_features=1,bias=False)\n)\n클래스에 대한 이해가 부족한 학생을 위한 암기방법\nstep1: 아래와 코드를 복사하여 틀을 만든다. (이건 무조건 고정임, XXXX 자리는 원하는 이름을 넣는다)\nclass XXXX(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        ## 우리가 사용할 레이어를 정의 \n        \n        ## 레이어 정의 끝\n    def forward(self,x):\n        ## yhat을 어떻게 구할것인지 정의 \n        \n        ## 정의 끝\n        return yhat\n\nnet(x)에 사용하는 x임, yhat은 net.forward(x) 함수의 리턴값임\n사실, x/yhat은 다른 변수로 써도 무방하나 (예를들면 input/output 이라든지) 설명의 편의상 x와 yhat을 고정한다.\n\nstep2: def __init__(self):에 사용할 레이어를 정의하고 이름을 붙인다. 이름은 항상 self.xxx 와 같은 식으로 정의한다.\nclass XXXX(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        ## 우리가 사용할 레이어를 정의 \n        self.xxx1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.xxx2 = torch.nn.Tanh()\n        self.xxx3 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        ## 레이어 정의 끝\n    def forward(self,x):\n        ## yhat을 어떻게 구할것인지 정의 \n        \n        ## 정의 끝\n        return yhat\nstep3: def forward:에 “x –> yhat” 으로 가는 과정을 묘사한 코드를 작성하고 yhat을 리턴하도록 한다.\nclass XXXX(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        ## 우리가 사용할 레이어를 정의 \n        self.xxx1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.xxx2 = torch.nn.Tanh()\n        self.xxx3 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        ## 레이어 정의 끝\n    def forward(self,x):\n        ## yhat을 어떻게 구할것인지 정의 \n        u = self.xxx1(x) \n        v = self.xxx2(u)\n        yhat = self.xxx3(v) \n        ## 정의 끝\n        return yhat\n예비학습 끝\n\n- 우리가 하려고 했던 것: 아래의 아키텍처에서\ntorch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.ACTIVATION_FUNCTION(),\n    torch.nn.Linear(in_features=1,out_features=1,bias=False)\n)\nACTIVATION의 자리에 tanh가 왜 적절한지 직관을 얻어보자.\n- 실험결과1(Sig): Sigmoid activation을 포함한 아키텍처로 학습시킨 25개의 적합결과\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        net = Mynet1()\n        loss_fn = torch.nn.BCEWithLogitsLoss()\n        optimizr = torch.optim.Adam(net.parameters())\n        for epoc in range(1000):\n            ## 1\n            yhat = net(x)\n            ## 2\n            loss = loss_fn(yhat,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        ax[i][j].plot(y[:5],'o')\n        ax[i][j].plot(sig(net(x[:5])).data,'--o')\nfig.suptitle(r\"$a_1(x):=Sigmoid(x)$\",size=20)\nfig.tight_layout()\n\n\n\n\n- 실험결과2(ReLU): RuLU activation을 포함한 아키텍처로 학습시킨 25개의 적합결과\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        net = Mynet2()\n        loss_fn = torch.nn.BCEWithLogitsLoss()\n        optimizr = torch.optim.Adam(net.parameters())\n        for epoc in range(1000):\n            ## 1\n            yhat = net(x)\n            ## 2\n            loss = loss_fn(yhat,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        ax[i][j].plot(y[:5],'o')\n        ax[i][j].plot(sig(net(x[:5])).data,'--o')\nfig.suptitle(r\"$a_2(x):=ReLU(x)$\",size=20)\nfig.tight_layout()\n\n\n\n\n- 실험결과3(Tanh): Tanh activation을 포함한 아키텍처로 학습시킨 25개의 적합결과\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        net = Mynet3()\n        loss_fn = torch.nn.BCEWithLogitsLoss()\n        optimizr = torch.optim.Adam(net.parameters())\n        for epoc in range(1000):\n            ## 1\n            yhat = net(x)\n            ## 2\n            loss = loss_fn(yhat,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        ax[i][j].plot(y[:5],'o')\n        ax[i][j].plot(sig(net(x[:5])).data,'--o')\nfig.suptitle(r\"$a_2(x):=Tanh(x)$\",size=20)        \nfig.tight_layout()\n\n\n\n\n- 실험해석 - sig: 주황색선의 변동폭이 작음 + 항상 0.5근처로 머무는 적합값이 존재 - relu: 주황색선의 변동폭이 큼 + 항상 0.5근처로 머무는 적합값이 존재 - tanh: 주황색선의 변동폭이 큼 + 0.5근처로 머무는 적합값이 존재X\n- 실험해보니까 tanh가 우수한것 같다. \\(\\to\\) 앞으로는 tanh를 쓰자.\n\n\n\n소프트맥스로 확장\n\n(풀이1) 로지스틱모형에서 3개의 파라메터 버전을 그대로 확장\n\nmapping = {'a':[1,0],'b':[0,1]}\nx = torch.tensor(f(txt_x,mapping)).float().reshape(-1,2)\ny = torch.tensor(f(txt_y,mapping)).float().reshape(-1,2)\nx[:5],y[:5]\n\n(tensor([[1., 0.],\n         [0., 1.],\n         [1., 0.],\n         [0., 1.],\n         [1., 0.]]),\n tensor([[0., 1.],\n         [1., 0.],\n         [0., 1.],\n         [1., 0.],\n         [0., 1.]]))\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=2,out_features=1),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=1,out_features=2,bias=False)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\ny[:5][:,0]\n\ntensor([0., 1., 0., 1., 0.])\n\n\n\nplt.plot(y[:5][:,1],'o')\nplt.plot(soft(net(x[:5]))[:,1].data,'--r')\n\n\n\n\n\nfig,ax = plt.subplots(1,2)\nax[0].imshow(y[:5])\nax[1].imshow(soft(net(x[:5])).data)\n\n<matplotlib.image.AxesImage at 0x7f2633e40f90>"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_10_31_(9주차)_10월31일_ipynb의_사본.html#embedding-layer",
    "href": "posts/Machine Learning/3. RNN/2022_10_31_(9주차)_10월31일_ipynb의_사본.html#embedding-layer",
    "title": "기계학습 (1031) 9주차",
    "section": "Embedding Layer",
    "text": "Embedding Layer\n\nmotive\n- 결국 최종적으로는 아래와 같은 맵핑방식이 확장성이 있어보인다.\n\nmapping = {'a':[1,0,0],'b':[0,1,0],'c':[0,0,1]} # 원핫인코딩 방식 \n\n- 그런데 매번 \\(X\\)를 원핫인코딩하고 Linear 변환하는것이 번거로운데 이를 한번에 구현하는 함수가 있으면 좋겠다. \\(\\to\\) torch.nn.Embedding Layer가 그 역할을 한다.\n\nmapping = {'a':0,'b':1,'c':2}\nx = torch.tensor(f(list('abc')*100,mapping))\ny = torch.tensor(f(list('bca')*100,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 0, 1]), tensor([1, 2, 0, 1, 2]))\n\n\n\ntorch.manual_seed(43052)\nebdd = torch.nn.Embedding(num_embeddings=3,embedding_dim=1) # x-> Xonehot (n,1)->(n,3)\n\n\nebdd(x)[:5]\n\ntensor([[-0.8178],\n        [-0.7052],\n        [-0.5843],\n        [-0.8178],\n        [-0.7052]], grad_fn=<SliceBackward0>)\n\n\n- 그런데 사실 언뜻보면 아래의 linr 함수와 역할의 차이가 없어보인다.\n\ntorch.manual_seed(43052)\nlinr = torch.nn.Linear(in_features=1,out_features=1)\n\n\nlinr(x.float().reshape(-1,1))[:5]\n\ntensor([[-0.8470],\n        [-1.1937],\n        [-1.5404],\n        [-0.8470],\n        [-1.1937]], grad_fn=<SliceBackward0>)\n\n\n- 차이점: 파라메터수에 차이가 있다.\n\nebdd.weight\n\nParameter containing:\ntensor([[-0.8178],\n        [-0.7052],\n        [-0.5843]], requires_grad=True)\n\n\n\nlinr.weight, linr.bias\n\n(Parameter containing:\n tensor([[-0.3467]], requires_grad=True),\n Parameter containing:\n tensor([-0.8470], requires_grad=True))\n\n\n결국 ebdd는 아래의 구조에 해당하는 파라메터들이고\n\n$=\n\\[\\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 1 \\end{bmatrix}\\]\n\n\\[\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}\\]\nnet(x)=\n\\[\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}\\begin{bmatrix} -0.8178 \\\\ -0.7052 \\\\ -0.5843 \\end{bmatrix}\\]\n=\n\\[\\begin{bmatrix} -0.8178 \\\\ -0.7052 \\\\ -0.5843 \\\\ -0.8178 \\\\ -0.7052  \\end{bmatrix}\\]\n$\n\nlinr는 아래의 구조에 해당하는 파라메터이다.\n\n\\(\\text{x[:5]}= \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 1 \\end{bmatrix} \\quad net(x)= \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 1 \\end{bmatrix} \\times (-0.3467) + (-0.8470)=\\begin{bmatrix} -0.8470 \\\\ -1.1937 \\\\ -1.5404 \\\\ -0.8470 \\\\ -1.1937 \\end{bmatrix}\\)\n\n\n\n연습 (ab문제 소프트맥스로 확장한 것 다시 풀이)\n- 맵핑\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 0, 1, 0]), tensor([1, 0, 1, 0, 1]))\n\n\n- torch.nn.Embedding 을 넣은 네트워크\n\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=2,embedding_dim=1),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=1,out_features=2)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n- 학습\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y[:5],'o')\nplt.plot(soft(net(x[:5]))[:,1].data,'--r')\n\n\n\n\n\nplt.imshow(soft(net(x[:5])).data)\n\n<matplotlib.image.AxesImage at 0x7f2633f7f450>"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html",
    "title": "기계학습 (1116) 11주차",
    "section": "",
    "text": "RNN(2)– AbAcAd예제, GPU실험 // LSTM– abcabC, abcdabcD, LSTM의 계산과정, LSTM은 왜 강한가?"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#data",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#data",
    "title": "기계학습 (1116) 11주차",
    "section": "data",
    "text": "data\n- 기존의 정리방식\n\ntxt = list('AbAcAd')*100\ntxt[:10]\n\n['A', 'b', 'A', 'c', 'A', 'd', 'A', 'b', 'A', 'c']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['A', 'b', 'A', 'c', 'A'], ['b', 'A', 'c', 'A', 'd'])\n\n\n\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,{'A':0,'b':1,'c':2,'d':3}))).float()\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,{'A':0,'b':1,'c':2,'d':3}))).float()\n\n\nx,y\n\n(tensor([[1., 0., 0., 0.],\n         [0., 1., 0., 0.],\n         [1., 0., 0., 0.],\n         ...,\n         [1., 0., 0., 0.],\n         [0., 0., 1., 0.],\n         [1., 0., 0., 0.]]), tensor([[0., 1., 0., 0.],\n         [1., 0., 0., 0.],\n         [0., 0., 1., 0.],\n         ...,\n         [0., 0., 1., 0.],\n         [1., 0., 0., 0.],\n         [0., 0., 0., 1.]]))"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#순환신경망-구현1-손으로-직접구현-리뷰",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#순환신경망-구현1-손으로-직접구현-리뷰",
    "title": "기계학습 (1116) 11주차",
    "section": "순환신경망 구현1 (손으로 직접구현) – 리뷰",
    "text": "순환신경망 구현1 (손으로 직접구현) – 리뷰\n(1) 숙성담당 네트워크\n\nclass rNNCell(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.i2h = torch.nn.Linear(4,2) \n        self.h2h = torch.nn.Linear(2,2) \n        self.tanh = torch.nn.Tanh()\n    def forward(self,x,hidden):\n        hidden = self.tanh(self.i2h(x)+self.h2h(hidden))\n        return hidden\n\n\ntorch.manual_seed(43052)\nrnncell = rNNCell() # 숙성담당 네트워크 \n\n(2) 조리담당 네트워크\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4) \n\n(3) 손실함수, 옵티마이저 설계\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(cook.parameters()))\n\n(4) 학습 (6분정도 걸림)\n\nT = len(x) \nfor epoc in range(5000): \n    ## 1~2\n    loss = 0 \n    ht = torch.zeros(1,2) \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = rnncell(xt,ht) \n        ot = cook(ht) \n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화\n\nT = len(x) \nhidden = torch.zeros(T,2) # 599년치 h를 담을 변수 \n_water = torch.zeros(1,2) # 맹물 \nhidden[[0]] = rnncell(x[[0]],_water) \nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]]) \n\n\nyhat = soft(cook(hidden))\nyhat\n\ntensor([[1.6522e-02, 6.2036e-01, 1.0433e-01, 2.5879e-01],\n        [9.9965e-01, 6.5788e-05, 1.8450e-05, 2.6785e-04],\n        [7.6673e-05, 1.9704e-01, 8.0201e-01, 8.7218e-04],\n        ...,\n        [7.4634e-05, 1.9501e-01, 8.0407e-01, 8.4751e-04],\n        [9.4785e-01, 7.4711e-03, 6.1182e-04, 4.4064e-02],\n        [3.6306e-02, 1.2466e-01, 2.8862e-03, 8.3615e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n\nplt.matshow(yhat.data[-15:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f09e935fa50>"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#순환신경망-구현2-with-rnncell-hidden-node-2",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#순환신경망-구현2-with-rnncell-hidden-node-2",
    "title": "기계학습 (1116) 11주차",
    "section": "순환신경망 구현2 (with RNNCell, hidden node 2)",
    "text": "순환신경망 구현2 (with RNNCell, hidden node 2)\nref: https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html\n\n구현1과 같은 초기값 (확인용)\n(1) 숙성네트워크\n\ntorch.manual_seed(43052)\n_rnncell = rNNCell() # 숙성담당 네트워크 \n\n\nrnncell = torch.nn.RNNCell(4,2)   # 4=x , 2=h\n\nrNNCell() 는 사실 torch.nn.RNNCell()와 같은 동작을 하도록 설계를 하였음. 같은동작을 하는지 확인하기 위해서 동일한 초기상태에서 rNNCell()에 의하여 학습된 결과와 torch.nn.RNNCell()에 의하여 학습된 결과를 비교해보자.\n\nrnncell.weight_ih.data = _rnncell.i2h.weight.data\nrnncell.bias_ih.data = _rnncell.i2h.bias.data\nrnncell.weight_hh.data = _rnncell.h2h.weight.data\nrnncell.bias_hh.data = _rnncell.h2h.bias.data\n\n# 초기상태를 똑같이! 앞에서 손으로 직접구현한 것과 일치하는지 확인하기 위해서.\n\n(2) 조리네트워크\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4) # 숙성된 2차원의 단어를 다시 4차원으로 바꿔줘야지 나중에 softmax취할 수 있음\n\n(3) 손실함수와 옵티마이저\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(cook.parameters()))\n\n(4) 학습\n\nT = len(x) \nfor epoc in range(5000):\n    ## 1~2\n    loss = 0 \n    ht = torch.zeros(1,2) \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = rnncell(xt,ht)\n        ot = cook(ht)\n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화\n\nhidden = torch.zeros(T,2) \n\n\n# t=0 \n_water = torch.zeros(1,2)\nhidden[[0]] = rnncell(x[[0]],_water)\n# t=1~T \nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]])\n\n\nyhat = soft(cook(hidden))\nyhat\n\ntensor([[1.6522e-02, 6.2036e-01, 1.0433e-01, 2.5879e-01],\n        [9.9965e-01, 6.5788e-05, 1.8450e-05, 2.6785e-04],\n        [7.6673e-05, 1.9704e-01, 8.0201e-01, 8.7218e-04],\n        ...,\n        [7.4634e-05, 1.9501e-01, 8.0407e-01, 8.4751e-04],\n        [9.4785e-01, 7.4711e-03, 6.1182e-04, 4.4064e-02],\n        [3.6306e-02, 1.2466e-01, 2.8862e-03, 8.3615e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n\nplt.matshow(yhat[:15].data,cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f09e0352f90>\n\n\n\n\n\n\nplt.matshow(yhat[-15:].data,cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f09c75ffd90>\n\n\n\n\n\n\n\n새로운 초기값\n(1) 숙성네트워크\n\ntorch.manual_seed(43052)\ntorch.nn.RNNCell(4,2)\n\nRNNCell(4, 2)\n\n\n(2) 조리네트워크\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4) # 숙성된 2차원의 단어를 다시 4차원으로 바꿔줘야지 나중에 softmax취할 수 있음\n\n(3) 손실함수와 옵티마이저\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(cook.parameters()))\n\n(4) 학습\n\nT = len(x) \nfor epoc in range(5000):\n    ## 1~2\n    loss = 0 \n    ht = torch.zeros(1,2) \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = rnncell(xt,ht)\n        ot = cook(ht)\n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화\nyhat을 저장한 적이 없는데? x -> h -> outfut -> yhat 이렇게 되야하는데.. 히든레이어에 출력이 T시점에만 저장되어 있고 1부터 599에 해당되는 히든레이어가 저장이 안되어있는 네트워크만 저장된 상태 시각화를 위해 hidden레이어를 재정의 해야함\n\nhidden = torch.zeros(T,2)\n#맹물을 만들자\n_water=torch.zeros(1,2)\n#t=0\nhidden[[0]] = rnncell(x[[0]],_water)\n\n#t=1~T\nfor t in range(1,T):\n  hidden[[t]] = rnncell(x[[t]],hidden[[t-1]])\n\n\n\n\nyhat = soft(cook(hidden)) \nplt.matshow(yhat[:15].data,cmap='bwr')"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#순환신경망-구현3-with-rnn-hidden-node-2-성공",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#순환신경망-구현3-with-rnn-hidden-node-2-성공",
    "title": "기계학습 (1116) 11주차",
    "section": "순환신경망 구현3 (with RNN, hidden node 2) – 성공",
    "text": "순환신경망 구현3 (with RNN, hidden node 2) – 성공\n(예비학습)\n- 네트워크학습이후 yhat을 구하려면 번거로웠음\nhidden = torch.zeros(T,2) \n_water = torch.zeros(1,2)\nhidden[[0]] = rnncell(x[[0]],_water)\nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]])\nyhat = soft(cook(hidden))\n- 이렇게 하면 쉽게(?) 구할 수 있음\n\nrnn = torch.nn.RNN(4,2)\n\n\nrnn.weight_hh_l0.data = rnncell.weight_hh.data    # 2x2 매트릭스\nrnn.weight_ih_l0.data = rnncell.weight_ih.data\nrnn.bias_hh_l0.data = rnncell.bias_hh.data\nrnn.bias_ih_l0.data = rnncell.bias_ih.data\n\n- rnn(x,_water)의 결과는 (1) 599년치 간장 (2) 599번째 간장 이다\n\n_water = torch.zeros(1,2)\nrnn(x,_water), hidden \n# 두개의 값이 같다! \n\n# x: tupple.. \n\n((tensor([[-0.9912, -0.9117],\n          [ 0.0698, -1.0000],\n          [-0.9927, -0.9682],\n          ...,\n          [-0.9935, -0.9315],\n          [ 0.5777, -1.0000],\n          [-0.9960, -0.0109]], grad_fn=<SqueezeBackward1>),\n  tensor([[-0.9960, -0.0109]], grad_fn=<SqueezeBackward1>)),\n tensor([[-0.9912, -0.9117],\n         [ 0.0698, -1.0000],\n         [-0.9927, -0.9682],\n         ...,\n         [-0.9935, -0.9315],\n         [ 0.5777, -1.0000],\n         [-0.9960, -0.0109]], grad_fn=<IndexPutBackward0>))\n\n\n\nsoft(cook(rnn(x,_water)[0]))\n\ntensor([[1.9725e-02, 1.5469e-03, 8.2766e-01, 1.5106e-01],\n        [9.1875e-01, 1.6513e-04, 6.7702e-02, 1.3384e-02],\n        [2.0031e-02, 1.0660e-03, 8.5248e-01, 1.2642e-01],\n        ...,\n        [1.9640e-02, 1.3568e-03, 8.3705e-01, 1.4196e-01],\n        [9.9564e-01, 1.3114e-05, 3.5069e-03, 8.4108e-04],\n        [3.5473e-03, 1.5670e-01, 1.4102e-01, 6.9873e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n(예비학습결론) torch.nn.RNN(4,2)는 torch.nn.RNNCell(4,2)의 batch 버전이다. (for문이 포함된 버전이다)\n\ntorch.nn.RNN(4,2)를 이용하여 구현하자.\n(1) 숙성네트워크\n선언\n\nrnn = torch.nn.RNN(4,2)\n\n가중치초기화\n\ntorch.manual_seed(43052)\n_rnncell = torch.nn.RNNCell(4,2)\n\n\nrnn.weight_hh_l0.data = _rnncell.weight_hh.data \nrnn.weight_ih_l0.data = _rnncell.weight_ih.data\nrnn.bias_hh_l0.data = _rnncell.bias_hh.data\nrnn.bias_ih_l0.data = _rnncell.bias_ih.data\n\n(2) 조리네트워크\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4) \n\n(3) 손실함수와 옵티마이저\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(cook.parameters()))\n\n(4) 학습\n\n# 맹물을 넣어주기 위한 세팅\n_water = torch.zeros(1,2) \n\nfor epoc in range(5000):\n    ## 1  hT: 그냥 써논거 \n    hidden,hT = rnn(x,_water)\n    output = cook(hidden) #output이 배치로 나오게 된다.\n    ## 2 \n    loss = loss_fn(output,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()   #갱신\n    optimizr.zero_grad()  #초기화\n\n(5) 시각화1: yhat\n\nyhat = soft(output)\n\n\nplt.matshow(yhat.data[:15],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7fe67c231310>\n\n\n\n\n\n\n처음은 좀 틀렸음 ㅎㅎ\n\n\nplt.matshow(yhat.data[-15:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7fe67c1c5d90>\n\n\n\n\n\n\n뒤에는 잘맞음\n\n실전팁: _water 대신에 hT를 대입 (사실 큰 차이는 없음)\n\n# hT에 값이 있음\n# rnn(x,hT)[0][:6] 값과 hidden[:6] 값을 비교해보면 비슷함 \n\n\nrnn(x[:6],_water),rnn(x[:6],hT)\n\n((tensor([[-0.9912, -0.9117],\n          [ 0.0698, -1.0000],\n          [-0.9927, -0.9682],\n          [ 0.5761, -1.0000],\n          [-0.9960, -0.0173],\n          [ 0.9960, -1.0000]], grad_fn=<SqueezeBackward1>),\n  tensor([[ 0.9960, -1.0000]], grad_fn=<SqueezeBackward1>)),\n (tensor([[-0.9713, -1.0000],\n          [ 0.0535, -1.0000],\n          [-0.9925, -0.9720],\n          [ 0.5759, -1.0000],\n          [-0.9960, -0.0180],\n          [ 0.9960, -1.0000]], grad_fn=<SqueezeBackward1>),\n  tensor([[ 0.9960, -1.0000]], grad_fn=<SqueezeBackward1>)))\n\n\n(6) 시각화2: hidden, yhat\n\ncombinded = torch.concat([hidden,yhat],axis=1)\n\n\nplt.matshow(combinded[-15:].data,cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7fe67c13b7d0>\n\n\n\n\n\n\n히든노드의 해석이 어려움."
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#순환신경망-구현4-with-rnn-hidden-node-3-성공",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#순환신경망-구현4-with-rnn-hidden-node-3-성공",
    "title": "기계학습 (1116) 11주차",
    "section": "순환신경망 구현4 (with RNN, hidden node 3) – 성공",
    "text": "순환신경망 구현4 (with RNN, hidden node 3) – 성공\n(1) 숙성네트워크~ (2) 조리네트워크\n\ntorch.manual_seed(2) #1 \nrnn = torch.nn.RNN(4,3) \ncook = torch.nn.Linear(3,4) \n\n(3) 손실함수와 옵티마이저\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(cook.parameters()))\n\n(4) 학습\n\n_water = torch.zeros(1,3) \nfor epoc in range(5000):\n    ## 1\n    hidden,hT = rnn(x,_water) \n    output = cook(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화1: yhat\n\nyhat = soft(output)\n\n\nplt.matshow(yhat[-15:].data,cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7fe67c04f550>\n\n\n\n\n\n(6) 시각화2: hidden, yhat\n\ncombinded = torch.concat([hidden,yhat],axis=1)\n\n\nplt.matshow(combinded[-15:].data,cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7fe6747ba910>\n\n\n\n\n\n\n세번째 히든노드 = 대소문자를 구분\n1,2 히든노드 = bcd를 구분"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#len-20-hidden-nodes",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#len-20-hidden-nodes",
    "title": "기계학습 (1116) 11주차",
    "section": "20000 len + 20 hidden nodes",
    "text": "20000 len + 20 hidden nodes\ncpu\n\nx = torch.randn([20000,4]) \ny = torch.randn([20000,4]) \n\n\nrnn = torch.nn.RNN(4,20) \nlinr = torch.nn.Linear(20,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward()   # 역전파를 하는 곳이 시간을 제일 많이 잡아먹는다. \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n93.01761960983276\n\n\ngpu\n\nx = torch.randn([20000,4]).to(\"cuda:0\")\ny = torch.randn([20000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,20).to(\"cuda:0\")\nlinr = torch.nn.Linear(20,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n3.2665085792541504\n\n\n\n왜 빠른지?\n\n\n# for문이 중첩이 되어있는 형태인데, hidden에서 cpu에서는 그래서 오래걸리는 거구... 근데 이것보다는 역전파 하는곳 때문에 더 오래걸린다!"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#len-20-hidden-nodes-역전파주석처리",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#len-20-hidden-nodes-역전파주석처리",
    "title": "기계학습 (1116) 11주차",
    "section": "20000 len + 20 hidden nodes + 역전파주석처리",
    "text": "20000 len + 20 hidden nodes + 역전파주석처리\ncpu\n\nx = torch.randn([20000,4]) \ny = torch.randn([20000,4]) \n\n\nrnn = torch.nn.RNN(4,20) \nlinr = torch.nn.Linear(20,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n18.851768255233765\n\n\ngpu (역전파주석처리)\n\nx = torch.randn([20000,4]).to(\"cuda:0\")\ny = torch.randn([20000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,20).to(\"cuda:0\")\nlinr = torch.nn.Linear(20,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n1.2901742458343506"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#len-20-hidden-nodes-1",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#len-20-hidden-nodes-1",
    "title": "기계학습 (1116) 11주차",
    "section": "2000 len + 20 hidden nodes",
    "text": "2000 len + 20 hidden nodes\ncpu\n\nx = torch.randn([2000,4]) \ny = torch.randn([2000,4]) \n\n\nrnn = torch.nn.RNN(4,20) \nlinr = torch.nn.Linear(20,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n6.533619165420532\n\n\ngpu\n\nx = torch.randn([2000,4]).to(\"cuda:0\")\ny = torch.randn([2000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,20).to(\"cuda:0\")\nlinr = torch.nn.Linear(20,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n0.7532594203948975"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#len-20-hidden-nodes-역전파주석처리-1",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#len-20-hidden-nodes-역전파주석처리-1",
    "title": "기계학습 (1116) 11주차",
    "section": "2000 len + 20 hidden nodes + 역전파주석처리",
    "text": "2000 len + 20 hidden nodes + 역전파주석처리\ncpu\n\nx = torch.randn([2000,4]) \ny = torch.randn([2000,4]) \n\n\nrnn = torch.nn.RNN(4,20) \nlinr = torch.nn.Linear(20,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n1.2477965354919434\n\n\ngpu\n\nx = torch.randn([2000,4]).to(\"cuda:0\")\ny = torch.randn([2000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,20).to(\"cuda:0\")\nlinr = torch.nn.Linear(20,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n0.14130854606628418"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#len-5000-hidden-nodes",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#len-5000-hidden-nodes",
    "title": "기계학습 (1116) 11주차",
    "section": "2000 len + 5000 hidden nodes",
    "text": "2000 len + 5000 hidden nodes\ncpu\n\nx = torch.randn([2000,4]) \ny = torch.randn([2000,4]) \n\n\nrnn = torch.nn.RNN(4,1000) \nlinr = torch.nn.Linear(1000,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,1000)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n58.99820685386658\n\n\ngpu\n\nx = torch.randn([2000,4]).to(\"cuda:0\")\ny = torch.randn([2000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,1000).to(\"cuda:0\")\nlinr = torch.nn.Linear(1000,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,1000).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n4.7596595287323"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#len-5000-hidden-nodes-역전파주석처리",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#len-5000-hidden-nodes-역전파주석처리",
    "title": "기계학습 (1116) 11주차",
    "section": "2000 len + 5000 hidden nodes + 역전파주석처리",
    "text": "2000 len + 5000 hidden nodes + 역전파주석처리\ncpu\n\nx = torch.randn([2000,4]) \ny = torch.randn([2000,4]) \n\n\nrnn = torch.nn.RNN(4,1000) \nlinr = torch.nn.Linear(1000,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,1000)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n13.163657188415527\n\n\ngpu\n\nx = torch.randn([2000,4]).to(\"cuda:0\")\ny = torch.randn([2000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,1000).to(\"cuda:0\")\nlinr = torch.nn.Linear(1000,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,1000).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n2.2989864349365234"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#실험결과-요약",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#실험결과-요약",
    "title": "기계학습 (1116) 11주차",
    "section": "실험결과 요약",
    "text": "실험결과 요약\n\n\n\nlen\n# of hidden nodes\nbackward\ncpu\ngpu\nratio\n\n\n\n\n20000\n20\nO\n93.02\n3.26\n28.53\n\n\n20000\n20\nX\n18.85\n1.29\n14.61\n\n\n2000\n20\nO\n6.53\n0.75\n8.70\n\n\n2000\n20\nX\n1.25\n0.14\n8.93\n\n\n2000\n1000\nO\n58.99\n4.75\n12.41\n\n\n2000\n1000\nX\n13.16\n2.29\n5.74"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#data-1",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#data-1",
    "title": "기계학습 (1116) 11주차",
    "section": "data",
    "text": "data\n\ntxt = list('abcabC')*100\ntxt[:8]\n\n['a', 'b', 'c', 'a', 'b', 'C', 'a', 'b']\n\n\n\ntxt_x = txt[:-1] \ntxt_y = txt[1:]\n\n\nmapping = {'a':0,'b':1,'c':2,'C':3} \nx= torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny= torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()\n\n\nx = x.to(\"cuda:0\")\ny = y.to(\"cuda:0\") \n\n\nx.shape\n\ntorch.Size([599, 4])\n\n\na1, b1, c, a2, b2, C - 보이는 문자수가 a,b,c,C 이므로 4개 - 문맥까지 고려하면 6개(a1, a2와 같이,.)"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#rnn",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#rnn",
    "title": "기계학습 (1116) 11주차",
    "section": "RNN",
    "text": "RNN\n\ntorch.manual_seed(43052) \nrnn = torch.nn.RNN(4,3)    # 문맥의 차이 고려가 힘드니까 히든레이어를 3개 정도는 있어야 문맥에 따른걸 생각할 수 있을 거 같다!  \nlinr = torch.nn.Linear(3,4) \nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnn.parameters())+ list(linr.parameters()))\n\n\nrnn.to(\"cuda:0\") \nlinr.to(\"cuda:0\")\n\nLinear(in_features=3, out_features=4, bias=True)\n\n\n- 3000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f47e032f890>\n\n\n\n\n\n- 6000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f47e1078b90>\n\n\n\n\n\n- 9000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f47e0358590>\n\n\n\n\n\n- 12000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f47e2de6f10>\n\n\n\n\n\n- 15000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f47cc12ae50>"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#lstm",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#lstm",
    "title": "기계학습 (1116) 11주차",
    "section": "LSTM",
    "text": "LSTM\n- LSTM\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(4,3) #RNN->lstm\nlinr = torch.nn.Linear(3,4) \nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstm.parameters())+ list(linr.parameters()))\n\n\nlstm.to(\"cuda:0\") \nlinr.to(\"cuda:0\")\n\nLinear(in_features=3, out_features=4, bias=True)\n\n\n- 3000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, (hT,cT) = lstm(x,(_water,_water))   # lstm은 물을 두개 넣어줘야 하고 hT랑 cT랑이 나온다..\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f47cc0608d0>\n\n\n\n\n\n- 6000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, (hT,cT) = lstm(x,(_water,_water))\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f47c61dd750>"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#rnn-vs-lstm-성능비교실험",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#rnn-vs-lstm-성능비교실험",
    "title": "기계학습 (1116) 11주차",
    "section": "RNN vs LSTM 성능비교실험",
    "text": "RNN vs LSTM 성능비교실험\n- RNN\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        rnn = torch.nn.RNN(4,3).to(\"cuda:0\")\n        linr = torch.nn.Linear(3,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,3).to(\"cuda:0\")\n        for epoc in range(3000):\n            ## 1\n            hidden, hT = rnn(x,_water)\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(r\"$RNN$\",size=20)\nfig.tight_layout()\n\n\n\n\n- LSTM\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        lstm = torch.nn.LSTM(4,3).to(\"cuda:0\")\n        linr = torch.nn.Linear(3,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,3).to(\"cuda:0\")\n        for epoc in range(3000):\n            ## 1\n            hidden, (hT,cT) = lstm(x,(_water,_water))\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(r\"$LSTM$\",size=20)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#data-2",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#data-2",
    "title": "기계학습 (1116) 11주차",
    "section": "data",
    "text": "data\n\ntxt = list('abcdabcD')*100\ntxt[:8]\n\n['a', 'b', 'c', 'd', 'a', 'b', 'c', 'D']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\nmapping = {'a':0, 'b':1, 'c':2, 'd':3, 'D':4}\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()\n\n\nx=x.to(\"cuda:0\")\ny=y.to(\"cuda:0\")"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#rnn-vs-lstm-성능비교실험-1",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#rnn-vs-lstm-성능비교실험-1",
    "title": "기계학습 (1116) 11주차",
    "section": "RNN vs LSTM 성능비교실험",
    "text": "RNN vs LSTM 성능비교실험\n- RNN\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        rnn = torch.nn.RNN(5,4).to(\"cuda:0\")\n        linr = torch.nn.Linear(4,5).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,4).to(\"cuda:0\")\n        for epoc in range(3000):\n            ## 1\n            hidden, hT = rnn(x,_water)\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-8:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(r\"$RNN$\",size=20)\nfig.tight_layout()\n\n\n\n\n- LSTM\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        lstm = torch.nn.LSTM(5,4).to(\"cuda:0\")\n        linr = torch.nn.Linear(4,5).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,4).to(\"cuda:0\")\n        for epoc in range(3000):\n            ## 1\n            hidden, (hT,cT) = lstm(x,(_water,_water))\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-8:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(r\"$LSTM$\",size=20)\nfig.tight_layout()\n\n\n\n\n- 관찰1: LSTM이 확실히 장기기억에 강하다.\n- 관찰2: LSTM은 hidden에 0이 잘 나온다.\n\n사실 확실히 구분되는 특징을 판별할때는 -1,1 로 히든레이어 값들이 설정되면 명확하다.\n히든레이어에 -1~1사이의 값이 나온다면 애매한 판단이 내려지게 된다.\n그런데 이 애매한 판단이 어떻게 보면 문맥의 뉘앙스를 이해하는데 더 잘 맞다.\n그런데 RNN은 -1,1로 셋팅된 상황에서 -1~1로의 변화가 더디다는 것이 문제임.\n\n\n# 히든레이어는 하얀색.."
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#data-abab",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#data-abab",
    "title": "기계학습 (1116) 11주차",
    "section": "data: abaB",
    "text": "data: abaB\n\ntxt = list('abaB')*100\ntxt[:5]\n\n['a', 'b', 'a', 'B', 'a']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\nmapping = {'a':0, 'b':1, 'B':2}\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#epoch-ver1-with-torch.nn.lstmcell",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#epoch-ver1-with-torch.nn.lstmcell",
    "title": "기계학습 (1116) 11주차",
    "section": "1 epoch ver1 (with torch.nn.LSTMCell)",
    "text": "1 epoch ver1 (with torch.nn.LSTMCell)\n\ntorch.manual_seed(43052) \nlstm_cell = torch.nn.LSTMCell(3,2) #LSTM말고 LSTMCell (LSTM을 batch버전으로..)  # 단어수가 3개니까 3!!! abB, 근데 문맥상 a1,a2,b,B 4개의 문자가 있으니까 히든노드를 2개정도로 잡자.\nlinr = torch.nn.Linear(2,3)\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm_cell.parameters())+list(linr.parameters()),lr=0.1)   #lr=학습률\n\n\nT = len(x) \nfor epoc in range(1):\n    ht = torch.zeros(1,2)  # 히든노드가 2개니까 차원이 2인 맹물을 만들어주자.\n    ct = torch.zeros(1,2)\n    loss = 0 \n    ## 1~2\n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht,ct = lstm_cell(xt,(ht,ct))\n        ot = linr(ht) \n        loss = loss + loss_fn(ot,yt)\n    loss = loss / T\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nht,ct \n\n(tensor([[-0.0406,  0.2505]], grad_fn=<MulBackward0>),\n tensor([[-0.0975,  0.7134]], grad_fn=<AddBackward0>))"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#epoch-ver2-완전-손으로-구현",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#epoch-ver2-완전-손으로-구현",
    "title": "기계학습 (1116) 11주차",
    "section": "1 epoch ver2 (완전 손으로 구현)",
    "text": "1 epoch ver2 (완전 손으로 구현)\n\nt=0 \\(\\to\\) t=1\n- lstm_cell 을 이용한 계산 (결과비교용)\n\ntorch.manual_seed(43052) \nlstm_cell = torch.nn.LSTMCell(3,2) \nlinr = torch.nn.Linear(2,3)\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm_cell.parameters())+list(linr.parameters()),lr=0.1)\n\n\nT = len(x) \nfor epoc in range(1):\n    ht = torch.zeros(1,2)\n    ct = torch.zeros(1,2)\n    loss = 0 \n    ## 1~2\n    for t in range(1):\n        xt,yt = x[[t]], y[[t]]\n        ht,ct = lstm_cell(xt,(ht,ct))\n    #     ot = linr(ht) \n    #     loss = loss + loss_fn(ot,yt)\n    # loss = loss / T\n    # ## 3 \n    # loss.backward()\n    # ## 4 \n    # optimizr.step()\n    # optimizr.zero_grad()\n\n\nht,ct \n\n(tensor([[-0.0541,  0.0892]], grad_fn=<MulBackward0>),\n tensor([[-0.1347,  0.2339]], grad_fn=<AddBackward0>))\n\n\n\n이런결과를 어떻게 만드는걸까?\nhttps://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n\n- 직접계산\n\nht = torch.zeros(1,2)\nct = torch.zeros(1,2)\n\n\n_ifgo = xt @ lstm_cell.weight_ih.T + ht @ lstm_cell.weight_hh.T + lstm_cell.bias_ih + lstm_cell.bias_hh\n\n\ninput_gate = sig(_ifgo[:,0:2])\nforget_gate = sig(_ifgo[:,2:4])\ngt = tanh(_ifgo[:,4:6])\noutput_gate = sig(_ifgo[:,6:8])\n\n\nct = forget_gate * ct + input_gate * gt\nht = output_gate * tanh(ct)\n\n\nht,ct\n\n(tensor([[-0.0541,  0.0892]], grad_fn=<MulBackward0>),\n tensor([[-0.1347,  0.2339]], grad_fn=<AddBackward0>))\n\n\n\n\nt=0 \\(\\to\\) t=T\n\ntorch.manual_seed(43052) \nlstm_cell = torch.nn.LSTMCell(3,2) \nlinr = torch.nn.Linear(2,3)\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm_cell.parameters())+list(linr.parameters()),lr=0.1)\n\n\nT = len(x) \nfor epoc in range(1):\n    ht = torch.zeros(1,2)\n    ct = torch.zeros(1,2)\n    loss = 0 \n    ## 1~2\n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        \n        ## lstm_cell step1: calculate _ifgo \n        _ifgo = xt @ lstm_cell.weight_ih.T + ht @ lstm_cell.weight_hh.T + lstm_cell.bias_ih + lstm_cell.bias_hh\n        ## lstm_cell step2: decompose _ifgo \n        input_gate = sig(_ifgo[:,0:2])\n        forget_gate = sig(_ifgo[:,2:4])\n        gt = tanh(_ifgo[:,4:6])\n        output_gate = sig(_ifgo[:,6:8])\n        ## lstm_cell step3: calculate ht,ct \n        ct = forget_gate * ct + input_gate * gt\n        ht = output_gate * tanh(ct)\n        \n    #     ot = linr(ht) \n    #     loss = loss + loss_fn(ot,yt)\n    # loss = loss / T\n    # ## 3 \n    # loss.backward()\n    # ## 4 \n    # optimizr.step()\n    # optimizr.zero_grad()\n\n\nht,ct\n\n#LSMT_Cell로 쉽게 계산한것이랑 값이 같다.\n\n(tensor([[-0.0406,  0.2505]], grad_fn=<MulBackward0>),\n tensor([[-0.0975,  0.7134]], grad_fn=<AddBackward0>))"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#epoch-ver3-with-torch.nn.lstm",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#epoch-ver3-with-torch.nn.lstm",
    "title": "기계학습 (1116) 11주차",
    "section": "1 epoch ver3 (with torch.nn.LSTM)",
    "text": "1 epoch ver3 (with torch.nn.LSTM)\n\ntorch.manual_seed(43052) \nlstm_cell = torch.nn.LSTMCell(3,2)\nlinr = torch.nn.Linear(2,3) \n\n\nlstm = torch.nn.LSTM(3,2) \n\n\n# batch버전 통해서 확인해보기, 가중치값 덮어씌워보기\nlstm.weight_hh_l0.data = lstm_cell.weight_hh.data \nlstm.bias_hh_l0.data = lstm_cell.bias_hh.data \nlstm.weight_ih_l0.data = lstm_cell.weight_ih.data \nlstm.bias_ih_l0.data = lstm_cell.bias_ih.data \n\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstm.parameters()) + list(linr.parameters()), lr=0.1) \n\n\n_water = torch.zeros(1,2) \nfor epoc in range(1): \n    ## step1 \n    hidden, (ht,ct) = lstm(x,(_water,_water))\n    output = linr(hidden)\n    # ## step2\n    # loss = loss_fn(output,y) \n    # ## step3\n    # loss.backward()\n    # ## step4 \n    # optimizr.step()\n    # optimizr.zero_grad() \n\n\nht,ct\n\n(tensor([[-0.0406,  0.2505]], grad_fn=<SqueezeBackward1>),\n tensor([[-0.0975,  0.7134]], grad_fn=<SqueezeBackward1>))"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#data-abab-1",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#data-abab-1",
    "title": "기계학습 (1116) 11주차",
    "section": "data: abaB",
    "text": "data: abaB\n\ntxt = list('abaB')*100\ntxt[:5]\n\n['a', 'b', 'a', 'B', 'a']\n\n\n\nn_words = 3\n\n\nmapping = {'a':0, 'b':1, 'B':2}\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:10],txt_y[:10]\n\n(['a', 'b', 'a', 'B', 'a', 'b', 'a', 'B', 'a', 'b'],\n ['b', 'a', 'B', 'a', 'b', 'a', 'B', 'a', 'b', 'a'])\n\n\n\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()\n\n\nx,y\n\n(tensor([[1., 0., 0.],\n         [0., 1., 0.],\n         [1., 0., 0.],\n         ...,\n         [1., 0., 0.],\n         [0., 1., 0.],\n         [1., 0., 0.]]),\n tensor([[0., 1., 0.],\n         [1., 0., 0.],\n         [0., 0., 1.],\n         ...,\n         [0., 1., 0.],\n         [1., 0., 0.],\n         [0., 0., 1.]]))"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#epoch",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#epoch",
    "title": "기계학습 (1116) 11주차",
    "section": "1000 epoch",
    "text": "1000 epoch\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(3,2) \nlinr = torch.nn.Linear(2,3) \n\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm.parameters())+ list(linr.parameters()),lr=0.1)\n\n\n_water = torch.zeros(1,2) \nfor epoc in range(1000): \n    ## step1 \n    hidden, (ht,ct) = lstm(x,(_water,_water))\n    output = linr(hidden)\n    ## step2\n    loss = loss_fn(output,y) \n    ## step3\n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#시각화",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#시각화",
    "title": "기계학습 (1116) 11주차",
    "section": "시각화",
    "text": "시각화\n\nT = len(x)\ninput_gate = torch.zeros(T,2)\nforget_gate = torch.zeros(T,2)\noutput_gate = torch.zeros(T,2)\ng = torch.zeros(T,2)\ncell = torch.zeros(T,2)\nh = torch.zeros(T,2)  # 히든노드를 2개로 잡아놨으니까.. \n\n# 계산식에 의해서 위에 값들이 다 (T,2)형태여야 한다.\n\n\n# LSTM 계산과정을 다시 따라가면,\n\nfor t in range(T): \n    ## 1: calculate _ifgo \n    _ifgo = x[[t]] @ lstm.weight_ih_l0.T + h[[t]] @ lstm.weight_hh_l0.T + lstm.bias_ih_l0 + lstm.bias_hh_l0 \n    ## 2: decompose _ifgo \n    input_gate[[t]] = sig(_ifgo[:,0:2])\n    forget_gate[[t]] = sig(_ifgo[:,2:4])\n    g[[t]] = tanh(_ifgo[:,4:6])\n    output_gate[[t]] = sig(_ifgo[:,6:8])\n    ## 3: calculate ht,ct \n    cell[[t]] = forget_gate[[t]] * cell[[t]] + input_gate[[t]] * g[[t]]\n    h[[t]] = output_gate[[t]] * tanh(cell[[t]])\n\n\ncombinded1 = torch.concat([input_gate,forget_gate,output_gate],axis=1)  # gate끼리 묶어서 시각화\ncombinded2 = torch.concat([g,cell,h,soft(output)],axis=1)               # 나머지 묶어서 시각화\n\n\nplt.matshow(combinded1[-8:].data,cmap='bwr',vmin=-1,vmax=1);\nplt.xticks(range(combinded1.shape[-1]),labels=['i']*2 + ['f']*2 + ['o']*2);\nplt.matshow(combinded2[-8:].data,cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(combinded2.shape[-1]),labels=['g']*2 + ['c']*2 + ['h']*2 + ['yhat']*3);\n\n\n\n\n\n\n\n\n상단그림은 게이트의 값들만 시각화, 하단그림은 게이트 이외의 값들을 시각화"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#시각화의-해석i",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#시각화의-해석i",
    "title": "기계학습 (1116) 11주차",
    "section": "시각화의 해석I",
    "text": "시각화의 해석I\n\nplt.matshow(combinded1[-8:].data,cmap='bwr',vmin=-1,vmax=1);\nplt.xticks(range(combinded1.shape[-1]),labels=['i']*2 + ['f']*2 + ['o']*2);\n\nNameError: ignored\n\n\n- input_gate, forget_gate, output_gate는 모두 0~1 사이의 값을 가진다.\n파 -1 흰 0 빨 1 이니까 .. xt, ht-1 가지고 sig취해서 i,f,o를 만들었으니 0~1사이의 값을 가진다.\n- 이 값들은 각각 모두 \\({\\boldsymbol g}_t, {\\boldsymbol c}_{t-1}, \\tanh({\\boldsymbol c}_t)\\)에 곱해진다. 따라서 input_gate, forget_gate, output_gate 는 gate의 역할로 비유가능하다. (1이면 통과, 0이면 차단)\n\ninput_gate: \\({\\boldsymbol g}_t\\)의 값을 얼만큼 통과시킬지 0~1사이의 숫자로 결정\nforget_gate: \\({\\boldsymbol c}_{t-1}\\)의 값을 얼만큼 통과시킬지 0~1사이의 숫자로 결정\noutput_gate: \\(\\tanh({\\boldsymbol c}_t)\\)의 값을 얼만큼 통과시킬지 0~1사이의 숫자로 결정"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#시각화의-해석ii",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#시각화의-해석ii",
    "title": "기계학습 (1116) 11주차",
    "section": "시각화의 해석II",
    "text": "시각화의 해석II\n\nplt.matshow(combinded2[-8:].data,cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(combinded2.shape[-1]),labels=['g']*2 + ['c']*2 + ['h']*2 + ['yhat']*3);\n\n\n\n\n- 결국 \\({\\boldsymbol g}_t\\to {\\boldsymbol c}_t \\to {\\boldsymbol h}_t \\to \\hat{\\boldsymbol y}\\) 의 느낌이다. (\\({\\boldsymbol h}_t\\)를 계산하기 위해서는 \\({\\boldsymbol c}_t\\)가 필요했고 \\({\\boldsymbol c}_t\\)를 계산하기 위해서는 \\({\\boldsymbol c}_{t-1}\\)과 \\({\\boldsymbol g}_t\\)가 필요했음)\n\n\\({\\boldsymbol h}_t= \\tanh({\\boldsymbol c}_t) \\odot {\\boldsymbol o}_t\\)\n\\({\\boldsymbol c}_t ={\\boldsymbol c}_{t-1} \\odot {\\boldsymbol f}_t + {\\boldsymbol g}_{t} \\odot {\\boldsymbol i}_t\\)\n\n- \\({\\boldsymbol g}_t,{\\boldsymbol c}_t,{\\boldsymbol h}_t\\) 모두 \\({\\boldsymbol x}_t\\)의 정보를 숙성시켜 가지고 있는 느낌이 든다.\n- \\({\\boldsymbol g}_t\\) 특징: 보통 -1,1 중 하나의 값을 가지도록 학습되어 있다. (마치 RNN의 hidden node처럼!)\n\n\\(\\boldsymbol{g}_t = \\tanh({\\boldsymbol x}_t {\\bf W}_{ig} + {\\boldsymbol h}_{t-1} {\\bf W}_{hg}+ {\\boldsymbol b}_{ig}+{\\boldsymbol b}_{hg})\\)\n\n- \\({\\boldsymbol c}_t\\) 특징: \\({\\boldsymbol g}_t\\)와 매우 비슷하지만 약간 다른값을 가진다. 그래서 \\({\\boldsymbol g}_t\\)와는 달리 -1,1 이외의 값도 종종 등장.\n\nprint(\"first row: gt={}, ct={}\".format(g[-8].data, cell[-8].data))\nprint(\"second row: gt={}, ct={}\".format(g[-7].data, cell[-7].data))\n#g[-7], cell[-7]\n\nfirst row: gt=tensor([ 0.9999, -0.9999]), ct=tensor([ 0.9647, -0.9984])\nsecond row: gt=tensor([ 0.9970, -0.9554]), ct=tensor([ 0.3592, -0.9373])\n\n\n- \\({\\boldsymbol h}_t\\) 특징: (1) \\({\\boldsymbol c}_t\\)의 느낌이 있음 하지만 약간의 변형이 있음. (2) -1~1 사이에의 값을 훨씬 다양하게 가진다. (tanh때문)\n\nprint(\"first row: gt={}, ct={}, ht={}\".format(g[-8].data, cell[-8].data,h[-8].data))\nprint(\"second row: gt={}, ct={}, ht={}\".format(g[-7].data, cell[-7].data,h[-7].data))\n#g[-7], cell[-7]\n\nfirst row: gt=tensor([ 0.9999, -0.9999]), ct=tensor([ 0.9647, -0.9984]), ht=tensor([ 0.7370, -0.3323])\nsecond row: gt=tensor([ 0.9970, -0.9554]), ct=tensor([ 0.3592, -0.9373]), ht=tensor([ 0.0604, -0.6951])\n\n\n- 예전의문 해결\n\n실험적으로 살펴보니 LSTM이 RNN보다 장기기억에 유리했음.\n그 이유: RRN은 \\({\\boldsymbol h}_t\\)의 값이 -1 혹은 1로 결정되는 경우가 많았음. 그러나 경우에 따라서는 \\({\\boldsymbol h}_t\\)이 -1~1의 값을 가지는 것이 문맥적 뉘앙스를 포착하기에는 유리한데 LSTM이 이러한 방식으로 학습되는 경우가 많았음.\n왜 LSTM의 \\({\\boldsymbol h}_t\\)은 -1,1 이외의 값을 쉽게 가질 수 있는가? (1) gate들의 역할 (2) 마지막에 취해지는 tanh 때문"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#lstm의-알고리즘-리뷰-i-수식위주",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#lstm의-알고리즘-리뷰-i-수식위주",
    "title": "기계학습 (1116) 11주차",
    "section": "LSTM의 알고리즘 리뷰 I (수식위주)",
    "text": "LSTM의 알고리즘 리뷰 I (수식위주)\n(step1) calculate \\({\\tt ifgo}\\)\n\\({\\tt ifgo} = {\\boldsymbol x}_t \\big[{\\bf W}_{ii} | {\\bf W}_{if}| {\\bf W}_{ig} |{\\bf W}_{io}\\big] + {\\boldsymbol h}_{t-1} \\big[ {\\bf W}_{hi}|{\\bf W}_{hf} |{\\bf W}_{hg} | {\\bf W}_{ho} \\big] + bias\\)\n\\(=\\big[{\\boldsymbol x}_t{\\bf W}_{ii} + {\\boldsymbol h}_{t-1}{\\bf W}_{hi} ~\\big|~ {\\boldsymbol x}_t{\\bf W}_{if}+ {\\boldsymbol h}_{t-1}{\\bf W}_{hf}~ \\big|~ {\\boldsymbol x}_t{\\bf W}_{ig} + {\\boldsymbol h}_{t-1}{\\bf W}_{hg} ~\\big|~ {\\boldsymbol x}_t{\\bf W}_{io} + {\\boldsymbol h}_{t-1}{\\bf W}_{ho} \\big] + bias\\)\n참고: 위의 수식은 아래코드에 해당하는 부분\nifgo = xt @ lstm_cell.weight_ih.T +\\\n       ht @ lstm_cell.weight_hh.T +\\\n       lstm_cell.bias_ih + lstm_cell.bias_hh\n(step2) decompose \\({\\tt ifgo}\\) and get \\({\\boldsymbol i}_t\\), \\({\\boldsymbol f}_t\\), \\({\\boldsymbol g}_t\\), \\({\\boldsymbol o}_t\\)\n\\({\\boldsymbol i}_t = \\sigma({\\boldsymbol x}_t {\\bf W}_{ii} + {\\boldsymbol h}_{t-1} {\\bf W}_{hi} +bias )\\)\n\\({\\boldsymbol f}_t = \\sigma({\\boldsymbol x}_t {\\bf W}_{if} + {\\boldsymbol h}_{t-1} {\\bf W}_{hf} +bias )\\)\n\\({\\boldsymbol g}_t = \\tanh({\\boldsymbol x}_t {\\bf W}_{ig} + {\\boldsymbol h}_{t-1} {\\bf W}_{hg} +bias )\\)\n\\({\\boldsymbol o}_t = \\sigma({\\boldsymbol x}_t {\\bf W}_{io} + {\\boldsymbol h}_{t-1} {\\bf W}_{ho} +bias )\\)\n(step3) calculate \\({\\boldsymbol c}_t\\) and \\({\\boldsymbol h}_t\\)\n\\({\\boldsymbol c}_t = {\\boldsymbol i}_t \\odot {\\boldsymbol g}_t+ {\\boldsymbol f}_t \\odot {\\boldsymbol c}_{t-1}\\)\n\\({\\boldsymbol h}_t = \\tanh({\\boldsymbol o}_t \\odot {\\boldsymbol c}_t)\\)"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#lstm의-알고리즘-리뷰-ii-느낌위주",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#lstm의-알고리즘-리뷰-ii-느낌위주",
    "title": "기계학습 (1116) 11주차",
    "section": "LSTM의 알고리즘 리뷰 II (느낌위주)",
    "text": "LSTM의 알고리즘 리뷰 II (느낌위주)\n\n이해 및 암기를 돕기위해서 비유적으로 설명한 챕터입니다..\n\n- 느낌1: RNN이 콩물에서 간장을 한번에 숙성시키는 방법이라면 LSTM은 콩물에서 간장을 3차로 나누어 숙성하는 느낌이다.\n\n콩물: \\({\\boldsymbol x}_t\\)\n1차숙성: \\({\\boldsymbol g}_t\\)\n2차숙성: \\({\\boldsymbol c}_t\\)\n3차숙성: \\({\\boldsymbol h}_t\\)\n\n- 느낌2: \\({\\boldsymbol g}_t\\)에 대하여\n\n계산방법: \\({\\boldsymbol x}_t\\)와 \\({\\boldsymbol h}_{t-1}\\)를 \\({\\bf W}_{ig}, {\\bf W}_{hg}\\)를 이용해 선형결합하고 \\(\\tanh\\)를 취한 결과\nRNN에서 간장을 만들던 그 수식에서 \\(h_t\\)를 \\(g_t\\)로 바꾼것\n크게 2가지의 의미를 가진다 (1) 과거와 현재의 결합 (2) 활성화함수 \\(\\tanh\\)를 적용\n\n- 느낌3: \\({\\boldsymbol c}_t\\)에 대하여 (1)\n\n계산방법: \\({\\boldsymbol g}_{t}\\)와 \\({\\boldsymbol c}_{t-1}\\)를 요소별로 선택하고 더하는 과정\n\\(g_t\\)는 (1) 과거와 현재의 결합 (2) 활성화함수 tanh를 적용으로 나누어지는데 이중에서 (1) 과거와 현재의 정보를 결합하는 과정만 해당한다. 차이점은 요소별 선택 후 덧셈\n이러한 결합을 쓰는 이유? 게이트를 이용하여 과거와 현재의 정보를 제어 (일반적인 설명, 솔직히 내가 좋아하는 설명은 아님)\n\n- 느낌4: \\({\\boldsymbol c}_t\\)에 대하여 (2) // \\({\\boldsymbol c}_t\\)는 왜 과거와 현재의 정보를 제어한다고 볼 수 있는가?\n\\(t=1\\) 시점 계산과정관찰\n\ninput_gate[1],g[1],forget_gate[1],cell[0]    # g[1]:현재시점 cell[0]:과거시점\n\n(tensor([0.9065, 0.9999], grad_fn=<SelectBackward0>),\n tensor([0.9931, 0.9999], grad_fn=<SelectBackward0>),\n tensor([0.9931, 0.0014], grad_fn=<SelectBackward0>),\n tensor([ 0.3592, -0.9373], grad_fn=<SelectBackward0>))\n\n\n\\([0.9,1.0] \\odot {\\boldsymbol g}_t + [1.0,0.0] \\odot {\\boldsymbol c}_{t-1}\\)\n\nforget_gate는 \\(c_{t-1}\\)의 첫번째 원소는 기억하고, 두번째 원소는 잊으라고 말하고 있음 // forget_gate는 과거(\\(c_{t-1}\\))의 정보를 얼마나 잊을지 (= 얼마나 기억할지) 를 결정한다고 해석할 수 있다.\ninput_gate는 \\(g_{t}\\)의 첫번째 원소와 두번째 원소를 모두 기억하되 두번째 원소를 좀 더 중요하게 기억하라고 말하고 있음 // input_gate는 현재(\\(g_{t}\\))의 정보를 얼만큼 강하게 반영할지 결정한다.\n이 둘을 조합하면 \\({\\boldsymbol c}_t\\)가 현재와 과거의 정보중 어떠한 정보를 더 중시하면서 기억할지 결정한다고 볼 수 있다.\n\n\n이 설명은 제가 좀 싫어해요, 싫어하는 이유는 (1) “기억의 정도를 조절한다”와 “망각의 정도를 조절한다”는 사실 같은말임. 그래서 forget_gate의 용어가 모호함. (2) 기억과 망각을 조정하는 방식으로 꼭 gate의 개념을 사용해야 하는건 아님\n\n- 느낌5: \\({\\boldsymbol c}_t\\)에 대하여 (3)\n\n사실상 LSTM 알고리즘의 꽃이라 할 수 있음.\nLSTM은 long short term memory의 약자임. 기존의 RNN은 장기기억을 활용함에 약점이 있는데 LSTM은 단기기억/장기기억 모두 잘 활용함.\nLSTM이 장기기억을 잘 활용하는 비법은 바로 \\({\\boldsymbol c}_t\\)에 있다.\n\n- 느낌6: \\({\\boldsymbol h}_t\\)에 대하여 - 계산방법: \\(\\tanh({\\boldsymbol c}_t)\\)를 요소별로 선택\n- RNN, LSTM의 변수들 비교 테이블\n\n\n\n\n\n\n\n\n\n\n\n\n\n과거정보\n현재정보\n과거와 현재의 결합방식\n활성화\n느낌\n비고\n\n\n\n\nRNN-\\({\\boldsymbol h}_t\\)\n\\({\\boldsymbol h}_{t-1}\\)\n\\({\\boldsymbol x}_t\\)\n\\(\\times\\) \\(\\to\\) \\(+\\)\n\\(\\tanh\\)\n간장\n\n\n\n\n\n\n\n\n\n\n\n\nLSTM-\\({\\boldsymbol g}_t\\)\n\\({\\boldsymbol h}_{t-1}\\)\n\\({\\boldsymbol x}_t\\)\n\\(\\times\\) \\(\\to\\) \\(+\\)\n\\(\\tanh\\)\n1차간장\n\n\n\nLSTM-\\({\\boldsymbol c}_t\\)\n\\({\\boldsymbol c}_{t-1}\\)\n\\({\\boldsymbol g}_t\\)\n\\(\\odot\\) \\(\\to\\) \\(+\\)\nNone\n2차간장\ngate를 열림정도를 판단할때 \\({\\boldsymbol x}_t\\)와 \\({\\boldsymbol h}_{t-1}\\)을 이용\n\n\nLSTM-\\({\\boldsymbol h}_t\\)\nNone\n\\({\\boldsymbol c}_t\\)\nNone\n\\(\\tanh\\), \\(\\odot\\)\n3차간장\ngate를 열림정도를 판단할때 \\({\\boldsymbol x}_t\\)와 \\({\\boldsymbol h}_{t-1}\\)을 이용\n\n\n\n\nRNN은 기억할 과거정보가 \\({\\boldsymbol h}_{t-1}\\) 하나이지만 LSTM은 \\({\\boldsymbol c}_{t-1}\\), \\({\\boldsymbol h}_{t-1}\\) 2개이다.\n\n- 알고리즘리뷰 :\n\n콩물,과거3차간장 \\(\\overset{\\times,+,\\tanh}{\\longrightarrow}\\) 현재1차간장\n현재1차간장, 과거2차간장 \\(\\overset{\\odot,+,\\tanh}{\\longrightarrow}\\) 현재2차간장\n현재2차간장 \\(\\overset{\\tanh,\\odot}{\\longrightarrow}\\) 현재3차간장"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#lstm이-강한이유",
    "href": "posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.html#lstm이-강한이유",
    "title": "기계학습 (1116) 11주차",
    "section": "LSTM이 강한이유",
    "text": "LSTM이 강한이유\n- LSTM이 장기기억에 유리함. 그 이유는 input, forget, output gate 들이 과거기억을 위한 역할을 하기 때문.\n\n비판: 아키텍처에 대한 이론적 근거는 없음. 장기기억을 위하여 꼭 LSTM같은 구조일 필요는 없음. (왜 3차간장을 만들때 tanh를 써야하는지? 게이트는 꼭3개이어야 하는지?)\n\n- 저는 사실 아까 살펴본 아래의 이유로 이해하고 있습니다.\n\n실험적으로 살펴보니 LSTM이 RNN보다 장기기억에 유리했음.\n그 이유: RRN은 \\({\\boldsymbol h}_t\\)의 값이 -1 혹은 1로 결정되는 경우가 많았음. 그러나 경우에 따라서는 \\({\\boldsymbol h}_t\\)이 -1~1의 값을 가지는 것이 문맥적 뉘앙스를 포착하기에는 유리한데 LSTM이 이러한 방식으로 학습되는 경우가 많았음.\n왜 LSTM의 \\({\\boldsymbol h}_t\\)은 -1,1 이외의 값을 쉽게 가질 수 있는가? (1) gate들의 역할 (2) 마지막에 취해지는 tanh 때문"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html",
    "title": "기계학습 (1130) 12주차",
    "section": "",
    "text": "순환신경망 minor topics"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#data-abcabc",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#data-abcabc",
    "title": "기계학습 (1130) 12주차",
    "section": "data: abcabC",
    "text": "data: abcabC\n\ntxt = list('abcabC')*100\ntxt[:8]\ntxt_x = txt[:-1] \ntxt_y = txt[1:]\n\n\nmapping = {'a':0,'b':1,'c':2,'C':3} \nx= torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny= torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()\n\n\nx = x.to(\"cuda:0\")\ny = y.to(\"cuda:0\") \n\n\nx.shape\n\ntorch.Size([599, 4])"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#실험",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#실험",
    "title": "기계학습 (1130) 12주차",
    "section": "실험",
    "text": "실험\n- 실험1\n\nHIDDEN = 3\n\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        rnn = torch.nn.RNN(4,HIDDEN).to(\"cuda:0\")\n        linr = torch.nn.Linear(HIDDEN,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,HIDDEN).to(\"cuda:0\")\n        for epoc in range(500):\n            ## 1\n            hidden, hT = rnn(x,_water)\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(\"experiment1: RNN with {} hidden nodes\".format(HIDDEN),size=20)\nfig.tight_layout()\n\n\n\n\n- 실험2\n\nHIDDEN = 4\n\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        rnn = torch.nn.RNN(4,HIDDEN).to(\"cuda:0\")\n        linr = torch.nn.Linear(HIDDEN,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,HIDDEN).to(\"cuda:0\")\n        for epoc in range(500):\n            ## 1\n            hidden, hT = rnn(x,_water)\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(\"experiment2: RNN with {} hidden nodes\".format(HIDDEN),size=20)\nfig.tight_layout()\n\n\n\n\n- 실험3\n\nHIDDEN = 8\n\n\nfig, ax = plt.subplots(5,5,figsize=(10,8))\nfor i in range(5):\n    for j in range(5):\n        rnn = torch.nn.RNN(4,HIDDEN).to(\"cuda:0\")\n        linr = torch.nn.Linear(HIDDEN,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,HIDDEN).to(\"cuda:0\")\n        for epoc in range(500):\n            ## 1\n            hidden, hT = rnn(x,_water)\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(\"experiment3: RNN with {} hidden nodes\".format(HIDDEN),size=20)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#결론",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#결론",
    "title": "기계학습 (1130) 12주차",
    "section": "결론",
    "text": "결론\n- 노드수가 많으면 학습에 유리함"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#data-abcc",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#data-abcc",
    "title": "기계학습 (1130) 12주차",
    "section": "data: ab(c,C)",
    "text": "data: ab(c,C)\n\n# torch.manual_seed(43052)\n# txta = 'a'*50\n# txtb = 'b'*50\n# prob_upper = torch.bernoulli(torch.zeros(50)+0.5) \n# txtc = list(map(lambda x: 'c' if x==1 else 'C', prob_upper))\n# txt = ''.join([txta[i]+','+txtb[i]+','+txtc[i]+',' for i in range(50)]).split(',')[:-1]\n# txt_x = txt[:-1] \n# txt_y = txt[1:]\n# pd.DataFrame({'txt_x':txt_x,'txt_y':txt_y}).to_csv(\"2022-11-25-ab(c,C).csv\",index=False)\n\n\ndf= pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2022/main/posts/IV.%20RNN/2022-11-25-ab(c%2CC).csv\")\ndf\n\n\n\n\n\n  \n    \n      \n      txt_x\n      txt_y\n    \n  \n  \n    \n      0\n      a\n      b\n    \n    \n      1\n      b\n      c\n    \n    \n      2\n      c\n      a\n    \n    \n      3\n      a\n      b\n    \n    \n      4\n      b\n      c\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      144\n      a\n      b\n    \n    \n      145\n      b\n      C\n    \n    \n      146\n      C\n      a\n    \n    \n      147\n      a\n      b\n    \n    \n      148\n      b\n      c\n    \n  \n\n149 rows × 2 columns\n\n\n\n\nmapping = {'a':0,'b':1,'c':2,'C':3} \nx= torch.nn.functional.one_hot(torch.tensor(f(df.txt_x,mapping))).float()\ny= torch.nn.functional.one_hot(torch.tensor(f(df.txt_y,mapping))).float()\n\n\nx = x.to(\"cuda:0\")\ny = y.to(\"cuda:0\")"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#실험-1",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#실험-1",
    "title": "기계학습 (1130) 12주차",
    "section": "실험",
    "text": "실험\n- 실험1\n\nHIDDEN = 3\n\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        lstm = torch.nn.LSTM(4,HIDDEN).to(\"cuda:0\")\n        linr = torch.nn.Linear(HIDDEN,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,HIDDEN).to(\"cuda:0\")\n        for epoc in range(500):\n            ## 1\n            hidden, (hT,cT) = lstm(x,(_water,_water))\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combinded = torch.concat([yhat,y],axis=1)\n        ax[i][j].matshow(combinded.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(\"experiment1: LSTM with {} hidden nodes\".format(HIDDEN),size=20)\nfig.tight_layout()\n\n# 2행 4열->과적합되어있음.. c,C 확실히 알수 없는데 확실하게 맞추고있네? -> 과적합이라고 보자!\n\n\n\n\n- 실험2\n\nHIDDEN = 16\n\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        lstm = torch.nn.LSTM(4,HIDDEN).to(\"cuda:0\")\n        linr = torch.nn.Linear(HIDDEN,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,HIDDEN).to(\"cuda:0\")\n        for epoc in range(500):\n            ## 1\n            hidden, (hT,cT) = lstm(x,(_water,_water))\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combinded = torch.concat([yhat,y],axis=1)\n        ax[i][j].matshow(combinded.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(\"experiment2: LSTM with {} hidden nodes\".format(HIDDEN),size=20)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#결론-1",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#결론-1",
    "title": "기계학습 (1130) 12주차",
    "section": "결론",
    "text": "결론\n- 노드수가 너무 많으면 오버피팅 경향도 있음"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#data-human-numbers-5",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#data-human-numbers-5",
    "title": "기계학습 (1130) 12주차",
    "section": "data: human numbers 5",
    "text": "data: human numbers 5\n\ntxt = (['one',',','two',',','three',',','four',',','five',',']*100)[:-1]\n\n\nmapping = {',':0, 'one':1, 'two':2, 'three':3, 'four':4, 'five':5} \nmapping\n\n{',': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5}\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:] \n\n\ntxt_x[0:5], txt_y[0:5]\n\n(['one', ',', 'two', ',', 'three'], [',', 'two', ',', 'three', ','])\n\n\n\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#torch를-이용한-learn",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#torch를-이용한-learn",
    "title": "기계학습 (1130) 12주차",
    "section": "torch를 이용한 learn",
    "text": "torch를 이용한 learn\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(6,20).to(\"cuda:0\")                  #히든레이어 20개\nlinr = torch.nn.Linear(20,6).to(\"cuda:0\") \nloss_fn = torch.nn.CrossEntropyLoss()                    #손실함수 적당히 정의해주기\noptimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\n_water = torch.zeros(1,20).to(\"cuda:0\")\nfor epoc in range(50):\n    ## 1 \n    hidden, (hT,cT) =lstm(x,(_water,_water))\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()     \n\n\nplt.matshow(soft(output).data[-10:].to(\"cpu\"),cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f6bbf69b890>"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#fastai-이용한-learn",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#fastai-이용한-learn",
    "title": "기계학습 (1130) 12주차",
    "section": "fastai 이용한 learn",
    "text": "fastai 이용한 learn\n\nds1 = torch.utils.data.TensorDataset(x,y)\nds2 = torch.utils.data.TensorDataset(x,y) # dummy \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=998)  #X의 full batch사이즈임\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=998) # dummy    #test에 해당하는 거.. 그냥 트레이닝이랑 똑같이 만들자. dls가 두개를 이용해서 만들어야 하니까 \ndls = DataLoaders(dl1,dl2) #데이터로드 두개를 이용해서 만들어야 한다.\n\n\n# lrnr=Learner(dls,net,loss_fn) 이렇게 하려고 했는데 \n# loss_fn은 만들 수 있어\n# 근데 net에서.. 두개의 네트워크를 같이 쓰고 있으니까 이걸 하나의 네트워크로 통일해서 넣기가 애매하다. lstm을 넣어야할지? linear를 넣어야할지? 애매함.\n# 두개이 연속동작을 한번에 해야해\n# class이용해서 해보자!\n\n\nclass MyLSTM(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lstm = torch.nn.LSTM(6,20)\n        self.linr = torch.nn.Linear(20,6) \n    def forward(self,x):\n        _water = torch.zeros(1,20).to(\"cuda:0\")\n        hidden, (hT,cT) =self.lstm(x,(_water,_water))\n        output = self.linr(hidden)\n        return output         \n\n\nnet = MyLSTM().to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\n\n\nlrnr = Learner(dls,net,loss_fn,lr=0.1)\n\n\nlrnr.fit(50)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.918821\n      1.547683\n      00:00\n    \n    \n      1\n      1.731377\n      1.771274\n      00:00\n    \n    \n      2\n      1.744945\n      1.490624\n      00:00\n    \n    \n      3\n      1.679425\n      1.400951\n      00:00\n    \n    \n      4\n      1.621457\n      1.431488\n      00:00\n    \n    \n      5\n      1.588175\n      1.398044\n      00:00\n    \n    \n      6\n      1.559340\n      1.291965\n      00:00\n    \n    \n      7\n      1.523507\n      1.127941\n      00:00\n    \n    \n      8\n      1.475921\n      0.959611\n      00:00\n    \n    \n      9\n      1.419471\n      0.861778\n      00:00\n    \n    \n      10\n      1.363497\n      0.815888\n      00:00\n    \n    \n      11\n      1.312624\n      0.780459\n      00:00\n    \n    \n      12\n      1.266544\n      0.742232\n      00:00\n    \n    \n      13\n      1.223979\n      0.715809\n      00:00\n    \n    \n      14\n      1.185103\n      0.671282\n      00:00\n    \n    \n      15\n      1.147897\n      0.620188\n      00:00\n    \n    \n      16\n      1.111588\n      0.575581\n      00:00\n    \n    \n      17\n      1.076424\n      0.529901\n      00:00\n    \n    \n      18\n      1.042135\n      0.475089\n      00:00\n    \n    \n      19\n      1.008015\n      0.418487\n      00:00\n    \n    \n      20\n      0.973913\n      0.368120\n      00:00\n    \n    \n      21\n      0.940148\n      0.322788\n      00:00\n    \n    \n      22\n      0.906926\n      0.285818\n      00:00\n    \n    \n      23\n      0.874595\n      0.254371\n      00:00\n    \n    \n      24\n      0.843313\n      0.218208\n      00:00\n    \n    \n      25\n      0.812716\n      0.187723\n      00:00\n    \n    \n      26\n      0.782985\n      0.158780\n      00:00\n    \n    \n      27\n      0.754088\n      0.133884\n      00:00\n    \n    \n      28\n      0.726112\n      0.112403\n      00:00\n    \n    \n      29\n      0.699107\n      0.093460\n      00:00\n    \n    \n      30\n      0.673082\n      0.075678\n      00:00\n    \n    \n      31\n      0.647987\n      0.059713\n      00:00\n    \n    \n      32\n      0.623807\n      0.047068\n      00:00\n    \n    \n      33\n      0.600592\n      0.037162\n      00:00\n    \n    \n      34\n      0.578363\n      0.029585\n      00:00\n    \n    \n      35\n      0.557125\n      0.023816\n      00:00\n    \n    \n      36\n      0.536864\n      0.019337\n      00:00\n    \n    \n      37\n      0.517551\n      0.015811\n      00:00\n    \n    \n      38\n      0.499145\n      0.013043\n      00:00\n    \n    \n      39\n      0.481606\n      0.010892\n      00:00\n    \n    \n      40\n      0.464891\n      0.009220\n      00:00\n    \n    \n      41\n      0.448957\n      0.007893\n      00:00\n    \n    \n      42\n      0.433761\n      0.006812\n      00:00\n    \n    \n      43\n      0.419261\n      0.005925\n      00:00\n    \n    \n      44\n      0.405417\n      0.005203\n      00:00\n    \n    \n      45\n      0.392191\n      0.004621\n      00:00\n    \n    \n      46\n      0.379547\n      0.004154\n      00:00\n    \n    \n      47\n      0.367454\n      0.003775\n      00:00\n    \n    \n      48\n      0.355879\n      0.003463\n      00:00\n    \n    \n      49\n      0.344794\n      0.003202\n      00:00\n    \n  \n\n\n\n\nplt.matshow(soft(lrnr.model(x)[-10:]).data.to(\"cpu\"),cmap = 'bwr', vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f6bbbb779d0>"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#data-hihello",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#data-hihello",
    "title": "기계학습 (1130) 12주차",
    "section": "data: hi?hello!!",
    "text": "data: hi?hello!!\n\ntxt = list('hi?hello!!')*100 \ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\nmapping = {'!':0, '?':1,'h':2,'i':3,'e':4,'l':5,'o':6} \nx= torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny= torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#세트1-_water의-생략",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#세트1-_water의-생략",
    "title": "기계학습 (1130) 12주차",
    "section": "세트1: _water의 생략",
    "text": "세트1: _water의 생략\n- 코드1: 정석코드\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\n\n\n_water = torch.zeros(1,4).to(\"cuda:0\")\nlstm(x, (_water,_water))\n\n(tensor([[-0.1547,  0.0673,  0.0695,  0.1563],\n         [-0.0786, -0.1430, -0.0250,  0.1189],\n         [-0.0300, -0.2256, -0.1324,  0.1439],\n         ...,\n         [-0.0723,  0.0620,  0.1913,  0.2015],\n         [-0.1155,  0.0746,  0.1747,  0.2938],\n         [-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n (tensor([[-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>),\n  tensor([[-0.4451, -0.2456, -0.1900,  0.6232]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>)))\n\n\n\n# 히든레이거 값\n# HT\n# CT\n\n- 코드2: _water 는 사실 없어도 괜찮았어..\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\n\n\nlstm(x)\n\n(tensor([[-0.1547,  0.0673,  0.0695,  0.1563],\n         [-0.0786, -0.1430, -0.0250,  0.1189],\n         [-0.0300, -0.2256, -0.1324,  0.1439],\n         ...,\n         [-0.0723,  0.0620,  0.1913,  0.2015],\n         [-0.1155,  0.0746,  0.1747,  0.2938],\n         [-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n (tensor([[-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>),\n  tensor([[-0.4451, -0.2456, -0.1900,  0.6232]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>)))"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#세트2-x.shape-l-h_in-or-lnh_in",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#세트2-x.shape-l-h_in-or-lnh_in",
    "title": "기계학습 (1130) 12주차",
    "section": "세트2: x.shape = (\\(L\\), \\(H_{in}\\)) or (\\(L\\),\\(N\\),\\(H_{in}\\))",
    "text": "세트2: x.shape = (\\(L\\), \\(H_{in}\\)) or (\\(L\\),\\(N\\),\\(H_{in}\\))\n- 파라메터 설명\n\n\\(L\\) = sequece length = 시계열의 길이 = 간장을 몇 년 전통으로 이어갈지 (time시리지의 length)\n\\(N\\) = batch size = 전체데이터는 몇 개의 시계열이 있는지 = 전체 데이터를 몇개의 시계열로 쪼갤지 <– 왜 이걸 해야해?\n\\(H_{in}\\) = input_size = 시점을 고정하였을 경우 입력자료의 차원 = 입력시계열이 시점별로 몇개의 변수로 나타내어 지는지? = 만약에 원핫인코딩으로 단어를 정리하면 단어수를 의미함\n\n\n# x.shape = [999,7]  <- len가 999이고 구별되는 것이 7개 \n# 7개 이거를 Hin으로 생각.. \n# Hin: Hnet 라고 생각..\n# x,shape=[999,N,Hin] 이렇게 생긴 N이 있대 \n\n# Hin \"시점을 고정했을때\"   만약 x[0] = 0., 0., 1., 0., 0.,  h를 2로 맵핑했으니까 저 1은 h를 의미해 \n# 즉 하나의 시점에는 7개 차원인 정보들에 대한 입력..! \n\n# 만약 x.shape=[1000,7] dlaus 1000x7인데, 반으로 쪼개서 500x7, 500x7로 만들면 여기서 N=2이다.\n# 지금우리는 쪼개고 있지 않고 N=1로ㅓ만 진행하눈중 \n\n- 코드2: _water 는 사실 없어도 괜찮았어..\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\n\n\nlstm(x)\n\n(tensor([[-0.1547,  0.0673,  0.0695,  0.1563],\n         [-0.0786, -0.1430, -0.0250,  0.1189],\n         [-0.0300, -0.2256, -0.1324,  0.1439],\n         ...,\n         [-0.0723,  0.0620,  0.1913,  0.2015],\n         [-0.1155,  0.0746,  0.1747,  0.2938],\n         [-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n (tensor([[-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>),\n  tensor([[-0.4451, -0.2456, -0.1900,  0.6232]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>)))\n\n\n- 코드3: x의 차원은 사실 엄밀하게는 (\\(L\\),\\(N\\),\\(H_{in}\\)) 와 같다…\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\n\n\nlstm(x.reshape(999,1,7))\n# lstm(x) 한것과 같은 숫자가 나온당.\n# batch_first=False가 기본 \n\n(tensor([[[-0.1547,  0.0673,  0.0695,  0.1563]],\n \n         [[-0.0786, -0.1430, -0.0250,  0.1189]],\n \n         [[-0.0300, -0.2256, -0.1324,  0.1439]],\n \n         ...,\n \n         [[-0.0723,  0.0620,  0.1913,  0.2015]],\n \n         [[-0.1155,  0.0746,  0.1747,  0.2938]],\n \n         [[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n        grad_fn=<CudnnRnnBackward0>),\n (tensor([[[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>),\n  tensor([[[-0.4451, -0.2456, -0.1900,  0.6232]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>)))\n\n\n- 코드4: batch_first=True옵션을 사용하여 lstm을 만든경우\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4,batch_first=True).to(\"cuda:0\")\n\n\n# lstm(x.reshape(999,1,7)) 하면 값이 이상하게 나온다! \n# batch_first=true옵션을 사용하면 (N,L,Hin) 으로 써줘야 한당. \n\n\nlstm(x.reshape(1,999,7))\n\n(tensor([[[-0.1547,  0.0673,  0.0695,  0.1563],\n          [-0.0786, -0.1430, -0.0250,  0.1189],\n          [-0.0300, -0.2256, -0.1324,  0.1439],\n          ...,\n          [-0.0723,  0.0620,  0.1913,  0.2015],\n          [-0.1155,  0.0746,  0.1747,  0.2938],\n          [-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n        grad_fn=<CudnnRnnBackward0>),\n (tensor([[[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>),\n  tensor([[[-0.4451, -0.2456, -0.1900,  0.6232]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>)))"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#세트3-hidden.shape-dtimes-num_layers-h_out-or-dtimes-num_layers-n-h_out",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#세트3-hidden.shape-dtimes-num_layers-h_out-or-dtimes-num_layers-n-h_out",
    "title": "기계학습 (1130) 12주차",
    "section": "세트3: hidden.shape = (\\(D\\times\\) num_layers, \\(H_{out}\\)) or (\\(D\\times\\) num_layers, \\(N\\), \\(H_{out}\\))",
    "text": "세트3: hidden.shape = (\\(D\\times\\) num_layers, \\(H_{out}\\)) or (\\(D\\times\\) num_layers, \\(N\\), \\(H_{out}\\))\n- 파라메터 설명\n\n\\(D\\) = 2 if bidirectional=True otherwise 1 = 양방향이면 2, 단방향이면 1 (우리는 단방향만 배움)\nnum_layres = 중첩된 RNN일 경우 (우리는 중첩을 안시켰음)\n\\(N\\) = batch size = 전체데이터는 몇 개의 시계열이 있는지 = 전체 데이터를 몇개의 시계열로 쪼갤지 <– 왜 이걸 해야해?\n\\(H_{out}\\) = 히든노드의 수\n\n\n# _water는 (1,히든)이였는데 여기서 1은 D X num_layers의 계산값이였다. \n# num_layres는 중첩시킨적없어서 이값도 1\n# N= 쪼갠적없으니까 1\n\n- 코드5: x.shape = (\\(L\\),\\(1\\),\\(H_{in}\\)) \\(\\to\\) hidden.shape = (\\(1\\),\\(1\\),\\(H_{out}\\))\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\n\n\n_water = torch.zeros(1,1,4).to(\"cuda:0\")    #zeros(1,4)하면 차원 에러가남 \nlstm(x.reshape(999,1,7),(_water,_water))\n\n(tensor([[[-0.1547,  0.0673,  0.0695,  0.1563]],\n \n         [[-0.0786, -0.1430, -0.0250,  0.1189]],\n \n         [[-0.0300, -0.2256, -0.1324,  0.1439]],\n \n         ...,\n \n         [[-0.0723,  0.0620,  0.1913,  0.2015]],\n \n         [[-0.1155,  0.0746,  0.1747,  0.2938]],\n \n         [[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n        grad_fn=<CudnnRnnBackward0>),\n (tensor([[[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>),\n  tensor([[[-0.4451, -0.2456, -0.1900,  0.6232]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>)))\n\n\n- 사실 _water.shape = (1,\\(H_{out}\\)) 에서 1은 observation의 차원을 의미하는게 아님 (그런데 대충 그렇게 생각해도 무방함)\n\n한 시점의 콩물에 대하여 양방향으로 간장을 만들면 _water.shape = (2,h)\n한 시점의 콩물에 대하여 3중첩으로 간장을 만들면 _water.shape = (3,h)\n한 시점의 콩물에 대하여 3중첩간장을 양방향으로 만들면 _water.shape = (6,h)\n\n\n# 원래 1은 D X 넘버오브레이어인데"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#data-hihello-1",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#data-hihello-1",
    "title": "기계학습 (1130) 12주차",
    "section": "data: hi?hello!!",
    "text": "data: hi?hello!!\n\ntxt = list('hi?hello!!')*100 \ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\nmapping = {'!':0, '?':1,'h':2,'i':3,'e':4,'l':5,'o':6} \nx= torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny= torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#세트1-_water의-생략-1",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#세트1-_water의-생략-1",
    "title": "기계학습 (1130) 12주차",
    "section": "세트1: _water의 생략",
    "text": "세트1: _water의 생략\n- 코드1: 정석코드\n\ntorch.manual_seed(43052) \nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\") \n\n\nxt = x[[1]]\n_water = torch.zeros(1,4).to(\"cuda:0\")\nxt.shape, _water.shape\n\n(torch.Size([1, 7]), torch.Size([1, 4]))\n\n\n\nlstmcell(xt,(_water,_water))\n\n(tensor([[-0.0290, -0.1758, -0.0537,  0.0598]], device='cuda:0',\n        grad_fn=<ThnnFusedLstmCellBackward0>),\n tensor([[-0.0582, -0.4566, -0.1256,  0.1922]], device='cuda:0',\n        grad_fn=<ThnnFusedLstmCellBackward0>))\n\n\n- 코드2: _water의 생략\n\ntorch.manual_seed(43052) \nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\") \n\n\nxt = x[[1]]\nxt.shape\n\ntorch.Size([1, 7])\n\n\n\nlstmcell(xt)\n\n(tensor([[-0.0290, -0.1758, -0.0537,  0.0598]], device='cuda:0',\n        grad_fn=<ThnnFusedLstmCellBackward0>),\n tensor([[-0.0582, -0.4566, -0.1256,  0.1922]], device='cuda:0',\n        grad_fn=<ThnnFusedLstmCellBackward0>))"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#세트2-xt.shape-nh_in-or-h_in",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#세트2-xt.shape-nh_in-or-h_in",
    "title": "기계학습 (1130) 12주차",
    "section": "세트2: xt.shape = (\\(N\\),\\(H_{in}\\)) or (\\(H_{in}\\))",
    "text": "세트2: xt.shape = (\\(N\\),\\(H_{in}\\)) or (\\(H_{in}\\))\n- 코드2: _water의 생략\n\ntorch.manual_seed(43052) \nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\") \n\n\nxt = x[[1]]\nxt.shape\n\ntorch.Size([1, 7])\n\n\n\nlstmcell(xt)\n\n(tensor([[-0.0290, -0.1758, -0.0537,  0.0598]], device='cuda:0',\n        grad_fn=<ThnnFusedLstmCellBackward0>),\n tensor([[-0.0582, -0.4566, -0.1256,  0.1922]], device='cuda:0',\n        grad_fn=<ThnnFusedLstmCellBackward0>))\n\n\n- 코드3:\n\ntorch.manual_seed(43052) \nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\") \n\n\nxt = x[1]\nxt.shape\n\ntorch.Size([7])\n\n\n\nlstmcell(xt)\n\n(tensor([-0.0290, -0.1758, -0.0537,  0.0598], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n tensor([-0.0582, -0.4566, -0.1256,  0.1922], device='cuda:0',\n        grad_fn=<SqueezeBackward1>))"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#세트3-hidden.shape-nh_out-or-h_out",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#세트3-hidden.shape-nh_out-or-h_out",
    "title": "기계학습 (1130) 12주차",
    "section": "세트3: hidden.shape = (\\(N\\),\\(H_{out}\\)) or (\\(H_{out}\\))",
    "text": "세트3: hidden.shape = (\\(N\\),\\(H_{out}\\)) or (\\(H_{out}\\))\n- 코드4: xt.shape = (\\(H_{in}\\)) \\(\\to\\) _water.shape = \\((H_{out})\\)\n\ntorch.manual_seed(43052) \nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\") \n\n\nxt = x[1]\n_water = torch.zeros(4).to(\"cuda:0\")\nxt.shape,_water.shape\n\n(torch.Size([7]), torch.Size([4]))\n\n\n\nlstmcell(xt, (_water,_water))\n\n(tensor([-0.0290, -0.1758, -0.0537,  0.0598], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n tensor([-0.0582, -0.4566, -0.1256,  0.1922], device='cuda:0',\n        grad_fn=<SqueezeBackward1>))"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#똑같은-코드들-정리",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#똑같은-코드들-정리",
    "title": "기계학습 (1130) 12주차",
    "section": "똑같은 코드들 정리",
    "text": "똑같은 코드들 정리\n- 원래 1은 단순히 observation의 차원이 아니다. 즉 \\({\\bf X}_{n \\times p}\\)에서 \\(n\\)에 대응하는 차원으로 생각할 수 없다.\n- 그런데 (1) 단방향 (2) 조각내지 않은 시계열 (3) 중첩하지 않은 순환망에 한정하여서는 observation 처럼 생각해도 무방하다. <– 엄밀하게는 이게 위험한 생각임. 하지만 정식으로 모두 따지려면 너무 헷갈림"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#실제구현시-기억할-것",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#실제구현시-기억할-것",
    "title": "기계학습 (1130) 12주차",
    "section": "실제구현시 기억할 것",
    "text": "실제구현시 기억할 것\n- 현실적으로 (1)-(3)이 아닌 조건에서는 Cell 단위로 연산을 이용할 일이 없다. (느리거든요) // 그냥 이해용으로 구현\n- torch.nn.RNN 혹은 torch.nn.LSTM 으로 네트워크를 구성할시 _water의 dim을 명시할 일도 없다.\n- 오로지 고려해야 할 것은 입력시계열을 조각낼지 조각내지 않을지"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#data",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#data",
    "title": "기계학습 (1130) 12주차",
    "section": "data",
    "text": "data\n\ntxt = list('hi!')*3 + list('hi?')*3"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#조각내지-않은-시계열",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#조각내지-않은-시계열",
    "title": "기계학습 (1130) 12주차",
    "section": "조각내지 않은 시계열",
    "text": "조각내지 않은 시계열\n\ntxt_x = txt[:-1] \ntxt_y = txt[1:] \n\n\nmapping = {'!':0, '?':1, 'h':2, 'i':3} \nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")\n\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(4,10).to(\"cuda:0\")\nlinr = torch.nn.Linear(10,4).to(\"cuda:0\")\n\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1 \n    hidden, _ = lstm(x) \n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.matshow(soft(output)).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n# 이것도 밑 그래프랑 같은! \n\n\nhidden, _ = lstm(x)\nplt.matshow(soft(linr(hidden)).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f6b994a6f50>"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#조각난-시계열",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#조각난-시계열",
    "title": "기계학습 (1130) 12주차",
    "section": "조각난 시계열",
    "text": "조각난 시계열\n\ntxt1= txt[:9]\ntxt2= txt[9:]\n\n\ntxt1,txt2\n\n(['h', 'i', '!', 'h', 'i', '!', 'h', 'i', '!'],\n ['h', 'i', '?', 'h', 'i', '?', 'h', 'i', '?'])\n\n\n\ntxt1_x = txt1[:-1] \ntxt1_y = txt1[1:] \ntxt2_x = txt2[:-1] \ntxt2_y = txt2[1:] \n\n\nmapping = {'!':0, '?':1, 'h':2, 'i':3} \nx1 = torch.nn.functional.one_hot(torch.tensor(f(txt1_x,mapping))).float().to(\"cuda:0\")\ny1 = torch.nn.functional.one_hot(torch.tensor(f(txt1_y,mapping))).float().to(\"cuda:0\")\nx2 = torch.nn.functional.one_hot(torch.tensor(f(txt2_x,mapping))).float().to(\"cuda:0\")\ny2 = torch.nn.functional.one_hot(torch.tensor(f(txt2_y,mapping))).float().to(\"cuda:0\")\n\n\nx1.shape, y1.shape, x2.shape, y2.shape\n\n(torch.Size([8, 4]),\n torch.Size([8, 4]),\n torch.Size([8, 4]),\n torch.Size([8, 4]))\n\n\n\nxx = torch.stack([x1,x2],axis=1)   # x1과 x2를 합치자\nyy = torch.stack([y1,y2],axis=1)\nxx.shape, yy.shape\n\n(torch.Size([8, 2, 4]), torch.Size([8, 2, 4]))\n\n\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(4,10).to(\"cuda:0\")\nlinr = torch.nn.Linear(10,4).to(\"cuda:0\")\n\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1 \n    hidden, _ = lstm(xx) \n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output[:,0,:],yy[:,0,:]) + loss_fn(output[:,1,:],yy[:,1,:])\n    # (8,4), (8,4)가 stack되어있는데 첫번째 스택 봅고. yy도 뽑고.. 그럼 로스가 한번 계산이 되는데 다시 로스를 더하면 \n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nfig , ax = plt.subplots(1,2) \nax[0].matshow(soft(output[:,0,:]).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\nax[1].matshow(soft(output[:,1,:]).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f6b70111650>\n\n\n\n\n\n\nhidden, _ = lstm(x)\nplt.matshow(soft(linr(hidden)).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f6b70111350>\n\n\n\n\n\n- 조각난 시계열로 학습한 경우는 hi!에서 hi?로 바뀔 수 없다. 왜냐햐면 그러한 연결정보가 끊어져 있으니까"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#재미있는-실험",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#재미있는-실험",
    "title": "기계학습 (1130) 12주차",
    "section": "재미있는 실험",
    "text": "재미있는 실험\n- x1만 배운다면?\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(4,10).to(\"cuda:0\")\nlinr = torch.nn.Linear(10,4).to(\"cuda:0\")\n\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1 \n    hidden, _ = lstm(x1) \n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y1)\n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nhidden, _ = lstm(x2)\nplt.matshow(soft(linr(hidden)).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f6b701ba890>\n\n\n\n\n\n- x2만 배운다면?\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(4,10).to(\"cuda:0\")\nlinr = torch.nn.Linear(10,4).to(\"cuda:0\")\n\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1 \n    hidden, _ = lstm(x2) \n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y2)\n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nhidden, _ = lstm(x1)\nplt.matshow(soft(linr(hidden)).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f6b9809ef50>"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#data-human-numbers-5-1",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#data-human-numbers-5-1",
    "title": "기계학습 (1130) 12주차",
    "section": "data: human numbers 5",
    "text": "data: human numbers 5\n\ntxt = (['one',',','two',',','three',',','four',',','five',',']*100)[:-1]\n\n\nmapping = {',':0, 'one':1, 'two':2, 'three':3, 'four':4, 'five':5} \nmapping\n\n{',': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5}\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:] \n\n\ntxt_x[0:5], txt_y[0:5]\n\n(['one', ',', 'two', ',', 'three'], [',', 'two', ',', 'three', ','])\n\n\n\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#fastai-이용한-learn-1",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#fastai-이용한-learn-1",
    "title": "기계학습 (1130) 12주차",
    "section": "fastai 이용한 learn",
    "text": "fastai 이용한 learn\n\nds1 = torch.utils.data.TensorDataset(x,y)\nds2 = torch.utils.data.TensorDataset(x,y) # dummy \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=998)\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=998) # dummy \ndls = DataLoaders(dl1,dl2) \n\n\nclass MyLSTM(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        torch.manual_seed(43052)\n        self.lstm = torch.nn.LSTM(6,20)\n        self.linr = torch.nn.Linear(20,6) \n    def forward(self,x):\n        _water = torch.zeros(1,20).to(\"cuda:0\")\n        hidden, (hT,cT) =self.lstm(x,(_water,_water))\n        output = self.linr(hidden)\n        return output         \n\n\nnet = MyLSTM().to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\n\n\nlrnr = Learner(dls,net,loss_fn,lr=0.1)\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.762846\n      1.502211\n      00:00\n    \n    \n      1\n      1.631212\n      1.620583\n      00:00\n    \n    \n      2\n      1.627597\n      1.443686\n      00:00\n    \n    \n      3\n      1.580216\n      1.368762\n      00:00\n    \n    \n      4\n      1.536200\n      1.307310\n      00:00\n    \n    \n      5\n      1.496099\n      1.216339\n      00:00\n    \n    \n      6\n      1.453670\n      1.113821\n      00:00\n    \n    \n      7\n      1.408125\n      1.019931\n      00:00\n    \n    \n      8\n      1.361426\n      0.941434\n      00:00\n    \n    \n      9\n      1.315507\n      0.884034\n      00:00\n    \n  \n\n\n\n\nsoft(lrnr.model(x)).data.to(\"cpu\").numpy().round(3)\n\narray([[0.935, 0.009, 0.015, 0.011, 0.016, 0.014],\n       [0.133, 0.164, 0.242, 0.172, 0.141, 0.147],\n       [0.982, 0.003, 0.004, 0.003, 0.004, 0.003],\n       ...,\n       [0.122, 0.171, 0.242, 0.174, 0.146, 0.144],\n       [0.984, 0.003, 0.004, 0.002, 0.004, 0.003],\n       [0.119, 0.172, 0.244, 0.175, 0.144, 0.145]], dtype=float32)"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#torch를-이용한-learn-1",
    "href": "posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.html#torch를-이용한-learn-1",
    "title": "기계학습 (1130) 12주차",
    "section": "torch를 이용한 learn",
    "text": "torch를 이용한 learn\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(6,20).to(\"cuda:0\") \nlinr = torch.nn.Linear(20,6).to(\"cuda:0\") \nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(10):\n    ## 1 \n    hidden, _ = lstm(x)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()     \n\n\nhidden, _ = lstm(x)\noutput = linr(hidden) \nsoft(output).data.to(\"cpu\").numpy().round(3)\n\narray([[0.935, 0.009, 0.015, 0.011, 0.016, 0.014],\n       [0.133, 0.164, 0.242, 0.172, 0.141, 0.147],\n       [0.982, 0.003, 0.004, 0.003, 0.004, 0.003],\n       ...,\n       [0.122, 0.171, 0.242, 0.174, 0.146, 0.144],\n       [0.984, 0.003, 0.004, 0.002, 0.004, 0.003],\n       [0.119, 0.172, 0.244, 0.175, 0.145, 0.145]], dtype=float32)"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_12_08_13wk_checkpoint.html",
    "href": "posts/Machine Learning/3. RNN/2022_12_08_13wk_checkpoint.html",
    "title": "기계학습 (1201)",
    "section": "",
    "text": "IMDB자료의 분석 (텍스트생성과 감성분류), 잡담"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_12_08_13wk_checkpoint.html#잡담1-순환신경망-텍스트마이닝-시계열분석",
    "href": "posts/Machine Learning/3. RNN/2022_12_08_13wk_checkpoint.html#잡담1-순환신경망-텍스트마이닝-시계열분석",
    "title": "기계학습 (1201)",
    "section": "잡담1: 순환신경망, 텍스트마이닝, 시계열분석",
    "text": "잡담1: 순환신경망, 텍스트마이닝, 시계열분석\n- 순환신경망은 순서가 있는 (말이 좀 애매하지만 아무튼 이렇게 많이 표현해요) 자료를 분석할때 사용할 수 있다. 순서가 있는 자료는 대표적으로 시계열자료과 텍스트자료가 있다.\n- 그래서 언뜻 생각하면 텍스트마이닝이나 시계열분석과 내용이 비슷할 것 같지만 사실 그렇지 않다.\n\n텍스트마이닝의 토픽: 단어를 어떻게 숫자로 잘 만들지, 토픽모델 // 자잘하고 실용적인 느낌? 공학적임..\n\n시계열분석의 토픽: 예측(forecasting)과 신뢰구간, 변화점과 관련한 연구 (detection/test), 정상/비정상시계열모형 (ARIMA, GARCH), Cointegration Test, // 느낌이 좀 거창해.. 경제와 관련 많음.\n순환신경망의 토픽(재작년까지): 텍스트생성, 텍스트분류 + 시계열 자료의 예측, 단어의 숫자화 … 텍스트마이닝과 시계열분석의 거의 모든 토픽에 관여함\n순환신경망의 토픽(작년부터?): 딥러닝의 거의 모든 영역에 관여하기 시작함 (심지어 요즘 이미지 분석도 순환망으로 합니다)\n\n\nhttps://youtu.be/thsXGOkcGGg"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_12_08_13wk_checkpoint.html#잡담2-순환신경망의-아키텍처를-얼마나-깊이-이해해야-할까",
    "href": "posts/Machine Learning/3. RNN/2022_12_08_13wk_checkpoint.html#잡담2-순환신경망의-아키텍처를-얼마나-깊이-이해해야-할까",
    "title": "기계학습 (1201)",
    "section": "잡담2: 순환신경망의 아키텍처를 얼마나 깊이 이해해야 할까?",
    "text": "잡담2: 순환신경망의 아키텍처를 얼마나 깊이 이해해야 할까?\n- 과거기준(텍스트생성, 텍스트분류, 시계열자료예측 등에만 순환망이 이용되었을 때): 학부수준에서 순수 RNN만 알아도 충분했던 것 같음. LSTM이나 GRU는 석사수준?\n- 현재기준: 석사기준 LSTM 같은건 기본이고 어텐션, 트랜스포머등에 대한 개념도 잘 알고 있어야 함. (학부는 잘 모르겠네..)\n- 내 생각: 결국 아키텍처는 근데 유행이라 아키텍처는 한번 따라하면서 이해해보고 핵심 아이디어만 이해하면 된다고 생각함. 즉 LSTM 같은 특정모형의 아키텍처를 달달 외울필요는 없다, 수식써있는거 보고 이해하면 그만임. (수식정도를 이해할 능력은 필요한게.. 코드를 짤때 옵션을 이해할 수는 있어야하니까)\n- 망상: 나중에는 순환신경망이 거의 모든 딥러닝 방법의 base가 되지 않을까?"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_12_08_13wk_checkpoint.html#잡담3-fastai-pytorch-lightning",
    "href": "posts/Machine Learning/3. RNN/2022_12_08_13wk_checkpoint.html#잡담3-fastai-pytorch-lightning",
    "title": "기계학습 (1201)",
    "section": "잡담3: fastai, pytorch lightning",
    "text": "잡담3: fastai, pytorch lightning\n- 비 컴퓨터공학 출신이 쓰기에는 fastai가 좀 더 쓰기 편한건 사실\n- pytorch lightning은 fastai보다 쓰기 어렵지만 (진짜 약간의 클래스관련 지식이 필요함, 솔직히 별로 어렵진 않아요) 좀 더 순수 파이토치에 가깝고 따라서 코드를 뜯어보기 편리하다.\n- 과거의 생각\n\n전문가: pytorch + fastai // pytorch + pytorch lightning (컴공출신)\n비 전문가: 순수 fastai\n\n- 요즘 생각\n\n모두: pytorch + pytorch lightning\n특정한경우: 순수 fastai <– 모형이 구현되어 있다면 fastai가 좋긴 좋아.. 그런데 모형의 구현속도가 못따라감"
  },
  {
    "objectID": "posts/Machine Learning/3. RNN/2022_12_08_13wk_checkpoint.html#잡담4-우린-뭘-해야-할까-학석사-레벨에서..",
    "href": "posts/Machine Learning/3. RNN/2022_12_08_13wk_checkpoint.html#잡담4-우린-뭘-해야-할까-학석사-레벨에서..",
    "title": "기계학습 (1201)",
    "section": "잡담4: 우린 뭘 해야 할까 (학석사 레벨에서..)",
    "text": "잡담4: 우린 뭘 해야 할까 (학석사 레벨에서..)\n- 능력1: 코드이해력 (= 구현능력 = 코드 베끼는 능력)\n\n이미지분석? 해봤음. 텍스트자료? 해봤음. 시계열? 해봤음. 등등등등? 다 해본적 있음. 어떤 원리인지 정확하게 몰라도 다 해본적 있고 그래서 일할 수 있음!!\n돌아가는 코드 최대한 많이 모아놓으세요. torch, fastai, pytorch lightning, tensorflow, keras 등등\n\n- 능력2: 최신트렌드를 파악할 수 있는 힘 (= 논문이해력)\n\n공부, 공부, 공부… A to Z 까지 수식 다 뜯어보고 코드 다 뜯어보면서 집요하게 공부해야함. (LSTM에서 했던것 처럼!) 물론 차근차근 알려주면 수업이 있다면 좋겠지 그런데 보통은 적당히 두리뭉실하게 설명하지 detail 하게 설명하는 수업은 잘 없음. (지루하거든요)\n수식이나 코드중 하나라도 볼 줄 모르면 능력2를 얻는것 자체가 불가능."
  },
  {
    "objectID": "posts/Machine Learning/2022_09_14_(2주차)_9월14일_ipynb의_사본.html",
    "href": "posts/Machine Learning/2022_09_14_(2주차)_9월14일_ipynb의_사본.html",
    "title": "기계학습 (0914) 2주차",
    "section": "",
    "text": "#\nfrom fastai.vision.all import *  ## 이미지분석\nfrom fastai.collab import * ## 추천시스템\nfrom fastai.text.all import * ## 텍스트분석 \nfrom fastai.vision.gan import * ## GAN (이미지생성)\n\n\nimport pandas as pd"
  },
  {
    "objectID": "posts/Machine Learning/2022_09_14_(2주차)_9월14일_ipynb의_사본.html#이미지-자료분석-실습-지난시간-복습",
    "href": "posts/Machine Learning/2022_09_14_(2주차)_9월14일_ipynb의_사본.html#이미지-자료분석-실습-지난시간-복습",
    "title": "기계학습 (0914) 2주차",
    "section": "이미지 자료분석 실습 (지난시간 복습)",
    "text": "이미지 자료분석 실습 (지난시간 복습)\n\n1단계: 데이터의 정리\n\npath=untar_data(URLs.PETS)/'images'\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 00:09<00:00]\n    \n    \n\n\n\npath\n\n#path뒤에 점찍으면 뒤에 함수 나옴. \n#path.ls 하면 뒤에 목록이 나온다!!\n\nPath('/root/.fastai/data/oxford-iiit-pet/images')\n\n\n\n#path에서 이미지 파일만 가져오기\nfnames = get_image_files(path)\n\n\nfnames\n#이미지 파일이라는 것만 가져옴\n\n(#7390) [Path('/root/.fastai/data/oxford-iiit-pet/images/Maine_Coon_52.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/newfoundland_73.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Siamese_75.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/samoyed_170.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/basset_hound_172.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/saint_bernard_37.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/saint_bernard_186.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/leonberger_152.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/pug_188.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/saint_bernard_156.jpg')...]\n\n\n\npath.ls()\n#위에랑 다른것=이미지파일이 아닌게 3개가 있겠지!\n\n(#7393) [Path('/root/.fastai/data/oxford-iiit-pet/images/Maine_Coon_52.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/newfoundland_73.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Siamese_75.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/samoyed_170.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/basset_hound_172.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/saint_bernard_37.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/saint_bernard_186.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/leonberger_152.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/pug_188.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/saint_bernard_156.jpg')...]\n\n\n\n\n\n # f(x)=x+1이라는 함수가 있는데\n # lambda=x+1이라는 식으로 표현하고 싶어. 그럼 f:lamda x: x+1 하면 이 자체가 함수가 되는거임!\n\n f=lambda fname: 'cat' if fname[0].isupper() else 'dog'\n\n\n\ndls = ImageDataLoaders.from_name_func(\n    path, \n    fnames,\n    f, # f대신 (lambda fname: 'cat' if fname[0].isupper() else 'dog') 를 넣어도 가능\n    item_tfms=Resize(224))\n\n#함수잘 모르면 ? 물음표해서 성질 보기!!\n\n\ndls.show_batch()\n\n\n\n\n\n#object를 하나 만드는데, 학습을 하고 학습된 결과를 토대로 예측데이터를만드는거!\n\n\n\n2단계: lrnr 오브젝트 생성\n\n# 러너 오브젝트 만들기 위해서\n# cnn_learner?\ncnn_learner?\n\n#코드가 있는 곳에 들어가서 찾아보면.. file로 찾아가 봅시다!!\n#return이 비전러너,, \n\n#Signature: cnn_learner(*args, **kwargs)\n#Docstring: Deprecated name for `vision_learner` -- do not use\n#File:      /usr/local/lib/python3.7/dist-packages/fastai/vision/learner.py\n#Type:      function\n\n\ncnn_learner??\n\n\n?cnn_learner\n\n\nlrnr = cnn_learner(dls,resnet34,metrics=error_rate)\n\n/usr/local/lib/python3.7/dist-packages/fastai/vision/learner.py:284: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n\nlrnr.dls.show_batch()\n\n\n\n\n\nid(dls)\n\n139724031921680\n\n\n\nid(lrnr.dls)\n\n# 위와 아래의 주소값이 같다! 등호(=)는 포스트잇?같은걸 붙여서 너는 부르면,, 나와야햄 \n# 숫자가 의미하는 것은 dls는 이름일 뿐이고 실제 오브젝트 메모리가 있는 장소\n# dls 에도 포스트잇 붙여놓고 lrnr.dls에도 포스트잇 붙여논당..\n\n139724031921680\n\n\n\n\n3단계: lrnr.학습()\n\nlrnr.fine_tune(1)\n\n#이거 너무 오래걸려.. 그래서 그때 GPU인가 뭐로 바꾸라고 했는데 바꿔도 너어어어어어무 느림..ㅠㅠ  그래서 뒤에 내용 다 놓쳤어 엉엉\n\n# 오! 다시 됬땅.\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.094556\n      0.028600\n      0.007442\n      32:59\n    \n  \n\n\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00<?]\n    \n    \n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n  \n\n\n    \n      \n      60.87% [56/92 27:03<17:23 0.0558]\n    \n    \n\n\n\n# 학습을 하는 방법은 fine_tune만 있는 것이 아니다. \n# fit, method... 등등 \n# mbti확인할때 장례식장이라는 특수한 상황에서 튜닝을 해야함->fine_tune   \n# 이미학습된 내 정보를 일부는 유지하고 미세한 영향을 주는 것만 조정하는 것. 기존 모델은 활용하고 새로운 모델을 조금 반영-> trnasfer model\n# CNN에서 투디아키펙처? 원..?아키펙처,,,가있고 원 어쩌고 아키펙처에서 튜닝...@@@@@ \n\n\nfine_tune()은 모든 가중치를 학습하는 것이 아니라 일부만 학습하는 것임.\nfine_tune()이외이 방법으로 학습할 수도 있음.\n\n\n\n4단계: lrnr.예측()\n(방법1) lrnr.predict() 함수를 이용\n\n#lrnr.predict('2022-09-06-hani03.jpg')\n\n\n# X,y=dis.one_batch()\n# X.shape\n# torch\n## 이쪽 잘 못들었어!! 다시들어야함!1\n# 왼쪼\n\n\n#type(_rtn)\n#튜플,, 대괄호면 튜플\n\n(방법2) lrnr.model(X) 를 이용: X의 shape이 (?,3,224,224)의 형태의 텐서이어야함"
  },
  {
    "objectID": "posts/Machine Learning/2022_09_14_(2주차)_9월14일_ipynb의_사본.html#프로그래밍-과정",
    "href": "posts/Machine Learning/2022_09_14_(2주차)_9월14일_ipynb의_사본.html#프로그래밍-과정",
    "title": "기계학습 (0914) 2주차",
    "section": "프로그래밍 과정",
    "text": "프로그래밍 과정\n\n프로그래밍 과정 overview\n- overview\n\ndls 오브젝트 생성\nlrnr 오브젝트 생성\nlrnr.학습()\nlrnr.예측()\n\n\n\n이미지분석, 추천시스템, 텍스트분석, GAN 분석과정 비교\n- 비교\n\n\n\n\n\n\n\n\n\n\n\n이미지분석(CNN)\n추천시스템\n텍스트분석\nGAN\n\n\n\n\n1단계\nImageDataLoaders\nCollabDataLoaders\nTextDataLoaders\nDataBlock -> dls\n\n\n2단계\ncnn_learner()\ncollab_learner()\nlanguage_model_learner()\nGANLearner.wgan()\n\n\n3단계\nlrnr.fine_tune(1)\nlrnr.fit()\nlrnr.fit()\nlrnr.fit()\n\n\n4단계\nlrnr.predict(), lrnr.model(X)\nlrnr.model(X)\nlrnr.predict()"
  },
  {
    "objectID": "posts/Machine Learning/2022_09_14_(2주차)_9월14일_ipynb의_사본.html#추천시스템-실습",
    "href": "posts/Machine Learning/2022_09_14_(2주차)_9월14일_ipynb의_사본.html#추천시스템-실습",
    "title": "기계학습 (0914) 2주차",
    "section": "추천시스템 실습",
    "text": "추천시스템 실습\n\n1단계\n\ndf_view=pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_view.csv')\n#웹에 있는걸 바로 가져오기\ndf_view\n# !wget ~뒤에 링크 하면 옆에 파일로 떠서 볼수있다는듯\n# 빈칸이 있는 건 메모리를 많이 잡아 먹음,\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      커피1\n      커피2\n      커피3\n      커피4\n      커피5\n      커피6\n      커피7\n      커피8\n      커피9\n      커피10\n      홍차1\n      홍차2\n      홍차3\n      홍차4\n      홍차5\n      홍차6\n      홍차7\n      홍차8\n      홍차9\n      홍차10\n    \n  \n  \n    \n      0\n      4.149209\n      NaN\n      NaN\n      4.078139\n      4.033415\n      4.071871\n      NaN\n      NaN\n      NaN\n      NaN\n      1.142659\n      1.109452\n      NaN\n      0.603118\n      1.084308\n      NaN\n      0.906524\n      NaN\n      NaN\n      0.903826\n    \n    \n      1\n      4.031811\n      NaN\n      NaN\n      3.822704\n      NaN\n      NaN\n      NaN\n      4.071410\n      3.996206\n      NaN\n      NaN\n      0.839565\n      1.011315\n      NaN\n      1.120552\n      0.911340\n      NaN\n      0.860954\n      0.871482\n      NaN\n    \n    \n      2\n      4.082178\n      4.196436\n      NaN\n      3.956876\n      NaN\n      NaN\n      NaN\n      4.450931\n      3.972090\n      NaN\n      NaN\n      NaN\n      NaN\n      0.983838\n      NaN\n      0.918576\n      1.206796\n      0.913116\n      NaN\n      0.956194\n    \n    \n      3\n      NaN\n      4.000621\n      3.895570\n      NaN\n      3.838781\n      3.967183\n      NaN\n      NaN\n      NaN\n      4.105741\n      1.147554\n      NaN\n      1.346860\n      NaN\n      0.614099\n      1.297301\n      NaN\n      NaN\n      NaN\n      1.147545\n    \n    \n      4\n      NaN\n      NaN\n      NaN\n      NaN\n      3.888208\n      NaN\n      3.970330\n      3.979490\n      NaN\n      4.010982\n      NaN\n      0.920995\n      1.081111\n      0.999345\n      NaN\n      1.195183\n      NaN\n      0.818332\n      1.236331\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      0.511905\n      1.066144\n      NaN\n      1.315430\n      NaN\n      1.285778\n      NaN\n      0.678400\n      1.023020\n      0.886803\n      NaN\n      4.055996\n      NaN\n      NaN\n      4.156489\n      4.127622\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      96\n      NaN\n      1.035022\n      NaN\n      1.085834\n      NaN\n      0.812558\n      NaN\n      1.074543\n      NaN\n      0.852806\n      3.894772\n      NaN\n      4.071385\n      3.935935\n      NaN\n      NaN\n      3.989815\n      NaN\n      NaN\n      4.267142\n    \n    \n      97\n      NaN\n      1.115511\n      NaN\n      1.101395\n      0.878614\n      NaN\n      NaN\n      NaN\n      1.329319\n      NaN\n      4.125190\n      NaN\n      4.354638\n      3.811209\n      4.144648\n      NaN\n      NaN\n      4.116915\n      3.887823\n      NaN\n    \n    \n      98\n      NaN\n      0.850794\n      NaN\n      NaN\n      0.927884\n      0.669895\n      NaN\n      NaN\n      0.665429\n      1.387329\n      NaN\n      NaN\n      4.329404\n      4.111706\n      3.960197\n      NaN\n      NaN\n      NaN\n      3.725288\n      4.122072\n    \n    \n      99\n      NaN\n      NaN\n      1.413968\n      0.838720\n      NaN\n      NaN\n      1.094826\n      0.987888\n      NaN\n      1.177387\n      3.957383\n      4.136731\n      NaN\n      4.026915\n      NaN\n      NaN\n      4.164773\n      4.104276\n      NaN\n      NaN\n    \n  \n\n100 rows × 20 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n#위에 링크: 교수님 깃허브-> DL2022/_notebooks/2022-09-08-rcmd_anal.csv\n#컴퓨터가 좋아하는 타입이 아님. 컴퓨터가 좋아하는 타입으로...\n\n\n#'-' 컴퓨터가 좋아하는 데이터 타입\n# https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_anal.csv\n\n\n!wget https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_anal.csv\n\n--2022-09-14 11:32:06--  https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_anal.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 31987 (31K) [text/plain]\nSaving to: ‘2022-09-08-rcmd_anal.csv’\n\n2022-09-08-rcmd_ana 100%[===================>]  31.24K  --.-KB/s    in 0.002s  \n\n2022-09-14 11:32:06 (13.2 MB/s) - ‘2022-09-08-rcmd_anal.csv’ saved [31987/31987]\n\n\n\n\ndf= pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_anal.csv')\n\n\ndls = CollabDataLoaders.from_df(df)\n\n\nCollabDataLoaders.from_df?\n\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      user\n      item\n      rating\n    \n  \n  \n    \n      0\n      47\n      12\n      0.937742\n    \n    \n      1\n      29\n      12\n      0.964676\n    \n    \n      2\n      96\n      4\n      1.315430\n    \n    \n      3\n      9\n      13\n      0.967607\n    \n    \n      4\n      8\n      14\n      1.092273\n    \n    \n      5\n      91\n      10\n      1.453194\n    \n    \n      6\n      41\n      11\n      0.973238\n    \n    \n      7\n      26\n      10\n      3.794259\n    \n    \n      8\n      23\n      2\n      4.048529\n    \n    \n      9\n      45\n      17\n      0.608018\n    \n  \n\n\n\n\ndls.one_batch()\n#가로로시자하니까 타입이 튜플\n\n(tensor([[84, 10],\n         [55, 16],\n         [62, 10],\n         [91,  8],\n         [98,  2],\n         [60, 17],\n         [92,  4],\n         [58, 13],\n         [58,  8],\n         [99,  6],\n         [30,  5],\n         [96,  4],\n         [15, 20],\n         [59, 12],\n         [ 3, 20],\n         [ 9, 10],\n         [77,  1],\n         [67, 14],\n         [71, 15],\n         [ 4, 13],\n         [27, 16],\n         [67, 17],\n         [15,  1],\n         [36, 11],\n         [41,  2],\n         [76, 18],\n         [52,  8],\n         [10, 18],\n         [ 9,  7],\n         [22, 15],\n         [42,  1],\n         [33,  7],\n         [74, 13],\n         [67, 18],\n         [36,  6],\n         [83,  1],\n         [33, 16],\n         [24, 18],\n         [97, 20],\n         [51,  7],\n         [84,  2],\n         [76,  1],\n         [74,  5],\n         [44,  6],\n         [98, 15],\n         [75, 13],\n         [62, 18],\n         [53, 15],\n         [26, 10],\n         [25,  9],\n         [55,  9],\n         [52,  6],\n         [57, 17],\n         [37, 11],\n         [73,  4],\n         [86,  4],\n         [ 1,  6],\n         [26,  4],\n         [64, 16],\n         [33, 14],\n         [83, 18],\n         [70,  2],\n         [75,  1],\n         [33, 12]]), tensor([[1.2715],\n         [4.0267],\n         [0.7438],\n         [0.9987],\n         [1.1155],\n         [3.8992],\n         [1.0577],\n         [3.9485],\n         [0.8547],\n         [0.6699],\n         [4.0411],\n         [1.3154],\n         [0.8391],\n         [4.0661],\n         [0.9562],\n         [3.6942],\n         [1.2878],\n         [4.1035],\n         [4.1144],\n         [1.3469],\n         [1.1454],\n         [3.7744],\n         [4.2453],\n         [1.4332],\n         [3.8708],\n         [4.1442],\n         [0.9954],\n         [0.7891],\n         [4.1260],\n         [0.5774],\n         [4.3466],\n         [4.1756],\n         [4.2065],\n         [3.5310],\n         [4.0288],\n         [0.9260],\n         [0.8332],\n         [0.9404],\n         [4.2671],\n         [0.9587],\n         [1.0322],\n         [1.2164],\n         [1.0687],\n         [4.0421],\n         [4.1446],\n         [3.7454],\n         [4.2632],\n         [3.8596],\n         [3.7943],\n         [3.9132],\n         [0.7096],\n         [0.7217],\n         [4.0679],\n         [1.1996],\n         [1.1524],\n         [0.9177],\n         [4.0719],\n         [4.1183],\n         [3.7412],\n         [1.0213],\n         [3.7535],\n         [0.8947],\n         [0.8515],\n         [1.3081]]))\n\n\n\ntype(dls.one_batch())\n\ntuple\n\n\n\nX,y=dls.one_batch()\n\n\nX[:5]\n\ntensor([[29,  8],\n        [41, 15],\n        [27, 17],\n        [58, 19],\n        [87, 11]])\n\n\n\ny[:5]\n\n#y는 평점 x는 사람의 인덱스,아이템인덱스\n#파이썬은 인덱스가 0번으로 되어잇는지 1번으로 되어잇는지 헷갈료\n#dls를 만들때 제일작은게 0인지 1인지 궁금쓰\n\ntensor([[4.1002],\n        [0.7859],\n        [1.0369],\n        [4.0163],\n        [4.0558]])\n\n\n\ndf.user\n\n0        1\n1        1\n2        1\n3        1\n4        1\n      ... \n995    100\n996    100\n997    100\n998    100\n999    100\nName: user, Length: 1000, dtype: int64\n\n\n\ndf.user.unique(), df.item.unique()\n#중복제거하고 유니크한 숫자만 보고싶을때\n#유저는 1~100까지, 아이템은 1~20까지 있다는 걸 확인할 수 있음\n\n(array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n         27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n         40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n         53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n         66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n         79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n         92,  93,  94,  95,  96,  97,  98,  99, 100]),\n array([15,  1, 11,  5,  4, 14,  6, 20, 12, 17,  8,  9, 13, 19, 18, 16,  2,\n         3, 10,  7]))\n\n\n\n\n2단계\n\n?collab_learner\n\n\nlrnr = collab_learner(dls, y_range=(0.5))\n\n\n\n3단계\n\nlrnr.fit(10) \n#이거왜안되지?ㅠㅠ 위에 파인튠학습안되서 그런가...흠 \n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      gen_loss\n      crit_loss\n      time\n    \n  \n  \n    \n      0\n      -0.564446\n      0.408502\n      0.408502\n      -0.763844\n      02:29\n    \n    \n      1\n      -0.580162\n      0.261919\n      0.261919\n      -0.767689\n      02:27\n    \n    \n      2\n      -0.573394\n      0.211019\n      0.211019\n      -0.748596\n      02:26\n    \n    \n      3\n      -0.565945\n      0.312586\n      0.312586\n      -0.731640\n      02:26\n    \n    \n      4\n      -0.533000\n      0.208635\n      0.208635\n      -0.709664\n      02:26\n    \n    \n      5\n      -0.563198\n      0.189235\n      0.189235\n      -0.736768\n      02:26\n    \n    \n      6\n      -0.565810\n      0.210935\n      0.210935\n      -0.741373\n      02:26\n    \n    \n      7\n      -0.565554\n      0.257288\n      0.257288\n      -0.737568\n      02:26\n    \n    \n      8\n      -0.562049\n      0.309152\n      0.309152\n      -0.743085\n      02:26\n    \n    \n      9\n      -0.565225\n      0.227808\n      0.227808\n      -0.726368\n      02:27\n    \n  \n\n\n\n\n\n4단계\n\n!nvidia-smi\n#GPU를 써야 학습이 빨리 된다... \n#CPU로 되어있어서?.. .?????? batch라는 개념을 알아야함..\n\nNVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n\n\n\n\n#lrnr.model(X.to(\"cuda:0\"))\n#y.reshpe(-1) 학습이 얼마 안된면 잘 몰라,, 그래서 위에 3단계에서 fit옆에 좀더 숫자를 키워,,\n#3단계안되서 4단계 다 안되는듯..\n\nRuntimeError: ignored"
  },
  {
    "objectID": "posts/Machine Learning/2022_09_14_(2주차)_9월14일_ipynb의_사본.html#텍스트분석-실습",
    "href": "posts/Machine Learning/2022_09_14_(2주차)_9월14일_ipynb의_사본.html#텍스트분석-실습",
    "title": "기계학습 (0914) 2주차",
    "section": "텍스트분석 실습",
    "text": "텍스트분석 실습\n\n1단계\n\n# 텍스트 데이터는 순환신경망을 사용한다!!\n# 만약 hello라는 단어를 생각할때, \n# h -> e\n# e -> l\n# l -> l\n# l -> o\n# 근데 l이 애매하다!! 그래서 두개 시점으로 하는게 좋을 거 같아\n\n# he -> l\n# el -> l\n# ll -> o\n\n\n#순서가 중요한게 있음. 텍스트랑 시계열~~\n\n# 다음텍스트가 뭐가 나오는지 적용시키는것 중 하나가 챗봇!\n# 나는 학교에 갔다.\n# 나는 다음에 학교에 가 나와야함. 반복되는 단위가 단어... 또는 문장 단위로 반복될 수도 있음!!\n\n# 나는 학교에 갔다. => 공부를 했다. => 집에 왓당..\n\n# 텍스트는 문맥에 맞게 그럴듯한걸 결과값을 주면 된다. cf)주식은 그 뒤에 값을 정해야함!! ㅠ 내일의 주식장...\n\n'h e l l o . h e l l o ! h e l l o ? h e l l o !!'\n\n\n\n2단계\n\n\n3단계\n\n\n4단계"
  },
  {
    "objectID": "posts/Machine Learning/2022_09_14_(2주차)_9월14일_ipynb의_사본.html#gan-intro",
    "href": "posts/Machine Learning/2022_09_14_(2주차)_9월14일_ipynb의_사본.html#gan-intro",
    "title": "기계학습 (0914) 2주차",
    "section": "GAN intro",
    "text": "GAN intro\n- 저자: 이안굿펠로우 (이름이 특이함. 좋은친구..) - 천재임 - 지도교수가 요수아 벤지오\n- 논문 NIPS, 저는 이 논문 읽고 소름돋았어요.. - https://arxiv.org/abs/1406.2661 (현재시점, 38751회 인용되었음 \\(\\to\\) 48978회 인용..)\n- 최근 10년간 머신러닝 분야에서 가장 혁신적인 아이디어이다. (얀르쿤, 2014년 시점..)\n- 무슨내용? 생성모형\n\n생성모형이란? (쉬운 설명)\n\n만들수 없다면 이해하지 못한 것이다, 리처드 파인만 (천재 물리학자)\n\n- 사진속에 들어있는 동물이 개인지 고양이인지 맞출수 있는 기계와 개와 고양이를 그릴수 있는 기계중 어떤것이 더 시각적보에 대한 이해가 깊다고 볼수 있는가?\n- 진정으로 인공지능이 이미지를 이해했다면, 이미지를 만들수도 있어야 한다. \\(\\to\\) 이미지를 생성하는 모형을 만들어보자 \\(\\to\\) 성공\n\n\nGAN의 응용분야\n- 내가 찍은 사진이 피카소의 화풍으로 표현된다면? - https://www.lgsl.kr/sto/stories/60/ALMA2020070001\n- 퀸의 라이브에이드가 4k로 나온다면?\n- 1920년대 서울의 모습이 칼라로 복원된다면?\n- 딥페이크: 유명인의 가짜 포르노, 가짜뉴스, 협박(거짓기소)\n- 게임영상 (파이널판타지)\n- 거북이의 커버..\n- 너무 많아요…..\n\n\n\n생성모형이란? 통계학과 버전의 설명\n\n제한된 정보만으로 어떤 문제를 풀 때, 그 과정에서 원래의 문제보다 일반적인 문제를 풀지 말고, 가능한 원래의 문제를 직접 풀어야한다. 배프닉 (SVM 창시자)\n\n- 이미지 \\(\\boldsymbol{x}\\)가 주어졌을 경우 라벨을 \\(y\\)라고 하자.\n- 이미지를 보고 라벨을 맞추는 일은 \\(p(y| \\boldsymbol{x})\\)에 관심이 있다.\n- 이미지를 생성하는 일은 \\(p(\\boldsymbol{x},y)\\)에 관심이 있는것이다.\n- 데이터의 생성확률 \\(p(\\boldsymbol{x},y)\\)을 알면 클래스의 사후확률 \\(p(y|\\boldsymbol{x})\\)를 알 수 있음. (아래의 수식 참고) 하지만 역은 불가능\n\\[p(y|x) = \\frac{p(x,y)}{p(x)} = \\frac{p(x,y)}{\\sum_{y}p(x,y)} \\]\n\n즉 이미지를 생성하는일은 분류문제보다 더 어려운 일이라 해석가능\n\n- 따라서 배프닉의 원리에 의하면 식별적 분류가 생성적 분류보다 바람직한 접근법이라 할 수 있음.\n- 하지만 다양한 현실문제에서 생성모형이 유용할때가 많다.\n\n\nGAN의 원리\n- GAN은 생성모형중 하나임\n- GAN의 원리는 경찰과 위조지폐범이 서로 선의의(?) 경쟁을 통하여 서로 발전하는 모형으로 설명할 수 있다.\n\nThe generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles.\n\n- 서로 적대적인(adversarial) 네트워크(network)를 동시에 학습시켜 가짜이미지를 만든다(generate)\n- 무식한 상황극..\n\n위조범: 가짜돈을 만들어서 부자가 되어야지! (가짜돈을 그림)\n경찰: (위조범이 만든 돈을 보고) 이건 가짜다!\n위조범: 걸렸군.. 더 정교하게 만들어야지..\n경찰: 이건 진짠가?… –> 상사에게 혼남. 그것도 구분못하냐고\n위조범: 더 정교하게 만들자..\n경찰: 더 판별능력을 업그레이드 하자!\n반복..\n\n- 굉장히 우수한 경찰조차도 진짜와 가짜를 구분하지 못할때(=진짜 이미지를 0.5의 확률로만 진짜라고 말할때 = 가짜 이미지를 0.5의 확률로만 가짜라고 말할때) 학습을 멈춘다."
  },
  {
    "objectID": "posts/Machine Learning/2022_09_14_(2주차)_9월14일_ipynb의_사본.html#gan-실습",
    "href": "posts/Machine Learning/2022_09_14_(2주차)_9월14일_ipynb의_사본.html#gan-실습",
    "title": "기계학습 (0914) 2주차",
    "section": "GAN 실습",
    "text": "GAN 실습\n\n1단계\n\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\n\n\n\n\n    \n      \n      100.14% [3219456/3214948 00:00<00:00]\n    \n    \n\n\n\ndblock = DataBlock(blocks=(TransformBlock,ImageBlock),\n          get_x = generate_noise,\n          get_items=get_image_files,\n          item_tfms=Resize(32))\ndls = dblock.dataloaders(path) \n\n\ndls.show_batch()\n\n\n\n\n\n\n2단계\n\ncounterfeiter = basic_generator(32,n_channels=3,n_extra_layers=1)\npolice = basic_critic(32,n_channels=3,n_extra_layers=1)\n\n\nlrnr = GANLearner.wgan(dls,counterfeiter,police) \n\n\n\n3단계\n- lrnr.fit(10) 진행\n\nlrnr.fit(10)\n\n/usr/local/lib/python3.7/dist-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (generator) that exists in the learner. Use `self.learn.generator` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n/usr/local/lib/python3.7/dist-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (critic) that exists in the learner. Use `self.learn.critic` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n/usr/local/lib/python3.7/dist-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (gen_mode) that exists in the learner. Use `self.learn.gen_mode` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/10 00:00<?]\n    \n    \n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      gen_loss\n      crit_loss\n      time\n    \n  \n  \n  \n\n\n    \n      \n      51.67% [93/180 01:13<01:08 -0.4932]\n    \n    \n\n\nKeyboardInterrupt: ignored\n\n\n\nlrnr.show_results()\n\n- lrnr.fit(10) 추가로 진행 // 총20회\n\nlrnr.fit(10)\n\n\nlrnr.show_results()\n\n- lrnr.fit(10) 추가로 진행 // 총30회\n\nlrnr.fit(10)\n\n\nlrnr.show_results()\n\n\n\n4단계 (없음)"
  },
  {
    "objectID": "posts/Machine Learning/(202250926)기계학습특강_final (2).html",
    "href": "posts/Machine Learning/(202250926)기계학습특강_final (2).html",
    "title": "기계학습 final",
    "section": "",
    "text": "기계학습특강 기말고사\n\nimport torch \nfrom fastai.text.all import *\n\n\ndf = pd.read_csv('/content/Corona_NLP_train.csv',encoding=\"ISO-8859-1\")\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      UserName\n      ScreenName\n      Location\n      TweetAt\n      OriginalTweet\n      Sentiment\n    \n  \n  \n    \n      0\n      3799\n      48751\n      London\n      16-03-2020\n      @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8\n      Neutral\n    \n    \n      1\n      3800\n      48752\n      UK\n      16-03-2020\n      advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order\n      Positive\n    \n    \n      2\n      3801\n      48753\n      Vagabonds\n      16-03-2020\n      Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-19 outbreak https://t.co/bInCA9Vp8P\n      Positive\n    \n    \n      3\n      3802\n      48754\n      NaN\n      16-03-2020\n      My food stock is not the only one which is empty...\\r\\r\\n\\r\\r\\nPLEASE, don't panic, THERE WILL BE ENOUGH FOOD FOR EVERYONE if you do not take more than you need. \\r\\r\\nStay calm, stay safe.\\r\\r\\n\\r\\r\\n#COVID19france #COVID_19 #COVID19 #coronavirus #confinement #Confinementotal #ConfinementGeneral https://t.co/zrlG0Z520j\n      Positive\n    \n    \n      4\n      3803\n      48755\n      NaN\n      16-03-2020\n      Me, ready to go at supermarket during the #COVID19 outbreak.\\r\\r\\n\\r\\r\\nNot because I'm paranoid, but because my food stock is litteraly empty. The #coronavirus is a serious thing, but please, don't panic. It causes shortage...\\r\\r\\n\\r\\r\\n#CoronavirusFrance #restezchezvous #StayAtHome #confinement https://t.co/usmuaLq72n\n      Extremely Negative\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      41152\n      44951\n      89903\n      Wellington City, New Zealand\n      14-04-2020\n      Airline pilots offering to stock supermarket shelves in #NZ lockdown #COVID-19 https://t.co/cz89uA0HNp\n      Neutral\n    \n    \n      41153\n      44952\n      89904\n      NaN\n      14-04-2020\n      Response to complaint not provided citing COVID-19 related delays. Yet prompt in rejecting policy before consumer TAT is over. Way to go ?\n      Extremely Negative\n    \n    \n      41154\n      44953\n      89905\n      NaN\n      14-04-2020\n      You know itÂs getting tough when @KameronWilds  is rationing toilet paper #coronavirus #toiletpaper @kroger martinsville, help us out!!\n      Positive\n    \n    \n      41155\n      44954\n      89906\n      NaN\n      14-04-2020\n      Is it wrong that the smell of hand sanitizer is starting to turn me on?\\r\\r\\n\\r\\r\\n#coronavirus #COVID19 #coronavirus\n      Neutral\n    \n    \n      41156\n      44955\n      89907\n      i love you so much || he/him\n      14-04-2020\n      @TartiiCat Well new/used Rift S are going for $700.00 on Amazon rn although the normal market price is usually $400.00 . Prices are really crazy right now for vr headsets since HL Alex was announced and it's only been worse with COVID-19. Up to you whethe\n      Negative\n    \n  \n\n41157 rows × 6 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# 텍스트 분석\n# 1단계 : TextDataLoaders\n# 2단계 : language_model_learner()\n# 3단계 : lrnr.fit()\n# 4단계 : lrnr.predict()\n\ndf = pd.read_csv('Corona_NLP_train.csv',encoding=\"ISO-8859-1\")\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      UserName\n      ScreenName\n      Location\n      TweetAt\n      OriginalTweet\n      Sentiment\n    \n  \n  \n    \n      0\n      3799\n      48751\n      London\n      16-03-2020\n      @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8\n      Neutral\n    \n    \n      1\n      3800\n      48752\n      UK\n      16-03-2020\n      advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order\n      Positive\n    \n    \n      2\n      3801\n      48753\n      Vagabonds\n      16-03-2020\n      Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-19 outbreak https://t.co/bInCA9Vp8P\n      Positive\n    \n    \n      3\n      3802\n      48754\n      NaN\n      16-03-2020\n      My food stock is not the only one which is empty...\\r\\r\\n\\r\\r\\nPLEASE, don't panic, THERE WILL BE ENOUGH FOOD FOR EVERYONE if you do not take more than you need. \\r\\r\\nStay calm, stay safe.\\r\\r\\n\\r\\r\\n#COVID19france #COVID_19 #COVID19 #coronavirus #confinement #Confinementotal #ConfinementGeneral https://t.co/zrlG0Z520j\n      Positive\n    \n    \n      4\n      3803\n      48755\n      NaN\n      16-03-2020\n      Me, ready to go at supermarket during the #COVID19 outbreak.\\r\\r\\n\\r\\r\\nNot because I'm paranoid, but because my food stock is litteraly empty. The #coronavirus is a serious thing, but please, don't panic. It causes shortage...\\r\\r\\n\\r\\r\\n#CoronavirusFrance #restezchezvous #StayAtHome #confinement https://t.co/usmuaLq72n\n      Extremely Negative\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      41152\n      44951\n      89903\n      Wellington City, New Zealand\n      14-04-2020\n      Airline pilots offering to stock supermarket shelves in #NZ lockdown #COVID-19 https://t.co/cz89uA0HNp\n      Neutral\n    \n    \n      41153\n      44952\n      89904\n      NaN\n      14-04-2020\n      Response to complaint not provided citing COVID-19 related delays. Yet prompt in rejecting policy before consumer TAT is over. Way to go ?\n      Extremely Negative\n    \n    \n      41154\n      44953\n      89905\n      NaN\n      14-04-2020\n      You know itÂs getting tough when @KameronWilds  is rationing toilet paper #coronavirus #toiletpaper @kroger martinsville, help us out!!\n      Positive\n    \n    \n      41155\n      44954\n      89906\n      NaN\n      14-04-2020\n      Is it wrong that the smell of hand sanitizer is starting to turn me on?\\r\\r\\n\\r\\r\\n#coronavirus #COVID19 #coronavirus\n      Neutral\n    \n    \n      41156\n      44955\n      89907\n      i love you so much || he/him\n      14-04-2020\n      @TartiiCat Well new/used Rift S are going for $700.00 on Amazon rn although the normal market price is usually $400.00 . Prices are really crazy right now for vr headsets since HL Alex was announced and it's only been worse with COVID-19. Up to you whethe\n      Negative\n    \n  \n\n41157 rows × 6 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nTextDataLoaders?\n\n\ndls = TextDataLoaders.from_df(df,text_col='OriginalTweet',is_lm=True, seq_len=64)\n\n\n\n\n\n\n\n\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      xxbos xxmaj japanese symbol for xxunk xxmaj germany at supermarket xxmaj edeka the will teach you how to appropriately social distance and give you shit when you don t xxbos xxup tp xxmaj shortages ? ? ! ? ! xxmaj not us we are fully stocked and you can help a great cause ? \\r\\r\\n https : / / t.co / xxunk \\r\\r\\n .\n      xxmaj japanese symbol for xxunk xxmaj germany at supermarket xxmaj edeka the will teach you how to appropriately social distance and give you shit when you don t xxbos xxup tp xxmaj shortages ? ? ! ? ! xxmaj not us we are fully stocked and you can help a great cause ? \\r\\r\\n https : / / t.co / xxunk \\r\\r\\n . \\r\\r\\n\n    \n    \n      1\n      , when there was shortage of food and # xxmaj corona at its peak ? xxbos xxmaj consider donating to a local shelter if you have the means … xxunk was today 's pick for me ! xxmaj they are also always looking for retailer gift cards if that suits you better . xxmaj they would love the extra support as they work to\n      when there was shortage of food and # xxmaj corona at its peak ? xxbos xxmaj consider donating to a local shelter if you have the means … xxunk was today 's pick for me ! xxmaj they are also always looking for retailer gift cards if that suits you better . xxmaj they would love the extra support as they work to combat\n    \n    \n      2\n      a little xxmaj wednesday humor for you . \\r\\r\\n\\r\\r\\n▁ # coronavirus # toiletpaper # xxunk # xxunk # xxmaj satire # humor # xxunk https : / / t.co / xxunk xxbos xxmaj so a friend of mine at a division of has to supply her own gloves and safety equipment xxmaj grocery store workers deserve hazard pay and the means to protect themselves\n      little xxmaj wednesday humor for you . \\r\\r\\n\\r\\r\\n▁ # coronavirus # toiletpaper # xxunk # xxunk # xxmaj satire # humor # xxunk https : / / t.co / xxunk xxbos xxmaj so a friend of mine at a division of has to supply her own gloves and safety equipment xxmaj grocery store workers deserve hazard pay and the means to protect themselves from\n    \n    \n      3\n      https : / / t.co / xxunk xxbos xxmaj as a former supermarket fairy , i think itâs about time all of the food shop workers get some credit . xxmaj theyâre always looked down on as xxunk working in a xxunk but theyâre working hard to put the stock on the shelves that everyone is panic buying every day ! ? ? #\n      : / / t.co / xxunk xxbos xxmaj as a former supermarket fairy , i think itâs about time all of the food shop workers get some credit . xxmaj theyâre always looked down on as xxunk working in a xxunk but theyâre working hard to put the stock on the shelves that everyone is panic buying every day ! ? ? # coronavirus\n    \n    \n      4\n      care home staff xxmaj care at home teams xxmaj volunteers xxmaj call help lines xxmaj supermarket workers xxmaj xxunk transport teams xxmaj social xxmaj xxunk xxmaj thank xxmaj you 19uk xxbos \" the real risk now is that the xxmaj government sets terms to pay so xxunk that it brings mass social unrest . \" \\r\\r\\n\\r\\r\\n xxmaj the xxmaj government has never spent more\n      home staff xxmaj care at home teams xxmaj volunteers xxmaj call help lines xxmaj supermarket workers xxmaj xxunk transport teams xxmaj social xxmaj xxunk xxmaj thank xxmaj you 19uk xxbos \" the real risk now is that the xxmaj government sets terms to pay so xxunk that it brings mass social unrest . \" \\r\\r\\n\\r\\r\\n xxmaj the xxmaj government has never spent more in\n    \n    \n      5\n      at third and last reading that allows the government to limit the prices of non - vital medicine and medical devices . xxmaj as a result , the state has more influence on price regulation . # xxup covid2019 # covid19russia ahk - liveticker https : / / t.co / xxunk https : / / t.co / xxunk xxbos a graduate from our xxmaj\n      third and last reading that allows the government to limit the prices of non - vital medicine and medical devices . xxmaj as a result , the state has more influence on price regulation . # xxup covid2019 # covid19russia ahk - liveticker https : / / t.co / xxunk https : / / t.co / xxunk xxbos a graduate from our xxmaj english\n    \n    \n      6\n      called racist . xxmaj on cue , he is called racist by globalists in denial . xxmaj pathetic ! https : / / t.co / xxunk xxbos @susannareid100 xxmaj but it 's ok for hundreds of people to be shopping in 300 argos stores across the country that are allowed to remain open even though they are nt essential retailers xxunk @bbcwatchdog @sainsburys @argos_online\n      racist . xxmaj on cue , he is called racist by globalists in denial . xxmaj pathetic ! https : / / t.co / xxunk xxbos @susannareid100 xxmaj but it 's ok for hundreds of people to be shopping in 300 argos stores across the country that are allowed to remain open even though they are nt essential retailers xxunk @bbcwatchdog @sainsburys @argos_online #\n    \n    \n      7\n      panicbuyinguk xxbos xxmaj did a supply run today . xxmaj walk all the way from my condo to the nearest supermarket ( still pretty far ! xxmaj walkthrough xxup xxunk ) and back in broad xxunk . 7 kg rice not included in the pic cuz its in my xxunk sucks . https : / / t.co / xxunk xxbos # xxup covid19 :\n      xxbos xxmaj did a supply run today . xxmaj walk all the way from my condo to the nearest supermarket ( still pretty far ! xxmaj walkthrough xxup xxunk ) and back in broad xxunk . 7 kg rice not included in the pic cuz its in my xxunk sucks . https : / / t.co / xxunk xxbos # xxup covid19 : xxmaj\n    \n    \n      8\n      the phone . # toiletpaper # xxmaj coronavirus xxbos xxmaj denver xxmaj news xxup ag warns xxmaj coloradans against coronavirus scams https : / / t.co / xxunk https : / / t.co / xxunk xxbos xxmaj this bus driver said he felt violated when a passenger coughed and sneezed on the bus without covering her mouth . xxmaj he died of # coronavirus\n      phone . # toiletpaper # xxmaj coronavirus xxbos xxmaj denver xxmaj news xxup ag warns xxmaj coloradans against coronavirus scams https : / / t.co / xxunk https : / / t.co / xxunk xxbos xxmaj this bus driver said he felt violated when a passenger coughed and sneezed on the bus without covering her mouth . xxmaj he died of # coronavirus 11\n    \n  \n\n\n\n\nlrnr = language_model_learner(dls,AWD_LSTM,metrics=[accuracy,Perplexity()])\n\n\nlrnr.fine_tune(3,1e-1) \n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      perplexity\n      time\n    \n  \n  \n    \n      0\n      4.812939\n      4.488820\n      0.288512\n      89.016312\n      02:03\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      perplexity\n      time\n    \n  \n  \n    \n      0\n      4.009799\n      3.902233\n      0.327761\n      49.512905\n      02:30\n    \n    \n      1\n      3.722594\n      3.662943\n      0.355355\n      38.975876\n      02:33\n    \n    \n      2\n      3.447793\n      3.590583\n      0.365500\n      36.255199\n      02:30\n    \n  \n\n\n\n\nlrnr.predict('the price of',20) \n\n\n\n\n\n\n\n\n'the price of milk and toilet roll havenâ\\x92t tripled surges by the end of the week but we have had a sense of'\n\n\n\n\n2. COVID10 tweets -> 분류\n\ndf = pd.read_csv('Corona_NLP_train.csv',encoding=\"ISO-8859-1\")\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      UserName\n      ScreenName\n      Location\n      TweetAt\n      OriginalTweet\n      Sentiment\n    \n  \n  \n    \n      0\n      3799\n      48751\n      London\n      16-03-2020\n      @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8\n      Neutral\n    \n    \n      1\n      3800\n      48752\n      UK\n      16-03-2020\n      advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order\n      Positive\n    \n    \n      2\n      3801\n      48753\n      Vagabonds\n      16-03-2020\n      Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-19 outbreak https://t.co/bInCA9Vp8P\n      Positive\n    \n    \n      3\n      3802\n      48754\n      NaN\n      16-03-2020\n      My food stock is not the only one which is empty...\\r\\r\\n\\r\\r\\nPLEASE, don't panic, THERE WILL BE ENOUGH FOOD FOR EVERYONE if you do not take more than you need. \\r\\r\\nStay calm, stay safe.\\r\\r\\n\\r\\r\\n#COVID19france #COVID_19 #COVID19 #coronavirus #confinement #Confinementotal #ConfinementGeneral https://t.co/zrlG0Z520j\n      Positive\n    \n    \n      4\n      3803\n      48755\n      NaN\n      16-03-2020\n      Me, ready to go at supermarket during the #COVID19 outbreak.\\r\\r\\n\\r\\r\\nNot because I'm paranoid, but because my food stock is litteraly empty. The #coronavirus is a serious thing, but please, don't panic. It causes shortage...\\r\\r\\n\\r\\r\\n#CoronavirusFrance #restezchezvous #StayAtHome #confinement https://t.co/usmuaLq72n\n      Extremely Negative\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      41152\n      44951\n      89903\n      Wellington City, New Zealand\n      14-04-2020\n      Airline pilots offering to stock supermarket shelves in #NZ lockdown #COVID-19 https://t.co/cz89uA0HNp\n      Neutral\n    \n    \n      41153\n      44952\n      89904\n      NaN\n      14-04-2020\n      Response to complaint not provided citing COVID-19 related delays. Yet prompt in rejecting policy before consumer TAT is over. Way to go ?\n      Extremely Negative\n    \n    \n      41154\n      44953\n      89905\n      NaN\n      14-04-2020\n      You know itÂs getting tough when @KameronWilds  is rationing toilet paper #coronavirus #toiletpaper @kroger martinsville, help us out!!\n      Positive\n    \n    \n      41155\n      44954\n      89906\n      NaN\n      14-04-2020\n      Is it wrong that the smell of hand sanitizer is starting to turn me on?\\r\\r\\n\\r\\r\\n#coronavirus #COVID19 #coronavirus\n      Neutral\n    \n    \n      41156\n      44955\n      89907\n      i love you so much || he/him\n      14-04-2020\n      @TartiiCat Well new/used Rift S are going for $700.00 on Amazon rn although the normal market price is usually $400.00 . Prices are really crazy right now for vr headsets since HL Alex was announced and it's only been worse with COVID-19. Up to you whethe\n      Negative\n    \n  \n\n41157 rows × 6 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndls = TextDataLoaders.from_df(df,text_col='OriginalTweet', label_col='Sentiment', seq_len=64)\ndls.show_batch()\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      text\n      category\n    \n  \n  \n    \n      0\n      xxbos xxrep 5 ? ? ? xxrep 7 ? ? ? xxrep 7 ? xxrep 4 ? xxrep 4 ? xxrep 11 ? ? ? xxrep 6 ? xxrep 4 ? , xxrep 3 ? xxrep 3 ? ? ? xxrep 3 ? xxrep 4 ? xxrep 3 ? ? ? ? ? xxrep 4 ? ? ? xxrep 3 ? , xxrep 4 ? ? ? ? ? xxrep 6 ? xxrep 3 ? xxrep 3 ? xxrep 3 ? ? ? xxrep 3 ? \\r\\r\\n▁ xxrep 5 ? xxrep 6 ? ? ? xxrep 3 ? xxrep 4 ? xxrep 4 ? ? ? xxrep 4 ? xxrep 6 ? xxrep 4 ? xxrep 8 ? ? ? xxrep 6 ? ? ? xxrep 5 ? ? ? xxrep 3 ? xxrep 4 ? ? ? xxrep 7 ? xxrep 5 ? - xxrep 8 ? xxrep 5\n      Neutral\n    \n    \n      1\n      xxbos xxup ask xxup your xxup self xxup what xxup do xxup you xxup think xxup is xxup going xxup to xxup happen xxup the xxup time xxup to xxup wake xxup up xxup is xxup now xxup do xxup you xxup think xxup food xxup going xxup to xxup be xxup xxunk xxup on xxup shop xxup shelfs .. no \\r\\r\\n xxup do xxup you xxup think xxup food xxup rise xxup in xxup price .. yes \\r\\r\\n xxup i m xxup going xxup to xxup stock xxup up xxup as xxup much i xxup can \\r\\r\\n xxup food xxup ladies xxup gentleman xxup is xxup most xxup valuable xxup asset \\r\\r\\n▁ # xxmaj coronavirus # xxup covid19 https : / / t.co / xxunk\n      Extremely Positive\n    \n    \n      2\n      xxbos xxup keep xxup your xxup home xxup safe & & xxup clean \\r\\r\\n xxmaj the xxmaj best xxmaj way to xxmaj avoid the # xxmaj coronavirus is in xxmaj clean xxmaj home \\r\\r\\n xxmaj absolutely xxmaj outstanding xxmaj cleaning @ xxmaj awesome xxmaj rates \\r\\r\\n xxmaj prices : 2 xxmaj hours 2 xxmaj maids $ 75 + \\r\\r\\n xxmaj serving xxmaj las # xxmaj vegas , # xxmaj summerlin , # xxmaj xxunk xxmaj city & & xxmaj more \\r\\r\\n https : / / t.co / xxunk \\r\\r\\n ( xxunk - xxunk \\r\\r\\n▁ # xxup xxunk # xxup xxunk # xxup xxunk https : / / t.co / xxunk\n      Extremely Positive\n    \n    \n      3\n      xxbos # xxup xxunk : xxup xxunk ' xxup back & & xxup forth xxup in xxup my xxup chair , xxup wearin ' xxup my xxup xxunk , xxup wrapped xxup in xxup my xxup blanket , xxup xxunk ' xxup exhausted , xxup xxunk ' xxunk xxup xxunk ' xxup in xxup line xxup at xxup the xxup supermarket , xxup xxunk ' xxup like xxup i m xxup cool xxup wit ' # xxup socialdistancing xxup there … . xxup why i xxup have xxup to xxup wait xxup so xxup long xxup before xxup xxunk  https : / / t.co / xxunk\n      Positive\n    \n    \n      4\n      xxbos xxup sweet xxup baby xxup jesus & & xxup all xxup his xxup xxunk ! i swear 2 xxmaj god xxmaj i 'm going 2 throat punch these xxup covid-19 xxup food xxup hoarders . xxmaj the world xxmaj is n't going 2 end u selfish pricks . i went 2 get milk tonight & & they were out of stock . 4 the love of xxup xxunk xxup hoarding & & xxup save xxup some xxup products xxup for xxup the xxup rest xxup of xxup us xxrep 3 ! https : / / t.co / xxunk\n      Extremely Positive\n    \n    \n      5\n      xxbos xxmaj this # xxmaj afternoon : xxmaj at xxup bs i could n't buy scratchers b / c it was # closed b / c of # coronavirus . xxmaj after xxup bs i walked to # xxmaj water xxmaj store to buy $ 2 scratchers . i won $ 10 with xxup xxunk and $ 1 with xxup xxunk . xxmaj after xxup ws i walked to # xxmaj mexican # xxmaj grocery to buy $ 2 scratchers . i lost $ 1 with xxup xxunk and $ 1 with xxup xxunk .\n      Positive\n    \n    \n      6\n      xxbos xxmaj running xxmaj in xxmaj place , xxmaj working xxmaj out # 2k20 # xxmaj park # xxmaj workouts # xxmaj xxunk # xxmaj xxunk # xxmaj basketball # xxmaj court # xxmaj bored # xxmaj coronavirus # toiletpaper # xxmaj running # xxmaj lockdown # xxmaj home # xxmaj governor # xxmaj browns # xxup nfl # xxup nba # xxmaj cleveland # xxmaj art # xxmaj poetry # xxmaj peaceful # xxmaj beauty # xxmaj meditation # xxmaj ventilator \\r\\r\\n https : / / t.co / xxunk via @youtube\n      Neutral\n    \n    \n      7\n      xxbos xxup mbbs - xxup rmc xxmaj pakistan \\r\\r\\n msc xxmaj public xxmaj health - xxup lsh xxup uk \\r\\r\\n ex - global xxmaj coordinator xxup who \\r\\r\\n ex - regional xxmaj adviser xxup who \\r\\r\\n xxmaj founder & & xxmaj executive xxmaj coordinator - xxmaj the xxmaj network for xxmaj consumer xxmaj protection xxmaj pakistan \\r\\r\\n\\r\\r\\n xxup vs \\r\\r\\n\\r\\r\\n xxup ba - xxmaj national xxmaj college xxmaj karachi . \\r\\r\\n xxup llb - xxmaj sindh xxmaj muslim xxmaj law xxmaj college \\r\\r\\n\\r\\r\\n▁ # coronaviruspakistan # xxmaj coronavirus\n      Neutral\n    \n    \n      8\n      xxbos xxup stop xxup hoarding - u r xxup causing xxup problems 4 / xxup people xxup who xxup canât get around xxup easy & & quick ( the elderly & & those w / physical disabilities ) most r xxup over xxup buying products xxunk at higher prices xxup not 4 / xxup need & & treating toilet paper like xxup roll xxup gold - look at xxup what u r xxup doing . \\r\\r\\n▁ # stophoarding \\r\\r\\n▁ # coronavirus https : / / t.co / xxunk\n      Negative\n    \n  \n\n\n\n\nlrnr = text_classifier_learner(dls,AWD_LSTM,metrics=accuracy)\n\n\nlrnr.fine_tune(5, 1e-2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.466701\n      1.372223\n      0.390597\n      00:46\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.171761\n      1.022376\n      0.572956\n      00:53\n    \n    \n      1\n      0.947488\n      0.843770\n      0.669056\n      00:51\n    \n    \n      2\n      0.802706\n      0.684167\n      0.740858\n      00:52\n    \n    \n      3\n      0.671411\n      0.648740\n      0.758110\n      00:52\n    \n    \n      4\n      0.605033\n      0.645920\n      0.759203\n      00:54\n    \n  \n\n\n\n\nlrnr.predict(\"the government’s approach to the pendemic has been a complete disaster\") \n\n\n\n\n\n\n\n\n('Extremely Negative',\n tensor(0),\n tensor([6.7275e-01, 5.5622e-06, 3.2659e-01, 2.4446e-05, 6.2955e-04]))\n\n\n\nlrnr.predict(\"the new vaccines hold the promise of a quick return to economic growth\") \n\n\n\n\n\n\n\n\n('Extremely Positive',\n tensor(1),\n tensor([1.6411e-06, 9.0713e-01, 1.5391e-04, 4.9677e-05, 9.2669e-02]))\n\n\n\n\n3. human numbers 5\n\ntxt = (['one',',','two',',','three',',','four',',','five',',']*100)[:-1]\nmapping = {',':0, 'one':1, 'two':2, 'three':3, 'four':4, 'five':5} \ntxt_x = txt[:-1]\ntxt_y = txt[1:] \n\n\ntxt_x[:5], txt_y[:5]\n\n(['one', ',', 'two', ',', 'three'], [',', 'two', ',', 'three', ','])\n\n\n\ndef f(txt,mapping):\n    return [mapping[key] for key in txt] \nsig = torch.nn.Sigmoid()\nsoft = torch.nn.Softmax(dim=1)\ntanh = torch.nn.Tanh()\n\n\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")\n\n\n #torch.nn.RNNCell()을 이용하여 다음단어를 예측하는 신경망을 설계하고 학습하라.\n\n\ntorch.manual_seed(202250926)\nrnncell = torch.nn.RNNCell(6,8).to(\"cuda:0\")\nlinr = torch.nn.Linear(8,6).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1\n    hidden = [] \n    ht = torch.zeros(8).to(\"cuda:0\")\n    for xt,yt in zip(x,y): \n        ht = rnncell(xt,ht) \n        hidden.append(ht) \n    hidden = torch.stack(hidden)\n    output = linr(hidden)\n    ## 2 \n    loss = loss_fn(output,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\nyhat[:10].to(\"cpu\").detach().numpy().round(3)\n\narray([[0.999, 0.   , 0.   , 0.001, 0.   , 0.   ],\n       [0.   , 0.002, 0.998, 0.   , 0.   , 0.   ],\n       [1.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.   , 0.   , 0.999, 0.   , 0.001],\n       [0.999, 0.   , 0.   , 0.001, 0.   , 0.   ],\n       [0.   , 0.   , 0.   , 0.   , 0.999, 0.   ],\n       [1.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.   , 0.   , 0.001, 0.001, 0.998],\n       [1.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.999, 0.   , 0.   , 0.   , 0.   ]], dtype=float32)\n\n\n\nplt.matshow(yhat.to(\"cpu\").data[:10],cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(6),labels=[',','1','2','3','4','5']);\n\n\n\n\n\n# torch.nn.RNN()을 이용하여 다음단어를 예측하는 신경망을 설계하고 학습하라.\nrnn = torch.nn.RNN(6,8).to(\"cuda:0\")\nlinr = torch.nn.Linear(8,6).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1\n    hidden, hT = rnn(x) \n    output = linr(hidden)\n    ## 2\n    loss = loss_fn(output,y)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nyhat=soft(output)    \nplt.matshow(yhat.to(\"cpu\").data[:10],cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(6),labels=[',','1','2','3','4','5']);\n\n\n\n\n\n#  torch.nn.LSTMCell()을 이용하여 다음단어를 예측하는 신경망을 설계하고 학습하라.\ntorch.manual_seed(202250926) \nlstmcell = torch.nn.LSTMCell(6,8).to(\"cuda:0\")\nlinr = torch.nn.Linear(8,6).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstmcell.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1\n    hidden = []\n    ht = torch.zeros(8).to(\"cuda:0\")\n    ct = torch.zeros(8).to(\"cuda:0\")\n    for xt,yt in zip(x,y): \n        ht,ct = lstmcell(xt,(ht,ct))\n        hidden.append(ht) \n    hidden = torch.stack(hidden)\n    output = linr(hidden)\n    ## 2 \n    loss = loss_fn(output,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\nyhat[:10].to(\"cpu\").detach().numpy().round(3)\n\narray([[0.997, 0.   , 0.002, 0.   , 0.001, 0.   ],\n       [0.   , 0.   , 0.991, 0.004, 0.005, 0.   ],\n       [1.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.028, 0.003, 0.969, 0.   , 0.   ],\n       [1.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.   , 0.004, 0.   , 0.975, 0.021],\n       [1.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.019, 0.   , 0.   , 0.021, 0.961],\n       [0.998, 0.002, 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.937, 0.   , 0.03 , 0.   , 0.032]], dtype=float32)\n\n\n\nplt.matshow(yhat.to(\"cpu\").data[:10],cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(6),labels=[',','1','2','3','4','5']);\n\n\n\n\n\n# torch.nn.LSTM()을 이용하여 다음단어를 예측하는 신경망을 설계하고 학습하라.\nlstm = torch.nn.LSTM(6,8).to(\"cuda:0\")\nlinr = torch.nn.Linear(8,6).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1\n    hidden, (hT,cT) = lstm(x)\n    output = linr(hidden)\n    ## 2\n    loss = loss_fn(output,y)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nyhat=soft(output)    \nplt.matshow(yhat.to(\"cpu\").data[:10],cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(6),labels=[',','1','2','3','4','5']);"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html",
    "href": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html",
    "title": "기계학습 (1012) 6주차",
    "section": "",
    "text": "깊은신경망(2)– 시벤코정리, 신경망의표현, CPU vs GPU, 확률적경사하강법, 오버피팅"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#지난시간-논리전개",
    "href": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#지난시간-논리전개",
    "title": "기계학습 (1012) 6주차",
    "section": "지난시간 논리전개",
    "text": "지난시간 논리전개\n- 아이디어: linear -> relu -> linear (-> sigmoid) 조합으로 꺽은선으로 표현되는 underlying 을 표현할 수 있었다.\n\n아이디어의 실용성: 실제자료에서 꺽은선으로 표현되는 underlying은 몇개 없을 것 같음. 그건 맞는데 꺽이는 점을 많이 설정하면 얼추 비슷하게는 “근사” 시킬 수 있음.\n아이디어의 확장성: 이러한 논리전개는 X:(n,2)인 경우도 가능했음. (이 경우 꺽인선은 꺽인평면이 된다)\n아이디어에 해당하는 용어정리: 이 구조가 x->y 로 바로 가는 것이 아니라 x->(u1->v1)->(u2->v2)=y 의 구조인데 이러한 네트워크를 하나의 은닉층을 포함하는 네트워크라고 표현한다. (이 용어는 이따가..)"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#시벤코정리-1",
    "href": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#시벤코정리-1",
    "title": "기계학습 (1012) 6주차",
    "section": "시벤코정리",
    "text": "시벤코정리\nuniversal approximation thm: (범용근사정리,보편근사정리,시벤코정리), 1989\n\n하나의 은닉층을 가지는 “linear -> sigmoid -> linear” 꼴의 네트워크를 이용하여 세상에 존재하는 모든 (다차원) 연속함수를 원하는 정확도로 근사시킬 수 있다. (계수를 잘 추정한다면)\n\n- 사실 엄청 이해안되는 정리임. 왜냐햐면,\n\n그렇게 잘 맞추면 1989년에 세상의 모든 문제를 다 풀어야 한거 아니야?\n요즘은 “linear -> sigmoid -> linear” 가 아니라 “linear -> relu -> linear” 조합으로 많이 쓰던데?\n요즘은 하나의 은닉층을 포함하는 네트워크는 잘 안쓰지 않나? 은닉층이 여러개일수록 좋다고 어디서 본 것 같은데?\n\n- 약간의 의구심이 있지만 아무튼 universal approximation thm에 따르면 우리는 아래와 같은 무기를 가진 꼴이 된다.\n\n우리의 무기: \\({\\bf X}: (n,p)\\) 꼴의 입력에서 \\({\\bf y}:(n,1)\\) 꼴의 출력으로 향하는 맵핑을 “linear -> relu -> linear”와 같은 네트워크를 이용해서 “근사”시킬 수 있다."
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#그림으로-보는-증명과정",
    "href": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#그림으로-보는-증명과정",
    "title": "기계학습 (1012) 6주차",
    "section": "그림으로 보는 증명과정",
    "text": "그림으로 보는 증명과정\n- 데이터\n\nx = torch.linspace(-10,10,200).reshape(-1,1)\n\n- 아래와 같은 네트워크를 고려하자.\n\nl1 = torch.nn.Linear(in_features=1,out_features=2)\na1 = torch.nn.Sigmoid()\nl2 = torch.nn.Linear(in_features=2,out_features=1)\n\n- 직관1: \\(l_1\\),\\(l_2\\)의 가중치를 잘 결합하다보면 우연히 아래와 같이 만들 수 있다.\n\nl1.weight.data = torch.tensor([[-5.00],[5.00]])\nl1.bias.data = torch.tensor([+10.00,+10.00])\n\n\nl2.weight.data = torch.tensor([[1.00,1.00]])\nl2.bias.data = torch.tensor([-1.00])\n\n\nfig,ax = plt.subplots(1,3,figsize=(9,3))\nax[0].plot(x,l1(x).data); ax[0].set_title('$l_1(x)$')\nax[1].plot(x,a1(l1(x)).data); ax[1].set_title('$(a_1 \\circ l_1)(x)$')\nax[2].plot(x,l2(a1(l1(x))).data,color='C2'); ax[2].set_title('$(l_2 \\circ a_1 \\circ \\l_1)(x)$') #모자의 시프팅(왼족아래쪽이동), 모자높이 이동 (1->2...)\n\nText(0.5, 1.0, '$(l_2 \\\\circ a_1 \\\\circ \\\\l_1)(x)$')\n\n\n\n\n\n- 직관2: 아래들도 가능할듯?\n\nl1.weight.data = torch.tensor([[-5.00],[5.00]])\nl1.bias.data = torch.tensor([+0.00,+20.00])\nl2.weight.data = torch.tensor([[1.00,1.00]])\nl2.bias.data = torch.tensor([-1.00])\nfig,ax = plt.subplots(1,3,figsize=(9,3))\nax[0].plot(x,l1(x).data,'--',color='C0'); ax[0].set_title('$l_1(x)$')\nax[1].plot(x,a1(l1(x)).data,'--',color='C0'); ax[1].set_title('$(a_1 \\circ l_1)(x)$')\nax[2].plot(x,l2(a1(l1(x))).data,'--',color='C0'); ax[2].set_title('$(l_2 \\circ a_1 \\circ \\l_1)(x)$');\n\n\n\n\n\nl1.weight.data = torch.tensor([[-5.00],[5.00]])\nl1.bias.data = torch.tensor([+20.00,+0.00])\nl2.weight.data = torch.tensor([[2.50,2.50]])\nl2.bias.data = torch.tensor([-2.50])\nax[0].plot(x,l1(x).data,'--',color='C1'); ax[0].set_title('$l_1(x)$')\nax[1].plot(x,a1(l1(x)).data,'--',color='C1'); ax[1].set_title('$(a_1 \\circ l_1)(x)$')\nax[2].plot(x,l2(a1(l1(x))).data,'--',color='C1'); ax[2].set_title('$(l_2 \\circ a_1 \\circ \\l_1)(x)$');\nfig\n\n\n\n\n- 은닉층의노드수=4로 하고 적당한 가중치를 조정하면 \\((l_2\\circ a_1 \\circ l_1)(x)\\)의 결과로 주황색선 + 파란색선도 가능할 것 같다. \\(\\to\\) 실제로 가능함\n\nl1 = torch.nn.Linear(in_features=1,out_features=4)\na1 = torch.nn.Sigmoid()\nl2 = torch.nn.Linear(in_features=4,out_features=1)\n\n\nl1.weight.data = torch.tensor([[-5.00],[5.00],[-5.00],[5.00]])\nl1.bias.data = torch.tensor([0.00, 20.00, 20.00, 0])\nl2.weight.data = torch.tensor([[1.00,  1.00, 2.50,  2.50]])\nl2.bias.data = torch.tensor([-1.0-2.5])\n\n\nplt.plot(l2(a1(l1(x))).data)\n\n\n\n\n- 2개의 시그모이드를 우연히 잘 결합하면 아래와 같은 함수 \\(h\\)를 만들 수 있다.\n\nh = lambda x: torch.sigmoid(200*(x+0.5))+torch.sigmoid(-200*(x-0.5))-1.0\n\n\nplt.plot(x,h(x))\nplt.title(\"$h(x)$\")\n\nText(0.5, 1.0, '$h(x)$')\n\n\n\n\n\n- 위와 같은 함수 \\(h\\)를 활성화함수로 하고 \\(m\\)개의 노드를 가지는 은닉층을 생각해보자. 이러한 은닉층을 사용한다면 전체 네트워크를 아래와 같이 표현할 수 있다.\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,m)}{\\boldsymbol u^{(1)}} \\overset{h}{\\to} \\underset{(n,m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n그리고 위의 네트워크와 동일한 효과를 주는 아래의 네트워크가 항상 존재함.\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,2m)}{\\boldsymbol u^{(1)}} \\overset{sig}{\\to} \\underset{(n,2m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n- \\(h(x)\\)를 활성화함수로 가지는 네트워크를 설계하여 보자.\n\nclass MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법\n    def __init__(self):\n        super().__init__() \n    def forward(self, input): #forward: x에서 y로가는거.. \n        return h(input) # activation 의 출력 \n\n\na1=MyActivation()\n# a1 = torch.nn.Sigmoid(), a1 = torch.nn.ReLU() 대신에 a1 = MyActivation()\n\n\nplt.plot(x,a1(x)) \n\n\n\n\n히든레이어가 1개의 노드를 가지는 경우\n\ntorch.manual_seed(43052)\nfig, ax = plt.subplots(4,4,figsize=(12,12))\nfor i in range(4):\n    for j in range(4):\n        net = torch.nn.Sequential(\n            torch.nn.Linear(1,1),\n            MyActivation(),\n            torch.nn.Linear(1,1)\n        )\n        ax[i,j].plot(x,net(x).data,'--')\n\n\n\n\n히든레이어가 2개의 노드를 가지는 경우\n\ntorch.manual_seed(43052)\nfig, ax = plt.subplots(4,4,figsize=(12,12))\nfor i in range(4):\n    for j in range(4):\n        net = torch.nn.Sequential(\n            torch.nn.Linear(1,2),\n            MyActivation(),\n            torch.nn.Linear(2,1)\n        )\n        ax[i,j].plot(x,net(x).data,'--')\n\n\n\n\n히든레이어가 3개의 노드를 가지는 경우\n\ntorch.manual_seed(43052)\nfig, ax = plt.subplots(4,4,figsize=(12,12))\nfor i in range(4):\n    for j in range(4):\n        net = torch.nn.Sequential(\n            torch.nn.Linear(1,3),\n            MyActivation(),\n            torch.nn.Linear(3,1)\n        )\n        ax[i,j].plot(x,net(x).data,'--')\n\n\n\n\n히든레이어가 1024개의 노드를 가지는 경우\n\ntorch.manual_seed(43052)\nfig, ax = plt.subplots(4,4,figsize=(12,12))\nfor i in range(4):\n    for j in range(4):\n        net = torch.nn.Sequential(\n            torch.nn.Linear(1,1024),\n            MyActivation(),\n            torch.nn.Linear(1024,1)\n        )\n        ax[i,j].plot(x,net(x).data,'--')"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#예제1-sin-exp",
    "href": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#예제1-sin-exp",
    "title": "기계학습 (1012) 6주차",
    "section": "예제1 (sin, exp)",
    "text": "예제1 (sin, exp)\n\ntorch.manual_seed(43052)\nx = torch.linspace(-10,10,200).reshape(-1,1)\nunderlying = torch.sin(2*x) + torch.sin(0.5*x) + torch.exp(-0.2*x)\neps = torch.randn(200).reshape(-1,1)*0.1  #오차항\ny = underlying + eps \nplt.plot(x,y,'o',alpha=0.5)\nplt.plot(x,underlying,lw=3)\n\n\n\n\n\nh = lambda x: torch.sigmoid(200*(x+0.5))+torch.sigmoid(-200*(x-0.5))-1.0\nclass MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법\n    def __init__(self):\n        super().__init__() \n    def forward(self, input):\n        return h(input) \n\n\nnet= torch.nn.Sequential(\n    torch.nn.Linear(1,2048),\n    MyActivation(),\n    torch.nn.Linear(2048,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters()) \n\n\nfor epoc in range(200):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.2)\nplt.plot(x,underlying,lw=3)\nplt.plot(x,net(x).data,'--')"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#예제2-스펙높아도-취업x",
    "href": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#예제2-스펙높아도-취업x",
    "title": "기계학습 (1012) 6주차",
    "section": "예제2 (스펙높아도 취업X)",
    "text": "예제2 (스펙높아도 취업X)\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex0.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      x\n      underlying\n      y\n    \n  \n  \n    \n      0\n      -1.000000\n      0.000045\n      0.0\n    \n    \n      1\n      -0.998999\n      0.000046\n      0.0\n    \n    \n      2\n      -0.997999\n      0.000047\n      0.0\n    \n    \n      3\n      -0.996998\n      0.000047\n      0.0\n    \n    \n      4\n      -0.995998\n      0.000048\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      1995\n      0.995998\n      0.505002\n      0.0\n    \n    \n      1996\n      0.996998\n      0.503752\n      0.0\n    \n    \n      1997\n      0.997999\n      0.502501\n      0.0\n    \n    \n      1998\n      0.998999\n      0.501251\n      1.0\n    \n    \n      1999\n      1.000000\n      0.500000\n      1.0\n    \n  \n\n2000 rows × 3 columns\n\n\n\n\nx = torch.tensor(df.x).reshape(-1,1).float()\ny = torch.tensor(df.y).reshape(-1,1).float()\nplt.plot(x,y,'o',alpha=0.1)\nplt.plot(df.x,df.underlying,lw=3)\n\n\n\n\n\nh = lambda x: torch.sigmoid(200*(x+0.5))+torch.sigmoid(-200*(x-0.5))-1.0\nclass MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법\n    def __init__(self):\n        super().__init__() \n    def forward(self, input):\n        return h(input) \n\n\ntorch.manual_seed(43052)\nnet= torch.nn.Sequential(\n    torch.nn.Linear(1,2048),\n    MyActivation(),\n    torch.nn.Linear(2048,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters()) \n\n\nfor epoc in range(100):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.2)\nplt.plot(df.x,df.underlying,lw=3)\nplt.plot(x,net(x).data,'--')"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#예제3-mnist-data-with-dnn",
    "href": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#예제3-mnist-data-with-dnn",
    "title": "기계학습 (1012) 6주차",
    "section": "예제3 (MNIST data with DNN)",
    "text": "예제3 (MNIST data with DNN)\n\n# 예비학습\n(예비학습1) Path\n\npath = untar_data(URLs.MNIST) \npath\n\nPath('/home/cgb4/.fastai/data/mnist_png')\n\n\n\npath 도 오브젝트임\npath 도 정보+기능이 있음\n\n- path의 정보\n\npath._str # 숨겨놓았네? #path도 object 동작을 정의하는 기능이 있을거야..\n#path 오브젝트에 저장된 정보(attribute, 기능은 method)\n\n'/home/cgb4/.fastai/data/mnist_png'\n\n\n- 기능1\n\npath.ls()  # path 오브젝트의 안에 있는 목록(폴더)를 보여줘!\n\n(#2) [Path('/home/cgb4/.fastai/data/mnist_png/training'),Path('/home/cgb4/.fastai/data/mnist_png/testing')]\n\n\n- 기능2\n\npath/'training'  #경로를 결합\n\nPath('/home/cgb4/.fastai/data/mnist_png/training')\n\n\n\npath/'testing'\n\nPath('/home/cgb4/.fastai/data/mnist_png/testing')\n\n\n- 기능1과 기능2의 결합\n\n(path/'training/3').ls()\n\n(#6131) [Path('/home/cgb4/.fastai/data/mnist_png/training/3/37912.png'),Path('/home/cgb4/.fastai/data/mnist_png/training/3/12933.png'),Path('/home/cgb4/.fastai/data/mnist_png/training/3/3576.png'),Path('/home/cgb4/.fastai/data/mnist_png/training/3/59955.png'),Path('/home/cgb4/.fastai/data/mnist_png/training/3/23144.png'),Path('/home/cgb4/.fastai/data/mnist_png/training/3/40836.png'),Path('/home/cgb4/.fastai/data/mnist_png/training/3/25536.png'),Path('/home/cgb4/.fastai/data/mnist_png/training/3/42669.png'),Path('/home/cgb4/.fastai/data/mnist_png/training/3/7046.png'),Path('/home/cgb4/.fastai/data/mnist_png/training/3/47380.png')...]\n\n\n\n‘/home/cgb4/.fastai/data/mnist_png/training/3/37912.png’ 이 파일을 더블클릭하면 이미지가 보인단 말임\n\n(예비학습2) plt.imshow\n\n# plt.imshow 값에 따라 밝게 어둡게 보여줌\n\n\nimgtsr = torch.tensor([[1.0,2],[2.0,4.0]])\nimgtsr\n\ntensor([[1., 2.],\n        [2., 4.]])\n\n\n\nplt.imshow(imgtsr,cmap='gray')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x7fceac108e50>\n\n\n\n\n\n(예비학습3) torchvision\n- ’/home/cgb4/.fastai/data/mnist_png/training/3/37912.png’의 이미지파일을 torchvision.io.read_image 를 이용하여 텐서로 만듬\n\n#!s /home/cgb4/.fastai/data/mnist_png/training/3\n#ls아닌가? 뭐지\n\n\nimgtsr = torchvision.io.read_image('/home/cgb4/.fastai/data/mnist_png/training/3/37912.png')\nimgtsr\n\ntensor([[[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  17,  66, 138,\n          149, 180, 138, 138,  86,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,  22, 162, 161, 228, 252, 252,\n          253, 252, 252, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0, 116, 253, 252, 252, 252, 189,\n          184, 110, 119, 252, 252,  32,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,  74, 161, 160,  77,  45,   4,\n            0,   0,  70, 252, 210,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,  22, 205, 252,  32,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0, 162, 253, 245,  21,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           36, 219, 252, 139,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          222, 252, 202,  13,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  43,\n          253, 252,  89,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  85, 240,\n          253, 157,   6,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   7, 160, 253,\n          231,  42,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 142, 252, 252,\n           42,  30,  78, 161,  36,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 184, 252, 252,\n          185, 228, 252, 252, 168,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 184, 252, 252,\n          253, 252, 252, 252, 116,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 101, 179, 252,\n          253, 252, 252, 210,  12,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  22,\n          255, 253, 215,  21,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  34,  89, 244,\n          253, 223,  98,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0, 116, 123, 142, 234, 252, 252,\n          184,  67,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0, 230, 253, 252, 252, 252, 168,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0, 126, 253, 252, 168,  43,   2,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]]],\n       dtype=torch.uint8)\n\n\n- 이 텐서는 (1,28,28)의 shape을 가짐\n\nimgtsr.shape\n\ntorch.Size([1, 28, 28])\n\n\n\n# 1: 채널의 숫자, 28*28은 픽셀의 숫자\n\n- imgtsr를 plt.imshow 로 시각화\n\nplt.imshow(imgtsr.reshape(28,28),cmap='gray')\n\n<matplotlib.image.AxesImage at 0x7fceabd49a90>\n\n\n\n\n\n\n진짜 숫자3이 있음\n\n\n\n# 데이터정리\n- 데이터정리\n\nthrees = (path/'training/3').ls() #6131개, 1,28,28\nsevens = (path/'training/7').ls() #6265개, 1,28,28\nlen(threes),len(sevens)\n\n(6131, 6265)\n\n\n\nX3 = torch.stack([torchvision.io.read_image(str(threes[i])) for i in range(6131)]) #리스트 형태로 만들고.. \nX7 = torch.stack([torchvision.io.read_image(str(sevens[i])) for i in range(6265)])\n\n\n# X3 = torch.stack([torchvision.io.read_image(str(fn)])) for i in three_fnames]) \n# X7 = torch.stack([torchvision.io.read_image(str(fn)])) for i in seven_fnames])\n# 위와 같은 코드\n\n\nX3.shape,X7.shape\n\n(torch.Size([6131, 1, 28, 28]), torch.Size([6265, 1, 28, 28]))\n\n\n\nX=torch.concat([X3,X7]) #n * p 의 shape \n#float로 바꿔줘야함\nX.shape\n\ntorch.Size([12396, 1, 28, 28])\n\n\n\nXnp = X.reshape(-1,1*28*28).float()\nXnp.shape\n\ntorch.Size([12396, 784])\n\n\n\ny = torch.tensor([0.0]*6131 + [1.0]*6265).reshape(-1,1)  # 3을 0으로 7을 1로.. 나중에 sigmoin하기 편하게 하려고  6131대신에 len(X3)이렇게 써도 됨\ny.shape\n\ntorch.Size([12396, 1])\n\n\n\nplt.plot(y,'o')\n\n\n\n\n\n“y=0”은 숫자3을 의미, “y=1”은 숫자7을 의미\n숫자3은 6131개, 숫자7은 6265개 있음\n\n\n\n# 학습\n- 네트워크의 설계\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(  #묶어주기..\n    torch.nn.Linear(in_features=1*28*28,out_features=30),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=30,out_features=1),\n    torch.nn.Sigmoid()\n)\n\n\n\\(\\underset{(n,784)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,30)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,30)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{a_2}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n\nloss_fn = torch.nn.BCELoss()\n\n\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(200):\n    ## 1\n    yhat = net(Xnp) \n    ## 2\n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y,'o')\nplt.plot(net(Xnp).data,'.',alpha=0.2)\n\n\n\n\n\n대부분 잘 적합되었음"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#예제1-undersetn1bf-x-oversetl_1to-undersetn1boldsymbol-u1-oversetsigto-undersetn1boldsymbol-v1-undersetn1hatboldsymbol-y",
    "href": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#예제1-undersetn1bf-x-oversetl_1to-undersetn1boldsymbol-u1-oversetsigto-undersetn1boldsymbol-v1-undersetn1hatboldsymbol-y",
    "title": "기계학습 (1012) 6주차",
    "section": "예제1: \\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(1)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(1)}} =\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)",
    "text": "예제1: \\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(1)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(1)}} =\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n- 모든 observation과 가중치를 명시한 버전\n(표현1)\n\n\nCode\ngv(''' \n    \"1\" -> \"ŵ₀ + xₙ*ŵ₁,    bias=False\"[label=\"* ŵ₀\"]\n    \"xₙ\" -> \"ŵ₀ + xₙ*ŵ₁,    bias=False\"[label=\"* ŵ₁\"]\n    \"ŵ₀ + xₙ*ŵ₁,    bias=False\" -> \"ŷₙ\"[label=\"sigmoid\"]\n\n    \".\" -> \"....................................\"[label=\"* ŵ₀\"]\n    \"..\" -> \"....................................\"[label=\"* ŵ₁\"]\n    \"....................................\" -> \"...\"[label=\" \"]\n\n    \"1 \" -> \"ŵ₀ + x₂*ŵ₁,    bias=False\"[label=\"* ŵ₀\"]\n    \"x₂\" -> \"ŵ₀ + x₂*ŵ₁,    bias=False\"[label=\"* ŵ₁\"]\n    \"ŵ₀ + x₂*ŵ₁,    bias=False\" -> \"ŷ₂\"[label=\"sigmoid\"]\n    \n    \"1  \" -> \"ŵ₀ + x₁*ŵ₁,    bias=False\"[label=\"* ŵ₀\"]\n    \"x₁\" -> \"ŵ₀ + x₁*ŵ₁,    bias=False\"[label=\"* ŵ₁\"]\n    \"ŵ₀ + x₁*ŵ₁,    bias=False\" -> \"ŷ₁\"[label=\"sigmoid\"]\n''')\n\n\n\n\n\n\n단점: 똑같은 그림의 반복이 너무 많음\n\n- observation 반복을 생략한 버전들\n(표현2) 모든 \\(i\\)에 대하여 아래의 그림을 반복한다고 하면 (표현1)과 같다.\n\n\nCode\ngv(''' \n    \"1\" -> \"ŵ₀ + xᵢ*ŵ₁,    bias=False\"[label=\"* ŵ₀\"]\n    \"xᵢ\" -> \"ŵ₀ + xᵢ*ŵ₁,    bias=False\"[label=\"* ŵ₁\"]\n    \"ŵ₀ + xᵢ*ŵ₁,    bias=False\" -> \"ŷᵢ\"[label=\"sigmoid\"]\n\n''')\n\n\n\n\n\n(표현3) 그런데 (표현2)에서 아래와 같이 \\(x_i\\), \\(y_i\\) 대신에 간단히 \\(x\\), \\(y\\)로 쓰는 경우도 많음\n\n\nCode\ngv(''' \n    \"1\" -> \"ŵ₀ + x*ŵ₁,    bias=False\"[label=\"* ŵ₀\"]\n    \"x\" -> \"ŵ₀ + x*ŵ₁,    bias=False\"[label=\"* ŵ₁\"]\n    \"ŵ₀ + x*ŵ₁,    bias=False\" -> \"ŷ\"[label=\"sigmoid\"]\n\n''')\n\n\n\n\n\n- 1을 생략한 버전들\n(표현4) bais=False 대신에 bias=True를 주면 1을 생략할 수 있음\n\n\nCode\ngv('''\n\"x\" -> \"x*ŵ₁,    bias=True\"[label=\"*ŵ₁\"] ;\n\"x*ŵ₁,    bias=True\" -> \"ŷ\"[label=\"sigmoid\"] ''')\n\n\n\n\n\n(표현4의 수정) \\(\\hat{w}_1\\)대신에 \\(\\hat{w}\\)를 쓰는 것이 더 자연스러움\n\n\nCode\ngv('''\n\"x\" -> \"x*ŵ,    bias=True\"[label=\"*ŵ\"] ;\n\"x*ŵ,    bias=True\" -> \"ŷ\"[label=\"sigmoid\"] ''')\n\n\n\n\n\n(표현5) 선형변환의 결과는 아래와 같이 \\(u\\)로 표현하기도 한다.\n\n\nCode\ngv('''\n\"x\" -> \"u\";\n\"u\" -> \"y\"[label=\"sigmoid\"] ''')\n\n\n\n\n\n\n다이어그램은 그리는 사람의 취향에 따라 그리는 방법이 조금씩 다릅니다. 즉 교재마다 달라요."
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#예제2-undersetn1bf-x-oversetl_1to-undersetn2boldsymbol-u1-oversetreluto-undersetn2boldsymbol-v1-oversetl_2to-undersetn1boldsymbol-u2-oversetsigto-undersetn1boldsymbol-v2-undersetn1hatboldsymbol-y",
    "href": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#예제2-undersetn1bf-x-oversetl_1to-undersetn2boldsymbol-u1-oversetreluto-undersetn2boldsymbol-v1-oversetl_2to-undersetn1boldsymbol-u2-oversetsigto-undersetn1boldsymbol-v2-undersetn1hatboldsymbol-y",
    "title": "기계학습 (1012) 6주차",
    "section": "예제2: \\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,2)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,2)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}} =\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)",
    "text": "예제2: \\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,2)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,2)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}} =\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n참고: 코드로 표현\ntorch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=2),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=2,out_features=1),\n    torch.nn.Sigmoid()\n)\n- 이해를 위해서 10월4일 강의노트에서 다루었던 아래의 상황을 고려하자.\n\n(강의노트의 표현)\n\n\nCode\ngv('''\n\"x\" -> \" -x\"[label=\"*(-1)\"];\n\"x\" -> \" x\"[label=\"*1\"]\n\" x\" -> \"rlu(x)\"[label=\"relu\"] \n\" -x\" -> \"rlu(-x)\"[label=\"relu\"] \n\"rlu(x)\" -> \"u\"[label=\"*(-4.5)\"] \n\"rlu(-x)\" -> \"u\"[label=\"*(-9.0)\"] \n\"u\" -> \"sig(u)=yhat\"[label=\"sig\"] \n'''\n)\n\n\n\n\n\n(좀 더 일반화된 표현) 10월4일 강의노트 상황을 일반화하면 아래와 같다.\n\n\nCode\ngv('''\n\"x\" -> \"u1[:,0]\"[label=\"*(-1)\"];\n\"x\" -> \"u1[:,1]\"[label=\"*1\"]\n\"u1[:,0]\" -> \"v1[:,0]\"[label=\"relu\"] \n\"u1[:,1]\" -> \"v1[:,1]\"[label=\"relu\"] \n\"v1[:,0]\" -> \"u2\"[label=\"*(-9.0)\"] \n\"v1[:,1]\" -> \"u2\"[label=\"*(-4.5)\"] \n\"u2\" -> \"v2=yhat\"[label=\"sig\"] \n'''\n)\n\n\n\n\n\n* Layer의 개념: \\({\\bf X}\\)에서 \\(\\hat{\\boldsymbol y}\\)로 가는 과정은 “선형변환+비선형변환”이 반복되는 구조이다. “선형변환+비선형변환”을 하나의 세트로 보면 아래와 같이 표현할 수 있다.\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\left( \\underset{(n,2)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,2)}{\\boldsymbol v^{(1)}} \\right) \\overset{l_2}{\\to} \\left(\\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}\\right), \\quad \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{net({\\bf X})}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n이것을 다이어그램으로 표현한다면 아래와 같다.\n(선형+비선형을 하나의 Layer로 묶은 표현)\n\n\nCode\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -> \"u1[:,0]\"\n    \"X\" -> \"u1[:,1]\"\n    \"u1[:,0]\" -> \"v1[:,0]\"[label=\"relu\"]\n    \"u1[:,1]\" -> \"v1[:,1]\"[label=\"relu\"]\n    label = \"Layer 1\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"v1[:,0]\" -> \"u2\"\n    \"v1[:,1]\" -> \"u2\"\n    \"u2\" -> \"v2=yhat\"[label=\"sigmoid\"]\n    label = \"Layer 2\"\n}\n''')\n\n\n\n\n\nLayer를 세는 방법\n\n정석: 학습가능한 파라메터가 몇층으로 있는지…\n일부 교재 설명: 입력층은 계산하지 않음, activation layer는 계산하지 않음.\n위의 예제의 경우 number of layer = 2 이다.\n\n\n사실 input layer, activation layer 등의 표현을 자주 사용해서 layer를 세는 방법이 처음에는 헷갈립니다..\n\nHidden Layer의 수를 세는 방법\n\nLayer의 수 = Hidden Layer의 수 + 출력층의 수 = Hidden Layer의 수 + 1\n위의 예제의 경우 number of hidden layer = 1 이다.\n\n* node의 개념: \\(u\\to v\\)로 가는 쌍을 간단히 노드라는 개념을 이용하여 나타낼 수 있음.\n(노드의 개념이 포함된 그림)\n\n\nCode\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -> \"node1\"\n    \"X\" -> \"node2\"\n    label = \"Layer 1:relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"yhat \"\n    \"node2\" -> \"yhat \"\n    label = \"Layer 2:sigmoid\"\n}\n''')\n\n\n\n\n\n여기에서 node의 숫자 = feature의 숫자와 같이 이해할 수 있다. 즉 아래와 같이 이해할 수 있다.\n(“number of nodes = number of features”로 이해한 그림)\n\n\nCode\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -> \"feature1\"\n    \"X\" -> \"feature2\"\n    label = \"Layer 1:relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"feature1\" -> \"yhat \"\n    \"feature2\" -> \"yhat \"\n    label = \"Layer 2:sigmoid\"\n}\n''')\n\n\n\n\n\n\n다이어그램의 표현방식은 교재마다 달라서 모든 예시를 달달 외울 필요는 없습니다. 다만 임의의 다이어그램을 보고 대응하는 네트워크를 pytorch로 구현하는 능력은 매우 중요합니다."
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#예제3-undersetn784bf-x-oversetl_1to-undersetn32boldsymbol-u1-oversetreluto-undersetn32boldsymbol-v1-oversetl_1to-undersetn1boldsymbol-u2-oversetsigto-undersetn1boldsymbol-v2undersetn1hatboldsymbol-y",
    "href": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#예제3-undersetn784bf-x-oversetl_1to-undersetn32boldsymbol-u1-oversetreluto-undersetn32boldsymbol-v1-oversetl_1to-undersetn1boldsymbol-u2-oversetsigto-undersetn1boldsymbol-v2undersetn1hatboldsymbol-y",
    "title": "기계학습 (1012) 6주차",
    "section": "예제3: \\(\\underset{(n,784)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,32)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,32)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)",
    "text": "예제3: \\(\\underset{(n,784)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,32)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,32)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n(다이어그램표현)\n\n\nCode\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Input Layer\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -> \"node1\"\n    \"x2\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \n    \"x784\" -> \"node1\"\n    \"x1\" -> \"node2\"\n    \"x2\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"x784\" -> \"node2\"\n    \n    \"x1\" -> \"...\"\n    \"x2\" -> \"...\"\n    \"..\" -> \"...\"\n    \"x784\" -> \"...\"\n\n    \"x1\" -> \"node32\"\n    \"x2\" -> \"node32\"\n    \"..\" -> \"node32\"\n    \"x784\" -> \"node32\"\n\n\n    label = \"Hidden Layer: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n\n    \"node1\" -> \"yhat\"\n    \"node2\" -> \"yhat\"\n    \"...\" -> \"yhat\"\n    \"node32\" -> \"yhat\"\n    \n    label = \"Outplut Layer: sigmoid\"\n}\n''')\n\n\n\n\n\n\nLayer0,1,2 대신에 Input Layer, Hidden Layer, Output Layer로 표현함\n\n- 위의 다이어그램에 대응하는 코드\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=28*28*1,out_features=32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=32,out_features=1),\n    torch.nn.Sigmoid() \n)"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#gpu-사용방법",
    "href": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#gpu-사용방법",
    "title": "기계학습 (1012) 6주차",
    "section": "GPU 사용방법",
    "text": "GPU 사용방법\n- cpu 연산이 가능한 메모리에 데이터 저장\n\ntorch.manual_seed(43052)\nx_cpu = torch.tensor([0.0,0.1,0.2]).reshape(-1,1) #net에 넣어야니까 shape을 바꿔주기\ny_cpu = torch.tensor([0.0,0.2,0.4]).reshape(-1,1) \nnet_cpu = torch.nn.Linear(1,1) \n\n- gpu 연산이 가능한 메모리에 데이터 저장\n\ntorch.manual_seed(43052)\nx_gpu = x_cpu.to(\"cuda:0\")\ny_gpu = y_cpu.to(\"cuda:0\")\nnet_gpu = torch.nn.Linear(1,1).to(\"cuda:0\")  #net_cpu.to(\"cuda:0\") 하게 되면 net_cpu도 gpu로 넘어가게 되므로 그대로 써주면 안뎀 \n\n- cpu 혹은 gpu 연산이 가능한 메모리에 저장된 값들을 확인\n\nx_cpu, y_cpu, net_cpu.weight, net_cpu.bias\n\n(tensor([[0.0000],\n         [0.1000],\n         [0.2000]]),\n tensor([[0.0000],\n         [0.2000],\n         [0.4000]]),\n Parameter containing:\n tensor([[-0.3467]], requires_grad=True),\n Parameter containing:\n tensor([-0.8470], requires_grad=True))\n\n\n\nx_gpu, y_gpu, net_gpu.weight, net_gpu.bias\n\n(tensor([[0.0000],\n         [0.1000],\n         [0.2000]], device='cuda:0'),\n tensor([[0.0000],\n         [0.2000],\n         [0.4000]], device='cuda:0'),\n Parameter containing:\n tensor([[-0.3467]], device='cuda:0', requires_grad=True),\n Parameter containing:\n tensor([-0.8470], device='cuda:0', requires_grad=True))\n\n\n- gpu는 gpu끼리 연산가능하고 cpu는 cpu끼리 연산가능함\n(예시1)\n\nnet_cpu(x_cpu) \n\ntensor([[-0.8470],\n        [-0.8817],\n        [-0.9164]], grad_fn=<AddmmBackward0>)\n\n\n(예시2)\n\nnet_gpu(x_gpu) \n\ntensor([[-0.8470],\n        [-0.8817],\n        [-0.9164]], device='cuda:0', grad_fn=<AddmmBackward0>)\n\n\n(예시3)\n\nnet_cpu(x_gpu) \n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n\n\n(예시4)\n\nnet_gpu(x_cpu)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)\n\n\n(예시5)\n\ntorch.mean((y_cpu-net_cpu(x_cpu))**2)\n\ntensor(1.2068, grad_fn=<MeanBackward0>)\n\n\n(예시6)\n\ntorch.mean((y_gpu-net_gpu(x_gpu))**2)\n\ntensor(1.2068, device='cuda:0', grad_fn=<MeanBackward0>)\n\n\n(예시7)\n\ntorch.mean((y_gpu-net_cpu(x_cpu))**2)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n\n\n(예시8)\n\ntorch.mean((y_cpu-net_gpu(x_gpu))**2)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#시간측정-예비학습",
    "href": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#시간측정-예비학습",
    "title": "기계학습 (1012) 6주차",
    "section": "시간측정 (예비학습)",
    "text": "시간측정 (예비학습)\n\nimport time \n\n\nt1 = time.time()  #현재시각\n\n\nt2 = time.time()\n\n\nt2-t1  # 현재시간 - 현재시간 : 위아래 실행하는 만큼의 초 나옴 \n\n4.9920783042907715"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#cpu-512",
    "href": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#cpu-512",
    "title": "기계학습 (1012) 6주차",
    "section": "CPU (512)",
    "text": "CPU (512)\n- 데이터준비\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1)\ny=torch.randn(100).reshape(-1,1)*0.01\n\n- for문 준비\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(512,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n- for문 + 학습시간측정\n\nt1= time.time()\nfor epoc in range(1000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n0.28586554527282715"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#gpu-512",
    "href": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#gpu-512",
    "title": "기계학습 (1012) 6주차",
    "section": "GPU (512)",
    "text": "GPU (512)\n- 데이터준비\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\ny=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n\n- for문돌릴준비\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(512,1)\n).to(\"cuda:0\")\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n- for문 + 학습시간측정\n\nt1= time.time()\nfor epoc in range(1000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n0.5355696678161621\n\n\n\n!! CPU가 더 빠르다?"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#cpu-vs-gpu-20480",
    "href": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#cpu-vs-gpu-20480",
    "title": "기계학습 (1012) 6주차",
    "section": "CPU vs GPU (20480)",
    "text": "CPU vs GPU (20480)\n- CPU (20480)\n\n#은닉충의 노드수: 20480\n\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1)\ny=torch.randn(100).reshape(-1,1)*0.01\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,20480),\n    torch.nn.ReLU(),\n    torch.nn.Linear(20480,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nt1= time.time()\nfor epoc in range(1000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n2.380666494369507\n\n\n- GPU (20480)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\ny=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,20480),\n    torch.nn.ReLU(),\n    torch.nn.Linear(20480,1)\n).to(\"cuda:0\")\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nt1= time.time()\nfor epoc in range(1000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n0.5442469120025635\n\n\n- 왜 이런 차이가 나는가? 연산을 하는 주체는 코어인데 CPU는 수는 적지만 일을 잘하는 코어들을 가지고 있고 GPU는 일은 못하지만 다수의 코어를 가지고 있기 때문"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#cpu-vs-gpu-204800",
    "href": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#cpu-vs-gpu-204800",
    "title": "기계학습 (1012) 6주차",
    "section": "CPU vs GPU (204800)",
    "text": "CPU vs GPU (204800)\n- CPU (204800)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1)\ny=torch.randn(100).reshape(-1,1)*0.01\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,204800),\n    torch.nn.ReLU(),\n    torch.nn.Linear(204800,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nt1= time.time()\nfor epoc in range(1000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n51.95550894737244\n\n\n- GPU (204800)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\ny=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,204800),\n    torch.nn.ReLU(),\n    torch.nn.Linear(204800,1)\n).to(\"cuda:0\")\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nt1= time.time()\nfor epoc in range(1000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n1.3824031352996826"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#좀-이상하지-않아요",
    "href": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#좀-이상하지-않아요",
    "title": "기계학습 (1012) 6주차",
    "section": "좀 이상하지 않아요?",
    "text": "좀 이상하지 않아요?\n- 우리가 쓰는 GPU: 다나와 PC견적 - GPU 메모리 끽해봐야 24GB\n- 우리가 분석하는 데이터: 빅데이터..?\n- 데이터의 크기가 커지는순간 X.to(\"cuda:0\"), y.to(\"cuda:0\") 쓰면 난리나겠는걸?\n\nx = torch.linspace(-10,10,100000).reshape(-1,1)\neps = torch.randn(100000).reshape(-1,1)\ny = x*2 + eps \n\n\nplt.plot(x,y,'o',alpha=0.05)\nplt.plot(x,2*x)\n\n\n\n\n- 데이터를 100개중에 1개만 꼴로만 쓰면 어떨까?\n\nplt.plot(x[::100],y[::100],'o',alpha=0.05)\nplt.plot(x,2*x)\n\n\n\n\n\n대충 이거만 가지고 적합해도 충분히 정확할것 같은데"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#xy-데이터를-굳이-모두-gpu에-넘겨야-하는가",
    "href": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#xy-데이터를-굳이-모두-gpu에-넘겨야-하는가",
    "title": "기계학습 (1012) 6주차",
    "section": "X,y 데이터를 굳이 모두 GPU에 넘겨야 하는가?",
    "text": "X,y 데이터를 굳이 모두 GPU에 넘겨야 하는가?\n- 데이터셋을 짝홀로 나누어서 번갈아가면서 GPU에 올렸다 내렸다하면 안되나?\n- 아래의 알고리즘을 생각해보자.\n\n데이터를 반으로 나눈다.\n짝수obs의 x,y 그리고 net의 모든 파라메터를 GPU에 올린다.\nyhat, loss, grad, update 수행\n짝수obs의 x,y를 GPU메모리에서 내린다. 그리고 홀수obs의 x,y를 GPU메모리에 올린다.\nyhat, loss, grad, update 수행\n홀수obs의 x,y를 GPU메모리에서 내린다. 그리고 짝수obs의 x,y를 GPU메모리에 올린다.\n반복"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#경사하강법-확률적경사하강법-미니배치-경사하강법",
    "href": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#경사하강법-확률적경사하강법-미니배치-경사하강법",
    "title": "기계학습 (1012) 6주차",
    "section": "경사하강법, 확률적경사하강법, 미니배치 경사하강법",
    "text": "경사하강법, 확률적경사하강법, 미니배치 경사하강법\n10개의 샘플이 있다고 가정. \\(\\{(x_i,y_i)\\}_{i=1}^{10}\\)\n- ver1: 모든 샘플을 이용하여 slope 계산\n(epoch1) \\(loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\)\n(epoch2) \\(loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\)\n…\n- ver2: 하나의 샘플만을 이용하여 slope 계산\n(epoch1) - \\(loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\to slope \\to update\\) - \\(loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\to slope \\to update\\) - … - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\to slope \\to update\\)\n(epoch2) - \\(loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\to slope \\to update\\) - \\(loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\to slope \\to update\\) - … - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\to slope \\to update\\)\n…\n- ver3: \\(m (\\leq n)\\) 개의 샘플을 이용하여 slope 계산\n\\(m=3\\)이라고 하자.\n(epoch1) - \\(loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\) - \\(loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\) - \\(loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\) - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\to slope \\to update\\)\n(epoch2) - \\(loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\) - \\(loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\) - \\(loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\) - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\to slope \\to update\\)\n…"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#용어의-정리",
    "href": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#용어의-정리",
    "title": "기계학습 (1012) 6주차",
    "section": "용어의 정리",
    "text": "용어의 정리\n옛날\n- ver1: gradient descent, batch gradient descent\n- ver2: stochastic gradient descent\n- ver3: mini-batch gradient descent, mini-batch stochastic gradient descent\n\n# gpu메모리가 떨어져서 ver1은 못쓴당.. ver2는 불안한 느낌 for문이 너무 많이 돌ㄹ아가->느림..\n\n요즘\n- ver1: gradient descent\n- ver2: stochastic gradient descent with batch size = 1\n- ver3: stochastic gradient descent - https://www.deeplearningbook.org/contents/optimization.html, 알고리즘 8-1 참고."
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#ds-dl",
    "href": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#ds-dl",
    "title": "기계학습 (1012) 6주차",
    "section": "ds, dl",
    "text": "ds, dl\n\n# 데이터셋\n\n- ds\n\nx=torch.tensor(range(10)).float()#.reshape(-1,1) reshape원래는 해야는데 보여주기 위해서 생략쓰,,\ny=torch.tensor([1.0]*5+[0.0]*5)#.reshape(-1,1)\n\n\nds=torch.utils.data.TensorDataset(x,y)\nds\n\n<torch.utils.data.dataset.TensorDataset at 0x7f62db294710>\n\n\n\nds.tensors # 그냥 (x,y)의 튜플\n\n(tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]),\n tensor([1., 1., 1., 1., 1., 0., 0., 0., 0., 0.]))\n\n\n\n# 데이터로더\n\n- dl\n\ndl=torch.utils.data.DataLoader(ds,batch_size=3) #batch_size: 3개씩 묶어서 배치를 해줌\n#set(dir(dl)) & {'__iter__'}\n\n#dir(dl):숨겨진 습성 \n#dl.__ : 숨겨진 습성\n# iter 오브젝트: for문에 돌릴수 있다는 특성..!\n\n\nfor xx,yy in dl:  #in 뒤에 iter오브젝트는 다 쓸수 있음\n    print(xx,yy)\n\ntensor([0., 1., 2.]) tensor([1., 1., 1.])\ntensor([3., 4., 5.]) tensor([1., 1., 0.])\ntensor([6., 7., 8.]) tensor([0., 0., 0.])\ntensor([9.]) tensor([0.])"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#ds-dl을-이용한-mnist-구현",
    "href": "posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.html#ds-dl을-이용한-mnist-구현",
    "title": "기계학습 (1012) 6주차",
    "section": "ds, dl을 이용한 MNIST 구현",
    "text": "ds, dl을 이용한 MNIST 구현\n- 데이터정리\n\npath = untar_data(URLs.MNIST)\n\n\nzero_fnames = (path/'training/0').ls()\none_fnames = (path/'training/1').ls()\n\n\nX0 = torch.stack([torchvision.io.read_image(str(zf)) for zf in zero_fnames])\nX1 = torch.stack([torchvision.io.read_image(str(of)) for of in one_fnames])\nX = torch.concat([X0,X1],axis=0).reshape(-1,1*28*28)/255  #255로나누는 이유 숙제\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\n\nX.shape,y.shape\n\n(torch.Size([12665, 784]), torch.Size([12665, 1]))\n\n\n- ds \\(\\to\\) dl\n\nds = torch.utils.data.TensorDataset(X,y)\ndl = torch.utils.data.DataLoader(ds,batch_size=2048) \n\n\n12665/2048\n\n6.18408203125\n\n\n\ni = 0 \nfor xx,yy in dl: # 총 7번 돌아가는 for문 \n    print(i)\n    i=i+1\n\n0\n1\n2\n3\n4\n5\n6\n\n\n- 미니배치 안쓰는 학습\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(70): \n    ## 1 \n    yhat = net(X) \n    ## 2 \n    loss= loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n\ntorch.sum((yhat>0.5) == y) / len(y) \n# 분자: 전체 데이터 중 잘 맞춘게 몇개인지.\n# 분모: y의 갯수???\n# torch.mean((yhat>0.5) == y)*1.0) 계산하면 위와 같음\n\ntensor(0.9981)\n\n\n- 미니배치 쓰는 학습 (GPU 올리고 내리는 과정은 생략)\n\n# len(y)/2048 = 6.18408203125 \n# 1~2048, 2049~ 하면 6번 조금넘게나옴 (7번)\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(10):\n    for xx,yy in dl: ## 7번\n        ## 1\n        #yhat = net(xx)\n        ## 2 \n        loss = loss_fn(net(xx),yy) \n        ## 3 \n        loss.backward() \n        ## 4 \n        optimizr.step()\n        optimizr.zero_grad()\n\n\ntorch.mean(((net(X)>0.5) == y)*1.0)\n\ntensor(0.9950)"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_28_(4주차)_9월 28일__ipynb의_사본.html",
    "href": "posts/Machine Learning/1. DNN/2022_09_28_(4주차)_9월 28일__ipynb의_사본.html",
    "title": "기계학습 (0928) 4주차",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt \nimport pandas as pd\nimport torch"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_28_(4주차)_9월 28일__ipynb의_사본.html#numpy-torch-선택학습",
    "href": "posts/Machine Learning/1. DNN/2022_09_28_(4주차)_9월 28일__ipynb의_사본.html#numpy-torch-선택학습",
    "title": "기계학습 (0928) 4주차",
    "section": "numpy, torch (선택학습)",
    "text": "numpy, torch (선택학습)\n\nnumpy, torch는 엄청 비슷해요\n- torch.tensor() = np.array() 처럼 생각해도 무방\n\nnp.array([1,2,3]), torch.tensor([1,2,3])\n\n(array([1, 2, 3]), tensor([1, 2, 3]))\n\n\n- 소수점의 정밀도에서 차이가 있음 (torch가 좀 더 쪼잔함)\n\nnp.array([3.123456789])\n\narray([3.12345679])\n\n\n\ntorch.tensor([3.123456789]) #GPU메모리에 저장해서 \n\ntensor([3.1235])\n\n\n- 기본적인 numpy 문법은 np 대신에 torch를 써도 무방 // 완전 같지는 않음\n\nnp.arange(10), torch.arange(10)\n\n(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n\n\n\nnp.linspace(0,1,10), torch.linspace(0,1,10)\n\n(array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n        0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]),\n tensor([0.0000, 0.1111, 0.2222, 0.3333, 0.4444, 0.5556, 0.6667, 0.7778, 0.8889,\n         1.0000]))\n\n\n\nnp.random.randn(10)\n\narray([ 0.68732684, -0.53367188,  0.27916096,  0.28236708,  0.03800702,\n       -0.66236923,  1.32472364, -0.11671166, -0.77019834, -1.14755872])\n\n\n\ntorch.randn(10)\n\ntensor([ 0.8525,  0.2257,  0.3406, -0.4713,  1.5393, -2.0060, -0.4257,  3.0482,\n        -0.7659,  0.3265])\n\n\n\n\nlength \\(n\\) vector, \\(n \\times 1\\) col-vector, \\(1 \\times n\\) row-vector\n- 길이가 3인 벡터 선언방법\n\na = torch.tensor([1,2,3])\na.shape\n\ntorch.Size([3])\n\n\n- 3x1 col-vec 선언방법\n(방법1)\n\na = torch.tensor([[1],[2],[3]])\na.shape\n\ntorch.Size([3, 1])\n\n\n(방법2)\n\na = torch.tensor([1,2,3]).reshape(3,1)\na.shape\n\ntorch.Size([3, 1])\n\n\n- 1x3 row-vec 선언방법\n(방법1)\n\na = torch.tensor([[1,2,3]])\na.shape\n\ntorch.Size([1, 3])\n\n\n(방법2)\n\na = torch.tensor([1,2,3]).reshape(1,3)\na.shape\n\ntorch.Size([1, 3])\n\n\n- 3x1 col-vec 선언방법, 1x3 row-vec 선언방법에서 [[1],[2],[3]] 혹은 [[1,2,3]] 와 같은 표현이 이해안되면 아래링크로 가셔서\nhttps://guebin.github.io/STBDA2022/2022/03/14/(2주차)-3월14일.html\n첫번째 동영상 12:15 - 22:45 에 해당하는 분량을 학습하시길 바랍니다.\n\n\ntorch의 dtype\n- 기본적으로 torch는 소수점으로 저장되면 dtype=torch.float32 가 된다. (이걸로 맞추는게 편리함)\n\ntsr = torch.tensor([1.23,2.34])\ntsr\n\ntensor([1.2300, 2.3400])\n\n\n\ntsr.dtype\n\ntorch.float32\n\n\n\n#float64보다 데이터를 적게 쓴다는 뜻-> float32\n\n- 정수로 선언하더라도 dtype를 torch.float32로 바꾸는게 유리함\n(안 좋은 선언예시)\n\ntsr = torch.tensor([1,2])\ntsr \n\ntensor([1, 2])\n\n\n\ntsr.dtype\n\ntorch.int64\n\n\n(좋은 선언예시1)\n\ntsr = torch.tensor([1,2],dtype=torch.float32)\ntsr \n\ntensor([1., 2.])\n\n\n\ntsr.dtype\n\ntorch.float32\n\n\n(좋은 선언예시2)\n\ntsr = torch.tensor([1,2.0])\ntsr \n\ntensor([1., 2.])\n\n\n\ntsr.dtype\n\ntorch.float32\n\n\n(사실 int로 선언해도 나중에 float으로 바꾸면 큰 문제없음)\n\ntsr = torch.tensor([1,2]).float()\ntsr\n\ntensor([1., 2.])\n\n\n\ntsr.dtype\n\ntorch.float32\n\n\n- 왜 정수만으로 torch.tensor를 만들때에도 torch.float32로 바꾸는게 유리할까? \\(\\to\\) torch.tensor끼리의 연산에서 문제가 될 수 있음\n별 문제 없을수도 있지만\n\ntorch.tensor([1,2])-torch.tensor([1.0,2.0]) \n\ntensor([0., 0.])\n\n\n아래와 같이 에러가 날수도 있다\n(에러1)\n\ntorch.tensor([[1.0,0.0],[0.0,1.0]]) @ torch.tensor([[1],[2]]) \n\nRuntimeError: expected scalar type Float but found Long\n\n\n(에러2)\n\ntorch.tensor([[1,0],[0,1]]) @ torch.tensor([[1.0],[2.0]])\n\nRuntimeError: expected scalar type Long but found Float\n\n\n(해결1) 둘다 정수로 통일\n\ntorch.tensor([[1,0],[0,1]]) @ torch.tensor([[1],[2]])\n\ntensor([[1],\n        [2]])\n\n\n(해결2) 둘다 소수로 통일 <– 더 좋은 방법임\n\ntorch.tensor([[1.0,0.0],[0.0,1.0]]) @ torch.tensor([[1.0],[2.0]])\n\ntensor([[1.],\n        [2.]])\n\n\n\n\nshape of vector\n- 행렬곱셈에 대한 shape 조심\n\nA = torch.tensor([[2.00,0.00],[0.00,3.00]]) \nb1 = torch.tensor([[-1.0,-5.0]])\nb2 = torch.tensor([[-1.0],[-5.0]])\nb3 = torch.tensor([-1.0,-5.0])\n\n\nA.shape,b1.shape,b2.shape,b3.shape\n\n(torch.Size([2, 2]), torch.Size([1, 2]), torch.Size([2, 1]), torch.Size([2]))\n\n\n- A@b1: 계산불가, b1@A: 계산가능\n\nA@b1 #행렬계산이라고 생각\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (2x2 and 1x2)\n\n\n\nb1@A\n\ntensor([[ -2., -15.]])\n\n\n- A@b2: 계산가능, b2@A: 계산불가\n\nA@b2\n\ntensor([[ -2.],\n        [-15.]])\n\n\n\nb2@A\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (2x1 and 2x2)\n\n\n- A@b3: 계산가능, b3@A: 계산가능\n\n(A@b3).shape ## b3를 마치 col-vec 처럼 해석\n\ntorch.Size([2])\n\n\n\n(b3@A).shape ## b3를 마지 row-vec 처럼 해석\n\ntorch.Size([2])\n\n\n- 브로드캐스팅\n\na = torch.tensor([1,2,3]) #a는 길이가 3인 벡터지만... 연산이 된다.\na - 1\n\ntensor([0, 1, 2])\n\n\n\nb = torch.tensor([[1],[2],[3]]) #b는 컬럼 벡터\nb - 1\n\ntensor([[0],\n        [1],\n        [2]])\n\n\n\na - b # a를 row-vec 로 해석 \n#불필요한 오류를 막기 위해서 dimension잘 써놓기\n\ntensor([[ 0,  1,  2],\n        [-1,  0,  1],\n        [-2, -1,  0]])"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_28_(4주차)_9월 28일__ipynb의_사본.html#review-step14",
    "href": "posts/Machine Learning/1. DNN/2022_09_28_(4주차)_9월 28일__ipynb의_사본.html#review-step14",
    "title": "기계학습 (0928) 4주차",
    "section": "Review: step1~4",
    "text": "Review: step1~4\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-22-regression.csv\") \ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      -2.482113\n      -8.542024\n    \n    \n      1\n      -2.362146\n      -6.576713\n    \n    \n      2\n      -1.997295\n      -5.949576\n    \n    \n      3\n      -1.623936\n      -4.479364\n    \n    \n      4\n      -1.479192\n      -4.251570\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      95\n      2.244400\n      10.325987\n    \n    \n      96\n      2.393501\n      12.266493\n    \n    \n      97\n      2.605604\n      13.098280\n    \n    \n      98\n      2.605658\n      12.546793\n    \n    \n      99\n      2.663240\n      13.834002\n    \n  \n\n100 rows × 2 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ntorch.tensor(df.x)\n#dtype=float32로 지정하면 밑에 dtype=torch.float64가 안붙는다. 메모리를 아끼기위해서 데이터타입을 float32로바꾼다리\n\ntensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632], dtype=torch.float64)\n\n\n\nx= torch.tensor(df.x,dtype=torch.float32).reshape(100,1)   \ny= torch.tensor(df.y,dtype=torch.float32).reshape(100,1)\n\n# _1 = torch.ones([100,1])\n# X = torch.concat([_1,x]),axis=1\n\nX= torch.tensor([[1]*100,x]).T    #torch.ones([100,1])로 써도 됨\n\n\n\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True) # 아무 점이나 주어보자! (-5,10)\n\n\nWhat\n\ntensor([[-5.],\n        [10.]], requires_grad=True)\n\n\n\nplt.plot(x,y,'o')\n#plt.plot(x,-5+10*x,'--')\nplt.plot(x,X@What.data,'--')\n\n\n\n\n\nver1: loss = sum of squares error\n\nalpha = 1/1000    #학습하는과정에 대한 분류 4가지\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nfor epoc in range(30): \n    # step1: yhat \n    yhat = X@What \n    # step2: loss \n    loss = torch.sum((y-yhat)**2)\n    # step3: 미분 \n    loss.backward()\n    # step4: update \n  #  What.data = What.data - 1/000 * What.grad   # alpha = 1/000\n  #  What.grad = None #              # gradient청소...\n\n    What.data = What.data - alpha * What.grad \n    What.grad = None # \n\n\nWhat\n\ntensor([[2.4290],\n        [4.0144]], requires_grad=True)\n\n\n\nplt.plot(x,y,'o') \nplt.plot(x,X@What.data,'--')\n\n\n\n\n\nnote: 왜 What = What - alpha*What.grad 는 안되는지?\n\n\n\nver2: loss = mean squared error = MSE\n\nalpha = 1/10\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nfor epoc in range(30): \n    # step1: yhat \n    yhat = X@What \n    # step2: loss \n    loss = torch.mean((y-yhat)**2)   # 위랑 다른거 여기 mean!!!! \n    # step3: 미분 \n    loss.backward()\n    # step4: update \n    What.data = What.data - alpha * What.grad \n    What.grad = None # \n\n    # mean으로 하면 좋은거: 100개읟 ㅔ이터 1/1000 학습률 \n    # sample size가 달라질때마다 학습률 설정이 힘든데, mean으로 하면 데이터set이 계속 할수잇어서!!\n\n\nWhat\n\ntensor([[2.4290],\n        [4.0144]], requires_grad=True)"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_28_(4주차)_9월 28일__ipynb의_사본.html#step1의-다른버전-net-설계만",
    "href": "posts/Machine Learning/1. DNN/2022_09_28_(4주차)_9월 28일__ipynb의_사본.html#step1의-다른버전-net-설계만",
    "title": "기계학습 (0928) 4주차",
    "section": "step1의 다른버전 – net 설계만",
    "text": "step1의 다른버전 – net 설계만\n\nver1: net = torch.nn.Linear(1,1,bias=True)\n\ntorch.manual_seed(43052)\nnet = torch.nn.Linear(in_features=1, out_features=1, bias=True)  # 함수를 만들어준다. x가들어가면 y가 나오는 것 가틍ㄴ..\n\n# x.shape 했을때 torch.size(100,1) 이 나온다. 100은 observation이고 뒤쪽에 있는 1이 in_features!!\n# out_features는 y.shape의 뒤쪽,,\n\n# net.bias, net.weight 하면 tensor 0.2366 -> w0역할... tensor -0.8791 -> w1역할 \n# 위 숫자는 최초의 숫자라 아무거나 찍은거 실행할때마다 달라질수 있음.\n# 맨 위에 seed를 주면 나중에 교수님 강의할때 편하게` 하려고 \n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n- net에서 \\(\\hat{w}_0, \\hat{w}_1\\) 의 값은?\n\nnet.weight # w1 \n\nParameter containing:\ntensor([[-0.3467]], requires_grad=True)\n\n\n\nnet.bias # w0 \n\nParameter containing:\ntensor([-0.8470], requires_grad=True)\n\n\n\n_yhat = -0.8470 + -0.3467*x \n\n\nplt.plot(x,y,'o')\nplt.plot(x, _yhat,'--')\nplt.plot(x,net(x).data,'-.')\n\n\n\n\n- 수식표현: \\(\\hat{y}_i = \\hat{w}_0 + \\hat{w}_1 x_i = \\hat{b} + \\hat{w}x_i = -0.8470 + -0.3467 x_i\\) for all \\(i=1,2,\\dots,100\\).\n\n\nver2: net = torch.nn.Linear(2,1,bias=False)\n- 입력이 x가 아닌 X를 넣고 싶다면? (보통 잘 안하긴 해요, 왜? bias=False로 주는게 귀찮거든요) - X는 바이어스가 고려된 상황\n\nnet(X) ## 그대로 쓰면 당연히 에러\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (100x2 and 1x1)\n\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Linear(in_features=2, out_features=1, bias=False) #bias=false:뒤쪽에 더해지는 값인거 같으니까....\n\n# out_features=3으로 쓰면 shape이 [100,3] 된다,,,,,,,,,,, 1이 되야해,,\n\n\nnet.weight\n\nParameter containing:\ntensor([[-0.2451, -0.5989]], requires_grad=True)\n\n\n\nnet.bias # false로 설정해서 아무것도 안뜸\n\n\nplt.plot(x,y,'o') \nplt.plot(x,net(X).data, '--')\nplt.plot(x,X@torch.tensor([[-0.2451],[-0.5989]]), '-.')\n\n\n\n\n- 수식표현: \\(\\hat{\\bf y} = {\\bf X} {\\bf \\hat W} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots & \\dots \\\\ 1 & x_{100} \\end{bmatrix} \\begin{bmatrix} -0.2451 \\\\ -0.5989 \\end{bmatrix}\\)\n\n\n잘못된사용1\n\n_x = x.reshape(-1)\n\n\n_x\n\ntensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632])\n\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Linear(in_features=1,out_features=1) \n\n\nnet(_x) #이렇게 하면 에러메시지뜬다리 \n# net(_x.reshape(100,1))로 바궈줘야 한다.\n\nRuntimeError: size mismatch, got 1, 1x1,100\n\n\n\n\n잘못된사용2\n\ntorch.manual_seed(43052)\nnet = torch.nn.Linear(in_features=2,out_features=1) # bias=False를 깜빡.. bias=true로 설정됨 기본으로 \n\n\nnet.weight\n\nParameter containing:\ntensor([[-0.2451, -0.5989]], requires_grad=True)\n\n\n\nnet.bias\n\nParameter containing:\ntensor([0.2549], requires_grad=True)\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(X).data,'--')\nplt.plot(x,X@torch.tensor([[-0.2451],[-0.5989]])+0.2549,'-.')\n# b hat = 0.2549 의도와는 다르게 모델링 된것..\n\n\n# plt.plot(x,X@torch.tensor([[-0.2451],[-0.5989]]),'-.')\n# bias=f일때\n\n\n\n\n\n수식표현: \\(\\hat{\\bf y} = {\\bf X} {\\bf \\hat W} + \\hat{b}= \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots & \\dots \\\\ 1 & x_{100} \\end{bmatrix} \\begin{bmatrix} -0.2451 \\\\ -0.5989 \\end{bmatrix} + 0.2549\\)"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_28_(4주차)_9월 28일__ipynb의_사본.html#step1의-다른버전-끝까지",
    "href": "posts/Machine Learning/1. DNN/2022_09_28_(4주차)_9월 28일__ipynb의_사본.html#step1의-다른버전-끝까지",
    "title": "기계학습 (0928) 4주차",
    "section": "step1의 다른버전 – 끝까지",
    "text": "step1의 다른버전 – 끝까지\n\nver1: net = torch.nn.Linear(1,1,bias=True)\n- 준비\n\nnet = torch.nn.Linear(1,1,bias=True) # in_features=1 에서 1만 써도 뎀, bias 생략해도 뎀\nnet.weight.data = torch.tensor([[10.0]])\nnet.bias.data = torch.tensor([-5.0])\nnet.weight,net.bias\n\n(Parameter containing:\n tensor([[10.]], requires_grad=True),\n Parameter containing:\n tensor([-5.], requires_grad=True))\n\n\n- step1\n\nyhat = net(x)  # -5 + 10x 가 첫 값으로 나올것,,,\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n- step2\n\nloss = torch.mean((y-yhat)**2)\n\n- step3\n(미분전)\n\nnet.bias,net.weight\n\n(Parameter containing:\n tensor([-5.], requires_grad=True),\n Parameter containing:\n tensor([[10.]], requires_grad=True))\n\n\n\nnet.bias.grad, net.weight.grad   #grad값이 없는데.... \n\n(None, None)\n\n\n(미분)\n\nloss.backward()\n\n(미분후)\n\nnet.bias,net.weight\n\n(Parameter containing:\n tensor([-5.], requires_grad=True),\n Parameter containing:\n tensor([[10.]], requires_grad=True))\n\n\n\nnet.bias.grad,net.weight.grad    # 미분후에 값 자체는 변화가 없지만 grad값이 \n\n(tensor([-13.4225]), tensor([[11.8893]]))\n\n\n- step4\n(업데이트전)\n\nnet.bias,net.weight\n\n(Parameter containing:\n tensor([-5.], requires_grad=True),\n Parameter containing:\n tensor([[10.]], requires_grad=True))\n\n\n\nnet.bias.grad, net.weight.grad\n\n(tensor([-13.4225]), tensor([[11.8893]]))\n\n\n(업데이트)\n\nnet.bias.data = net.bias.data - 0.1*net.bias.grad   # 기울기 0.1\nnet.weight.data = net.weight.data - 0.1*net.weight.grad \n\n\nnet.bias.grad = None  # 바뀌기만 하고 청소가 안된상태ㅣ니까 none값으로 지정해주기...\nnet.weight.grad = None \n\n(업데이트후)\n\nnet.bias,net.weight\n\n(Parameter containing:\n tensor([-3.6577], requires_grad=True),\n Parameter containing:\n tensor([[8.8111]], requires_grad=True))\n\n\n\nnet.bias.grad, net.weight.grad\n\n(None, None)\n\n\n- 반복\n\nfor epoc in range(30):\n    # step1\n    yhat = net(x) \n    # step2\n    loss = torch.mean((y-yhat)**2)\n    # step3\n    loss.backward()\n    # step4\n    net.weight.data = net.weight.data - 0.1*net.weight.grad\n    net.bias.data = net.bias.data - 0.1*net.bias.grad\n    net.weight.grad = None\n    net.bias.grad = None\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\nver2: net = torch.nn.Linear(2,1,bias=False)\n- 준비\n\nnet = torch.nn.Linear(2,1,bias=False)\nnet.weight.data = torch.tensor([[-5.0, 10.0]])\n\n- step1\n\nyhat = net(X)\n\n- step2\n\nloss = torch.mean((y-yhat)**2)\n\n- step3\n(미분전)\n\nnet.weight\n\nParameter containing:\ntensor([[-5., 10.]], requires_grad=True)\n\n\n\nnet.weight.grad\n\n(미분)\n\nloss.backward()\n\n(미분후)\n\nnet.weight\n\nParameter containing:\ntensor([[-5., 10.]], requires_grad=True)\n\n\n\nnet.weight.grad\n\ntensor([[-13.4225,  11.8893]])\n\n\n- step4\n(업데이트전)\n\nnet.weight\n\nParameter containing:\ntensor([[-5., 10.]], requires_grad=True)\n\n\n\nnet.weight.grad\n\ntensor([[-13.4225,  11.8893]])\n\n\n(업데이트)\n\nnet.weight.data = net.weight.data - 0.1*net.weight.grad\n\n\nnet.weight.grad = None\n\n(업데이트후)\n\nnet.weight\n\nParameter containing:\ntensor([[-3.6577,  8.8111]], requires_grad=True)\n\n\n\nnet.weight.grad\n\n- 반복\n\nnet = torch.nn.Linear(2,1,bias=False)\nnet.weight.data = torch.tensor([[-5.0, 10.0]])\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(X).data,'--')\n\n\n\n\n\nfor epoc in range(30):\n    # step1\n    yhat = net(X)\n    # step2 \n    loss = torch.mean((y-yhat)**2)\n    # step3\n    loss.backward()\n    # step4\n    net.weight.data = net.weight.data - 0.1*net.weight.grad\n    net.weight.grad = None\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(X).data,'--')"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_28_(4주차)_9월 28일__ipynb의_사본.html#step4의-다른버전-옵티마이저",
    "href": "posts/Machine Learning/1. DNN/2022_09_28_(4주차)_9월 28일__ipynb의_사본.html#step4의-다른버전-옵티마이저",
    "title": "기계학습 (0928) 4주차",
    "section": "step4의 다른버전: 옵티마이저!",
    "text": "step4의 다른버전: 옵티마이저!\n\nver1: net = torch.nn.Linear(1,1,bias=True)\n- 준비\n\nnet = torch.nn.Linear(1,1) \nnet.weight.data = torch.tensor([[10.0]]) \nnet.bias.data = torch.tensor([[-5.0]]) \n\n\noptimizr = torch.optim.SGD(net.parameters(),lr=1/10) # step4가 너무 귀찮기 때문에 net파라미터를 받아서 업데이트하고 청소해주는 오브젝트를 하나 만들기\n# optim.SGD(parameter, lr(alpha)=0.1)\n# net.parameters() = generator 어쩌고 튀어나오는데 이거 넣어주기 \n\n- step1~3\n\nyhat = net(x)     \n\n\nloss = torch.mean((y-yhat)**2) \n\n\nloss.backward() \n\n- step4\n(update 전)\n\nnet.weight.data, net.bias.data ## 값은 업데이트 전\n\n(tensor([[10.]]), tensor([[-5.]]))\n\n\n\nnet.weight.grad, net.bias.grad ## 미분값은 청소전 \n\n(tensor([[11.8893]]), tensor([[-13.4225]]))\n\n\n(update)\n\noptimizr.step()  # update 진행해줌\noptimizr.zero_grad() # grad값 청소\n\n(update 후)\n\nnet.weight.data, net.bias.data ## 값은 업데이트 되었음 \n\n(tensor([[8.8111]]), tensor([[-3.6577]]))\n\n\n\nnet.weight.grad, net.bias.grad ## 미분값은 0으로 초기화하였음 \n\n(tensor([[0.]]), tensor([[0.]]))\n\n\n- 반복\n\nnet = torch.nn.Linear(1,1) \nnet.weight.data = torch.tensor([[10.0]])\nnet.bias.data = torch.tensor([-5.0])\noptimizr = torch.optim.SGD(net.parameters(),lr=1/10) \n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')\n\n\nfor epoc in range(30): \n    # step1\n    yhat = net(x)\n    # step2\n    loss = torch.mean((y-yhat)**2) \n    # step3\n    loss.backward()\n    # step4 \n    optimizr.step(); optimizr.zero_grad() \n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\nver2: net = torch.nn.Linear(2,1,bias=False)\n- 바로 반복하겠습니다..\n\nnet = torch.nn.Linear(2,1,bias=False) \nnet.weight.data = torch.tensor([[-5.0, 10.0]])\noptimizr = torch.optim.SGD(net.parameters(),lr=1/10) \n\n\nfor epoc in range(30): \n    yhat = net(X)              # ver1에서는 스몰x였는데 여기서는 라지X\n    loss = torch.mean((y-yhat)**2) \n    loss.backward() \n    optimizr.step(); optimizr.zero_grad() \n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(X).data,'--')"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_28_(4주차)_9월 28일__ipynb의_사본.html#appendix-net.parameters의-의미-선택학습",
    "href": "posts/Machine Learning/1. DNN/2022_09_28_(4주차)_9월 28일__ipynb의_사본.html#appendix-net.parameters의-의미-선택학습",
    "title": "기계학습 (0928) 4주차",
    "section": "Appendix: net.parameters()의 의미? (선택학습)",
    "text": "Appendix: net.parameters()의 의미? (선택학습)\n- iterator, generator의 개념필요 - https://guebin.github.io/IP2022/2022/06/06/(14주차)-6월6일.html, 클래스공부 8단계 참고\n- 탐구시작: 네트워크 생성\n\nnet = torch.nn.Linear(in_features=1,out_features=1)\nnet.weight\n\nParameter containing:\ntensor([[-0.1656]], requires_grad=True)\n\n\n\nnet.bias\n\nParameter containing:\ntensor([0.8529], requires_grad=True)\n\n\n- torch.optim.SGD? 를 확인하면 params에 대한설명에 아래와 같이 되어있음\nparams (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n- 설명을 읽어보면 params에 iterable object를 넣으라고 되어있음 (iterable object는 숨겨진 명령어로 __iter__를 가지고 있는 오브젝트를 의미)\n\nset(dir(net.parameters)) & {'__iter__'}\n\nset()\n\n\n\nset(dir(net.parameters())) & {'__iter__'}\n\n{'__iter__'}\n\n\n- 무슨의미?\n\n_generator = net.parameters()\n\n\n_generator.__next__()\n\nParameter containing:\ntensor([[-0.1656]], requires_grad=True)\n\n\n\n_generator.__next__()\n\nParameter containing:\ntensor([0.8529], requires_grad=True)\n\n\n\n_generator.__next__()\n\nStopIteration: \n\n\n- 이건 이런느낌인데?\n\n_generator2 = iter([net.weight,net.bias])\n\n\n_generator2\n\n<list_iterator at 0x7efce86d5dd0>\n\n\n\n_generator2.__next__()\n\nParameter containing:\ntensor([[-0.1656]], requires_grad=True)\n\n\n\n_generator2.__next__()\n\nParameter containing:\ntensor([0.8529], requires_grad=True)\n\n\n\n_generator2.__next__()\n\nStopIteration: \n\n\n- 즉 아래는 같은코드이다.\n### 코드1\n_generator = net.parameters() \ntorch.optim.SGD(_generator,lr=1/10) \n### 코드2\n_generator = iter([net.weight,net.bias])\ntorch.optim.SGD(_generator,lr=1/10) \n### 코드3 (이렇게 써도 코드2가 실행된다고 이해할 수 있음)\n_iterator = [net.weight,net.bias]\ntorch.optim.SGD(_iterator,lr=1/10) \n결론: net.parameters()는 net오브젝트에서 학습할 파라메터를 모두 모아 리스트(iterable object)로 만드는 함수라 이해할 수 있다.\n- 응용예제1\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\noptimizr = torch.optim.SGD([What],lr=1/10) \n\n\nplt.plot(x,y,'o')\nplt.plot(x,(X@What).data,'--')\n\n\n\n\n\nfor epoc in range(30):\n    yhat = X@What \n    loss = torch.mean((y-yhat)**2)\n    loss.backward()\n    optimizr.step();optimizr.zero_grad() \n\n\nplt.plot(x,y,'o')\nplt.plot(x,(X@What).data,'--')\n\n\n\n\n- 응용예제2\n\nb = torch.tensor(-5.0,requires_grad=True)\nw = torch.tensor(10.0,requires_grad=True)\noptimizr = torch.optim.SGD([b,w],lr=1/10)\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(w*x+b).data,'--')\n\n\n\n\n\nfor epoc in range(30):\n    yhat = b+ w*x \n    loss = torch.mean((y-yhat)**2)\n    loss.backward()\n    optimizr.step(); optimizr.zero_grad()\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(w*x+b).data,'--')"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_28_(4주차)_9월 28일__ipynb의_사본.html#logistic-regression",
    "href": "posts/Machine Learning/1. DNN/2022_09_28_(4주차)_9월 28일__ipynb의_사본.html#logistic-regression",
    "title": "기계학습 (0928) 4주차",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nmotive\n- 현실에서 이런 경우가 많음 - \\(x\\)가 커질수록 (혹은 작아질수록) 성공확률이 증가함.\n\n# EX) x는 학점이고.. y는 취업할 확률\n\n- (X,y)는 어떤모양?\n\n_df = pd.DataFrame({'x':range(-6,7),'y':[0,0,0,0,0,0,1,0,1,1,1,1,1]})\n_df \n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      -6\n      0\n    \n    \n      1\n      -5\n      0\n    \n    \n      2\n      -4\n      0\n    \n    \n      3\n      -3\n      0\n    \n    \n      4\n      -2\n      0\n    \n    \n      5\n      -1\n      0\n    \n    \n      6\n      0\n      1\n    \n    \n      7\n      1\n      0\n    \n    \n      8\n      2\n      1\n    \n    \n      9\n      3\n      1\n    \n    \n      10\n      4\n      1\n    \n    \n      11\n      5\n      1\n    \n    \n      12\n      6\n      1\n    \n  \n\n\n\n\n\nplt.plot(_df.x,_df.y,'o')\n\n\n\n\n- (예비학습) 시그모이드라는 함수가 있음\n\nxx = torch.linspace(-6,6,100)   # -6에서 6까지 100개..\ndef f(x):\n    return torch.exp(x)/(1+torch.exp(x))  \n\n\nplt.plot(_df.x,_df.y,'o')\nplt.plot(xx,f(xx))   # f(xx) = f(1*xx) 얌.. 근데 만약 f(5*xx)하면 기울기가 더 급해져.. 애매한 부분이 더 적어지고 스펙에 대한 영향을 ... f(2.5*xx)-1.2 (우측으로 1.2 이동) 이렇게 튜닝이 가능\n\n\n\n\n\n\nmodel\n- \\(x\\)가 커질수록 \\(y=1\\)이 잘나오는 모형은 아래와 같이 설계할 수 있음 <— 외우세요!!!\n\n$y_i Ber(_i),$ where \\(\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\)\n\\(\\hat{y}_i= \\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\frac{1}{1+\\exp(-\\hat{w}_0-\\hat{w}_1x_i)}\\)\n\\(loss= - \\sum_{i=1}^{n} \\big(y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i)\\big)\\) <— 외우세요!!\n\n\n# 베르누이.. pi i 라는 건.. 왜 p가 아니고 pi냐? 사람마다 합격할 확률이 다르기 때문에.\n# x가 무한대로 가면 pi i 는 1에 가까워지고 마이너스 무한대로 가면 0에가까워진다\n\n# loss는 MSE로 하긴 어렵고,, 라이클리우드?????????? 설명이기니까 위에 그냥 외우기\n# y i = 0 일대랑 1 일때 저식에 넣어서 그래프 그려서 생각해보기... loss는 yi랑 y값이 비슷하면 loss 값이 작아짐\n\n\n\ntoy example\n- 예제시작\n\nx=torch.linspace(-1,1,2000).reshape(2000,1)\nw0= -1 \nw1= 5 \nu = w0+x*w1 \nv = torch.exp(u)/(1+torch.exp(u)) # v=πi, 즉 확률을 의미함,  v는 성공할확률\ny = torch.bernoulli(v) \n\n# torch.bernoulli(toch.tensor([0.5]*100))   0,1 반복해서 뽑힘. 0.5는 확률!!!!!\n\n\nplt.scatter(x,y,alpha=0.05)   # 여기서 알파가 투명도인듯??????????? \nplt.plot(x,v,'--r')\n\n\n\n\n\n우리의 목적: \\(x\\)가 들어가면 빨간선 \\(\\hat{y}\\)의 값을 만들어주는 mapping을 학습해보자.\n\n\n# 최초의 곡선\n# w0hat = -1\n# w1hat = 3\n\n\n# yhat = f(w0hat+x*w1hat)\n# plt.plot(x,y, 'o', alpha=0.05)   \n# plt.plot(x,v,'--')\n# plt.plot(x,yhat,'--r')\n\nSyntaxError: ignored\n\n\n\n# sigmoid함수만들엇던걸..............."
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_28_(4주차)_9월 28일__ipynb의_사본.html#숙제",
    "href": "posts/Machine Learning/1. DNN/2022_09_28_(4주차)_9월 28일__ipynb의_사본.html#숙제",
    "title": "기계학습 (0928) 4주차",
    "section": "숙제",
    "text": "숙제"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_21_(3주차)_9월21일_ipynb의_사본.html",
    "href": "posts/Machine Learning/1. DNN/2022_09_21_(3주차)_9월21일_ipynb의_사본.html",
    "title": "기계학습 (0921) 3주차",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_21_(3주차)_9월21일_ipynb의_사본.html#로드맵",
    "href": "posts/Machine Learning/1. DNN/2022_09_21_(3주차)_9월21일_ipynb의_사본.html#로드맵",
    "title": "기계학습 (0921) 3주차",
    "section": "로드맵",
    "text": "로드맵\n- 회귀분석 \\(\\to\\) 로지스틱 \\(\\to\\) 심층신경망(DNN) \\(\\to\\) 합성곱신경망(CNN)\n- 강의계획서"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_21_(3주차)_9월21일_ipynb의_사본.html#ref",
    "href": "posts/Machine Learning/1. DNN/2022_09_21_(3주차)_9월21일_ipynb의_사본.html#ref",
    "title": "기계학습 (0921) 3주차",
    "section": "ref",
    "text": "ref\n- 넘파이 문법이 약하다면? (reshape, concatenate, stack)\n\nreshape: 아래 링크의 넘파이공부 2단계 reshape 참고\n\nhttps://guebin.github.io/IP2022/2022/04/06/(6%EC%A3%BC%EC%B0%A8)-4%EC%9B%946%EC%9D%BC.html\n\nconcatenate, stack: 아래 링크의 넘파이공부 4단계 참고\n\nhttps://guebin.github.io/IP2022/2022/04/11/(6%EC%A3%BC%EC%B0%A8)-4%EC%9B%9411%EC%9D%BC.html"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_21_(3주차)_9월21일_ipynb의_사본.html#회귀모형-소개",
    "href": "posts/Machine Learning/1. DNN/2022_09_21_(3주차)_9월21일_ipynb의_사본.html#회귀모형-소개",
    "title": "기계학습 (0921) 3주차",
    "section": "회귀모형 소개",
    "text": "회귀모형 소개\n- model: \\(y_i= w_0+w_1 x_i +\\epsilon_i = 2.5 + 4x_i +\\epsilon_i, \\quad i=1,2,\\dots,n\\)\n- model: \\({\\bf y}={\\bf X}{\\bf W} +\\boldsymbol{\\epsilon}\\)\n\n\\({\\bf y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\dots \\\\ y_n\\end{bmatrix}, \\quad {\\bf X}=\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots \\\\ 1 & x_n\\end{bmatrix}, \\quad {\\bf W}=\\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}, \\quad \\boldsymbol{\\epsilon}= \\begin{bmatrix} \\epsilon_1 \\\\ \\dots \\\\ \\epsilon_n\\end{bmatrix}\\)"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_21_(3주차)_9월21일_ipynb의_사본.html#회귀모형에서-데이터-생성",
    "href": "posts/Machine Learning/1. DNN/2022_09_21_(3주차)_9월21일_ipynb의_사본.html#회귀모형에서-데이터-생성",
    "title": "기계학습 (0921) 3주차",
    "section": "회귀모형에서 데이터 생성",
    "text": "회귀모형에서 데이터 생성\n\n_rnt=torch.randn(100).sort() #100개의 난수 생성 .sort()는 정렬된 값 표현\n\n\ntype(_rnt) #type해보니까 모르는 거네? lengh를 보면 2니까 리스트를 해볼수 있음!\n\ntorch.return_types.sort\n\n\n\na,_ = _rnt[0], _rnt[1] #첫번쨰 원소가 a에 들어가고 두번째 원소가 언더바에 들어가게 된다.\n\n\nx,_ = torch.randn(100).sort()\nx     # X벡터 안에 들어가는 x1, x2, x3 ... \n\ntensor([-2.6694e+00, -2.6132e+00, -2.2525e+00, -2.0763e+00, -1.9791e+00,\n        -1.8444e+00, -1.7486e+00, -1.7284e+00, -1.6991e+00, -1.6634e+00,\n        -1.6364e+00, -1.5948e+00, -1.5710e+00, -1.5043e+00, -1.5002e+00,\n        -1.4035e+00, -1.3328e+00, -1.3239e+00, -1.2964e+00, -1.2064e+00,\n        -1.1857e+00, -1.1184e+00, -1.0559e+00, -1.0148e+00, -1.0105e+00,\n        -9.7771e-01, -9.2156e-01, -8.9929e-01, -8.8333e-01, -7.6213e-01,\n        -6.8896e-01, -6.2386e-01, -6.0660e-01, -5.9161e-01, -5.7884e-01,\n        -4.4417e-01, -4.3631e-01, -3.8129e-01, -3.5062e-01, -3.4311e-01,\n        -3.1632e-01, -2.7753e-01, -2.7065e-01, -2.7020e-01, -2.6189e-01,\n        -2.2925e-01, -1.4359e-01, -1.2405e-01, -6.8853e-02, -5.1603e-02,\n        -4.9887e-02, -2.3798e-02, -1.6275e-03,  7.4200e-02,  1.6760e-01,\n         1.7279e-01,  2.3754e-01,  2.5730e-01,  2.6886e-01,  2.8250e-01,\n         2.9296e-01,  3.0017e-01,  3.1466e-01,  3.2627e-01,  3.5380e-01,\n         3.5664e-01,  3.6345e-01,  3.6429e-01,  4.3469e-01,  4.3551e-01,\n         4.6556e-01,  4.9491e-01,  4.9940e-01,  5.4481e-01,  6.4859e-01,\n         6.7236e-01,  6.8683e-01,  7.2763e-01,  7.3832e-01,  7.8508e-01,\n         8.0376e-01,  8.1716e-01,  8.2234e-01,  8.8814e-01,  9.1453e-01,\n         9.8436e-01,  1.0107e+00,  1.0332e+00,  1.0441e+00,  1.0577e+00,\n         1.1333e+00,  1.1406e+00,  1.2557e+00,  1.3057e+00,  1.3221e+00,\n         1.3361e+00,  1.6109e+00,  1.7063e+00,  1.8415e+00,  2.0672e+00])\n\n\n\nones= torch.ones(100)   #torch.ones(100) 1이 100개 들어간거. X벡터만들기 위해서 \n\n\ntorch.stack([ones, x]) #stack 쌓는다!!!   근데 우리가 원하는 건 이거의 T (트랜스)를 가지고 싶으니까 벡터T 해주기!\n\ntensor([[ 1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00],\n        [-2.6694e+00, -2.6132e+00, -2.2525e+00, -2.0763e+00, -1.9791e+00,\n         -1.8444e+00, -1.7486e+00, -1.7284e+00, -1.6991e+00, -1.6634e+00,\n         -1.6364e+00, -1.5948e+00, -1.5710e+00, -1.5043e+00, -1.5002e+00,\n         -1.4035e+00, -1.3328e+00, -1.3239e+00, -1.2964e+00, -1.2064e+00,\n         -1.1857e+00, -1.1184e+00, -1.0559e+00, -1.0148e+00, -1.0105e+00,\n         -9.7771e-01, -9.2156e-01, -8.9929e-01, -8.8333e-01, -7.6213e-01,\n         -6.8896e-01, -6.2386e-01, -6.0660e-01, -5.9161e-01, -5.7884e-01,\n         -4.4417e-01, -4.3631e-01, -3.8129e-01, -3.5062e-01, -3.4311e-01,\n         -3.1632e-01, -2.7753e-01, -2.7065e-01, -2.7020e-01, -2.6189e-01,\n         -2.2925e-01, -1.4359e-01, -1.2405e-01, -6.8853e-02, -5.1603e-02,\n         -4.9887e-02, -2.3798e-02, -1.6275e-03,  7.4200e-02,  1.6760e-01,\n          1.7279e-01,  2.3754e-01,  2.5730e-01,  2.6886e-01,  2.8250e-01,\n          2.9296e-01,  3.0017e-01,  3.1466e-01,  3.2627e-01,  3.5380e-01,\n          3.5664e-01,  3.6345e-01,  3.6429e-01,  4.3469e-01,  4.3551e-01,\n          4.6556e-01,  4.9491e-01,  4.9940e-01,  5.4481e-01,  6.4859e-01,\n          6.7236e-01,  6.8683e-01,  7.2763e-01,  7.3832e-01,  7.8508e-01,\n          8.0376e-01,  8.1716e-01,  8.2234e-01,  8.8814e-01,  9.1453e-01,\n          9.8436e-01,  1.0107e+00,  1.0332e+00,  1.0441e+00,  1.0577e+00,\n          1.1333e+00,  1.1406e+00,  1.2557e+00,  1.3057e+00,  1.3221e+00,\n          1.3361e+00,  1.6109e+00,  1.7063e+00,  1.8415e+00,  2.0672e+00]])\n\n\n\ntype(torch.stack([ones, x]))\n\ntorch.Tensor\n\n\n\n# 역슬래시 하고 입실론 쓰고 탭 누르면 입실론 수학기호생김 신기하군\n\n\nW = torch.tensor([2.5,4])\nW\nW.shape #원래 shape이 매트릭스여야 하는데 벡터네? 그럼 X@W하면 매트릭스가 아닌 벡터가 된다. ~~~~~~~~~~ 2차원 1차원,,, 훔 \n\ntorch.Size([2])\n\n\n\ntorch.manual_seed(43052) #이건 원래 난수가 봅히는건뎅 교수님이 설명하기 편하게 ,, 숫자 정해논고\nones= torch.ones(100) #torch.ones(100) 1이 100개 들어간거. X벡터만들기 위해서 \nx,_ = torch.randn(100).sort()\nX = torch.stack([ones,x]).T # torch.stack([ones,x],axis=1)    T:트랜스포 해주는거. 근데 그렇게 안하고 axis=1해도 된당!\nW = torch.tensor([2.5,4])\nϵ = torch.randn(100)*0.5\ny = X@W + ϵ\n\n\nplt.plot(x,y,'o')\nplt.plot(x,2.5+4*x,'--') #트루펑션 점선으로 찍어보기\n\n#언더라인펑션~= Y=4x+2.5 \n# w0, w1를 추정하면 언더라인 펑션을 잘 추정했다고 확인 할 수 잇어염 \n\n\n\n\n\n# 파란점은 기본 데이터. 입실론을 뺀거(오차항 뺸거)=TRUU FUNCTION을 찾고 싶어!"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_21_(3주차)_9월21일_ipynb의_사본.html#회귀모형에서-학습이란",
    "href": "posts/Machine Learning/1. DNN/2022_09_21_(3주차)_9월21일_ipynb의_사본.html#회귀모형에서-학습이란",
    "title": "기계학습 (0921) 3주차",
    "section": "회귀모형에서 학습이란?",
    "text": "회귀모형에서 학습이란?\n\n# x에서 y로가는 맵핑 \n# 리니어 맵핑,, 2.5랑 4에 가깝게 맞추는거!!\n\n- 파란점만 주어졌을때, 주황색 점선을 추정하는것. 좀 더 정확하게 말하면 given data로 \\(\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}\\)를 최대한 \\(\\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}\\)와 비슷하게 찾는것.\n\ngiven data : \\(\\big\\{(x_i,y_i) \\big\\}_{i=1}^{n}\\)\nparameter: \\({\\bf W}=\\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix}\\)\nestimated parameter: \\({\\bf \\hat{W}}=\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}\\)\n\n- 더 쉽게 말하면 아래의 그림을 보고 적당한 추세선을 찾는것이다.\n\n# \"적당한\" 추세선이 뭐냐? 하면 웱,,\n# 숫자로 만드는게 제일 편하다!!\n# 적당한게 정도가 있어서 일단 안적당한 거 먼저 해볼게용\n\n\nplt.plot(x,y,'o')\nplt.plot(x, -5+10*x, '--')\n\n# 원데이터가 y1, y2 되고.. 언더바에 잇는게 y1 hat, y2 hat ..... \n\n\n\n\n- 시도: \\((\\hat{w}_0,\\hat{w}_1)=(-5,10)\\)을 선택하여 선을 그려보고 적당한지 판단.\n\n\\(\\hat{y}_i=-5 +10 x_i\\) 와 같이 \\(y_i\\)의 값을 적합시키겠다는 의미\n\n- 벡터표현으로 주황색점선을 계산\n\nWhat= torch.tensor([-5.0, 10.0])\nWhat\n\ntensor([-5., 10.])\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,X@What,'--')\n\n# 모델링: 데이터를 보고 아키텍처 설정,,"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_21_(3주차)_9월21일_ipynb의_사본.html#파라메터를-학습하는-방법-적당한-선으로-업데이트-하는-방법",
    "href": "posts/Machine Learning/1. DNN/2022_09_21_(3주차)_9월21일_ipynb의_사본.html#파라메터를-학습하는-방법-적당한-선으로-업데이트-하는-방법",
    "title": "기계학습 (0921) 3주차",
    "section": "파라메터를 학습하는 방법 (적당한 선으로 업데이트 하는 방법)",
    "text": "파라메터를 학습하는 방법 (적당한 선으로 업데이트 하는 방법)\n- 이론적으로 추론 <- 회귀분석시간에 배운것\n- 컴퓨터의 반복계산을 이용하여 추론 (손실함수도입 + 경사하강법) <- 우리가 오늘 파이토치로 실습해볼 내용.\n- 전략: 아래와 같은 3단계 전략을 취한다.\n\nstage1: 아무 점선이나 그어본다..\nstage2: stage1에서 그은 점선보다 더 좋은 점선으로 바꾼다.\nstage3: stage1 - 2 를 반복한다.\n\n\nStage1: 첫번째 점선 – 임의의 선을 일단 그어보자\n- \\(\\hat{w}_0=-5, \\hat{w}_1 = 10\\) 으로 설정하고 (왜? 그냥) 임의의 선을 그어보자.\n\nWhat= torch.tensor([-5.0, 10.0], requires_grad=True)\nWhat # 나중에 미분하기 위해서 requires_grad 필요한 옵션\n\n# 뒤에 꼬리표가 붙어있음! 따라다녀,, \n\n#텐서플로우 패키지에서 tf.variable이랑 tf. 어ㅓㅉ고 랑 선언하는데 tf.variable로 설정한거\n\ntensor([-5., 10.], requires_grad=True)\n\n\n\nWhat + 1\n# 뒤에 grad_fn 어쩌고가 따라오지만 신경쓰지 않아도 된당 ,, 벡터처럼 계산도 가능해!!\n# 꼬리표를 빼고싶을땐...... \nWhat.detach()\nWhat.data\n\ntensor([-5., 10.])\n\n\n\n처음에는 ${}=\n\\[\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}\\]\n=\n\\[\\begin{bmatrix} -5 \\\\ 10 \\end{bmatrix}\\]\n$ 를 대입해서 주황색 점선을 적당히 그려보자는 의미\n끝에 requires_grad=True는 나중에 미분을 위한 것\n\n그려보자!\n\n\nStage2: 첫번째 수정 – 최초의 점선에 대한 ‘적당한 정도’를 판단하고 더 ’적당한’ 점선으로 업데이트 한다.\n- ’적당한 정도’를 판단하기 위한 장치: loss function 도입!\n\\(loss=\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2=\\sum_{i=1}^{n}(y_i-(\\hat{w}_0+\\hat{w}_1x_i))^2\\)\n\\(=({\\bf y}-{\\bf\\hat{y}})^\\top({\\bf y}-{\\bf\\hat{y}})=({\\bf y}-{\\bf X}{\\bf \\hat{W}})^\\top({\\bf y}-{\\bf X}{\\bf \\hat{W}})\\)\n- loss 함수의 특징 - \\(y_i \\approx \\hat{y}_i\\) 일수록 loss값이 작다. - \\(y_i \\approx \\hat{y}_i\\) 이 되도록 \\((\\hat{w}_0,\\hat{w}_1)\\)을 잘 찍으면 loss값이 작다. - (중요) 주황색 점선이 ‘적당할 수록’ loss값이 작다.\n\n# y와 yhat의 값이 비슷하면 loss값이 0에 가까워진다.\n\n\nloss=torch.sum((y-X@What)**2) #이 값이 주황색 점선에 대한 loss!!!\nloss\n\ntensor(8587.6875, grad_fn=<SumBackward0>)\n\n\n- 우리의 목표: 이 loss(=8587.6875)을 더 줄이자. - 궁극적으로는 아예 모든 조합 \\((\\hat{w}_0,\\hat{w}_1)\\)에 대하여 가장 작은 loss를 찾으면 좋겠다. (stage2에서 할일은 아님)\n- 문제의 치환: 생각해보니까 우리의 문제는 아래와 같이 수학적으로 단순화 되었다. - 적당해보이는 주황색 선을 찾자 \\(\\to\\) \\(loss(w_0,w_1)\\)를 최소로하는 \\((w_0,w_1)\\)의 값을 찾자.\n- 수정된 목표: \\(loss(w_0,w_1)\\)를 최소로 하는 \\((w_0,w_1)\\)을 구하라. - 단순한 수학문제가 되었다. 마치 \\(loss(w)=w^2-2w+3\\) 을 최소화하는 \\(w\\)를 찾으라는 것과 같음. - 즉 “적당한 선으로 업데이트 하라 = 파라메터를 학습 하라 = 손실함수를 최소화 하라”\n\n# 그 function을 minimize하는 정의역의 세트를 찾으면 된다. = 적당한 선으로 업데이트 하라= ...=\n\n- 우리의 무기: 경사하강법, 벡터미분\n\n\nStage2를 위한 경사하강법 복습\n경사하강법 아이디어 (1차원)\n(step 1) 임의의 점을 찍는다.\n(step 2) 그 점에서 순간기울기를 구한다. (접선) <– 미분\n(step 3) 순간기울기(=미분계수)의 부호를 살펴보고 부호와 반대방향으로 움직인다.\n(팁) 기울기의 절대값 크기와 비례하여 보폭(=움직이는 정도)을 조절한다.\n\n# 접선의 기울기가 만약 -4 라는 음수가 나오면 양수값으로 가면 된다.. \n# 미분계수의 절대값이 작아지는 정도로.. 보폭 조절!!\n\n경사하강법 아이디어 (2차원)\n(step 1) 임의의 점을 찍는다.\n(step 2) 그 점에서 순간기울기를 구한다. (접평면) <– 편미분\n(step 3) 순간기울기(=미분계수)의 부호를 살펴보고 부호와 반대방향으로 각각 움직인다.\n(팁) 기울기의 절대값 크기와 비례하여 보폭(=움직이는 정도)을 각각 조절한다.\n\n# 왼쪽으로 갈래? 오른쪽으로 갈래?\n# 위로 갈래? 아래로 갈래? ->점에서 한쪽방향을 고정되어있다 생각하고 왼오, 또는 위아래로만 -> 편미분으로 가넝!!\n# 2차원, 3차원,, 원리는 결국 1차원과 똑같당!\n\nloss를 줄이도록 \\({\\bf W}\\)를 개선하는 방법\n- $수정값 원래값 - 기울어진크기(=미분계수) $\n\n여기에서 \\(\\alpha\\)는 전체적인 보폭의 크기를 결정한다. 즉 \\(\\alpha\\)값이 클수록 한번의 update에 움직이는 양이 크다.\n\n\n# 반대방향으로 가야하니까 - 를 붙인다. \n# a 알파값은 .. 만약 미분계수가 -8이 나왔엉. 근데 그렇다고 8곱해버리면 너무 크니까 0.8 이든 0.08이든.. 그런 알파값을 곱해줘야해!!\n\n# a 값은 양수여야함!!! 음수면 방향이 바꾸기 때문에 a는 정답이 없어. 0.0001 이렇게 걍 맞춰가면 뎀..\n\n- \\({\\bf W} \\leftarrow {\\bf W} - \\alpha \\times \\frac{\\partial}{\\partial {\\bf W}}loss(w_0,w_1)\\)\n\n마이너스의 의미: 기울기의 부호를 보고 반대방향으로 움직여라.\n\\(\\frac{\\partial}{\\partial {\\bf W}}loss(w_0,w_1):\\) 기울기의 절대값 크기와 비례하여 움직이는 정도를 조정하라.\n\\(\\alpha\\)의 의미: 전체적인 보폭의 속도를 조절, \\(\\alpha\\)가 크면 전체적으로 빠르게 움직인다. 다리의 길이로 비유할 수 있다.\n\n\n\nloss\n\ntensor(8587.6875, grad_fn=<SumBackward0>)\n\n\n- 우리의 목표: loss=8587.6875 인데, 이걸 줄이는 것이 목표라고 했었음. 이것을 줄이는 방법이 경사하강법이다.\n- 경사하강법으로 loss를 줄이기 위해서는 \\(\\frac{\\partial}{\\partial {\\bf W}}loss(w_0,w_1)\\)의 계산이 필요한데, 이를 위해서 벡터미분이 필요하다. (loss.backward()로 하면된다)\n\nWhat.grad\n\n\nloss.backward()\n\n# 자기 혼자 미분하고 결과값을 안보여죵  \n# 뭘로 미분하라는거야? ->꼬리표 추적... What까지 가서 얘를 미분하라는 거구나 하고 미분해줌\n\n\nX@What #꼬리표가 있어!!!\n\ntensor([-29.8211, -28.6215, -24.9730, -21.2394, -19.7919, -19.6354, -19.5093,\n        -19.4352, -18.7223, -18.0793, -16.9040, -16.0918, -16.0536, -15.8746,\n        -14.4690, -14.3193, -13.6426, -12.8578, -12.5486, -12.4213, -11.9484,\n        -11.1034, -10.8296, -10.6210, -10.5064, -10.0578,  -9.8063,  -9.7380,\n         -9.7097,  -9.6756,  -8.8736,  -8.7195,  -8.6880,  -8.1592,  -7.7752,\n         -7.7716,  -7.7339,  -7.7208,  -7.6677,  -7.1551,  -7.0004,  -6.8163,\n         -6.7081,  -6.5655,  -6.4480,  -6.3612,  -6.0566,  -5.6031,  -5.5589,\n         -5.2137,  -4.3446,  -4.3165,  -3.8047,  -3.5801,  -3.4793,  -3.4325,\n         -2.3545,  -2.3440,  -1.8434,  -1.7799,  -1.5386,  -1.0161,  -0.8103,\n          0.4426,   0.5794,   0.9125,   1.1483,   1.4687,   1.4690,   1.5234,\n          1.6738,   2.0592,   2.1414,   2.8221,   3.1536,   3.6682,   4.2907,\n          4.8037,   4.8531,   4.9414,   5.3757,   5.3926,   5.6973,   6.0239,\n          6.1261,   6.5317,   7.2891,   8.4032,   8.4936,   9.2794,   9.9943,\n         10.0310,  10.4369,  11.7886,  15.8323,  17.4440,  18.9350,  21.0560,\n         21.0566,  21.6324], grad_fn=<MvBackward0>)\n\n\n\ny-X@What #꼬리표가 있는 걸로 파생된 모든 것들은 다 꼬리표가 있땅\n\ntensor([21.2791, 22.0448, 19.0234, 16.7600, 15.5403, 16.5028, 15.4853, 15.2491,\n        15.3820, 15.8766, 14.8778, 13.5299, 14.7183, 13.8280, 14.0026, 12.9680,\n        11.9954, 12.7489, 12.2415, 11.7914, 11.9046, 11.5198, 11.2462, 10.5267,\n        10.7726, 10.5168, 10.6967, 10.6377, 10.3411, 11.0601,  9.6820,  9.9789,\n         9.8090, 10.0825,  8.8370,  9.1268,  9.8500,  8.8644,  9.2922,  8.9190,\n         8.6027,  8.5628,  7.6912,  8.3478,  8.5596,  9.2232,  8.1731,  7.1257,\n         8.1160,  8.0498,  7.7402,  6.3844,  6.6187,  7.0653,  7.0851,  6.0290,\n         5.2399,  6.2613,  5.4961,  5.8827,  5.8510,  4.4186,  4.0283,  4.1260,\n         3.7978,  3.3949,  3.3412,  3.0141,  3.8481,  3.9753,  3.7895,  3.9736,\n         3.1428,  2.2318,  2.3002,  2.3654,  1.4343,  0.9550,  1.3489,  1.6579,\n         1.0864,  1.1214,  0.9873,  1.3258,  1.9648,  0.5476, -0.4224, -0.9803,\n        -1.2392, -2.0827, -0.4937, -0.9971, -2.9482, -2.7127, -4.7377, -7.1180,\n        -6.6685, -7.9578, -8.5098, -7.7984], grad_fn=<SubBackward0>)\n\n\n\nloss.backward()의 의미: loss를 미분해라! 뭘로? requires_grad=True를 가진 텐서로!!\n\n\nloss=torch.sum((y-yhat)**2)= torch.sum((y-X@What)**2)\n# 이었고 \nWhat=torch.tensor([-5.0,10.0],requires_grad=True)\n# 이므로 결국 What으로 미분하라는 의미. \n# 미분한 식이 나오는 것이 아니고, \n# 그 식에 (-5.0, 10.0)을 대입한 계수값이 계산됨. \n- 위에서 loss.backward()의 과정은 미분을 활용하여 \\((-5,10)\\)에서의 순간기울기를 구했다는 의미임.\n\nWhat.grad\n\ntensor([-1342.2522,  1188.9305])\n\n\n- (-5,10)에서 loss의 순간기울기 값은 What.grad로 확인가능하다.\n\n이것이 의미하는건 \\((-5,10)\\)에서의 \\(loss(w_0,w_1)\\)의 순간기울기가 \\((-1342.2523, 1188.9307)\\) 이라는 의미\n\n- (확인1) loss.backward()가 미분을 잘 계산해 주는 것이 맞는가? 손계산으로 검증하여 보자.\n\n\\(loss(w_0,w_1)=({\\bf y}-\\hat{\\bf y})^\\top ({\\bf y}-\\hat{\\bf y})=({\\bf y}-{\\bf XW})^\\top ({\\bf y}-{\\bf XW})\\)\n\\(\\frac{\\partial}{\\partial {\\bf W} }loss(w_0,w_1)=-2{\\bf X}^\\top {\\bf y}+2{\\bf X}^\\top {\\bf X W}\\)\n\n\n- 2 * X.T @ y + 2 * X.T @ X @ What\n\ntensor([-1342.2523,  1188.9305], grad_fn=<AddBackward0>)\n\n\n- (확인2) loss.backward()가 미분을 잘 계산해 주는 것이 맞는가? 편미분을 간단히 구현하여 검증하여 보자.\n\n\\(\\frac{\\partial}{\\partial {\\bf W} } loss(w_0,w_1)=\\begin{bmatrix}\\frac{\\partial}{\\partial w_0} \\\\ \\frac{\\partial}{\\partial w_1} \\end{bmatrix}loss(w_0,w_1) =\\begin{bmatrix}\\frac{\\partial}{\\partial w_0}loss(w_0,w_1) \\\\ \\frac{\\partial}{\\partial w_1}loss(w_0,w_1) \\end{bmatrix}\\)\n\\(\\frac{\\partial}{\\partial w_0}loss(w_0,w_1) \\approx \\frac{loss(w_0+h,w_1)-loss(w_0,w_1)}{h}\\)\n\\(\\frac{\\partial}{\\partial w_1}loss(w_0,w_1) \\approx \\frac{loss(w_0,w_1+h)-loss(w_0,w_1)}{h}\\)\n\n\n_lossfn = lambda w0,w1: torch.sum((y-w0-w1*x)**2)\n_lossfn(-5,10)\n\ntensor(8587.6875)\n\n\n\nh=0.001\n(_lossfn(-5+h,10) - _lossfn(-5,10))/h,  (_lossfn(-5,10+h) - _lossfn(-5,10))/h\n\n(tensor(-1341.7968), tensor(1190.4297))\n\n\n\n약간 오차가 있지만 얼추비슷 \\(\\to\\) 잘 계산했다는 소리임\n\n- 수정전, 수정하는폭, 수정후의 값은 차례로 아래와 같다.\n\nWhat.data\n\ntensor([-5., 10.])\n\n\n\nstr(What.data) #문자열이 됨! \n\n'tensor([-5., 10.])'\n\n\n\nalpha=0.001 \nprint('수정전: ' + str(What.data)) # What 에서 미분꼬리표를 떼고 싶다면? What.data or What.detach()\nprint('수정하는폭: ' +str(-alpha * What.grad)) #1341*0.001, 1190*0.001\nprint('수정후: ' +str(What.data-alpha * What.grad))\nprint('*참값: (2.5,4)' )\n\n수정전: tensor([-5., 10.])\n수정하는폭: tensor([ 1.3423, -1.1889])\n수정후: tensor([-3.6577,  8.8111])\n*참값: (2.5,4)\n\n\n- Wbefore, Wafter 계산\n\nWbefore = What.data\nWafter = What.data- alpha * What.grad\nWbefore, Wafter\n\n(tensor([-5., 10.]), tensor([-3.6577,  8.8111]))\n\n\n- Wbefore, Wafter의 시각화\n\nplt.plot(x,y,'o')\nplt.plot(x,X@Wbefore,'--')  #주황색\nplt.plot(x,X@Wafter,'--')  #초록색\n\n\n\n\n\n\n\nStage3: Learn (=estimate \\(\\bf\\hat{W})\\)\n- 이 과정은 Stage1,2를 반복하면 된다.\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True) \nWhat\n\ntensor([-5., 10.], requires_grad=True)\n\n\n\nalpha=0.001 \nfor epoc in range(30): ## 30번 반복합니다!! \n    yhat=X@What \n    loss=torch.sum((y-yhat)**2)\n    loss.backward() #미분.. what.grad로 미분이 된 것을 볼 수 있따->편미분계수값\n    What.data = What.data-alpha * What.grad #경사하강법~~\n    What.grad=None  # 파이토치 특징.. 미분된 grad값에 새 미분값이 안들어가있고.. 이전미분값에 그다음미분값이 더해져서 들어가기 때문에->defalut기 대문에 초기상태로 바꿔줘야햄....\n\n\nWhat\n\ntensor([2.4290, 4.0144], requires_grad=True)\n\n\n\n원래 철자는 epoch이 맞아요\n\n- 반복결과는?! (최종적으로 구해지는 What의 값은?!) - 참고로 true\n\nWhat.data ## true인 (2.5,4)와 상당히 비슷함\n\ntensor([2.4290, 4.0144])\n\n\n- 반복결과를 시각화하면?\n\nplt.plot(x,y,'o')\nplt.plot(x,X@What.data,'--') #그림을 그리기위해서 꼬리표를 떼준당.."
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_21_(3주차)_9월21일_ipynb의_사본.html#파라메터의-학습과정-음미-학습과정-모니터링",
    "href": "posts/Machine Learning/1. DNN/2022_09_21_(3주차)_9월21일_ipynb의_사본.html#파라메터의-학습과정-음미-학습과정-모니터링",
    "title": "기계학습 (0921) 3주차",
    "section": "파라메터의 학습과정 음미 (학습과정 모니터링)",
    "text": "파라메터의 학습과정 음미 (학습과정 모니터링)\n\n학습과정의 기록\n- 기록을 해보자.\n\nloss_history = [] # 기록하고 싶은것 1  .\nyhat_history = [] # 기록하고 싶은것 2  yhat이 어떻게 변하는지\nWhat_history = [] # 기록하고 싶은것 3 \n\n\n#loss_history #처음엔 비어있지만.. 값을 점점 넣고 싶어!!!\n\n\n#loss_history.append(loss)\n#loss_history\n\n\n#loss.item()\n\n\n#loss_history.append(loss.item())\n#loss_history # 텐서를 리스트로 바꿔줘서 넣어주기..\n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.001 \nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())  #세미콜론만 주석처리하면.. 위랑 똑같은 코드..\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist())\n    What.grad=None\n\n\nloss_history\n\n[8587.6875,\n 5675.2109375,\n 3755.637451171875,\n 2489.581787109375,\n 1654.0390625,\n 1102.3206787109375,\n 737.8441162109375,\n 496.96514892578125,\n 337.7142028808594,\n 232.39694213867188,\n 162.72906494140625,\n 116.63263702392578,\n 86.1263656616211,\n 65.93397521972656,\n 52.566444396972656,\n 43.71583557128906,\n 37.855220794677734,\n 33.974090576171875,\n 31.403636932373047,\n 29.701112747192383,\n 28.57339096069336,\n 27.826366424560547,\n 27.331483840942383,\n 27.003639221191406,\n 26.78643798828125,\n 26.642536163330078,\n 26.547197341918945,\n 26.48402976989746,\n 26.442174911499023,\n 26.414440155029297]\n\n\n- \\(\\hat{y}\\) 관찰 (epoch=3, epoch=10, epoch=15)\n\nyhat_history\n\n[[-29.821128845214844,\n  -28.621461868286133,\n  -24.97295379638672,\n  -21.239360809326172,\n  -19.791915893554688,\n  -19.635364532470703,\n  -19.50925064086914,\n  -19.435216903686523,\n  -18.722301483154297,\n  -18.079282760620117,\n  -16.903972625732422,\n  -16.09178924560547,\n  -16.053556442260742,\n  -15.874590873718262,\n  -14.468965530395508,\n  -14.31933879852295,\n  -13.642648696899414,\n  -12.857781410217285,\n  -12.548619270324707,\n  -12.421306610107422,\n  -11.94838809967041,\n  -11.103449821472168,\n  -10.829590797424316,\n  -10.621047973632812,\n  -10.506429672241211,\n  -10.05777359008789,\n  -9.80626392364502,\n  -9.737953186035156,\n  -9.709654808044434,\n  -9.67557144165039,\n  -8.873588562011719,\n  -8.719473838806152,\n  -8.687962532043457,\n  -8.159214973449707,\n  -7.775153636932373,\n  -7.771570682525635,\n  -7.733872890472412,\n  -7.7207512855529785,\n  -7.667671203613281,\n  -7.155084609985352,\n  -7.000405311584473,\n  -6.816307067871094,\n  -6.708141326904297,\n  -6.565457820892334,\n  -6.4479804039001465,\n  -6.361170768737793,\n  -6.056612968444824,\n  -5.603134632110596,\n  -5.558872222900391,\n  -5.213672637939453,\n  -4.34455680847168,\n  -4.3164825439453125,\n  -3.8046953678131104,\n  -3.5801002979278564,\n  -3.479255437850952,\n  -3.4324843883514404,\n  -2.3544726371765137,\n  -2.3440215587615967,\n  -1.843425989151001,\n  -1.7798891067504883,\n  -1.5385699272155762,\n  -1.016080617904663,\n  -0.8102789521217346,\n  0.44257819652557373,\n  0.5793601274490356,\n  0.9125399589538574,\n  1.1482644081115723,\n  1.468665599822998,\n  1.468990445137024,\n  1.5233922004699707,\n  1.673753261566162,\n  2.059195041656494,\n  2.141373634338379,\n  2.8221492767333984,\n  3.1536107063293457,\n  3.6682331562042236,\n  4.290748119354248,\n  4.803698539733887,\n  4.853081226348877,\n  4.9413557052612305,\n  5.375688076019287,\n  5.392560005187988,\n  5.697267055511475,\n  6.023870468139648,\n  6.126120090484619,\n  6.531744956970215,\n  7.289087772369385,\n  8.403202056884766,\n  8.493597030639648,\n  9.279403686523438,\n  9.994264602661133,\n  10.030980110168457,\n  10.436870574951172,\n  11.788615226745605,\n  15.832330703735352,\n  17.444000244140625,\n  18.93501091003418,\n  21.05604362487793,\n  21.05657958984375,\n  21.632400512695312],\n [-25.527816772460938,\n  -24.470781326293945,\n  -21.25605583190918,\n  -17.96636199951172,\n  -16.691007614135742,\n  -16.553070068359375,\n  -16.44194793701172,\n  -16.37671661376953,\n  -15.748562812805176,\n  -15.181994438171387,\n  -14.1464204788208,\n  -13.430801391601562,\n  -13.397112846374512,\n  -13.239425659179688,\n  -12.000919342041016,\n  -11.869081497192383,\n  -11.272846221923828,\n  -10.581294059753418,\n  -10.30888843536377,\n  -10.196712493896484,\n  -9.780020713806152,\n  -9.035539627075195,\n  -8.794240951538086,\n  -8.610491752624512,\n  -8.509501457214355,\n  -8.114187240600586,\n  -7.892580509185791,\n  -7.832390785217285,\n  -7.807457447052002,\n  -7.777426242828369,\n  -7.070793151855469,\n  -6.935001850128174,\n  -6.907237529754639,\n  -6.441354274749756,\n  -6.102954864501953,\n  -6.09979772567749,\n  -6.066582202911377,\n  -6.055020809173584,\n  -6.008251190185547,\n  -5.556607723236084,\n  -5.420318603515625,\n  -5.258108615875244,\n  -5.1628031730651855,\n  -5.037083625793457,\n  -4.933573246002197,\n  -4.85708475112915,\n  -4.588736534118652,\n  -4.189174175262451,\n  -4.150174140930176,\n  -3.8460164070129395,\n  -3.0802321434020996,\n  -3.0554959774017334,\n  -2.6045565605163574,\n  -2.4066641330718994,\n  -2.3178091049194336,\n  -2.2765989303588867,\n  -1.326755166053772,\n  -1.3175466060638428,\n  -0.8764684200286865,\n  -0.8204857110977173,\n  -0.6078576445579529,\n  -0.14748874306678772,\n  0.03384458273649216,\n  1.13774573802948,\n  1.2582652568817139,\n  1.5518323183059692,\n  1.759530782699585,\n  2.0418384075164795,\n  2.0421247482299805,\n  2.0900585651397705,\n  2.2225425243377686,\n  2.5621581077575684,\n  2.634566307067871,\n  3.2344024181365967,\n  3.5264554023742676,\n  3.9798927307128906,\n  4.528395175933838,\n  4.980359077453613,\n  5.023870468139648,\n  5.101649761199951,\n  5.4843430519104,\n  5.499208927154541,\n  5.767688751220703,\n  6.055461406707764,\n  6.145554065704346,\n  6.502953052520752,\n  7.170252799987793,\n  8.151906967163086,\n  8.231554985046387,\n  8.923933982849121,\n  9.553803443908691,\n  9.586153030395508,\n  9.94378662109375,\n  11.134817123413086,\n  14.69776439666748,\n  16.117816925048828,\n  17.431556701660156,\n  19.300413131713867,\n  19.300886154174805,\n  19.808244705200195],\n [-22.067176818847656,\n  -21.124095916748047,\n  -18.25593376159668,\n  -15.320884704589844,\n  -14.183019638061523,\n  -14.059952735900879,\n  -13.960810661315918,\n  -13.90261173248291,\n  -13.34217643737793,\n  -12.836686134338379,\n  -11.912752151489258,\n  -11.274280548095703,\n  -11.244223594665527,\n  -11.103535652160645,\n  -9.998546600341797,\n  -9.880922317504883,\n  -9.348963737487793,\n  -8.731964111328125,\n  -8.48892593383789,\n  -8.388842582702637,\n  -8.017072677612305,\n  -7.352850437164307,\n  -7.137564659118652,\n  -6.9736247062683105,\n  -6.883521556854248,\n  -6.530824184417725,\n  -6.333107948303223,\n  -6.279407024383545,\n  -6.257161617279053,\n  -6.230367660522461,\n  -5.599913597106934,\n  -5.478761196136475,\n  -5.4539899826049805,\n  -5.038331031799316,\n  -4.73641300201416,\n  -4.733596324920654,\n  -4.703961372375488,\n  -4.693646430969238,\n  -4.651918888092041,\n  -4.248964786529541,\n  -4.127368450164795,\n  -3.982645273208618,\n  -3.8976142406463623,\n  -3.7854480743408203,\n  -3.69309663772583,\n  -3.6248538494110107,\n  -3.385435104370117,\n  -3.028947353363037,\n  -2.9941515922546387,\n  -2.7227838039398193,\n  -2.039555072784424,\n  -2.0174853801727295,\n  -1.6151596307754517,\n  -1.438601016998291,\n  -1.3593250513076782,\n  -1.3225574493408203,\n  -0.47511163353919983,\n  -0.46689584851264954,\n  -0.07336807250976562,\n  -0.023420605808496475,\n  0.16628508269786835,\n  0.5770238637924194,\n  0.7388085722923279,\n  1.7237036228179932,\n  1.8312305212020874,\n  2.0931496620178223,\n  2.2784571647644043,\n  2.5303306579589844,\n  2.530586004257202,\n  2.573352098464966,\n  2.691553831100464,\n  2.9945571422576904,\n  3.059159278869629,\n  3.594330072402954,\n  3.854898452758789,\n  4.259452819824219,\n  4.748823642730713,\n  5.152063846588135,\n  5.190884590148926,\n  5.260278701782227,\n  5.601716041564941,\n  5.614979267120361,\n  5.854515075683594,\n  6.111264705657959,\n  6.191644668579102,\n  6.510514736175537,\n  7.1058759689331055,\n  7.98170280456543,\n  8.052763938903809,\n  8.670501708984375,\n  9.232467651367188,\n  9.261330604553223,\n  9.580409049987793,\n  10.643040657043457,\n  13.821883201599121,\n  15.088847160339355,\n  16.26095962524414,\n  17.9283447265625,\n  17.92876625061035,\n  18.381427764892578],\n [-19.276042938232422,\n  -18.424091339111328,\n  -15.833085060119629,\n  -13.181654930114746,\n  -12.153742790222168,\n  -12.04256820678711,\n  -11.953006744384766,\n  -11.900431632995605,\n  -11.39415168762207,\n  -10.937507629394531,\n  -10.102855682373047,\n  -9.526080131530762,\n  -9.49892807006836,\n  -9.371834754943848,\n  -8.373621940612793,\n  -8.267363548278809,\n  -7.786808967590332,\n  -7.22943115234375,\n  -7.009877681732178,\n  -6.919466018676758,\n  -6.583621025085449,\n  -5.983583450317383,\n  -5.7891011238098145,\n  -5.641002655029297,\n  -5.559606552124023,\n  -5.24099063873291,\n  -5.062380313873291,\n  -5.01386833190918,\n  -4.993772506713867,\n  -4.969567775726318,\n  -4.400035381317139,\n  -4.290590286254883,\n  -4.26821231842041,\n  -3.8927195072174072,\n  -3.619976043701172,\n  -3.617431640625,\n  -3.590660333633423,\n  -3.5813422203063965,\n  -3.543646812438965,\n  -3.179630756378174,\n  -3.069784641265869,\n  -2.9390463829040527,\n  -2.862231731414795,\n  -2.760904550552368,\n  -2.6774771213531494,\n  -2.615828514099121,\n  -2.399545431137085,\n  -2.077505588531494,\n  -2.046072244644165,\n  -1.8009270429611206,\n  -1.1837197542190552,\n  -1.1637827157974243,\n  -0.8003342151641846,\n  -0.640836775302887,\n  -0.5692213177680969,\n  -0.5360066294670105,\n  0.22954916954040527,\n  0.23697106540203094,\n  0.5924716591835022,\n  0.637592613697052,\n  0.8089667558670044,\n  1.1800153255462646,\n  1.3261665105819702,\n  2.2158896923065186,\n  2.313025951385498,\n  2.549635410308838,\n  2.717036485671997,\n  2.944571018218994,\n  2.9448018074035645,\n  2.9834353923797607,\n  3.0902152061462402,\n  3.363938570022583,\n  3.4222981929779053,\n  3.905754804611206,\n  4.141143798828125,\n  4.506605625152588,\n  4.94868803024292,\n  5.312962532043457,\n  5.348031520843506,\n  5.410720348358154,\n  5.71916389465332,\n  5.73114538192749,\n  5.947534561157227,\n  6.179473876953125,\n  6.252087116241455,\n  6.540143966674805,\n  7.077974796295166,\n  7.869168758392334,\n  7.933363437652588,\n  8.491408348083496,\n  8.999070167541504,\n  9.02514362335205,\n  9.313389778137207,\n  10.273337364196777,\n  13.145004272460938,\n  14.289539337158203,\n  15.348388671875,\n  16.854652404785156,\n  16.855031967163086,\n  17.263954162597656],\n [-17.02360725402832,\n  -16.244606018066406,\n  -13.875457763671875,\n  -11.451059341430664,\n  -10.511163711547852,\n  -10.409507751464844,\n  -10.327615737915039,\n  -10.279541969299316,\n  -9.81661319732666,\n  -9.399069786071777,\n  -8.635885238647461,\n  -8.10849666595459,\n  -8.083669662475586,\n  -7.967459201812744,\n  -7.054719924926758,\n  -6.957560062408447,\n  -6.518153190612793,\n  -6.0085015296936035,\n  -5.807747840881348,\n  -5.7250776290893555,\n  -5.417989253997803,\n  -4.869330883026123,\n  -4.691501617431641,\n  -4.556084156036377,\n  -4.4816575050354,\n  -4.190323829650879,\n  -4.02700662612915,\n  -3.9826488494873047,\n  -3.964273452758789,\n  -3.942141532897949,\n  -3.4213757514953613,\n  -3.3213019371032715,\n  -3.3008406162261963,\n  -2.9574995040893555,\n  -2.7081100940704346,\n  -2.7057836055755615,\n  -2.681304693222046,\n  -2.6727843284606934,\n  -2.6383166313171387,\n  -2.3054697513580322,\n  -2.205029249191284,\n  -2.0854856967926025,\n  -2.0152485370635986,\n  -1.9225974082946777,\n  -1.846313714981079,\n  -1.7899439334869385,\n  -1.5921801328659058,\n  -1.2977153062820435,\n  -1.268973469734192,\n  -1.0448191165924072,\n  -0.48046091198921204,\n  -0.4622310400009155,\n  -0.12990324199199677,\n  0.015937041491270065,\n  0.08142035454511642,\n  0.11179099977016449,\n  0.8117952346801758,\n  0.8185816407203674,\n  1.1436420679092407,\n  1.1848994493484497,\n  1.3415995836257935,\n  1.680876612663269,\n  1.8145134449005127,\n  2.6280529499053955,\n  2.716871976852417,\n  2.9332215785980225,\n  3.0862884521484375,\n  3.2943403720855713,\n  3.294551134109497,\n  3.3298768997192383,\n  3.427513360977173,\n  3.6777989864349365,\n  3.731161594390869,\n  4.173221588134766,\n  4.388454914093018,\n  4.722623825073242,\n  5.126852512359619,\n  5.459935665130615,\n  5.492002010345459,\n  5.549322605133057,\n  5.831355094909668,\n  5.842310905456543,\n  6.0401716232299805,\n  6.252251148223877,\n  6.318646430969238,\n  6.582037925720215,\n  7.073816776275635,\n  7.797264099121094,\n  7.855961799621582,\n  8.366223335266113,\n  8.830416679382324,\n  8.854257583618164,\n  9.11782169342041,\n  9.995573043823242,\n  12.621350288391113,\n  13.667882919311523,\n  14.636066436767578,\n  16.013355255126953,\n  16.013702392578125,\n  16.387609481811523],\n [-15.204924583435059,\n  -14.484371185302734,\n  -12.292978286743164,\n  -10.050481796264648,\n  -9.181105613708496,\n  -9.087077140808105,\n  -9.01132869720459,\n  -8.966862678527832,\n  -8.538666725158691,\n  -8.152451515197754,\n  -7.446528434753418,\n  -6.958709716796875,\n  -6.9357452392578125,\n  -6.828253746032715,\n  -5.983996868133545,\n  -5.894127368927002,\n  -5.487689018249512,\n  -5.0162763595581055,\n  -4.830584526062012,\n  -4.754117012023926,\n  -4.470069885253906,\n  -3.9625768661499023,\n  -3.7980897426605225,\n  -3.672832727432251,\n  -3.603990077972412,\n  -3.33451509475708,\n  -3.1834518909454346,\n  -3.1424221992492676,\n  -3.125425338745117,\n  -3.1049540042877197,\n  -2.623260974884033,\n  -2.530695676803589,\n  -2.5117695331573486,\n  -2.1941893100738525,\n  -1.963511347770691,\n  -1.9613593816757202,\n  -1.938717007637024,\n  -1.9308359622955322,\n  -1.8989545106887817,\n  -1.591080904006958,\n  -1.4981764554977417,\n  -1.3876020908355713,\n  -1.3226348161697388,\n  -1.2369352579116821,\n  -1.1663750410079956,\n  -1.1142346858978271,\n  -0.9313089847564697,\n  -0.6589377522468567,\n  -0.6323524117469788,\n  -0.42501628398895264,\n  0.09699846059083939,\n  0.11386056244373322,\n  0.4212539494037628,\n  0.5561519265174866,\n  0.616722047328949,\n  0.6448140144348145,\n  1.2922972440719604,\n  1.298574447631836,\n  1.5992457866668701,\n  1.637407660484314,\n  1.7823506593704224,\n  2.0961718559265137,\n  2.2197821140289307,\n  2.9722821712493896,\n  3.0544371604919434,\n  3.254554033279419,\n  3.396136522293091,\n  3.588578224182129,\n  3.588773250579834,\n  3.621448516845703,\n  3.711759328842163,\n  3.9432661533355713,\n  3.9926249980926514,\n  4.401517391204834,\n  4.600602149963379,\n  4.909698486328125,\n  5.283597946166992,\n  5.5916900634765625,\n  5.621350288391113,\n  5.674370288848877,\n  5.935242176055908,\n  5.945375919342041,\n  6.128391265869141,\n  6.324558258056641,\n  6.385972023010254,\n  6.62960147857666,\n  7.084482192993164,\n  7.753649711608887,\n  7.807943344116211,\n  8.27992057800293,\n  8.709284782409668,\n  8.731337547302246,\n  8.975126266479492,\n  9.787020683288574,\n  12.215786933898926,\n  13.183798789978027,\n  14.079340934753418,\n  15.353291511535645,\n  15.353612899780273,\n  15.69946575164795],\n [-13.7357177734375,\n  -13.06203556060791,\n  -11.013188362121582,\n  -8.916561126708984,\n  -8.103736877441406,\n  -8.015825271606445,\n  -7.945003986358643,\n  -7.903429985046387,\n  -7.503087520599365,\n  -7.141994953155518,\n  -6.481991291046143,\n  -6.025904178619385,\n  -6.0044331550598145,\n  -5.903934001922607,\n  -5.114594459533691,\n  -5.0305705070495605,\n  -4.650570392608643,\n  -4.209822177886963,\n  -4.036209583282471,\n  -3.9647161960601807,\n  -3.6991453170776367,\n  -3.2246639728546143,\n  -3.070876359939575,\n  -2.9537672996520996,\n  -2.8894026279449463,\n  -2.6374564170837402,\n  -2.4962196350097656,\n  -2.4578588008880615,\n  -2.441967725753784,\n  -2.422827959060669,\n  -1.9724682569503784,\n  -1.885924220085144,\n  -1.8682290315628052,\n  -1.5713067054748535,\n  -1.3556339740753174,\n  -1.3536220788955688,\n  -1.3324525356292725,\n  -1.3250840902328491,\n  -1.2952765226364136,\n  -1.007429599761963,\n  -0.9205683469772339,\n  -0.8171865940093994,\n  -0.7564453482627869,\n  -0.6763203740119934,\n  -0.6103500127792358,\n  -0.5616012215614319,\n  -0.39057457447052,\n  -0.13592055439949036,\n  -0.11106456071138382,\n  0.0827847421169281,\n  0.5708433985710144,\n  0.5866086483001709,\n  0.8740066289901733,\n  1.0001296997070312,\n  1.0567599534988403,\n  1.083024501800537,\n  1.6883901357650757,\n  1.6942590475082397,\n  1.975372314453125,\n  2.011051893234253,\n  2.146566390991211,\n  2.439974308013916,\n  2.5555436611175537,\n  3.2590951919555664,\n  3.3359060287475586,\n  3.523005723953247,\n  3.655378580093384,\n  3.8353021144866943,\n  3.835484743118286,\n  3.8660342693328857,\n  3.9504706859588623,\n  4.1669182777404785,\n  4.213066577911377,\n  4.595361232757568,\n  4.781496047973633,\n  5.070486068725586,\n  5.4200639724731445,\n  5.708115100860596,\n  5.735846042633057,\n  5.785417556762695,\n  6.029320240020752,\n  6.03879451751709,\n  6.20990514755249,\n  6.393311977386475,\n  6.450730800628662,\n  6.6785125732421875,\n  7.103804111480713,\n  7.729443550109863,\n  7.780205726623535,\n  8.221481323242188,\n  8.622916221618652,\n  8.643534660339355,\n  8.871465682983398,\n  9.630547523498535,\n  11.901327133178711,\n  12.806371688842773,\n  13.643659591674805,\n  14.834742546081543,\n  14.835042953491211,\n  15.158398628234863],\n [-12.548260688781738,\n  -11.912196159362793,\n  -9.97775650024414,\n  -7.998204708099365,\n  -7.230768203735352,\n  -7.147764682769775,\n  -7.080898761749268,\n  -7.0416460037231445,\n  -6.663658618927002,\n  -6.322729110717773,\n  -5.69957971572876,\n  -5.268960475921631,\n  -5.248688697814941,\n  -5.153800964355469,\n  -4.408538341522217,\n  -4.3292059898376465,\n  -3.9704248905181885,\n  -3.554287910461426,\n  -3.3903696537017822,\n  -3.322868585586548,\n  -3.072127103805542,\n  -2.624140739440918,\n  -2.478940725326538,\n  -2.368370771408081,\n  -2.307600259780884,\n  -2.0697226524353027,\n  -1.9363723993301392,\n  -1.900153636932373,\n  -1.8851499557495117,\n  -1.8670789003372192,\n  -1.4418671131134033,\n  -1.360155701637268,\n  -1.3434486389160156,\n  -1.0631064176559448,\n  -0.8594767451286316,\n  -0.8575771450996399,\n  -0.8375897407531738,\n  -0.830632746219635,\n  -0.8024895787239075,\n  -0.5307159423828125,\n  -0.4487050175666809,\n  -0.35109609365463257,\n  -0.29374656081199646,\n  -0.2180957943201065,\n  -0.1558091938495636,\n  -0.10978250950574875,\n  0.051694076508283615,\n  0.2921282947063446,\n  0.315596342086792,\n  0.4986211657524109,\n  0.959426760673523,\n  0.9743117094039917,\n  1.2456614971160889,\n  1.3647419214248657,\n  1.4182099103927612,\n  1.4430078268051147,\n  2.0145699977874756,\n  2.020111322402954,\n  2.285527229309082,\n  2.319214344024658,\n  2.447161912918091,\n  2.7241859436035156,\n  2.8333020210266113,\n  3.4975674152374268,\n  3.570089101791382,\n  3.74674129486084,\n  3.871722459793091,\n  4.041599273681641,\n  4.041771411895752,\n  4.070615291595459,\n  4.150336742401123,\n  4.354698181152344,\n  4.398269176483154,\n  4.759216785430908,\n  4.934957981109619,\n  5.207810878753662,\n  5.537868499755859,\n  5.809834957122803,\n  5.836017608642578,\n  5.8828206062316895,\n  6.113103866577148,\n  6.122049331665039,\n  6.283605098724365,\n  6.456770420074463,\n  6.510982990264893,\n  6.726045608520508,\n  7.127589225769043,\n  7.718293190002441,\n  7.766220569610596,\n  8.182855606079102,\n  8.561875343322754,\n  8.581341743469238,\n  8.796545028686523,\n  9.513239860534668,\n  11.657219886779785,\n  12.511727333068848,\n  13.302261352539062,\n  14.426834106445312,\n  14.427118301391602,\n  14.732418060302734],\n [-11.588088035583496,\n  -10.982240676879883,\n  -9.139697074890137,\n  -7.254184246063232,\n  -6.523205280303955,\n  -6.444145202636719,\n  -6.380455493927002,\n  -6.343067646026611,\n  -5.983036518096924,\n  -5.658303260803223,\n  -5.064756870269775,\n  -4.6545939445495605,\n  -4.635285377502441,\n  -4.544905662536621,\n  -3.8350467681884766,\n  -3.7594833374023438,\n  -3.417746067047119,\n  -3.0213780403137207,\n  -2.8652467727661133,\n  -2.800952196121216,\n  -2.5621225833892822,\n  -2.1354176998138428,\n  -1.9971154928207397,\n  -1.8917983770370483,\n  -1.8339147567749023,\n  -1.6073375940322876,\n  -1.480322241783142,\n  -1.445824146270752,\n  -1.4315330982208252,\n  -1.4143205881118774,\n  -1.0093086957931519,\n  -0.9314789175987244,\n  -0.9155654907226562,\n  -0.6485410928726196,\n  -0.4545849859714508,\n  -0.45277559757232666,\n  -0.43373769521713257,\n  -0.4271112382411957,\n  -0.40030497312545776,\n  -0.14144207537174225,\n  -0.0633271187543869,\n  0.02964484691619873,\n  0.0842699483036995,\n  0.15632690489292145,\n  0.21565455198287964,\n  0.25949472188949585,\n  0.4133002758026123,\n  0.6423125863075256,\n  0.6646657586097717,\n  0.8389959335327148,\n  1.2779107093811035,\n  1.292088508605957,\n  1.55054771900177,\n  1.663971185684204,\n  1.7148991823196411,\n  1.7385190725326538,\n  2.282928943634033,\n  2.2882070541381836,\n  2.5410141944885254,\n  2.573101043701172,\n  2.6949703693389893,\n  2.958834171295166,\n  3.0627667903900146,\n  3.6954758167266846,\n  3.764552593231201,\n  3.9328126907348633,\n  4.051856517791748,\n  4.213663101196289,\n  4.213827133178711,\n  4.2413010597229,\n  4.317235469818115,\n  4.51188850402832,\n  4.553389549255371,\n  4.897190093994141,\n  5.064582347869873,\n  5.3244733810424805,\n  5.638851642608643,\n  5.897898197174072,\n  5.922836780548096,\n  5.967416763305664,\n  6.186760425567627,\n  6.1952805519104,\n  6.349161624908447,\n  6.514101028442383,\n  6.565738201141357,\n  6.7705841064453125,\n  7.153051853179932,\n  7.715693950653076,\n  7.761344909667969,\n  8.158186912536621,\n  8.519201278686523,\n  8.537742614746094,\n  8.74272346496582,\n  9.425371170043945,\n  11.467500686645508,\n  12.281414031982422,\n  13.034393310546875,\n  14.10554313659668,\n  14.105813980102539,\n  14.396610260009766],\n [-10.811363220214844,\n  -10.229805946350098,\n  -8.46113395690918,\n  -6.651216983795166,\n  -5.949544429779053,\n  -5.873653888702393,\n  -5.8125176429748535,\n  -5.776628494262695,\n  -5.431032180786133,\n  -5.11931848526001,\n  -4.5495686531066895,\n  -4.155850410461426,\n  -4.13731575012207,\n  -4.0505595207214355,\n  -3.3691604137420654,\n  -3.296626567840576,\n  -2.968590497970581,\n  -2.58811354637146,\n  -2.438242197036743,\n  -2.376525402069092,\n  -2.147270917892456,\n  -1.7376737594604492,\n  -1.6049163341522217,\n  -1.5038214921951294,\n  -1.448258638381958,\n  -1.2307655811309814,\n  -1.1088424921035767,\n  -1.0757274627685547,\n  -1.0620094537734985,\n  -1.0454870462417603,\n  -0.6567130088806152,\n  -0.582003653049469,\n  -0.5667282342910767,\n  -0.31040942668914795,\n  -0.12422949820756912,\n  -0.12249265611171722,\n  -0.104218028485775,\n  -0.09785724431276321,\n  -0.07212571799755096,\n  0.17635875940322876,\n  0.25134190917015076,\n  0.340586394071579,\n  0.39302143454551697,\n  0.4621894657611847,\n  0.519138514995575,\n  0.5612210631370544,\n  0.7088601589202881,\n  0.9286907911300659,\n  0.950147807598114,\n  1.1174886226654053,\n  1.5388063192367554,\n  1.5524157285690308,\n  1.800512671470642,\n  1.909388780593872,\n  1.9582748413085938,\n  1.9809478521347046,\n  2.503530979156494,\n  2.5085973739624023,\n  2.7512691020965576,\n  2.782069444656372,\n  2.899052858352661,\n  3.1523375511169434,\n  3.252103328704834,\n  3.859445571899414,\n  3.925752639770508,\n  4.08726692199707,\n  4.2015380859375,\n  4.356857776641846,\n  4.357015132904053,\n  4.383387088775635,\n  4.456276893615723,\n  4.643126010894775,\n  4.6829633712768555,\n  5.012979984283447,\n  5.173661231994629,\n  5.423132419586182,\n  5.7249064445495605,\n  5.973567485809326,\n  5.997506141662598,\n  6.040298938751221,\n  6.250848293304443,\n  6.259027004241943,\n  6.406738758087158,\n  6.565064907073975,\n  6.6146321296691895,\n  6.811265468597412,\n  7.178399085998535,\n  7.7184834480285645,\n  7.762304306030273,\n  8.14323616027832,\n  8.489776611328125,\n  8.507574081420898,\n  8.704336166381836,\n  9.359615325927734,\n  11.319870948791504,\n  12.101153373718262,\n  12.823944091796875,\n  13.852148056030273,\n  13.852408409118652,\n  14.131546020507812],\n [-10.182785987854004,\n  -9.620768547058105,\n  -7.911523342132568,\n  -6.162417888641357,\n  -5.484321117401123,\n  -5.410980701446533,\n  -5.351898670196533,\n  -5.317215442657471,\n  -4.9832305908203125,\n  -4.681990146636963,\n  -4.131383895874023,\n  -3.750894069671631,\n  -3.7329823970794678,\n  -3.6491410732269287,\n  -2.9906365871429443,\n  -2.9205398559570312,\n  -2.6035256385803223,\n  -2.235832691192627,\n  -2.090996742248535,\n  -2.03135347366333,\n  -1.8098018169403076,\n  -1.4139670133590698,\n  -1.2856701612472534,\n  -1.187972068786621,\n  -1.1342761516571045,\n  -0.9240906238555908,\n  -0.8062641024589539,\n  -0.7742617130279541,\n  -0.761004626750946,\n  -0.745037317276001,\n  -0.3693259060382843,\n  -0.29712674021720886,\n  -0.2823645770549774,\n  -0.03465794026851654,\n  0.14526645839214325,\n  0.14694494009017944,\n  0.1646055430173874,\n  0.170752614736557,\n  0.19561956822872162,\n  0.4357551038265228,\n  0.5082188844680786,\n  0.5944647789001465,\n  0.6451380848884583,\n  0.7119820713996887,\n  0.767017662525177,\n  0.8076862692832947,\n  0.9503647685050964,\n  1.1628092527389526,\n  1.1835453510284424,\n  1.3452636003494263,\n  1.752425193786621,\n  1.7655773162841797,\n  2.005338430404663,\n  2.1105563640594482,\n  2.15779972076416,\n  2.179711103439331,\n  2.6847357749938965,\n  2.689631938934326,\n  2.924149751663208,\n  2.9539153575897217,\n  3.0669682025909424,\n  3.3117427825927734,\n  3.408156394958496,\n  3.9950923919677734,\n  4.059171676635742,\n  4.215259075164795,\n  4.325690746307373,\n  4.4757914543151855,\n  4.475943565368652,\n  4.501429557800293,\n  4.571870803833008,\n  4.75244140625,\n  4.790940284729004,\n  5.109869003295898,\n  5.265151023864746,\n  5.506240367889404,\n  5.797874927520752,\n  6.038180828094482,\n  6.061315536499023,\n  6.102670192718506,\n  6.306145191192627,\n  6.314049243927002,\n  6.456798076629639,\n  6.609804630279541,\n  6.657706260681152,\n  6.8477325439453125,\n  7.202530860900879,\n  7.72446870803833,\n  7.766817092895508,\n  8.134949684143066,\n  8.469846725463867,\n  8.48704719543457,\n  8.677197456359863,\n  9.310460090637207,\n  11.204852104187012,\n  11.959882736206055,\n  12.658388137817383,\n  13.652046203613281,\n  13.652297019958496,\n  13.9220552444458],\n [-9.673905372619629,\n  -9.127617835998535,\n  -7.466211318969727,\n  -5.766060829162598,\n  -5.106943130493164,\n  -5.0356550216674805,\n  -4.978226661682129,\n  -4.944514274597168,\n  -4.619877338409424,\n  -4.327067852020264,\n  -3.791872262954712,\n  -3.422031879425049,\n  -3.4046216011047363,\n  -3.323126792907715,\n  -2.6830527782440186,\n  -2.6149179935455322,\n  -2.306776523590088,\n  -1.9493745565414429,\n  -1.808592438697815,\n  -1.750618577003479,\n  -1.5352678298950195,\n  -1.1505117416381836,\n  -1.0258057117462158,\n  -0.9308421015739441,\n  -0.8786489963531494,\n  -0.6743462681770325,\n  -0.5598175525665283,\n  -0.5287108421325684,\n  -0.5158247947692871,\n  -0.5003044009208679,\n  -0.13510854542255402,\n  -0.06493011862039566,\n  -0.050581131130456924,\n  0.1901925802230835,\n  0.36508116126060486,\n  0.36671265959739685,\n  0.3838789761066437,\n  0.3898540139198303,\n  0.4140249788761139,\n  0.6474394798278809,\n  0.7178751230239868,\n  0.8017071485519409,\n  0.8509621620178223,\n  0.9159352779388428,\n  0.9694305658340454,\n  1.0089608430862427,\n  1.1476460695266724,\n  1.35414457321167,\n  1.374300241470337,\n  1.5314922332763672,\n  1.927258014678955,\n  1.9400420188903809,\n  2.1730926036834717,\n  2.2753655910491943,\n  2.321286916732788,\n  2.3425848484039307,\n  2.833474636077881,\n  2.838233709335327,\n  3.066187858581543,\n  3.095120429992676,\n  3.2050089836120605,\n  3.4429328441619873,\n  3.5366477966308594,\n  4.107156276702881,\n  4.169442176818848,\n  4.321160793304443,\n  4.428501605987549,\n  4.574401378631592,\n  4.574549674987793,\n  4.599322319030762,\n  4.667791366577148,\n  4.843308448791504,\n  4.880730152130127,\n  5.190732002258301,\n  5.341668128967285,\n  5.576009750366211,\n  5.8594818115234375,\n  6.093061923980713,\n  6.115549087524414,\n  6.1557464599609375,\n  6.353526592254639,\n  6.361209392547607,\n  6.49996280670166,\n  6.64868688583374,\n  6.695247650146484,\n  6.879955768585205,\n  7.224823474884033,\n  7.732153415679932,\n  7.773316383361816,\n  8.131145477294922,\n  8.456668853759766,\n  8.473387718200684,\n  8.65821647644043,\n  9.273755073547363,\n  11.11512565612793,\n  11.849024772644043,\n  12.527979850769043,\n  13.493826866149902,\n  13.494071006774902,\n  13.756278991699219],\n [-9.261781692504883,\n  -8.728165626525879,\n  -7.105295658111572,\n  -5.444580554962158,\n  -4.800751209259033,\n  -4.731117248535156,\n  -4.675020694732666,\n  -4.642090320587158,\n  -4.3249831199646,\n  -4.038965702056885,\n  -3.516183853149414,\n  -3.1549222469329834,\n  -3.13791561126709,\n  -3.0583112239837646,\n  -2.433084011077881,\n  -2.3665294647216797,\n  -2.065535306930542,\n  -1.716423511505127,\n  -1.578906774520874,\n  -1.5222777128219604,\n  -1.3119219541549683,\n  -0.9360904097557068,\n  -0.8142769932746887,\n  -0.7215160727500916,\n  -0.6705335974693298,\n  -0.4709697663784027,\n  -0.35909754037857056,\n  -0.3287123739719391,\n  -0.3161252439022064,\n  -0.3009648025035858,\n  0.0557602196931839,\n  0.12431085109710693,\n  0.1383270025253296,\n  0.37351590394973755,\n  0.5443479418754578,\n  0.5459415912628174,\n  0.5627096891403198,\n  0.5685461759567261,\n  0.5921564698219299,\n  0.8201568722724915,\n  0.8889586925506592,\n  0.9708462357521057,\n  1.0189588069915771,\n  1.0824248790740967,\n  1.1346791982650757,\n  1.173292636871338,\n  1.3087610006332397,\n  1.510469675064087,\n  1.5301578044891357,\n  1.6837037801742554,\n  2.0702898502349854,\n  2.082777261734009,\n  2.31042218208313,\n  2.410322904586792,\n  2.45517897605896,\n  2.475982904434204,\n  2.955486536026001,\n  2.960134983062744,\n  3.1828017234802246,\n  3.2110631465911865,\n  3.3184027671813965,\n  3.5508079528808594,\n  3.6423492431640625,\n  4.199624538421631,\n  4.260465621948242,\n  4.408665657043457,\n  4.513516426086426,\n  4.656032085418701,\n  4.656176567077637,\n  4.680374622344971,\n  4.747255802154541,\n  4.918701648712158,\n  4.955255031585693,\n  5.258066654205322,\n  5.405501842498779,\n  5.6344075202941895,\n  5.911304473876953,\n  6.139466762542725,\n  6.161432266235352,\n  6.200697422027588,\n  6.393889904022217,\n  6.401394367218018,\n  6.536929130554199,\n  6.682203769683838,\n  6.727684497833252,\n  6.908108234405518,\n  7.244976997375488,\n  7.740539073944092,\n  7.780747413635254,\n  8.130276679992676,\n  8.448249816894531,\n  8.464580535888672,\n  8.645122528076172,\n  9.246382713317871,\n  11.045042991638184,\n  11.761919021606445,\n  12.425125122070312,\n  13.368569374084473,\n  13.368807792663574,\n  13.624934196472168],\n [-8.927906036376953,\n  -8.404502868652344,\n  -6.81269645690918,\n  -5.1837687492370605,\n  -4.552262783050537,\n  -4.483961582183838,\n  -4.428938865661621,\n  -4.3966383934021,\n  -4.085601329803467,\n  -3.805058479309082,\n  -3.292283058166504,\n  -2.9379360675811768,\n  -2.921255111694336,\n  -2.8431742191314697,\n  -2.229914426803589,\n  -2.1646337509155273,\n  -1.869400978088379,\n  -1.5269713401794434,\n  -1.3920868635177612,\n  -1.3365416526794434,\n  -1.1302123069763184,\n  -0.7615744471549988,\n  -0.6420926451683044,\n  -0.5511072278022766,\n  -0.5011005997657776,\n  -0.30535656213760376,\n  -0.1956256479024887,\n  -0.1658220887184143,\n  -0.15347588062286377,\n  -0.13860562443733215,\n  0.21129140257835388,\n  0.27852991223335266,\n  0.2922777831554413,\n  0.5229650139808655,\n  0.6905271410942078,\n  0.6920903325080872,\n  0.7085375189781189,\n  0.7142622470855713,\n  0.7374206185340881,\n  0.9610569477081299,\n  1.0285418033599854,\n  1.108862042427063,\n  1.1560535430908203,\n  1.2183048725128174,\n  1.2695591449737549,\n  1.3074333667755127,\n  1.4403088092803955,\n  1.6381566524505615,\n  1.6574679613113403,\n  1.8080748319625854,\n  2.1872613430023193,\n  2.199509859085083,\n  2.422797203063965,\n  2.5207858085632324,\n  2.5647833347320557,\n  2.5851891040802,\n  3.0555145740509033,\n  3.0600743293762207,\n  3.2784790992736816,\n  3.306199550628662,\n  3.411484479904175,\n  3.6394412517547607,\n  3.7292304039001465,\n  4.275839328765869,\n  4.335515975952148,\n  4.480878829956055,\n  4.583723068237305,\n  4.7235107421875,\n  4.723652362823486,\n  4.747387409210205,\n  4.81298828125,\n  4.981152534484863,\n  5.0170063972473145,\n  5.314021587371826,\n  5.458634853363037,\n  5.683159351348877,\n  5.954756259918213,\n  6.178551197052002,\n  6.200096130371094,\n  6.238609790802002,\n  6.428104400634766,\n  6.435465335845947,\n  6.568406105041504,\n  6.710899829864502,\n  6.755510330200195,\n  6.932480335235596,\n  7.262901306152344,\n  7.7489776611328125,\n  7.788416385650635,\n  8.131255149841309,\n  8.44314193725586,\n  8.459160804748535,\n  8.636246681213379,\n  9.225998878479004,\n  10.990230560302734,\n  11.693385124206543,\n  12.343897819519043,\n  13.269283294677734,\n  13.269516944885254,\n  13.520740509033203],\n [-8.657336235046387,\n  -8.142171859741211,\n  -6.575418949127197,\n  -4.972128868103027,\n  -4.35056209564209,\n  -4.2833356857299805,\n  -4.2291789054870605,\n  -4.197387218475342,\n  -3.8912453651428223,\n  -3.6151180267333984,\n  -3.110413074493408,\n  -2.761643409729004,\n  -2.745224714279175,\n  -2.668372869491577,\n  -2.064765214920044,\n  -2.000511884689331,\n  -1.709925889968872,\n  -1.372885823249817,\n  -1.240124225616455,\n  -1.1854532957077026,\n  -0.9823713898658752,\n  -0.6195355653762817,\n  -0.5019342303276062,\n  -0.4123808741569519,\n  -0.3631612956523895,\n  -0.1704980731010437,\n  -0.06249423325061798,\n  -0.033159755170345306,\n  -0.021007858216762543,\n  -0.006371652241796255,\n  0.3380183279514313,\n  0.4041985869407654,\n  0.41773006319999695,\n  0.6447864770889282,\n  0.8097113966941833,\n  0.8112499117851257,\n  0.827438235282898,\n  0.83307284116745,\n  0.8558667898178101,\n  1.0759832859039307,\n  1.1424059867858887,\n  1.2214620113372803,\n  1.2679108381271362,\n  1.3291823863983154,\n  1.3796298503875732,\n  1.4169081449508667,\n  1.5476921796798706,\n  1.7424260377883911,\n  1.7614333629608154,\n  1.9096699953079224,\n  2.282888412475586,\n  2.2949440479278564,\n  2.5147171020507812,\n  2.61116361618042,\n  2.654468536376953,\n  2.674553155899048,\n  3.1374762058258057,\n  3.1419641971588135,\n  3.356931447982788,\n  3.3842155933380127,\n  3.4878435134887695,\n  3.712212324142456,\n  3.800588369369507,\n  4.33859395980835,\n  4.397331237792969,\n  4.540406227111816,\n  4.641631603240967,\n  4.779219627380371,\n  4.779358863830566,\n  4.802720546722412,\n  4.867288589477539,\n  5.032806396484375,\n  5.068095684051514,\n  5.36043643951416,\n  5.502773761749268,\n  5.723764419555664,\n  5.991086483001709,\n  6.211359024047852,\n  6.232564926147461,\n  6.270472049713135,\n  6.456984519958496,\n  6.464229583740234,\n  6.595077991485596,\n  6.7353291511535645,\n  6.779237270355225,\n  6.9534220695495605,\n  7.278642177581787,\n  7.757068634033203,\n  7.795886516571045,\n  8.133329391479492,\n  8.4403076171875,\n  8.456073760986328,\n  8.630373001098633,\n  9.21084213256836,\n  10.947306632995605,\n  11.639394760131836,\n  12.279668807983398,\n  13.190489768981934,\n  13.190719604492188,\n  13.437989234924316],\n [-8.438004493713379,\n  -7.929488182067871,\n  -6.38295316696167,\n  -4.800353050231934,\n  -4.186807155609131,\n  -4.120448589324951,\n  -4.066990852355957,\n  -4.035609245300293,\n  -3.7334179878234863,\n  -3.4608538150787354,\n  -2.9626619815826416,\n  -2.6183929443359375,\n  -2.602186441421509,\n  -2.5263261795043945,\n  -1.930507779121399,\n  -1.8670837879180908,\n  -1.5802475214004517,\n  -1.2475568056106567,\n  -1.1165084838867188,\n  -1.0625430345535278,\n  -0.8620818257331848,\n  -0.5039282441139221,\n  -0.3878445327281952,\n  -0.29944679141044617,\n  -0.25086236000061035,\n  -0.06068538501858711,\n  0.04592471569776535,\n  0.07488065212965012,\n  0.08687572926282883,\n  0.10132306069135666,\n  0.44126883149147034,\n  0.5065950751304626,\n  0.5199519395828247,\n  0.7440782785415649,\n  0.9068748950958252,\n  0.9083935618400574,\n  0.9243730306625366,\n  0.9299349188804626,\n  0.9524346590042114,\n  1.169710636138916,\n  1.235276222229004,\n  1.31331205368042,\n  1.3591614961624146,\n  1.4196423292160034,\n  1.469438910484314,\n  1.506235957145691,\n  1.6353323459625244,\n  1.8275532722473145,\n  1.8463153839111328,\n  1.9926389455795288,\n  2.3610410690307617,\n  2.372941255569458,\n  2.5898783206939697,\n  2.685080051422119,\n  2.7278263568878174,\n  2.7476518154144287,\n  3.2046008110046387,\n  3.209030866622925,\n  3.4212241172790527,\n  3.4481561183929443,\n  3.5504469871520996,\n  3.7719204425811768,\n  3.8591558933258057,\n  4.390218734741211,\n  4.448198318481445,\n  4.5894269943237305,\n  4.6893463134765625,\n  4.82515811920166,\n  4.825295925140381,\n  4.848355770111084,\n  4.912091255187988,\n  5.075472831726074,\n  5.110306739807129,\n  5.398874759674072,\n  5.539375305175781,\n  5.757513999938965,\n  6.021386623382568,\n  6.238816738128662,\n  6.259748935699463,\n  6.29716682434082,\n  6.4812726974487305,\n  6.488424301147461,\n  6.617583751678467,\n  6.756025314331055,\n  6.799366474151611,\n  6.971303462982178,\n  7.292326927185059,\n  7.764579772949219,\n  7.802896499633789,\n  8.135985374450684,\n  8.439001083374023,\n  8.454564094543457,\n  8.62661361694336,\n  9.199593544006348,\n  10.913649559020996,\n  11.596805572509766,\n  12.2288179397583,\n  13.127883911132812,\n  13.128111839294434,\n  13.372190475463867],\n [-8.260159492492676,\n  -7.757010459899902,\n  -6.226800441741943,\n  -4.660905838012695,\n  -4.053836822509766,\n  -3.988178253173828,\n  -3.9352846145629883,\n  -3.9042344093322754,\n  -3.6052331924438477,\n  -3.3355460166931152,\n  -2.8426129817962646,\n  -2.5019781589508057,\n  -2.4859423637390137,\n  -2.4108831882476807,\n  -1.8213540315628052,\n  -1.7585995197296143,\n  -1.4747910499572754,\n  -1.145612120628357,\n  -1.0159471035003662,\n  -0.9625513553619385,\n  -0.7642061114311218,\n  -0.4098331332206726,\n  -0.29497477412223816,\n  -0.20751014351844788,\n  -0.1594385802745819,\n  0.028730938211083412,\n  0.13421568274497986,\n  0.16286596655845642,\n  0.174734428524971,\n  0.189029261469841,\n  0.5253866314888,\n  0.5900232791900635,\n  0.6032391786575317,\n  0.8249996900558472,\n  0.9860778450965881,\n  0.9875805377960205,\n  1.0033912658691406,\n  1.008894443511963,\n  1.0311566591262817,\n  1.2461391687393188,\n  1.3110127449035645,\n  1.388224720954895,\n  1.4335901737213135,\n  1.493432641029358,\n  1.5427035093307495,\n  1.5791122913360596,\n  1.7068458795547485,\n  1.8970377445220947,\n  1.9156018495559692,\n  2.0603809356689453,\n  2.424894332885742,\n  2.436668634414673,\n  2.651315927505493,\n  2.7455127239227295,\n  2.7878077030181885,\n  2.8074238300323486,\n  3.259549617767334,\n  3.263932704925537,\n  3.473886013031006,\n  3.5005338191986084,\n  3.6017448902130127,\n  3.820880651473999,\n  3.9071953296661377,\n  4.432652473449707,\n  4.490019798278809,\n  4.629757404327393,\n  4.728621959686279,\n  4.863000392913818,\n  4.8631367683410645,\n  4.885953426361084,\n  4.9490156173706055,\n  5.110672950744629,\n  5.145139217376709,\n  5.430661201477051,\n  5.56967830657959,\n  5.785514831542969,\n  6.04660177230835,\n  6.261736869812012,\n  6.2824482917785645,\n  6.31947135925293,\n  6.501633167266846,\n  6.50870943069458,\n  6.636505603790283,\n  6.7734856605529785,\n  6.816370010375977,\n  6.986491680145264,\n  7.304126739501953,\n  7.7713942527771,\n  7.809306621551514,\n  8.13887882232666,\n  8.43869686126709,\n  8.454095840454102,\n  8.62432861328125,\n  9.19126033782959,\n  10.887223243713379,\n  11.5631685256958,\n  12.188508033752441,\n  13.078084945678711,\n  13.078310012817383,\n  13.319812774658203],\n [-8.115914344787598,\n  -7.617101669311523,\n  -6.1000800132751465,\n  -4.547680854797363,\n  -3.9458436965942383,\n  -3.880751132965088,\n  -3.8283135890960693,\n  -3.7975308895111084,\n  -3.5011065006256104,\n  -3.233743667602539,\n  -2.745059013366699,\n  -2.4073598384857178,\n  -2.3914623260498047,\n  -2.317049980163574,\n  -1.7326017618179321,\n  -1.670388102531433,\n  -1.3890256881713867,\n  -1.0626837015151978,\n  -0.9341362714767456,\n  -0.8812006711959839,\n  -0.6845648884773254,\n  -0.3332460820674896,\n  -0.21937762200832367,\n  -0.13266681134700775,\n  -0.08500954508781433,\n  0.10153822600841522,\n  0.20611386001110077,\n  0.23451721668243408,\n  0.2462833821773529,\n  0.26045501232147217,\n  0.5939134955406189,\n  0.6579930782318115,\n  0.671095073223114,\n  0.8909443616867065,\n  1.0506342649459839,\n  1.0521239042282104,\n  1.067798376083374,\n  1.0732542276382446,\n  1.0953246355056763,\n  1.3084542751312256,\n  1.3727686405181885,\n  1.4493151903152466,\n  1.494289755821228,\n  1.5536164045333862,\n  1.602462649345398,\n  1.6385575532913208,\n  1.7651903629302979,\n  1.9537429809570312,\n  1.9721471071243286,\n  2.115678310394287,\n  2.4770500659942627,\n  2.4887232780456543,\n  2.7015204429626465,\n  2.794905424118042,\n  2.8368358612060547,\n  2.856282949447632,\n  3.3045120239257812,\n  3.3088574409484863,\n  3.5170013904571533,\n  3.543419361114502,\n  3.6437580585479736,\n  3.8610050678253174,\n  3.946575880050659,\n  4.467504501342773,\n  4.524377346038818,\n  4.6629109382629395,\n  4.760923385620117,\n  4.894143581390381,\n  4.894278526306152,\n  4.916898727416992,\n  4.97941780090332,\n  5.139681339263916,\n  5.1738505363464355,\n  5.456912040710449,\n  5.594730854034424,\n  5.808707237243652,\n  6.067543983459473,\n  6.280825138092041,\n  6.301357746124268,\n  6.338061809539795,\n  6.518653869628906,\n  6.525669097900391,\n  6.6523637771606445,\n  6.788163185119629,\n  6.830677509307861,\n  6.999333381652832,\n  7.314230918884277,\n  7.77747106552124,\n  7.815056800842285,\n  8.141789436340332,\n  8.4390230178833,\n  8.454288482666016,\n  8.623055458068848,\n  9.185099601745605,\n  10.866446495056152,\n  11.536565780639648,\n  12.156517028808594,\n  13.038426399230957,\n  13.038649559020996,\n  13.278070449829102],\n [-7.998893737792969,\n  -7.5035858154296875,\n  -5.997223377227783,\n  -4.4557318687438965,\n  -3.858123540878296,\n  -3.7934882640838623,\n  -3.7414190769195557,\n  -3.71085262298584,\n  -3.416511058807373,\n  -3.151026964187622,\n  -2.665776014328003,\n  -2.3304495811462402,\n  -2.314663887023926,\n  -2.240774154663086,\n  -1.6604324579238892,\n  -1.5986559391021729,\n  -1.319270372390747,\n  -0.9952215552330017,\n  -0.8675772547721863,\n  -0.815013587474823,\n  -0.6197594404220581,\n  -0.2709091007709503,\n  -0.15784072875976562,\n  -0.07173916697502136,\n  -0.024416757747530937,\n  0.16082027554512024,\n  0.26466113328933716,\n  0.29286491870880127,\n  0.3045484125614166,\n  0.3186204731464386,\n  0.6497359871864319,\n  0.7133653163909912,\n  0.7263752222061157,\n  0.9446797966957092,\n  1.1032476425170898,\n  1.1047269105911255,\n  1.1202912330627441,\n  1.1257086992263794,\n  1.1476240158081055,\n  1.3592561483383179,\n  1.4231185913085938,\n  1.4991273880004883,\n  1.5437859296798706,\n  1.6026957035064697,\n  1.6511987447738647,\n  1.687040090560913,\n  1.8127830028533936,\n  2.0000109672546387,\n  2.0182857513427734,\n  2.1608083248138428,\n  2.519641160964966,\n  2.5312321186065674,\n  2.7425341606140137,\n  2.8352630138397217,\n  2.876898765563965,\n  2.896209239959717,\n  3.3412890434265137,\n  3.3456039428710938,\n  3.5522851943969727,\n  3.5785176753997803,\n  3.6781513690948486,\n  3.893872022628784,\n  3.978841543197632,\n  4.496109962463379,\n  4.55258321762085,\n  4.69014310836792,\n  4.787467002868652,\n  4.9197516441345215,\n  4.919885635375977,\n  4.942346572875977,\n  5.004426002502441,\n  5.1635637283325195,\n  5.197493076324463,\n  5.478565692901611,\n  5.615416049957275,\n  5.8278889656066895,\n  6.084907054901123,\n  6.296689510345459,\n  6.317078113555908,\n  6.353524208068848,\n  6.5328474044799805,\n  6.539813041687012,\n  6.665617942810059,\n  6.8004631996154785,\n  6.842678546905518,\n  7.0101494789123535,\n  7.322834491729736,\n  7.782819747924805,\n  7.820141315460205,\n  8.144577980041504,\n  8.439723014831543,\n  8.45488166809082,\n  8.622462272644043,\n  9.180558204650879,\n  10.850090980529785,\n  11.515501976013184,\n  12.131096839904785,\n  13.00680923461914,\n  13.007031440734863,\n  13.244770050048828],\n [-7.903938293457031,\n  -7.411464214324951,\n  -5.913720607757568,\n  -4.381049156188965,\n  -3.7868597507476807,\n  -3.7225944995880127,\n  -3.670823335647583,\n  -3.6404316425323486,\n  -3.3477742671966553,\n  -3.0838091373443604,\n  -2.601334571838379,\n  -2.2679266929626465,\n  -2.2522313594818115,\n  -2.178764581680298,\n  -1.601743221282959,\n  -1.5403201580047607,\n  -1.262533187866211,\n  -0.9403384327888489,\n  -0.8134244680404663,\n  -0.7611615657806396,\n  -0.5670245885848999,\n  -0.22017022967338562,\n  -0.10774879902601242,\n  -0.022139888256788254,\n  0.02491176314651966,\n  0.20908893644809723,\n  0.31233564019203186,\n  0.3403780460357666,\n  0.35199472308158875,\n  0.36598625779151917,\n  0.6952072381973267,\n  0.7584725022315979,\n  0.771407961845398,\n  0.9884634613990784,\n  1.1461241245269775,\n  1.1475948095321655,\n  1.1630702018737793,\n  1.1684565544128418,\n  1.1902464628219604,\n  1.4006677865982056,\n  1.4641648530960083,\n  1.5397387742996216,\n  1.584141731262207,\n  1.642714500427246,\n  1.690940022468567,\n  1.7265762090682983,\n  1.8515998125076294,\n  2.0377564430236816,\n  2.055926561355591,\n  2.197633981704712,\n  2.5544135570526123,\n  2.5659382343292236,\n  2.776031255722046,\n  2.868229389190674,\n  2.9096271991729736,\n  2.9288270473480225,\n  3.3713600635528564,\n  3.375650405883789,\n  3.581149101257324,\n  3.607231616973877,\n  3.7062952518463135,\n  3.9207816123962402,\n  4.005264759063721,\n  4.51957368850708,\n  4.575723648071289,\n  4.712496757507324,\n  4.809263706207275,\n  4.940791130065918,\n  4.940924644470215,\n  4.9632568359375,\n  5.024981498718262,\n  5.183208465576172,\n  5.216943740844727,\n  5.496407985687256,\n  5.632475852966309,\n  5.8437323570251465,\n  6.09928035736084,\n  6.309850692749023,\n  6.330122947692871,\n  6.366360187530518,\n  6.544657230377197,\n  6.551583290100098,\n  6.676668167114258,\n  6.810741901397705,\n  6.852716445922852,\n  7.019228935241699,\n  7.330124378204346,\n  7.787477970123291,\n  7.824585914611816,\n  8.14716625213623,\n  8.44062328338623,\n  8.455695152282715,\n  8.622316360473633,\n  9.17721939086914,\n  10.837199211120605,\n  11.49880313873291,\n  12.110876083374023,\n  12.98157787322998,\n  12.98179817199707,\n  13.21817684173584],\n [-7.826869964599609,\n  -7.336688995361328,\n  -5.845917224884033,\n  -4.320380687713623,\n  -3.728957414627075,\n  -3.6649913787841797,\n  -3.6134610176086426,\n  -3.5832109451293945,\n  -3.2919158935546875,\n  -3.029179573059082,\n  -2.5489509105682373,\n  -2.217095136642456,\n  -2.201472759246826,\n  -2.1283481121063232,\n  -1.5540128946304321,\n  -1.4928756952285767,\n  -1.2163819074630737,\n  -0.8956869840621948,\n  -0.7693638205528259,\n  -0.7173442244529724,\n  -0.5241109728813171,\n  -0.1788712739944458,\n  -0.06697317957878113,\n  0.01823720894753933,\n  0.06506982445716858,\n  0.24838963150978088,\n  0.35115569829940796,\n  0.3790675699710846,\n  0.39063015580177307,\n  0.4045565724372864,\n  0.7322449684143066,\n  0.7952157258987427,\n  0.8080909848213196,\n  1.0241360664367676,\n  1.1810626983642578,\n  1.182526707649231,\n  1.1979299783706665,\n  1.2032912969589233,\n  1.2249797582626343,\n  1.434421420097351,\n  1.4976229667663574,\n  1.5728451013565063,\n  1.6170413494110107,\n  1.6753414869308472,\n  1.7233424186706543,\n  1.7588127851486206,\n  1.8832542896270752,\n  2.068544387817383,\n  2.08663010597229,\n  2.227677583694458,\n  2.582796335220337,\n  2.5942673683166504,\n  2.803382396697998,\n  2.8951516151428223,\n  2.936356544494629,\n  2.9554669857025146,\n  3.395940065383911,\n  3.400210380554199,\n  3.604752540588379,\n  3.63071346282959,\n  3.729315996170044,\n  3.9428038597106934,\n  4.0268940925598145,\n  4.538808345794678,\n  4.594696998596191,\n  4.730833530426025,\n  4.827149868011475,\n  4.958065032958984,\n  4.958198070526123,\n  4.98042631149292,\n  5.041863441467285,\n  5.19935417175293,\n  5.232932090759277,\n  5.5110955238342285,\n  5.646529674530029,\n  5.8568034172058105,\n  6.111161231994629,\n  6.320751667022705,\n  6.34092903137207,\n  6.376997947692871,\n  6.554465293884277,\n  6.56135892868042,\n  6.685861587524414,\n  6.819311141967773,\n  6.861090183258057,\n  7.026827335357666,\n  7.336275577545166,\n  7.791500091552734,\n  7.82843542098999,\n  8.149514198303223,\n  8.441604614257812,\n  8.4566068649292,\n  8.622452735900879,\n  9.174772262573242,\n  10.827024459838867,\n  11.485548973083496,\n  12.094772338867188,\n  12.961421012878418,\n  12.961640357971191,\n  13.196918487548828],\n [-7.764307975769043,\n  -7.27598237991333,\n  -5.790853023529053,\n  -4.271090030670166,\n  -3.681905508041382,\n  -3.6181814670562744,\n  -3.5668461322784424,\n  -3.536710739135742,\n  -3.246518135070801,\n  -2.984776258468628,\n  -2.5063650608062744,\n  -2.1757652759552,\n  -2.1602022647857666,\n  -2.0873541831970215,\n  -1.5151927471160889,\n  -1.4542869329452515,\n  -1.1788396835327148,\n  -0.8593584895133972,\n  -0.7335134744644165,\n  -0.6816907525062561,\n  -0.4891888499259949,\n  -0.14525583386421204,\n  -0.033781249076128006,\n  0.051106635481119156,\n  0.09776199609041214,\n  0.2803879678249359,\n  0.3827650845050812,\n  0.41057130694389343,\n  0.4220901429653168,\n  0.43596383929252625,\n  0.7624120116233826,\n  0.8251444101333618,\n  0.8379709720611572,\n  1.0531983375549316,\n  1.2095310688018799,\n  1.2109894752502441,\n  1.2263344526290894,\n  1.2316755056381226,\n  1.253281831741333,\n  1.4619308710098267,\n  1.5248931646347046,\n  1.5998305082321167,\n  1.6438595056533813,\n  1.7019389867782593,\n  1.7497583627700806,\n  1.7850943803787231,\n  1.9090650081634521,\n  2.093653678894043,\n  2.111670970916748,\n  2.2521846294403076,\n  2.605959415435791,\n  2.617387056350708,\n  2.8257105350494385,\n  2.9171321392059326,\n  2.958181142807007,\n  2.977219343185425,\n  3.416025400161743,\n  3.4202795028686523,\n  3.6240475177764893,\n  3.6499102115631104,\n  3.7481393814086914,\n  3.9608192443847656,\n  4.044590950012207,\n  4.554568290710449,\n  4.610245227813721,\n  4.745866775512695,\n  4.841818332672119,\n  4.972238063812256,\n  4.972370147705078,\n  4.994514465332031,\n  5.055719375610352,\n  5.212613582611084,\n  5.24606466293335,\n  5.523175239562988,\n  5.658096790313721,\n  5.867574691772461,\n  6.120970249176025,\n  6.329767227172852,\n  6.349868297576904,\n  6.385800361633301,\n  6.562595844268799,\n  6.569463729858398,\n  6.693495273590088,\n  6.826439380645752,\n  6.868060111999512,\n  7.033170223236084,\n  7.341447353363037,\n  7.794949054718018,\n  7.83174467086792,\n  8.151607513427734,\n  8.44259262084961,\n  8.457537651062012,\n  8.622756004333496,\n  9.172985076904297,\n  10.818984031677246,\n  11.475016593933105,\n  12.081933975219727,\n  12.945302963256836,\n  12.945521354675293,\n  13.179908752441406],\n [-7.713510990142822,\n  -7.226686954498291,\n  -5.746126174926758,\n  -4.231037616729736,\n  -3.643665313720703,\n  -3.580137252807617,\n  -3.5289597511291504,\n  -3.4989171028137207,\n  -3.2096168994903564,\n  -2.9486801624298096,\n  -2.47174072265625,\n  -2.142157793045044,\n  -2.1266424655914307,\n  -2.054018497467041,\n  -1.4836169481277466,\n  -1.422898530960083,\n  -1.1482985019683838,\n  -0.829800009727478,\n  -0.7043420076370239,\n  -0.6526787281036377,\n  -0.4607689380645752,\n  -0.11789380013942719,\n  -0.006762102246284485,\n  0.07786467671394348,\n  0.12437653541564941,\n  0.3064407706260681,\n  0.408502995967865,\n  0.43622371554374695,\n  0.44770708680152893,\n  0.4615381062030792,\n  0.7869821786880493,\n  0.8495216369628906,\n  0.8623087406158447,\n  1.0768741369247437,\n  1.232725977897644,\n  1.2341798543930054,\n  1.2494776248931885,\n  1.2548022270202637,\n  1.2763421535491943,\n  1.4843493700027466,\n  1.547118067741394,\n  1.6218249797821045,\n  1.6657185554504395,\n  1.7236193418502808,\n  1.7712914943695068,\n  1.8065189123153687,\n  1.9301081895828247,\n  2.1141293048858643,\n  2.1320908069610596,\n  2.272172451019287,\n  2.624859094619751,\n  2.636251449584961,\n  2.8439342975616455,\n  2.935074806213379,\n  2.9759974479675293,\n  2.9949772357940674,\n  3.4324333667755127,\n  3.4366743564605713,\n  3.63981556892395,\n  3.6655988693237305,\n  3.76352596282959,\n  3.9755516052246094,\n  4.059065818786621,\n  4.567473888397217,\n  4.622980117797852,\n  4.75818395614624,\n  4.8538408279418945,\n  4.983859539031982,\n  4.9839911460876465,\n  5.006067276000977,\n  5.067083835601807,\n  5.223495960235596,\n  5.2568440437316895,\n  5.533102035522461,\n  5.667608737945557,\n  5.876441955566406,\n  6.129057884216309,\n  6.337213039398193,\n  6.35725212097168,\n  6.393074035644531,\n  6.5693254470825195,\n  6.576172351837158,\n  6.699821949005127,\n  6.832357406616211,\n  6.873850345611572,\n  7.038452625274658,\n  7.345781326293945,\n  7.797888278961182,\n  7.834570407867432,\n  8.153450012207031,\n  8.4435396194458,\n  8.458438873291016,\n  8.623148918151855,\n  9.171685218811035,\n  10.8126220703125,\n  11.466635704040527,\n  12.071686744689941,\n  12.93239974975586,\n  12.932618141174316,\n  13.166284561157227],\n [-7.672260761260986,\n  -7.186653137207031,\n  -5.709791660308838,\n  -4.198488712310791,\n  -3.612584114074707,\n  -3.5492148399353027,\n  -3.4981651306152344,\n  -3.4681975841522217,\n  -3.1796202659606934,\n  -2.91933536529541,\n  -2.4435875415802,\n  -2.11482834815979,\n  -2.099351644515991,\n  -2.026909112930298,\n  -1.457932949066162,\n  -1.3973662853240967,\n  -1.1234523057937622,\n  -0.8057495951652527,\n  -0.6806051135063171,\n  -0.6290708780288696,\n  -0.43764063715934753,\n  -0.0956222265958786,\n  0.015231793746352196,\n  0.09964711964130402,\n  0.14604276418685913,\n  0.32765209674835205,\n  0.4294593036174774,\n  0.457110732793808,\n  0.468565434217453,\n  0.48236188292503357,\n  0.806992769241333,\n  0.869376003742218,\n  0.8821310997009277,\n  1.0961604118347168,\n  1.2516227960586548,\n  1.2530730962753296,\n  1.268332600593567,\n  1.273643970489502,\n  1.2951301336288452,\n  1.502617597579956,\n  1.5652294158935547,\n  1.639749526977539,\n  1.6835334300994873,\n  1.7412896156311035,\n  1.7888426780700684,\n  1.8239821195602417,\n  1.9472625255584717,\n  2.130823850631714,\n  2.148740530014038,\n  2.2884721755981445,\n  2.64027738571167,\n  2.651641368865967,\n  2.8588054180145264,\n  2.9497179985046387,\n  2.9905385971069336,\n  3.0094707012176514,\n  3.445833921432495,\n  3.4500644207000732,\n  3.652698040008545,\n  3.6784167289733887,\n  3.77609920501709,\n  3.9875950813293457,\n  4.0709004402160645,\n  4.578038692474365,\n  4.633405685424805,\n  4.768272399902344,\n  4.86368989944458,\n  4.993383407592773,\n  4.9935150146484375,\n  5.015536308288574,\n  5.076399803161621,\n  5.232420921325684,\n  5.26568603515625,\n  5.541253566741943,\n  5.675424575805664,\n  5.883735656738281,\n  6.135720729827881,\n  6.343355655670166,\n  6.363344669342041,\n  6.39907693862915,\n  6.574888229370117,\n  6.581717491149902,\n  6.705058574676514,\n  6.837263107299805,\n  6.878652095794678,\n  7.042842864990234,\n  7.3494038581848145,\n  7.800381183624268,\n  7.836971759796143,\n  8.155054092407227,\n  8.444419860839844,\n  8.459280967712402,\n  8.623579978942871,\n  9.170745849609375,\n  10.807581901550293,\n  11.459961891174316,\n  12.06350040435791,\n  12.922063827514648,\n  12.922280311584473,\n  13.155364036560059],\n [-7.63875675201416,\n  -7.154134273529053,\n  -5.680269718170166,\n  -4.17203426361084,\n  -3.5873184204101562,\n  -3.5240776538848877,\n  -3.4731316566467285,\n  -3.4432246685028076,\n  -3.155233144760132,\n  -2.8954765796661377,\n  -2.420694351196289,\n  -2.092602014541626,\n  -2.0771567821502686,\n  -2.004861354827881,\n  -1.4370397329330444,\n  -1.3765959739685059,\n  -1.1032378673553467,\n  -0.7861799001693726,\n  -0.661289393901825,\n  -0.6098597645759583,\n  -0.41881799697875977,\n  -0.07749363034963608,\n  0.03313542529940605,\n  0.11737944930791855,\n  0.16368094086647034,\n  0.34492170810699463,\n  0.4465223252773285,\n  0.474117636680603,\n  0.48554909229278564,\n  0.49931755661964417,\n  0.8232896327972412,\n  0.8855462670326233,\n  0.8982754945755005,\n  1.1118704080581665,\n  1.2670173645019531,\n  1.2684646844863892,\n  1.2836933135986328,\n  1.2889938354492188,\n  1.3104363679885864,\n  1.517502784729004,\n  1.5799875259399414,\n  1.6543564796447754,\n  1.6980515718460083,\n  1.7556904554367065,\n  1.8031470775604248,\n  1.8382152318954468,\n  1.9612454175949097,\n  2.1444342136383057,\n  2.1623146533966064,\n  2.301762580871582,\n  2.6528539657592773,\n  2.6641948223114014,\n  2.870938301086426,\n  2.9616665840148926,\n  3.00240421295166,\n  3.0212981700897217,\n  3.456775665283203,\n  3.4609975814819336,\n  3.663220167160034,\n  3.6888866424560547,\n  3.7863707542419434,\n  3.9974374771118164,\n  4.080574035644531,\n  4.5866827964782715,\n  4.641937732696533,\n  4.7765302658081055,\n  4.871754169464111,\n  5.001184940338135,\n  5.001316070556641,\n  5.023292541503906,\n  5.084033012390137,\n  5.239737510681152,\n  5.272934436798096,\n  5.547943115234375,\n  5.6818413734436035,\n  5.889730453491211,\n  6.141203880310059,\n  6.348417282104492,\n  6.36836576461792,\n  6.404025554656982,\n  6.579480171203613,\n  6.5862956047058105,\n  6.709386348724365,\n  6.841322422027588,\n  6.882627487182617,\n  7.046485424041748,\n  7.352424144744873,\n  7.802485942840576,\n  7.83900260925293,\n  8.156439781188965,\n  8.44521713256836,\n  8.460049629211426,\n  8.624013900756836,\n  9.170069694519043,\n  10.803584098815918,\n  11.45464038848877,\n  12.056954383850098,\n  12.913774490356445,\n  12.913991928100586,\n  13.146601676940918],\n [-7.611539840698242,\n  -7.127716064453125,\n  -5.656280517578125,\n  -4.1505303382873535,\n  -3.5667781829833984,\n  -3.5036416053771973,\n  -3.452779769897461,\n  -3.422921895980835,\n  -3.1354050636291504,\n  -2.8760764598846436,\n  -2.402076482772827,\n  -2.0745251178741455,\n  -2.059105396270752,\n  -1.986928939819336,\n  -1.4200431108474731,\n  -1.3596988916397095,\n  -1.0867912769317627,\n  -0.7702558040618896,\n  -0.6455711126327515,\n  -0.5942262411117554,\n  -0.4034992754459381,\n  -0.06273742020130157,\n  0.047709330916404724,\n  0.13181452453136444,\n  0.17803971469402313,\n  0.35898181796073914,\n  0.46041497588157654,\n  0.48796483874320984,\n  0.49937742948532104,\n  0.5131232142448425,\n  0.8365614414215088,\n  0.8987154364585876,\n  0.9114237427711487,\n  1.1246665716171265,\n  1.2795579433441162,\n  1.2810028791427612,\n  1.2962063550949097,\n  1.3014981746673584,\n  1.3229053020477295,\n  1.5296305418014526,\n  1.5920122861862183,\n  1.666258692741394,\n  1.7098817825317383,\n  1.7674256563186646,\n  1.8148040771484375,\n  1.8498144149780273,\n  1.972641944885254,\n  2.1555287837982178,\n  2.17337965965271,\n  2.3125979900360107,\n  2.6631107330322266,\n  2.6744329929351807,\n  2.880835771560669,\n  2.971414566040039,\n  3.0120849609375,\n  3.030947685241699,\n  3.465707778930664,\n  3.4699225425720215,\n  3.671811819076538,\n  3.6974360942840576,\n  3.794759511947632,\n  4.005478382110596,\n  4.088478088378906,\n  4.593752861022949,\n  4.648916721343994,\n  4.783287048339844,\n  4.878354549407959,\n  5.007571697235107,\n  5.007702827453613,\n  5.029642581939697,\n  5.090282917022705,\n  5.245730876922607,\n  5.278873443603516,\n  5.553429126739502,\n  5.687106609344482,\n  5.894652843475342,\n  6.145711898803711,\n  6.352583885192871,\n  6.372499465942383,\n  6.408100605010986,\n  6.583265781402588,\n  6.5900702476501465,\n  6.712958335876465,\n  6.844676494598389,\n  6.885913848876953,\n  7.049501419067383,\n  7.354936122894287,\n  7.804256439208984,\n  7.840712547302246,\n  8.15762710571289,\n  8.445928573608398,\n  8.460736274719238,\n  8.624430656433105,\n  9.169586181640625,\n  10.800409317016602,\n  11.45039176940918,\n  12.051713943481445,\n  12.907122611999512,\n  12.90733814239502,\n  13.139565467834473],\n [-7.589427947998047,\n  -7.1062517166137695,\n  -5.63678503036499,\n  -4.133049488067627,\n  -3.5500783920288086,\n  -3.4870262145996094,\n  -3.436232328414917,\n  -3.406414747238159,\n  -3.1192824840545654,\n  -2.8603007793426514,\n  -2.386935234069824,\n  -2.059821844100952,\n  -2.0444228649139404,\n  -1.97234308719635,\n  -1.4062156677246094,\n  -1.3459522724151611,\n  -1.073409914970398,\n  -0.757297933101654,\n  -0.6327800750732422,\n  -0.5815038681030273,\n  -0.3910321295261383,\n  -0.050726234912872314,\n  0.0595727264881134,\n  0.14356538653373718,\n  0.1897287219762802,\n  0.3704287111759186,\n  0.4717261493206024,\n  0.49923914670944214,\n  0.5106364488601685,\n  0.5243638753890991,\n  0.8473693132400513,\n  0.9094401597976685,\n  0.9221314191818237,\n  1.1350890398025513,\n  1.2897729873657227,\n  1.2912160158157349,\n  1.3063991069793701,\n  1.3116838932037354,\n  1.3330624103546143,\n  1.53951096534729,\n  1.6018093824386597,\n  1.67595636844635,\n  1.7195210456848145,\n  1.7769880294799805,\n  1.8243030309677124,\n  1.8592665195465088,\n  1.9819296598434448,\n  2.164571762084961,\n  2.182398796081543,\n  2.3214306831359863,\n  2.6714744567871094,\n  2.68278169631958,\n  2.8889081478118896,\n  2.979365825653076,\n  3.019981861114502,\n  3.0388193130493164,\n  3.4729976654052734,\n  3.4772069454193115,\n  3.678825855255127,\n  3.704415798187256,\n  3.8016092777252197,\n  4.0120463371276855,\n  4.094934463500977,\n  4.5995330810546875,\n  4.654623031616211,\n  4.788814067840576,\n  4.883754253387451,\n  5.012798309326172,\n  5.012929439544678,\n  5.034840106964111,\n  5.095399379730225,\n  5.250638961791992,\n  5.2837371826171875,\n  5.557925224304199,\n  5.691424369812012,\n  5.898692607879639,\n  6.149415969848633,\n  6.356010913848877,\n  6.3759002685546875,\n  6.4114532470703125,\n  6.586384296417236,\n  6.593179702758789,\n  6.715902805328369,\n  6.847445487976074,\n  6.888627052307129,\n  7.051996231079102,\n  7.357022285461426,\n  7.805741310119629,\n  7.842148303985596,\n  8.158638954162598,\n  8.446555137634277,\n  8.461341857910156,\n  8.624817848205566,\n  9.169244766235352,\n  10.797884941101074,\n  11.44699764251709,\n  12.047514915466309,\n  12.901779174804688,\n  12.901994705200195,\n  13.1339111328125],\n [-7.571461200714111,\n  -7.088809967041016,\n  -5.620939254760742,\n  -4.118837833404541,\n  -3.5364997386932373,\n  -3.4735164642333984,\n  -3.4227776527404785,\n  -3.3929922580718994,\n  -3.1061720848083496,\n  -2.8474717140197754,\n  -2.3746204376220703,\n  -2.0478625297546387,\n  -2.032480239868164,\n  -1.960478663444519,\n  -1.3949663639068604,\n  -1.3347684144973755,\n  -1.062522053718567,\n  -0.7467535138130188,\n  -0.6223708987236023,\n  -0.5711504220962524,\n  -0.38088560104370117,\n  -0.040949393063783646,\n  0.06922974437475204,\n  0.1531311571598053,\n  0.19924433529376984,\n  0.37974801659584045,\n  0.4809354245662689,\n  0.5084185004234314,\n  0.5198034644126892,\n  0.5335159301757812,\n  0.8561704754829407,\n  0.9181739091873169,\n  0.930851399898529,\n  1.1435775756835938,\n  1.298093557357788,\n  1.2995350360870361,\n  1.3147016763687134,\n  1.3199806213378906,\n  1.3413360118865967,\n  1.547560214996338,\n  1.6097909212112427,\n  1.6838574409484863,\n  1.727374792098999,\n  1.7847793102264404,\n  1.832042932510376,\n  1.8669683933258057,\n  1.989498257637024,\n  2.1719419956207275,\n  2.1897497177124023,\n  2.3286306858062744,\n  2.6782939434051514,\n  2.689588785171509,\n  2.895491600036621,\n  2.9858508110046387,\n  3.0264227390289307,\n  3.0452396869659424,\n  3.4789464473724365,\n  3.4831509590148926,\n  3.6845510005950928,\n  3.710113286972046,\n  3.8072009086608887,\n  4.017409324645996,\n  4.100207805633545,\n  4.604258060455322,\n  4.65928840637207,\n  4.793333530426025,\n  4.88817024230957,\n  5.0170745849609375,\n  5.017205238342285,\n  5.039092063903809,\n  5.09958553314209,\n  5.254656791687012,\n  5.287718772888184,\n  5.561609268188477,\n  5.694962978363037,\n  5.902006149291992,\n  6.152457237243652,\n  6.358827590942383,\n  6.378695011138916,\n  6.414209842681885,\n  6.588951110839844,\n  6.595738887786865,\n  6.718328952789307,\n  6.849728107452393,\n  6.890865325927734,\n  7.054056644439697,\n  7.35875129699707,\n  7.80698299407959,\n  7.843350887298584,\n  8.159497261047363,\n  8.447100639343262,\n  8.461872100830078,\n  8.62516975402832,\n  9.169004440307617,\n  10.795876502990723,\n  11.444284439086914,\n  12.044148445129395,\n  12.89748477935791,\n  12.897700309753418,\n  13.129364013671875],\n [-7.556859493255615,\n  -7.074634075164795,\n  -5.608058452606201,\n  -4.107281684875488,\n  -3.5254576206207275,\n  -3.4625296592712402,\n  -3.4118356704711914,\n  -3.3820767402648926,\n  -3.0955095291137695,\n  -2.8370375633239746,\n  -2.364603042602539,\n  -2.038133382797241,\n  -2.0227646827697754,\n  -1.9508267641067505,\n  -1.3858133554458618,\n  -1.325668454170227,\n  -1.0536623001098633,\n  -0.7381722927093506,\n  -0.6138994693756104,\n  -0.5627241134643555,\n  -0.3726271688938141,\n  -0.03299083933234215,\n  0.07709110528230667,\n  0.1609184890985489,\n  0.20699100196361542,\n  0.3873354494571686,\n  0.4884335994720459,\n  0.5158924460411072,\n  0.527267336845398,\n  0.5409677028656006,\n  0.8633376359939575,\n  0.9252863526344299,\n  0.9379526376724243,\n  1.1504912376403809,\n  1.304870843887329,\n  1.306311011314392,\n  1.3214643001556396,\n  1.326738715171814,\n  1.3480751514434814,\n  1.5541175603866577,\n  1.61629319190979,\n  1.6902943849563599,\n  1.7337733507156372,\n  1.7911272048950195,\n  1.8383492231369019,\n  1.873243808746338,\n  1.9956656694412231,\n  2.177948236465454,\n  2.1957404613494873,\n  2.334498882293701,\n  2.6838538646698,\n  2.695138692855835,\n  2.9008595943450928,\n  2.9911391735076904,\n  3.031675338745117,\n  3.050475835800171,\n  3.483799695968628,\n  3.4880008697509766,\n  3.689223051071167,\n  3.7147626876831055,\n  3.81176495552063,\n  4.021787643432617,\n  4.104513168334961,\n  4.608119010925293,\n  4.663100719451904,\n  4.797027587890625,\n  4.891780853271484,\n  5.020571231842041,\n  5.0207014083862305,\n  5.042569160461426,\n  5.103009223937988,\n  5.257943630218506,\n  5.290976524353027,\n  5.564625263214111,\n  5.697861671447754,\n  5.904722213745117,\n  6.154952049255371,\n  6.361140727996826,\n  6.380990505218506,\n  6.416473865509033,\n  6.591060638427734,\n  6.597842693328857,\n  6.720324516296387,\n  6.8516082763671875,\n  6.892708778381348,\n  7.055756568908691,\n  7.360182285308838,\n  7.808018207550049,\n  7.844354152679443,\n  8.160221099853516,\n  8.44757080078125,\n  8.462329864501953,\n  8.625483512878418,\n  9.168838500976562,\n  10.794275283813477,\n  11.442111015319824,\n  12.0414457321167,\n  12.89402961730957,\n  12.894245147705078,\n  13.125704765319824],\n [-7.544992446899414,\n  -7.063112258911133,\n  -5.5975871086120605,\n  -4.0978851318359375,\n  -3.5164780616760254,\n  -3.4535951614379883,\n  -3.402937412261963,\n  -3.373199701309204,\n  -3.0868377685546875,\n  -2.8285508155822754,\n  -2.356454849243164,\n  -2.030219078063965,\n  -2.0148613452911377,\n  -1.9429749250411987,\n  -1.3783661127090454,\n  -1.318264365196228,\n  -1.0464529991149902,\n  -0.7311890125274658,\n  -0.6070051789283752,\n  -0.555866539478302,\n  -0.3659057021141052,\n  -0.026512641459703445,\n  0.08349045366048813,\n  0.16725780069828033,\n  0.2132973074913025,\n  0.3935125768184662,\n  0.4945383071899414,\n  0.5219774842262268,\n  0.5333442687988281,\n  0.5470348000526428,\n  0.8691738247871399,\n  0.9310781359672546,\n  0.9437353610992432,\n  1.1561217308044434,\n  1.310390830039978,\n  1.311829924583435,\n  1.3269723653793335,\n  1.3322429656982422,\n  1.3535641431808472,\n  1.5594589710235596,\n  1.6215901374816895,\n  1.6955382823944092,\n  1.7389861345291138,\n  1.796298861503601,\n  1.843487024307251,\n  1.878356695175171,\n  2.0006906986236572,\n  2.182842969894409,\n  2.200622320175171,\n  2.3392813205718994,\n  2.6883859634399414,\n  2.699662685394287,\n  2.9052364826202393,\n  2.9954514503479004,\n  3.0359585285186768,\n  3.0547454357147217,\n  3.4877591133117676,\n  3.491956949234009,\n  3.693035125732422,\n  3.7185566425323486,\n  3.8154892921447754,\n  4.02536153793335,\n  4.108027458190918,\n  4.611272811889648,\n  4.666214942932129,\n  4.800045967102051,\n  4.894731521606445,\n  5.0234293937683105,\n  5.023560047149658,\n  5.045412063598633,\n  5.105808734893799,\n  5.260632038116455,\n  5.293641567230225,\n  5.567094326019287,\n  5.700234889984131,\n  5.906947612762451,\n  6.156998157501221,\n  6.363039016723633,\n  6.382874488830566,\n  6.418332576751709,\n  6.592794418334961,\n  6.599571228027344,\n  6.721965789794922,\n  6.853155136108398,\n  6.894226551055908,\n  7.057157039642334,\n  7.361364841461182,\n  7.80888032913208,\n  7.845190048217773,\n  8.160831451416016,\n  8.447975158691406,\n  8.462722778320312,\n  8.625760078430176,\n  9.168725967407227,\n  10.792997360229492,\n  11.440369606018066,\n  12.039276123046875,\n  12.89124870300293,\n  12.891464233398438,\n  13.122757911682129]]\n\n\n\nlen(yhat_history)\n\n30\n\n\n\nlen(yhat_history[0]) #0에 100개... [1]에 100개.. \n\n100\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat_history[2],'--')\n\n\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat_history[9],'--')\n\n\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat_history[14],'--')\n\n\n\n\n- \\(\\hat{\\bf W}\\) 관찰\n\nWhat_history\n\n[[-3.657747745513916, 8.81106948852539],\n [-2.554811477661133, 7.861191749572754],\n [-1.649186134338379, 7.101552963256836],\n [-0.9060712456703186, 6.49347448348999],\n [-0.2966785430908203, 6.006272315979004],\n [0.20277434587478638, 5.615575313568115],\n [0.6119105815887451, 5.302003383636475],\n [0.9469034671783447, 5.050129413604736],\n [1.2210699319839478, 4.847657680511475],\n [1.4453645944595337, 4.684779167175293],\n [1.6287915706634521, 4.553659439086914],\n [1.778746247291565, 4.448036193847656],\n [1.90129816532135, 4.3628973960876465],\n [2.0014259815216064, 4.294229507446289],\n [2.0832109451293945, 4.238814353942871],\n [2.149996757507324, 4.194070339202881],\n [2.204521894454956, 4.157923698425293],\n [2.249027729034424, 4.128708839416504],\n [2.285348415374756, 4.105085849761963],\n [2.31498384475708, 4.0859761238098145],\n [2.339160442352295, 4.070511341094971],\n [2.3588807582855225, 4.057991027832031],\n [2.3749637603759766, 4.0478515625],\n [2.3880786895751953, 4.039637088775635],\n [2.3987717628479004, 4.032979965209961],\n [2.40748929977417, 4.027583599090576],\n [2.414595603942871, 4.023208141326904],\n [2.4203879833221436, 4.019659042358398],\n [2.4251089096069336, 4.016779899597168],\n [2.4289560317993164, 4.014443874359131]]\n\n\n- loss 관찰\n\nloss_history\n\n[8587.6875,\n 5675.2109375,\n 3755.637451171875,\n 2489.581787109375,\n 1654.0390625,\n 1102.3206787109375,\n 737.8441162109375,\n 496.96514892578125,\n 337.7142028808594,\n 232.39694213867188,\n 162.72906494140625,\n 116.63263702392578,\n 86.1263656616211,\n 65.93397521972656,\n 52.566444396972656,\n 43.71583557128906,\n 37.855220794677734,\n 33.974090576171875,\n 31.403636932373047,\n 29.701112747192383,\n 28.57339096069336,\n 27.826366424560547,\n 27.331483840942383,\n 27.003639221191406,\n 26.78643798828125,\n 26.642536163330078,\n 26.547197341918945,\n 26.48402976989746,\n 26.442174911499023,\n 26.414440155029297]\n\n\n\nplt.plot(loss_history)\n\n\n\n\n\n\n학습과정을 animation으로 시각화\n\nfrom matplotlib import animation\n\n\nplt.rcParams['figure.figsize'] = (7.5,2.5)\nplt.rcParams[\"animation.html\"] = \"jshtml\" \n\n- 왼쪽에는 \\((x_i,y_i)\\) and \\((x_i,\\hat{y}_i)\\) 을 그리고 오른쪽에는 \\(loss(w_0,w_1)\\) 을 그릴것임\n\nfig = plt.figure()\nax1 = fig.add_subplot(1, 2, 1)\nax2 = fig.add_subplot(1, 2, 2, projection='3d')\n\n\n\n\n\n# 왼쪽 2d 오른쪽 3d 축만 생긴다\n\n- 왼쪽그림!\n\nax1.plot(x,y,'o')\nline, = ax1.plot(x,yhat_history[0]) # 나중에 애니메이션 할때 필요해요..\n\n\nfig\n\n\n\n\n- 오른쪽 그림1: \\(loss(w_0,w_1)\\)\n\n_w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) \n_w1 = np.arange(-6, 11, 0.5)\nw1,w0 = np.meshgrid(_w1,_w0)\nlss=w0*0\nfor i in range(len(_w0)):\n    for j in range(len(_w1)):\n        lss[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2)\nax2.plot_surface(w0, w1, lss, rstride=1, cstride=1, color='b',alpha=0.35) ## 파란색곡면을 그리는 코드(끝) \nax2.azim = 40  ## 3d plot의 view 조절 \nax2.dist = 8   ## 3d plot의 view 조절 \nax2.elev = 5   ## 3d plot의 view 조절 \n\n\nfig\n\n\n\n\n- 오른쪽 그림2: \\((w_0,w_1)=(2.5,4)\\) 와 \\(loss(2.5,4)\\) 값 <- loss 함수가 최소가 되는 값 (이거 진짜야? ㅋㅋ)\n\nax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color='red',marker='*') ## 최소점을 표시하는 코드 (붉은색 별) \n\n<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7fac8c960610>\n\n\n\nfig\n\n\n\n\n- 오른쪽 그림3: \\((w_0,w_1)=(-3.66, 8.81)\\) 와 \\(loss(-3.66,8.81)\\) 값\n\nWhat_history[0]\n\n[-3.657747745513916, 8.81106948852539]\n\n\n\nax2.scatter(What_history[0][0],What_history[0][1],loss_history[0],color='grey') ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) \n#처음 값!! 오른쪽 그림에서 저 동그라미 회색점이 별표로 가야해~~ 경사하강법생각하자!\n\n<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7fac8c7e5710>\n\n\n\nfig\n\n\n\n\n- 애니메이션\n\ndef animate(epoc):\n    line.set_ydata(yhat_history[epoc])\n    ax2.scatter(What_history[epoc][0],What_history[epoc][1],loss_history[epoc],color='grey')\n    return line\n\nani = animation.FuncAnimation(fig, animate, frames=30)\nplt.close()\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- 함수로 만들자..\n\ndef show_lrpr(data,history):\n    x,y = data \n    loss_history,yhat_history,What_history = history \n    \n    fig = plt.figure()\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n\n    ## ax1: 왼쪽그림 \n    ax1.plot(x,y,'o')\n    line, = ax1.plot(x,yhat_history[0]) \n    ## ax2: 오른쪽그림 \n    _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) \n    _w1 = np.arange(-6, 11, 0.5)\n    w1,w0 = np.meshgrid(_w1,_w0)\n    lss=w0*0\n    for i in range(len(_w0)):\n        for j in range(len(_w1)):\n            lss[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2)\n    ax2.plot_surface(w0, w1, lss, rstride=1, cstride=1, color='b',alpha=0.35) ## 파란색곡면을 그리는 코드(끝) \n    ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color='red',marker='*') ## 최소점을 표시하는 코드 (붉은색 별) \n    ax2.scatter(What_history[0][0],What_history[0][1],loss_history[0],color='b') ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) \n    ax2.azim = 40  ## 3d plot의 view 조절 \n    ax2.dist = 8   ## 3d plot의 view 조절 \n    ax2.elev = 5   ## 3d plot의 view 조절 \n\n    def animate(epoc):\n        line.set_ydata(yhat_history[epoc])\n        ax2.scatter(np.array(What_history)[epoc,0],np.array(What_history)[epoc,1],loss_history[epoc],color='grey')\n        return line\n\n    ani = animation.FuncAnimation(fig, animate, frames=30)\n    plt.close()\n    return ani\n\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_21_(3주차)_9월21일_ipynb의_사본.html#alpha에-대하여-alpha는-학습률",
    "href": "posts/Machine Learning/1. DNN/2022_09_21_(3주차)_9월21일_ipynb의_사본.html#alpha에-대하여-alpha는-학습률",
    "title": "기계학습 (0921) 3주차",
    "section": "\\(\\alpha\\)에 대하여 (\\(\\alpha\\)는 학습률)",
    "text": "\\(\\alpha\\)에 대하여 (\\(\\alpha\\)는 학습률)\n\n#머신러닝에서 a는 학습률...\n\n\n(1) \\(\\alpha=0.0001\\): \\(\\alpha\\) 가 너무 작다면? \\(\\to\\) 비효율적이다.\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3 \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.0001 \nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist())\n    What.grad=None\n\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n(2) \\(\\alpha=0.0083\\): \\(\\alpha\\)가 너무 크다면? \\(\\to\\) 다른의미에서 비효율적이다 + 위험하다..\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3 \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.0083\nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist())\n    What.grad=None\n\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n(3) \\(\\alpha=0.0085\\)\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3 \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.0085\nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad.data; What_history.append(What.data.tolist())\n    What.grad=None\n\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n(4) \\(\\alpha=0.01\\)\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3 \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.01\nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist())\n    What.grad=None\n\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_21_(3주차)_9월21일_ipynb의_사본.html#숙제",
    "href": "posts/Machine Learning/1. DNN/2022_09_21_(3주차)_9월21일_ipynb의_사본.html#숙제",
    "title": "기계학습 (0921) 3주차",
    "section": "숙제",
    "text": "숙제\n- 학습률(\\(\\alpha\\))를 조정하며 실습해보고 스크린샷 제출\n\n# α=0.00912\nloss_history = [] \nyhat_history = [] \nWhat_history = [] \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.00912\nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist())\n    What.grad=None\n\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_28_(5주차)_10월05일_ipynb의_사본.html",
    "href": "posts/Machine Learning/1. DNN/2022_09_28_(5주차)_10월05일_ipynb의_사본.html",
    "title": "기계학습 (1005) 5주차",
    "section": "",
    "text": "import torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \n\n\n\n준비1 loss_fn을 plot하는 함수\n\ndef plot_loss(loss_fn,ax=None):\n    if ax==None:\n        fig = plt.figure()\n        ax=fig.add_subplot(1,1,1,projection='3d')\n        ax.elev=15;ax.azim=75\n    w0hat,w1hat =torch.meshgrid(torch.arange(-10,3,0.15),torch.arange(-1,10,0.15),indexing='ij')\n    w0hat = w0hat.reshape(-1)\n    w1hat = w1hat.reshape(-1)\n    def l(w0hat,w1hat):\n        yhat = torch.exp(w0hat+w1hat*x)/(1+torch.exp(w0hat+w1hat*x))\n        return loss_fn(yhat,y) \n    loss = list(map(l,w0hat,w1hat))\n    ax.scatter(w0hat,w1hat,loss,s=0.1,alpha=0.2) \n    ax.scatter(-1,5,l(-1,5),s=200,marker='*') # 실제로 -1,5에서 최소값을 가지는건 아님.. \n\n\n$y_i Ber(_i),$ where \\(\\pi_i = \\frac{\\exp(-1+5x_i)}{1+\\exp(-1+5x_i)}\\) 에서 생성된 데이터 한정하여 손실함수가 그려지게 되어있음.\n\n준비2: for문 대신 돌려주고 epoch마다 필요한 정보를 기록하는 함수\n\ndef learn_and_record(net, loss_fn, optimizr):\n    yhat_history = [] \n    loss_history = []\n    what_history = [] \n\n    for epoc in range(1000): \n        ## step1 \n        yhat = net(x)\n        ## step2 \n        loss = loss_fn(yhat,y)\n        ## step3\n        loss.backward() \n        ## step4 \n        optimizr.step()\n        optimizr.zero_grad() \n\n        ## record \n        if epoc % 20 ==0: \n            yhat_history.append(yhat.reshape(-1).data.tolist())\n            loss_history.append(loss.item())\n            what_history.append([net[0].bias.data.item(), net[0].weight.data.item()])\n    return yhat_history, loss_history, what_history\n\n\n20에폭마다 yhat, loss, what을 기록\n\n준비3: 애니메이션을 만들어주는 함수\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\n\ndef show_lrpr2(net,loss_fn,optimizr,suptitle=''):\n    yhat_history,loss_history,what_history = learn_and_record(net,loss_fn,optimizr)\n    \n    fig = plt.figure(figsize=(7,2.5))\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n    ax1.set_xticks([]);ax1.set_yticks([])\n    ax2.set_xticks([]);ax2.set_yticks([]);ax2.set_zticks([])\n    ax2.elev = 15; ax2.azim = 75\n\n    ## ax1: 왼쪽그림 \n    ax1.plot(x,v,'--')\n    ax1.scatter(x,y,alpha=0.05)\n    line, = ax1.plot(x,yhat_history[0],'--') \n    plot_loss(loss_fn,ax2)\n    fig.suptitle(suptitle)\n    fig.tight_layout()\n\n    def animate(epoc):\n        line.set_ydata(yhat_history[epoc])\n        ax2.scatter(np.array(what_history)[epoc,0],np.array(what_history)[epoc,1],loss_history[epoc],color='grey')\n        return line\n\n    ani = animation.FuncAnimation(fig, animate, frames=30)\n    plt.close()\n    return ani\n\n\n준비1에서 그려진 loss 함수위에, 준비2의 정보를 조합하여 애니메이션을 만들어주는 함수"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_28_(5주차)_10월05일_ipynb의_사본.html#logistic-intro-review-alpha",
    "href": "posts/Machine Learning/1. DNN/2022_09_28_(5주차)_10월05일_ipynb의_사본.html#logistic-intro-review-alpha",
    "title": "기계학습 (1005) 5주차",
    "section": "Logistic intro (review + \\(\\alpha\\))",
    "text": "Logistic intro (review + \\(\\alpha\\))\n- 모델: \\(x\\)가 커질수록 \\(y=1\\)이 잘나오는 모형은 아래와 같이 설계할 수 있음 <— 외우세요!!!\n\n$y_i Ber(_i),$ where \\(\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\)\n\\(\\hat{y}_i= \\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\frac{1}{1+\\exp(-\\hat{w}_0-\\hat{w}_1x_i)}\\)\n\\(loss= - \\sum_{i=1}^{n} \\big(y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i)\\big)\\) <— 외우세요!!\n\n- toy example\n\nx=torch.linspace(-1,1,2000).reshape(2000,1)\nw0= -1 \nw1= 5 \nu = w0+x*w1 \nv = torch.exp(u)/(1+torch.exp(u)) # v=πi, 즉 확률을 의미함\ny = torch.bernoulli(v) \n\n\nplt.plot(x,y,'o')\n\n\n\n\n\nnote: \\((w_0,w_1)\\)의 true는 \\((-1,5)\\)이다. -> \\((\\hat{w}_0, \\hat{w}_1)\\)을 적당히 \\((-1,5)\\)근처로 추정하면 된다는 의미\n\n\nplt.scatter(x,y,alpha=0.05)\nplt.plot(x,v,'--r')\n\n\n\n\n- step1: yhat을 만들기\n(방법1)\n\ntorch.manual_seed(43052)\nl1 = torch.nn.Linear(1,1)  #x의 shape보면(2000,1)인데 뒤에 1이 중요..\na1 = torch.nn.Sigmoid() \nyhat = a1(l1(x))\nyhat\n\ntensor([[0.3775],\n        [0.3774],\n        [0.3773],\n        ...,\n        [0.2327],\n        [0.2327],\n        [0.2326]], grad_fn=<SigmoidBackward0>)\n\n\n(방법2)\n\ntorch.manual_seed(43052)\nl1 = torch.nn.Linear(1,1)\na1 = torch.nn.Sigmoid() \nnet = torch.nn.Sequential(l1,a1) #net는 l1과 a1의 합성함수\nyhat = net(x)\nyhat\n\ntensor([[0.3775],\n        [0.3774],\n        [0.3773],\n        ...,\n        [0.2327],\n        [0.2327],\n        [0.2326]], grad_fn=<SigmoidBackward0>)\n\n\n(방법3)\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nyhat = net(x)\nyhat\n\n# 단점: a1과 l1 각 통과하는게 궁금한데 이건 중간과정 보기가힘들다.\n# len(net) = 2 : 2가 나오네.. 우너소에 접근을 해보자\n#net[0], net[1]\n\ntensor([[0.3775],\n        [0.3774],\n        [0.3773],\n        ...,\n        [0.2327],\n        [0.2327],\n        [0.2326]], grad_fn=<SigmoidBackward0>)\n\n\n\nnet[0]\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\nnet[0](x)\n\ntensor([[-0.5003],\n        [-0.5007],\n        [-0.5010],\n        ...,\n        [-1.1930],\n        [-1.1934],\n        [-1.1937]], grad_fn=<AddmmBackward0>)\n\n\n\nnet[1] #a1의 기능\n\nSigmoid()\n\n\n\nnet[1](net[0](x))\n\ntensor([[0.3775],\n        [0.3774],\n        [0.3773],\n        ...,\n        [0.2327],\n        [0.2327],\n        [0.2326]], grad_fn=<SigmoidBackward0>)\n\n\n- step2: loss (일단 MSE로..)\n(방법1)\n\ntorch.mean((y-yhat)**2) #mse는 교수님들이 싫어한데.. 왜? 몰라 일단 걍 해보쟈\n\ntensor(0.2846, grad_fn=<MeanBackward0>)\n\n\n\nloss=torch.mean((y-yhat)**2)\nloss\n\ntensor(0.2846, grad_fn=<MeanBackward0>)\n\n\n(방법2)\n\nloss_fn = torch.nn.MSELoss()\nloss_fn(yhat,y) # yhat을 먼저쓰자!\n\ntensor(0.2846, grad_fn=<MseLossBackward0>)\n\n\n- step3~4는 동일\n- 반복 (준비+for문)\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.MSELoss() #MSELoss로 하면.. .. 별로? BCE이거로바꾸기\noptimizr = torch.optim.SGD(net.parameters(),lr=0.01)\n\n\nplt.plot(x,y,'o',alpha=0.01)\n\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\nfor epoc in range(3000):\n    ## step1 \n    yhat = net(x) \n    ## step2 \n    loss = loss_fn(yhat,y)\n    ## step3 \n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad() #청소\n\n\nplt.plot(x,y,'o',alpha=0.05)\nplt.plot(x,v,'--b')\nplt.plot(x,net(x).data,'--')"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_28_(5주차)_10월05일_ipynb의_사본.html#로지스틱bceloss",
    "href": "posts/Machine Learning/1. DNN/2022_09_28_(5주차)_10월05일_ipynb의_사본.html#로지스틱bceloss",
    "title": "기계학습 (1005) 5주차",
    "section": "로지스틱–BCEloss",
    "text": "로지스틱–BCEloss\n- BCEloss로 바꾸어서 적합하여 보자.\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.01)\n\n\nfor epoc in range(3000):\n    ## step1 \n    yhat = net(x) \n    ## step2 \n    loss = -torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) # loss_fn(yhat,y)\n    ## step3 \n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.05)\nplt.plot(x,v,'--b')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n- 왜 잘맞지? -> “linear -> sigmoid” 와 같은 net에 BCEloss를 이용하면 손실함수의 모양이 convex 하기 때문에\n\n#convex:볼록한... convex가 학습하기 좋은!!\n\n\nplot_loss 함수소개 = 이 예제에 한정하여 \\(\\hat{w}_0,\\hat{w}_1,loss(\\hat{w}_0,\\hat{w}_1)\\)를 각각 \\(x,y,z\\) 축에 그려줍니다.\n\n\nplot_loss(torch.nn.MSELoss())\n\n\n\n\n\nplot_loss(torch.nn.BCELoss())\n\n\n\n\n\n시각화1: MSE, 좋은초기값\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.SGD(net.parameters(),lr=0.05) #학습률\n\n\nl1,a1 = net #초기값 세팅\nl1.bias.data = torch.tensor([-3.0]) #세팅값\nl1.weight.data = torch.tensor([[-1.0]]) #세팅값\n\n\nshow_lrpr2(net,loss_fn,optimizr,'MSEloss, good_init // SGD')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n시각화2: MSE, 나쁜초기값\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.SGD(net.parameters(),lr=0.05) \n\n\nl1,a1 = net\nl1.bias.data = torch.tensor([-10.0])\nl1.weight.data = torch.tensor([[-1.0]])\n\n\nshow_lrpr2(net,loss_fn,optimizr,'MSEloss, bad_init // SGD')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n시각화3: BCE, 좋은초기값\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.BCELoss() \noptimizr = torch.optim.SGD(net.parameters(),lr=0.05) \n\n\nl1,a1 = net\nl1.bias.data = torch.tensor([-3.0])\nl1.weight.data = torch.tensor([[-1.0]])\n\n\nshow_lrpr2(net,loss_fn,optimizr,'BCEloss, good_init // SGD')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n시각화4: BCE, 나쁜초기값\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.BCELoss() \noptimizr = torch.optim.SGD(net.parameters(),lr=0.05) \n\n\nl1,a1 = net\nl1.bias.data = torch.tensor([-3.0])\nl1.weight.data = torch.tensor([[-1.0]])\n\n\nshow_lrpr2(net,loss_fn,optimizr,'BCEloss, good_init // SGD')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_28_(5주차)_10월05일_ipynb의_사본.html#로지스틱adam-국민옵티마이저",
    "href": "posts/Machine Learning/1. DNN/2022_09_28_(5주차)_10월05일_ipynb의_사본.html#로지스틱adam-국민옵티마이저",
    "title": "기계학습 (1005) 5주차",
    "section": "로지스틱–Adam (국민옵티마이저)",
    "text": "로지스틱–Adam (국민옵티마이저)\n\n# Adam은 SGD에 비하여 2가지 면에서 개선점이 있음\n# 1. 어려워서 몰라도 뎀\n# 2. 가속도의 개념\n\n\n시각화1: MSE, 좋은초기값 –> 이걸 아담으로!\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.Adam(net.parameters(),lr=0.05)  ## <-- 여기를 수정!\n\n\nl1,a1 = net\nl1.bias.data = torch.tensor([-3.0])\nl1.weight.data = torch.tensor([[-1.0]])\n\n\nshow_lrpr2(net,loss_fn,optimizr,'MSEloss, good_init // Adam')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n시각화2: MSE, 나쁜초기값 –> 이걸 아담으로!\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.Adam(net.parameters(),lr=0.05) \n\n\nl1,a1 = net\nl1.bias.data = torch.tensor([-10.0])\nl1.weight.data = torch.tensor([[-1.0]])\n\n\nshow_lrpr2(net,loss_fn,optimizr,'MSEloss, bad_init // Adam')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n시각화3: BCE, 좋은초기값 –> 이걸 아담으로! (혼자해봐요..)\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.BCELoss() \noptimizr = torch.optim.Adam(net.parameters(),lr=0.05) \n\n\nl1,a1 = net\nl1.bias.data = torch.tensor([-3.0])\nl1.weight.data = torch.tensor([[-1.0]])\n\n\nshow_lrpr2(net,loss_fn,optimizr,'BCEloss, good_init // Adam')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n시각화4: BCE, 나쁜초기값 –> 이걸 아담으로! (혼자해봐요..)\n(참고) Adam이 우수한 이유? SGD보다 두 가지 측면에서 개선이 있었음. 1. 그런게 있음.. 2. 가속도의 개념을 적용!!"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_28_(5주차)_10월05일_ipynb의_사본.html#깊은신경망로지스틱-회귀의-한계",
    "href": "posts/Machine Learning/1. DNN/2022_09_28_(5주차)_10월05일_ipynb의_사본.html#깊은신경망로지스틱-회귀의-한계",
    "title": "기계학습 (1005) 5주차",
    "section": "깊은신경망–로지스틱 회귀의 한계",
    "text": "깊은신경망–로지스틱 회귀의 한계\n\n신문기사 (데이터의 모티브)\n- 스펙이 높아도 취업이 안된다고 합니다..\n중소·지방 기업 “뽑아봤자 그만두니까”\n중소기업 관계자들은 고스펙 지원자를 꺼리는 이유로 높은 퇴직률을 꼽는다. 여건이 좋은 대기업으로 이직하거나 회사를 관두는 경우가 많다는 하소연이다. 고용정보원이 지난 3일 공개한 자료에 따르면 중소기업 청년취업자 가운데 49.5%가 2년 내에 회사를 그만두는 것으로 나타났다.\n중소 IT업체 관계자는 “기업 입장에서 가장 뼈아픈 게 신입사원이 그만둬서 새로 뽑는 일”이라며 “명문대 나온 스펙 좋은 지원자를 뽑아놔도 1년을 채우지 않고 그만두는 사원이 대부분이라 우리도 눈을 낮춰 사람을 뽑는다”고 말했다.\n\n\n가짜데이터\n- 위의 기사를 모티브로 한 데이터\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex0.csv')\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      x\n      underlying\n      y\n    \n  \n  \n    \n      0\n      -1.000000\n      0.000045\n      0.0\n    \n    \n      1\n      -0.998999\n      0.000046\n      0.0\n    \n    \n      2\n      -0.997999\n      0.000047\n      0.0\n    \n    \n      3\n      -0.996998\n      0.000047\n      0.0\n    \n    \n      4\n      -0.995998\n      0.000048\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      1995\n      0.995998\n      0.505002\n      0.0\n    \n    \n      1996\n      0.996998\n      0.503752\n      0.0\n    \n    \n      1997\n      0.997999\n      0.502501\n      0.0\n    \n    \n      1998\n      0.998999\n      0.501251\n      1.0\n    \n    \n      1999\n      1.000000\n      0.500000\n      1.0\n    \n  \n\n2000 rows × 3 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nplt.plot(df.x,df.y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\n\n\n\n\n\n\n로지스틱 회귀로 적합\n\n#nn: netral network?의 약자\n\n\nx= torch.tensor(df.x).float().reshape(-1,1)   #float(): 뒤에 거슬리는거 빼주기\ny= torch.tensor(df.y).float().reshape(-1,1)\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nyhat=net(x)\n\n\nloss_fn = torch.nn.BCELoss() \nloss = loss_fn(yhat,y) # loss = -torch.mean((y)*torch.log(yhat)+(1-y)*torch.log(1-yhat))\nloss\n\ntensor(0.9367, grad_fn=<BinaryCrossEntropyBackward0>)\n\n\n\noptimizr = torch.optim.Adam(net.parameters()) \n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'--b')\nplt.plot(x,net(x).data,'--') # 학습전\n\n\n\n\n\nfor epoc in range(6000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'--b')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n- 이건 epoc=6억번으로 설정해도 못 맞출 것 같다 (증가하다가 감소하는 underlying을 설계하는 것이 불가능) \\(\\to\\) 모형의 표현력이 너무 낮다.\n\n\n해결책\n- sigmoid 넣기 전의 상태가 꺽인 그래프 이어야 한다.\n\nsig = torch.nn.Sigmoid()\n\n\nfig,ax = plt.subplots(4,2,figsize=(8,8))\nu1 = torch.tensor([-6,-4,-2,0,2,4,6])\nu2 = torch.tensor([6,4,2,0,-2,-4,-6])\nu3 = torch.tensor([-6,-2,2,6,2,-2,-6])\nu4 = torch.tensor([-6,-2,2,6,4,2,0])\nax[0,0].plot(u1,'--o',color='C0');ax[0,1].plot(sig(u1),'--o',color='C0')\nax[1,0].plot(u2,'--o',color='C1');ax[1,1].plot(sig(u2),'--o',color='C1')\nax[2,0].plot(u3,'--o',color='C2');ax[2,1].plot(sig(u3),'--o',color='C2')\nax[3,0].plot(u4,'--o',color='C3');ax[3,1].plot(sig(u4),'--o',color='C3')"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_28_(5주차)_10월05일_ipynb의_사본.html#깊은신경망dnn을-이용한-해결",
    "href": "posts/Machine Learning/1. DNN/2022_09_28_(5주차)_10월05일_ipynb의_사본.html#깊은신경망dnn을-이용한-해결",
    "title": "기계학습 (1005) 5주차",
    "section": "깊은신경망–DNN을 이용한 해결",
    "text": "깊은신경망–DNN을 이용한 해결\n- 목표: 아래와 같은 벡터 \\({\\boldsymbol u}\\)를 만들어보자.\n\\({\\boldsymbol u} = [u_1,u_2,\\dots,u_{2000}], \\quad u_i = \\begin{cases} 9x_i +4.5& x_i <0 \\\\ -4.5x_i + 4.5& x_i >0 \\end{cases}\\)\n\n꺽인 그래프를 만드는 방법1\n\nu = [9*xi+4.5 if xi <0 else -4.5*xi+4.5 for xi in x.reshape(-1).tolist()]  #tolist하면 list화 \nplt.plot(u,'--')\n\n\n\n\n\n\n꺽인 그래프를 만드는 방법2\n- 전략: 선형변환 \\(\\to\\) ReLU \\(\\to\\) 선형변환\n(예비학습) ReLU 함수란?\n\\(ReLU(x) = \\max(0,x)\\)\n\nrelu=torch.nn.ReLU()\nplt.plot(x,'--r')\nplt.plot(relu(x),'--b')\n\n\n\n\n\n빨간색: x, 파란색: relu(x)\n\n예비학습끝\n우리 전략 다시 확인: 선형변환1 -> 렐루 -> 선형변환2\n(선형변환1)\n\nplt.plot(x);plt.plot(-x)\n\n\n\n\n(렐루)\n\nplt.plot(x,alpha=0.2);plt.plot(-x,alpha=0.5)\nplt.plot(relu(x),'--',color='C0');plt.plot(relu(-x),'--',color='C1')\n\n#out feature을 2로 잡는당->선을 두개로\n\n\n\n\n(선형변환2)\n\nplt.plot(x,alpha=0.2);plt.plot(-x,alpha=0.2)\nplt.plot(relu(x),'--',color='C0',alpha=0.2);plt.plot(relu(-x),'--',color='C1',alpha=0.2)\nplt.plot(-4.5*relu(x)-9.0*relu(-x)+4.5,'--',color='C2')\n\n#하늘색 점선과 노란색 점섬을 더해보자..\n\n\n\n\n이제 초록색선에 sig를 취하기만 하면?\n\nplt.plot(sig(-4.5*relu(x)-9.0*relu(-x)+4.5),'--',color='C2')\n\n\n\n\n정리하면!\n\nfig = plt.figure(figsize=(8, 4))\nspec = fig.add_gridspec(4, 4)\nax1 = fig.add_subplot(spec[:2,0]); ax1.set_title('x'); ax1.plot(x,'--',color='C0')\nax2 = fig.add_subplot(spec[2:,0]); ax2.set_title('-x'); ax2.plot(-x,'--',color='C1')\nax3 = fig.add_subplot(spec[:2,1]); ax3.set_title('relu(x)'); ax3.plot(relu(x),'--',color='C0')\nax4 = fig.add_subplot(spec[2:,1]); ax4.set_title('relu(-x)'); ax4.plot(relu(-x),'--',color='C1')\nax5 = fig.add_subplot(spec[1:3,2]); ax5.set_title('u'); ax5.plot(-4.5*relu(x)-9*relu(-x)+4.5,'--',color='C2')\nax6 = fig.add_subplot(spec[1:3,3]); ax6.set_title('yhat'); ax6.plot(sig(-4.5*relu(x)-9*relu(-x)+4.5),'--',color='C2')\nfig.tight_layout()\n\n\n\n\n\n이런느낌으로 \\(\\hat{\\boldsymbol y}\\)을 만들면 된다.\n\n\n\ntorch.nn.Linear()를 이용한 꺽인 그래프 구현\n\ntorch.manual_seed(43052)\nl1 = torch.nn.Linear(in_features=1,out_features=2,bias=True) \na1 = torch.nn.ReLU()\nl2 = torch.nn.Linear(in_features=2,out_features=1,bias=True) \na2 = torch.nn.Sigmoid() \n\n\nnet = torch.nn.Sequential(l1,a1,l2,a2) \n\n\nl1.weight,l1.bias,l2.weight,l2.bias\n\n(Parameter containing:\n tensor([[-0.3467],\n         [-0.8470]], requires_grad=True), Parameter containing:\n tensor([0.3604, 0.9336], requires_grad=True), Parameter containing:\n tensor([[ 0.2880, -0.6282]], requires_grad=True), Parameter containing:\n tensor([0.2304], requires_grad=True))\n\n\n\nl1.weight.data = torch.tensor([[1.0],[-1.0]])\nl1.bias.data = torch.tensor([0.0, 0.0])\nl2.weight.data = torch.tensor([[ -4.5, -9.0]])\nl2.bias.data= torch.tensor([4.5])\nl1.weight,l1.bias,l2.weight,l2.bias\n\n(Parameter containing:\n tensor([[ 1.],\n         [-1.]], requires_grad=True), Parameter containing:\n tensor([0., 0.], requires_grad=True), Parameter containing:\n tensor([[-4.5000, -9.0000]], requires_grad=True), Parameter containing:\n tensor([4.5000], requires_grad=True))\n\n\n\nplt.plot(l1(x).data)\n\n\n\n\n\nplt.plot(a1(l1(x)).data)\n\n\n\n\n\nplt.plot(l2(a1(l1(x))).data,color='C2')\n\n\n\n\n\nplt.plot(a2(l2(a1(l1(x)))).data,color='C2')\n#plt.plot(net(x).data,color='C2')\n\n\n\n\n- 수식표현\n\n\\({\\bf X}=\\begin{bmatrix} x_1 \\\\ \\dots \\\\ x_n \\end{bmatrix}\\)\n\\(l_1({\\bf X})={\\bf X}{\\bf W}^{(1)}\\overset{bc}{+} {\\boldsymbol b}^{(1)}=\\begin{bmatrix} x_1 & -x_1 \\\\ x_2 & -x_2 \\\\ \\dots & \\dots \\\\ x_n & -x_n\\end{bmatrix}\\)\n\n\\({\\bf W}^{(1)}=\\begin{bmatrix} 1 & -1 \\end{bmatrix}\\)\n\\({\\boldsymbol b}^{(1)}=\\begin{bmatrix} 0 & 0 \\end{bmatrix}\\)\n\n\\((a_1\\circ l_1)({\\bf X})=\\text{relu}\\big({\\bf X}{\\bf W}^{(1)}\\overset{bc}{+}{\\boldsymbol b}^{(1)}\\big)=\\begin{bmatrix} \\text{relu}(x_1) & \\text{relu}(-x_1) \\\\ \\text{relu}(x_2) & \\text{relu}(-x_2) \\\\ \\dots & \\dots \\\\ \\text{relu}(x_n) & \\text{relu}(-x_n)\\end{bmatrix}\\)\n\\((l_2 \\circ a_1\\circ l_1)({\\bf X})=\\text{relu}\\big({\\bf X}{\\bf W}^{(1)}\\overset{bc}{+}{\\boldsymbol b}^{(1)}\\big){\\bf W}^{(2)}\\overset{bc}{+}b^{(2)}\\\\ =\\begin{bmatrix} -4.5\\times\\text{relu}(x_1) -9.0 \\times \\text{relu}(-x_1) +4.5 \\\\ -4.5\\times\\text{relu}(x_2) -9.0 \\times\\text{relu}(-x_2) + 4.5 \\\\ \\dots \\\\ -4.5\\times \\text{relu}(x_n) -9.0 \\times\\text{relu}(-x_n)+4.5 \\end{bmatrix}\\)\n\n\\({\\bf W}^{(2)}=\\begin{bmatrix} -4.5 \\\\ -9 \\end{bmatrix}\\)\n\\(b^{(2)}=4.5\\)\n\n\\(net({\\bf X})=(a_2 \\circ l_2 \\circ a_1\\circ l_1)({\\bf X})=\\text{sig}\\Big(\\text{relu}\\big({\\bf X}{\\bf W}^{(1)}\\overset{bc}{+}{\\boldsymbol b}^{(1)}\\big){\\bf W}^{(2)}\\overset{bc}{+}b^{(2)}\\Big)\\\\=\\begin{bmatrix} \\text{sig}\\Big(-4.5\\times\\text{relu}(x_1) -9.0 \\times \\text{relu}(-x_1) +4.5\\Big) \\\\ \\text{sig}\\Big(-4.5\\times\\text{relu}(x_2) -9.0 \\times\\text{relu}(-x_2) + 4.5 \\Big)\\\\ \\dots \\\\ \\text{sig}\\Big(-4.5\\times \\text{relu}(x_n) -9.0 \\times\\text{relu}(-x_n)+4.5 \\Big)\\end{bmatrix}\\)\n\n- 차원만 따지자\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,2)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,2)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{a_2}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(x,df.underlying,'-b')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\nStep1 ~ Step4\n- 준비\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=2), #u1=l1(x), x:(n,1) --> u1:(n,2) \n    torch.nn.ReLU(), # v1=a1(u1), u1:(n,2) --> v1:(n,2) \n    torch.nn.Linear(in_features=2,out_features=1), # u2=l2(v1), v1:(n,2) --> u2:(n,1) \n    torch.nn.Sigmoid() # v2=a2(u2), u2:(n,1) --> v2:(n,1) \n) \n\n\nloss_fn = torch.nn.BCELoss()\n\n\noptimizr = torch.optim.Adam(net.parameters()) # lr은 디폴트값으로..\n\n- 반복\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(x,df.underlying,'-b')\nplt.plot(x,net(x).data,'--')\nplt.title(\"before\")\n\nText(0.5, 1.0, 'before')\n\n\n\n\n\n\nfor epoc in range(3000):\n    ## step1 \n    yhat = net(x) \n    ## step2 \n    loss = loss_fn(yhat,y) \n    ## step3\n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(x,df.underlying,'-b')\nplt.plot(x,net(x).data,'--',color='C1')\nplt.title(\"after 3000 epochs\")\n\nText(0.5, 1.0, 'after 3000 epochs')\n\n\n\n\n\n\nfor epoc in range(3000):\n    ## step1 \n    yhat = net(x) \n    ## step2 \n    loss = loss_fn(yhat,y) \n    ## step3\n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(x,df.underlying,'-b')\nplt.plot(x,net(x).data,'--',color='C1')\nplt.title(\"after 6000 epochs\")\n\nText(0.5, 1.0, 'after 6000 epochs')"
  },
  {
    "objectID": "posts/Machine Learning/1. DNN/2022_09_28_(5주차)_10월05일_ipynb의_사본.html#깊은신경망dnn으로-해결가능한-다양한-예제",
    "href": "posts/Machine Learning/1. DNN/2022_09_28_(5주차)_10월05일_ipynb의_사본.html#깊은신경망dnn으로-해결가능한-다양한-예제",
    "title": "기계학습 (1005) 5주차",
    "section": "깊은신경망–DNN으로 해결가능한 다양한 예제",
    "text": "깊은신경망–DNN으로 해결가능한 다양한 예제\n\n예제1\n- 언뜻 생각하면 방금 배운 기술은 sig를 취하기 전이 꺽은선인 형태만 가능할 듯 하다. \\(\\to\\) 그래서 이 역시 표현력이 부족할 듯 하다. \\(\\to\\) 그런데 생각보다 표현력이 풍부한 편이다. 즉 생각보다 쓸 만하다.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex1.csv')\n\n\n# 데이터정리\nx = torch.tensor(df.x).float().reshape(-1,1)\ny = torch.tensor(df.y).float().reshape(-1,1)\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\n\n\n\n\n\n이거 시그모이드 취하기 직전은 step이 포함된 듯 \\(\\to\\) 그래서 꺽은선으로는 표현할 수 없는 구조임 \\(\\to\\) 그런데 사실 대충은 표현가능\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=16), # x:(n,1) --> u1:(n,16) #최대 16번 꺾일 수 있음..\n    torch.nn.ReLU(), # u1:(n,16) --> v1:(n,16)\n    torch.nn.Linear(in_features=16,out_features=1), # v1:(n,16) --> u2:(n,1) \n    torch.nn.Sigmoid() # u2:(n,1) --> v2:(n,1) \n)\n\n\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,16)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,16)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{a_2}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n\nloss_fn = torch.nn.BCELoss()\n\n\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(6000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()    \n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b') #실제로는 관츷ㄱ 못하는거\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\n예제2\n- 사실 꺽은선의 조합으로 꽤 많은걸 표현할 수 있거든요? \\(\\to\\) 심지어 곡선도 대충 맞게 적합된다.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex2.csv')\n\n\nx = torch.tensor(df.x).float().reshape(-1,1)\ny = torch.tensor(df.y).float().reshape(-1,1)\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\n\n\n\n\n\nx=torch.tensor(df.x).float().reshape(-1,1)\ny=torch.tensor(df.y).float().reshape(-1,1)\n\n(풀이1)\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=32), # x:(n,1) --> u1:(n,32)\n    torch.nn.ReLU(), # u1:(n,32) --> v1:(n,32) \n    torch.nn.Linear(in_features=32,out_features=1) # v1:(n,32) --> u2:(n,1)\n)\n\n\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,32)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,32)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n\nloss_fn = torch.nn.MSELoss() #mseloss:마지막에 sigmoid형태가 아니니까!\n\n\noptimizr = torch.optim.Adam(net.parameters())\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\nfor epoc in range(6000): \n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\nplt.plot(x,net(x).data,lw=4) #lw:두겁게\n\n\n\n\n(풀이2) – 풀이1보다 좀 더 잘맞음. 잘 맞는 이유? 좋은초기값 (=운)\n\ntorch.manual_seed(5)  # seed값을 43052->5로바꿔주기...\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=32), # x:(n,1) --> u1:(n,32)\n    torch.nn.ReLU(), # u1:(n,32) --> v1:(n,32) \n    torch.nn.Linear(in_features=32,out_features=1) # v1:(n,32) --> u2:(n,1)\n)\n\n\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,32)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,32)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n\nloss_fn = torch.nn.MSELoss() \n\n\noptimizr = torch.optim.Adam(net.parameters())\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\nfor epoc in range(6000): \n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\nplt.plot(x,net(x).data,lw=4)\n\n\n\n\n\n풀이1에서 에폭을 많이 반복하면 풀이2의 적합선이 나올까? –> 안나옴!! (local min에 빠졌다)\n\n\n\n예제3\n\nimport seaborn as sns\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex3.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      x1\n      x2\n      y\n    \n  \n  \n    \n      0\n      -0.874139\n      0.210035\n      0.0\n    \n    \n      1\n      -1.143622\n      -0.835728\n      1.0\n    \n    \n      2\n      -0.383906\n      -0.027954\n      0.0\n    \n    \n      3\n      2.131652\n      0.748879\n      1.0\n    \n    \n      4\n      2.411805\n      0.925588\n      1.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      1995\n      -0.002797\n      -0.040410\n      0.0\n    \n    \n      1996\n      -1.003506\n      1.182736\n      0.0\n    \n    \n      1997\n      1.388121\n      0.079317\n      0.0\n    \n    \n      1998\n      0.080463\n      0.816024\n      1.0\n    \n    \n      1999\n      -0.416859\n      0.067907\n      0.0\n    \n  \n\n2000 rows × 3 columns\n\n\n\n\nsns.scatterplot(data=df,x='x1',y='x2',hue='y',alpha=0.5,palette={0:(0.5, 0.0, 1.0),1:(1.0,0.0,0.0)})\n\n<AxesSubplot:xlabel='x1', ylabel='x2'>\n\n\n\n\n\n\n# 데이터준비\nx1 = torch.tensor(df.x1).float().reshape(-1,1) \nx2 = torch.tensor(df.x2).float().reshape(-1,1) \nX = torch.concat([x1,x2],axis=1) \ny = torch.tensor(df.y).float().reshape(-1,1) \n\n\nX.shape\n\ntorch.Size([2000, 2])\n\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=2,out_features=32),#X의 shape이 2니까 in_features=2\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=32,out_features=1),\n    torch.nn.Sigmoid()\n)\n\n\n\\(\\underset{(n,2)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,32)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,32)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{a_2}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n\nloss_fn = torch.nn.BCELoss() \n\n\noptimizr = torch.optim.Adam(net.parameters()) \n\n\nfor epoc in range(3000):\n    ## 1 \n    yhat = net(X) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n\ndf2 = df.assign(yhat=yhat.reshape(-1).detach().tolist()) #seaborn을 그리려먼 dataframe형식으로 되어잇어야해\ndf2\n\n\n\n\n\n  \n    \n      \n      x1\n      x2\n      y\n      yhat\n    \n  \n  \n    \n      0\n      -0.874139\n      0.210035\n      0.0\n      0.345833\n    \n    \n      1\n      -1.143622\n      -0.835728\n      1.0\n      0.605130\n    \n    \n      2\n      -0.383906\n      -0.027954\n      0.0\n      0.111915\n    \n    \n      3\n      2.131652\n      0.748879\n      1.0\n      0.918491\n    \n    \n      4\n      2.411805\n      0.925588\n      1.0\n      0.912608\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1995\n      -0.002797\n      -0.040410\n      0.0\n      0.254190\n    \n    \n      1996\n      -1.003506\n      1.182736\n      0.0\n      0.508002\n    \n    \n      1997\n      1.388121\n      0.079317\n      0.0\n      0.410099\n    \n    \n      1998\n      0.080463\n      0.816024\n      1.0\n      0.262315\n    \n    \n      1999\n      -0.416859\n      0.067907\n      0.0\n      0.107903\n    \n  \n\n2000 rows × 4 columns\n\n\n\n\nsns.scatterplot(data=df2,x='x1',y='x2',hue='yhat',alpha=0.5,palette='rainbow')\n\n<AxesSubplot:xlabel='x1', ylabel='x2'>\n\n\n\n\n\n- 결과시각화\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nsns.scatterplot(data=df,x='x1',y='x2',hue='y',alpha=0.5,palette={0:(0.5, 0.0, 1.0),1:(1.0,0.0,0.0)},ax=ax[0])\nsns.scatterplot(data=df2,x='x1',y='x2',hue='yhat',alpha=0.5,palette='rainbow',ax=ax[1])\n\n<AxesSubplot:xlabel='x1', ylabel='x2'>\n\n\n\n\n\n- 교훈: underlying이 엄청 이상해보여도 생각보다 잘 맞춤"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-25-8wk.html",
    "href": "posts/Advanved Probability Theory/2023-04-25-8wk.html",
    "title": "8wk: 확률변수, 분포 (1)",
    "section": "",
    "text": "해당 강의노트는 전북대학교 최규빈교수님 AP2023 자료임"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-25-8wk.html#cap_n1infty-frac1nfrac1n0",
    "href": "posts/Advanved Probability Theory/2023-04-25-8wk.html#cap_n1infty-frac1nfrac1n0",
    "title": "8wk: 확률변수, 분포 (1)",
    "section": "\\(\\cap_{n=1}^{\\infty}(-\\frac{1}{n},\\frac{1}{n})=\\{0\\}\\)",
    "text": "\\(\\cap_{n=1}^{\\infty}(-\\frac{1}{n},\\frac{1}{n})=\\{0\\}\\)\n- \\(\\cap_{n=1}^{\\infty}(-\\frac{1}{n},\\frac{1}{n}) =\\{0\\}\\)을 증명하라.\n\\(lim_{n \\to \\infty}(-\\frac{1}{n},\\frac{1}{n})\\)\n(증명)\nstep1: \\(\\cap_{n=1}^{\\infty}(-\\frac{1}{n},\\frac{1}{n})\\)은 원소로 \\(0\\)을 포함한다.\n\\(\\forall n \\in \\mathbb{N}\\): \\(-\\frac{1}{n} < 0 < \\frac{1}{n}\\)\n\\(\\Leftrightarrow \\forall n \\in \\mathbb{N}\\): \\(0 \\in (-\\frac{1}{n},\\frac{1}{n})\\)\n\\(\\Leftrightarrow\\) \\(0 \\in (-\\frac{1}{1},\\frac{1}{1})\\) and \\(0 \\in (-\\frac{1}{2},\\frac{1}{2})\\) \\(\\dots\\)\n\\(\\Leftrightarrow\\) \\(0 \\in (-\\frac{1}{1},\\frac{1}{1}) \\cap (-\\frac{1}{2},\\frac{1}{2}) \\cap \\dots\\)\n\\(\\Leftrightarrow\\) \\(0 \\in \\cap_{n=1}^{\\infty}(-\\frac{1}{1},\\frac{1}{1})\\)\nstep2: \\(\\cap_{n=1}^{\\infty}(-\\frac{1}{n},\\frac{1}{n})\\)은 원소로 \\(0\\)보다 큰 임의의 양수를 포함하지 않는다.\n포함한다고 가정하자. 즉\n\\(\\exists \\delta >0\\) such that \\(0+\\delta \\in \\cap_{n=1}^{\\infty}(-\\frac{1}{n},\\frac{1}{n})\\)\nNOTE: From \\(\\delta>0\\), \\(\\exists N \\in \\mathbb{N}\\) such that \\(0<\\frac{1}{N}<\\delta\\)\nTHUS \\(\\delta \\notin (-\\frac{1}{N},\\frac{1}{N})\\) \\(\\Rightarrow\\) CONTRADICTION! (\\(\\because \\cap_{n=1}^{\\infty}(-\\frac{1}{n},\\frac{1}{n}) \\subset (-\\frac{1}{N},\\frac{1}{N}))\\)\nstep3: \\(\\cap_{n=1}^{\\infty}(-\\frac{1}{n},\\frac{1}{n})\\)은 원소로 \\(0\\)보다 큰 임의의 음수를 포함하지 않는다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-25-8wk.html#vacuous-truth",
    "href": "posts/Advanved Probability Theory/2023-04-25-8wk.html#vacuous-truth",
    "title": "8wk: 확률변수, 분포 (1)",
    "section": "Vacuous truth",
    "text": "Vacuous truth\n- \\(P \\Rightarrow Q\\) 에서, \\(P\\)가 틀렸거나 \\(P\\)를 만족하는 집합이 공집합일 경우 \\(P\\Rightarrow Q\\)라는 명제는 항상 참이되고 이러한 참을 배큐어스 트루 라고 말한다.\n- 이해를 돕기 위한 예시\n\n명제1: 최규빈교수보다 나이 많은 학생은 A+를 받지 못했다.\n명제2: 최규빈교수보다 나이 많은 학생은 A+를 받았다.\n\n여기에서 명제1,명제2는 모두 참이어야 한다. 그래야 명제1,명제2의 대우는 모두 참이 되며\n\n대우1: A+를 받은 학생은 최규빈교수보다 나이가 적다.\n대우2: A+를 받지 못한 학생은 최규빈교수보다 나이가 적다.\n\n두 대우의 합성명제인 아래도 참이 된다.\n\nA+을 받거나 받지 못한 학생은 최규빈교수보다 나이가 적다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-25-8wk.html#정의",
    "href": "posts/Advanved Probability Theory/2023-04-25-8wk.html#정의",
    "title": "8wk: 확률변수, 분포 (1)",
    "section": "정의",
    "text": "정의\n- 정의: \\(\\Omega\\)에 대한 부분집합의 모임 \\({\\cal T}\\)가 아래의 조건을 만족하면 \\({\\cal T}\\)를 \\(\\Omega\\)의 토폴로지라고 부른다.\n\n\\(\\emptyset, \\Omega \\in {\\cal T}\\)\n\\(\\forall A,B \\in {\\cal T}:~ A\\cap B \\in {\\cal T}\\) (finite intersection에 닫혀있음)\n\\(\\forall {\\cal A} \\subset {\\cal T}: ~ (\\cup_{A \\in {\\cal A}}A ) \\in {\\cal T}\\) (uncoutable union, arbitrary union에 닫혀있음)\n\n- \\((\\Omega,{\\cal T})\\)를 위상공간 (topological space) 이라고 부른다. 그리고 \\({\\cal T}\\)의 원소를 \\({\\cal T}\\)-open set이라고 부른다.\n- 모티브: 실수위에서의 열린구간 \\((a,b)\\)의 개념을 추상화하고 싶음. 즉 open interval \\(\\overset{일반화}{\\to}\\) open set 을 하고 싶음. 그리고 이러한 open set 만을 모은 collection \\({\\cal T}\\)라는 기호로 표현하고 싶음.\n\n관찰1: \\((1,3) \\cap (2,4) = (2,3)\\) // 2개의 open-interval을 교집합하니 open-interval이 나옴\n관찰2: \\(\\cap_{n=1}^{\\infty}(1-\\frac{1}{n},3+\\frac{1}{n}) =[1,3]\\) // countable many한 open-interval을 교집합하면 closed-interval이 나옴\n관찰3: \\(\\cup_{n=1}^{\\infty}(1+\\frac{1}{n},3-\\frac{1}{n})= (1,3)\\) // countable many한 open-interval을 합집합하면 open-interval이 나옴\n관찰4: \\(\\cup_{\\epsilon>0}^{\\infty}(1+\\epsilon,3-\\epsilon) =(1,3)\\) // uncountalbe many한 open-interval을 합집합해도 open-interval이 나옴\n\n\n관찰 1,2를 통해 open-interval은 finte한 intersection에 닫혀있음 을 알 수있다.\n관찰 3,4를 통해 arbitrary union에 닫혀있다.\n\n- 왜 open interval을 추상화하고 싶을까?\n\nopen interval은 엄청 특이한 성질이 있음. 구간 \\((a,b)\\)의 모든 점 \\(x\\)는 점 \\(x\\)를 포함하는 (아주 작은) 열린구간 \\((x-\\epsilon,x+\\epsilon)\\) 이 \\((a,b)\\)사이에 존재함.\n이 성질은 극한의 개념을 정의하기에 매우 유리하다. (따라서 연속, 끊어짐 등을 이해하기에도 좋다)\n\n- \\(\\Omega=\\mathbb{R}\\)일 경우 open-set\n\n\\((1,2)\\)\n\\((1,2)\\cup (5,6)\\)\n\\((a-\\epsilon, a+\\epsilon)\\), where \\(\\epsilon>0\\) and \\(a\\in\\mathbb{R}\\)\n\\(\\dots\\)\n\n\nopen-set이라는 것은 open-interval의 일반화\n\n- 체크\n\n\\(\\Omega=\\mathbb{R}\\), \\({\\cal T}=\\{\\emptyset, \\Omega\\}\\)라고 하자. \\({\\cal T}\\)는 \\(\\Omega\\)에 대한 토폴로지이며 따라서 \\((\\Omega, {\\cal T})\\)는 위상공간이 된다.\n\\(\\Omega=\\mathbb{R}\\), \\({\\cal T}=2^{\\mathbb{R}}\\)라고 하자. 그렇다면 \\({\\cal T}\\)는 \\(\\Omega\\)에 대한 토폴로지이며 따라서 \\((\\Omega,{\\cal T})\\)는 위상공간이 된다.\n\n그렇지만 우린 이런걸 쓰고 싶은게 아니야 (\\(\\star\\))"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-25-8wk.html#짧은지식",
    "href": "posts/Advanved Probability Theory/2023-04-25-8wk.html#짧은지식",
    "title": "8wk: 확률변수, 분포 (1)",
    "section": "짧은지식",
    "text": "짧은지식\n- 이론: \\(\\Omega=\\mathbb{R}\\) 일때 \\({\\cal U}=\\{O:O = \\cup_{i=1}^{\\infty}(a_i, b_i),~ a_i\\leq b_i \\in \\mathbb{R}\\}\\)라고 하자. 즉 \\({\\cal U}\\)는 open interval의 countable union으로 표현가능한 집합들의 모임이다. 그렇다면 \\((\\mathbb{R}, {\\cal U})\\)는 위상공간이 된다.\n\n그리고 특별히 이러한 위상 \\({\\cal U}\\)를 \\(\\mathbb{R}\\)에서의 standard topology, Euclidean topology, 혹은 usual topology 라고 부른다. 사실 \\({\\cal U}\\)가 바로 우리가 토폴로지를 정의하는 이유이다 (매우 중요하다는 뜻이에요)\n\n\n\\({\\cal U}\\)의 원소를 원래 엄밀하게는 \\({\\cal U}\\)-open set이라고 불러야 하지만 이 경우는 \\({\\cal U}\\)를 생략하여 open set 이라고 부르기도 한다. 즉 우리가 일반적으로 말하는 “실수 \\(\\mathbb{R}\\)에서의 열린집합, 혹은 그냥 열린집합” 은 \\({\\cal U}\\)-open set을 의미한다.\n\n\n이 이론이 의미하는 바는 (1) 실수에서의 열린구간의 일반화 버전은 열린집합이며 (2) 열린집합은 열린구간의 가산합집합으로 표현가능하다 라는 뜻이다.\n\n\n\\({\\cal U}\\)를 한글로는 보통위상이라고 표현하기도 하지만 그렇게 널리 사용되지는 않는다. 하지만 따로 지칭할 용어가 마땅치 않아서 나는 그냥 보통위상이라고 부르겠다.\n\n- 이론: \\((\\mathbb{R},{\\cal U})\\)를 보통위상공간 (usual topological space) 이라고 하자. 모든 \\(O \\in {\\cal U}\\) 는 아래를 만족한다.\n\n\\(\\forall o \\in O, \\exists a,b \\in \\mathbb{R}\\) such that \\(o \\in (a,b) \\subset O \\quad \\cdots (\\star)\\)\n\n\n참고로 어떠한 집합 \\(O\\)에 대하여 \\((\\star)\\)를 만족하는 원소 \\(o\\)를 interior point of \\(O\\) 라고 부른다. 따라서 어떤 집합의 모든 원소가 그 집합의 interior point(내점)라면 그 집합은 openset이라고 해석할 수 있다.\n\n\n저는 나이테정리라고 외웠어요..\n\n- 실수에서의 \\({\\cal U}\\)-openset 을 정의하는 방법\n\n열린구간의 가산합집합\n모든원소가 interior point인 집합\n\n- 위상공간 \\((\\mathbb{R},{\\cal U})\\)를 고려하자. 여기에서 \\({\\cal U}=\\{O:O = \\cup_{i=1}^{\\infty}(a_i, b_i),~ a_i\\leq b_i \\in \\mathbb{R}\\}\\)를 의미한다. 아래의 사실들을 관찰하라.\n\n모든 열린구간은 열린집합이다.\n\\((-\\infty, a)\\)와 \\((a,\\infty)\\)는 모두 열린집합이다.\n한점의 원소 \\(\\{a\\}\\)는 닫힌집합이다. (\\(\\{a\\}\\)의 여집합이 열린집합이므로)\n\\((-\\infty, a]\\)와 \\([a,\\infty)\\)는 모두 닫힌집합이다.\n공집합과 \\(\\mathbb{R}\\)은 열린집합이다.1 따라서 공집합과 \\(\\mathbb{R}\\)은 닫힌집합이다.\n\n\n열린구간의 가산합집합 에 대한 증명\n2번 증명 \\(\\cup_{n=1}^\\infty (-n, a) = (-\\infty, a)\\)\n3번 \\(\\{a\\}\\)는 닫힌집합이라는 것은 \\(\\{a^c\\}\\)이 열린집합이라는 것을 의미한다. \\(\\{a^c\\}=(-\\infty, a) \\cup (a,\\infty)\\)이고 열린집합의 합집합은 열린집합이다.\n4번 \\((-\\infty, a]^c = (a,\\infty)\\)이고 우변이 열린집합이므로 성립\n5번 \\((2,2)=\\emptyset\\)이고 \\(\\cup_{n=1}^\\infty (-n,n) = \\mathbb{R}\\). 공집합과 전체집합은 열린집합이면서 닫힌집합이라고 할 수 있다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-25-8wk.html#시그마필드-vs-토폴로지",
    "href": "posts/Advanved Probability Theory/2023-04-25-8wk.html#시그마필드-vs-토폴로지",
    "title": "8wk: 확률변수, 분포 (1)",
    "section": "시그마필드 vs 토폴로지",
    "text": "시그마필드 vs 토폴로지\n\n\n\n\n\n\n\n\n\n시그마필드\n토폴로지\n\n\n\n\n시작\n“길이를 잴 수 있는 집합”이란 개념을 일반화 하고 싶다\n“열린구간”의 개념을 일반화 하고 싶다\n\n\n기호\n\\({\\cal F}\\)\n\\({\\cal T}\\)\n\n\n공간\n\\((\\Omega,{\\cal F})\\)\n\\((\\Omega,{\\cal T})\\)\n\n\n원소\n\\({\\cal F}\\)-measurable set, measurable set\n\\({\\cal T}\\)-open set\n\n\n쓸모없는공간\n\\((\\mathbb{R},2^{\\mathbb R})\\)\n\\((\\mathbb{R},2^{\\mathbb R})\\)\n\n\n쓸모있는공간\n\\((\\mathbb{R},{\\cal R})\\)\n\\((\\mathbb{R},{\\cal U})\\)\n\n\n\n\n\\({\\cal R}\\)이 뭔데..?"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-25-8wk.html#borel-sigma-field",
    "href": "posts/Advanved Probability Theory/2023-04-25-8wk.html#borel-sigma-field",
    "title": "8wk: 확률변수, 분포 (1)",
    "section": "Borel \\(\\sigma\\)-field",
    "text": "Borel \\(\\sigma\\)-field\n- 정의: \\((\\mathbb{R}, {\\cal U})\\)를 보통위상공간이라고 하자. 아래와 같은 시그마필드를 Borel \\(\\sigma\\)-algebera on \\(\\mathbb{R}\\)이라고 한다.\n\\[{\\cal B}(\\mathbb{R}):=\\sigma({\\cal U})\\]\n그리고 \\({\\cal B}(\\mathbb{R})\\)의 원소를 Borel measurable sets이라고 부른다.\n- 참고: 교재에서는 \\({\\cal B}(\\mathbb{R})\\)를 \\({\\cal R}\\)로 표현하기도 한다.\n- 이론: 아래와 같은 집합을 고려하자.\n\n\\({\\cal A}_1:= \\{A\\subset \\mathbb{R}: A \\text{ is open}\\}\\)2\n\\({\\cal A}_2:= \\{(a,b): a,b \\in \\mathbb{R}, a<b\\}\\)\n\\({\\cal A}_3:= \\{[a,b): a,b \\in \\mathbb{R}, a<b\\}\\)\n\\({\\cal A}_4:= \\{(a,b]: a,b \\in \\mathbb{R}, a<b\\}\\)\n\\({\\cal A}_5:= \\{[a,b]: a,b \\in \\mathbb{R}, a<b\\}\\)\n\\({\\cal A}_6:= \\{(-\\infty,b): a,b \\in \\mathbb{R}, a<b\\}\\)\n\\({\\cal A}_7:= \\{(-\\infty,b]: a,b \\in \\mathbb{R}, a<b\\}\\)\n\\({\\cal A}_8:= \\{(a,\\infty): a,b \\in \\mathbb{R}, a<b\\}\\)\n\\({\\cal A}_9:= \\{[a,\\infty): a,b \\in \\mathbb{R}, a<b\\}\\)\n\n아래가 성립한다.\n\\[{\\cal R}:={\\cal B}(\\mathbb{R}) = \\sigma({\\cal A}_1)=\\sigma({\\cal A}_2)=\\dots=\\sigma({\\cal A}_9)\\]\n\n\\(R\\)의 모든 원소는 르벡메저로 측정 가능\n\n\n\\(R=\\sigma({\\cal A}_1)\\)의 의미는 open set을 재료로 하여 만들어진 집합은 모두 르벡측도로 잴 수 있다.\n\n(증명??) – 증명까지는 아니고 그냥 설명..\n\n예비학습1: countable union의 countable union은 countable union이다.\n\\[\\mathbb{Q}^+ = \\cup_{m \\in \\mathbb{N}}\\big(\\cup_{n \\in \\mathbb{N}}\\{m/n\\}\\big)\\]\n예비학습2: “\\(\\sigma({\\cal A}_1)\\)의 모든원소는 \\({\\cal A}_1\\)의 원소를 재료로하여 만들수 있다” 라고 표현할 수 있으며, 여기에서 “만들 수 있다” 라는 의미는 \\({\\cal A}_1\\)의 원소에 가산합집합, 가산교집합, 여집합, 차집합등의 연산을 적용하여 \\(\\sigma({\\cal A_1})\\)의 원소를 만들 수 있다라는 의미이다.\n\n가산합집합, 가산교집합, 여집합, 차집합 > 가산합집합, 여집합, 그리고 그것을 이용한 합성 연산\n\n예비학습3: 아래의 연산들은 모두 시그마필드에서 닫혀있다.\n\n가산합집합의 가산합집합\n가산합집합의 가산합집합의 가산합집합\n여집합의 가산교집합의 가산합집합의 차집합\n\\(\\dots\\)\n\n즉 시그마필드는 가산합집합과, 여집합에 닫혀있고 그들의 합성연산에 닫혀있다고 해석할 수 있다.\n\n이제 아래가 성립한다고 가정해보자.\n\n\\(\\sigma({\\cal A}_1)\\)의 모든 원소는 \\({\\cal A}_1\\)의 원소를 이용하여 만들 수 있다. 즉 \\({\\cal A}_1\\)의 모든원소에 가산합집합, 여집합, 혹은 그들의 합성연산을 적용하여 \\(\\sigma({\\cal A}_1)\\)의 모든 원소를 나타낼 수 있다.\n\\({\\cal A}_1\\)의 모든 원소는 \\({\\cal A}_2\\)의 원소를 이용하여 만들 수 있다. 즉 \\({\\cal A}_1\\)의 모든원소에 가산합집합, 여집합 혹은 그들의 합성연산을 적용하여 \\({\\cal A}_2\\)의 모든 원소를 나타낼 수 있다.\n\n그렇다면 궁극적으로는 \\({\\cal A}_2\\)의 원소를 가산합집합, 여집합, 혹은 그들의 합성연산을 적용하여 \\(\\sigma({\\cal A}_1)\\)를 표현할 수 있다는 의미이고 이는 \\({\\cal R}=\\sigma({\\cal A}_1)=\\sigma({\\cal A}_2)\\)를 의미한다. \\({\\cal R}=\\sigma({\\cal A}_3)=\\sigma({\\cal A}_4)=\\dots=\\sigma({\\cal A}_9)\\) 역시 유사하게 따질 수 있다.\n- 이론: 위의 이론의 \\({\\cal A}_2,\\dots,{\\cal A}_9\\)에서 \\(\\mathbb{R}\\) 대신에 \\(\\mathbb{Q}\\)를 사용해도 성립한다.\n- NOTE: \\({\\cal A}_1,\\dots,{\\cal A}_9\\)는 모두 파이시스템이다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-25-8wk.html#르벡메져",
    "href": "posts/Advanved Probability Theory/2023-04-25-8wk.html#르벡메져",
    "title": "8wk: 확률변수, 분포 (1)",
    "section": "르벡메져",
    "text": "르벡메져\n- Thm: \\(\\Omega=\\mathbb{R}\\) 에 대하여 아래와 같은 collection \\({\\cal A}\\)를 고려하자.\n\\[{\\cal A}=\\{(a,b]: a,b\\in \\mathbb{R}, a<b\\}\\]\n그리고 아래와 같은 함수 \\(\\tilde{m}:{\\cal A} \\to [0,\\infty]\\)을 고려하자.\n\\[\\tilde{m}((a,b]) = b-a\\]\n이러한 함수 \\(\\tilde{m}\\)은 \\((\\mathbb{R},{\\cal R})\\)에서의 메져 \\(m:{\\cal R} \\to [0,\\infty]\\)로 쉽게 업그레이드 가능하며 이 업그레이드 결과는 유일하다.\n\n업그레이된 메저를 르벡메져라고 한다.\n\n(증명)\n카라테오도리의 확장정리에 의하여\n\n\\({\\cal A}\\)가 세미링임을 체크하고\n\\(\\tilde{m}:{\\cal A}\\to[0,\\infty]\\)이 \\({\\cal A}\\)에서 (1) additive (2) \\(\\sigma\\)-subadditive (3) \\(\\sigma\\)-finite 을 만족한다는 사실을 체크하면 된다.\n\n된다. \\(\\tilde{m}\\)이 \\(\\sigma\\)-subaddtive 성질을 가진다는 것을 보이는 것이 어려운데 이는 받아들이자.\n- 정의: 위의 이론에 의하여 업그레이드 된 메져 \\(m\\)을 르벡메져라고 한다.\n- 이론: \\((\\mathbb{R},{\\cal R})\\)를 잴 수 있는 공간이라고 하고, \\(m\\)을 이 공간에서의 르벡메져라고 하자. 아래와 같은 집합들의 모임을 생각하자.\n\n\\({\\cal A}_1:= \\{A\\subset \\mathbb{R}: A \\text{ is open}\\}\\)\n\\({\\cal A}_2:= \\{(a,b): a,b \\in \\mathbb{R}, a<b\\}\\)\n\\({\\cal A}_3:= \\{[a,b): a,b \\in \\mathbb{R}, a<b\\}\\)\n\\({\\cal A}_4:= \\{(a,b]: a,b \\in \\mathbb{R}, a<b\\}\\)\n\\({\\cal A}_5:= \\{[a,b]: a,b \\in \\mathbb{R}, a<b\\}\\)\n\\({\\cal A}_6:= \\{(-\\infty,b): a,b \\in \\mathbb{R}, a<b\\}\\)\n\\({\\cal A}_7:= \\{(-\\infty,b]: a,b \\in \\mathbb{R}, a<b\\}\\)\n\\({\\cal A}_8:= \\{(a,\\infty): a,b \\in \\mathbb{R}, a<b\\}\\)\n\\({\\cal A}_9:= \\{[a,\\infty): a,b \\in \\mathbb{R}, a<b\\}\\)\n\n\\({\\cal A}_1,{\\cal A}_2,\\dots, {\\cal A}_9\\)에서의 르벡메져와 그 값이 일치하지만 \\({\\cal R} - {\\cal A}_1, \\dots, {\\cal R}-{\\cal A}_9\\) 등에서는 일치하지 않는 새로운 메져 \\(m'\\)은 존재할 수 없다. 즉 르벡메져는 \\({\\cal A}_1,\\dots,{\\cal A}_9\\)에서의 값으로 유일하게 결정된다.\n(설명)\n르벡메져는 \\(\\sigma\\)-finite한 메져이고, \\({\\cal A}_{1}\\dots{\\cal A}_{9}\\)는 모두 “7wk-파이시스템에서의 확장이론(메져버전)”에 소개된 이론의 조건 1,2를 만족하는 파이시스템이다. 따라서 르벡메져의 값은 \\({\\cal A}_1\\dots,{\\cal A}_9\\)에서의 값으로 유일하게 결정된다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-25-8wk.html#생략때문에-헷갈려",
    "href": "posts/Advanved Probability Theory/2023-04-25-8wk.html#생략때문에-헷갈려",
    "title": "8wk: 확률변수, 분포 (1)",
    "section": "생략때문에 헷갈려",
    "text": "생략때문에 헷갈려\n- 어떠한 수학교재에서, 아무말 없이 open set 이라고만 하면 보통위상공간 (usual topological space) 으로부터 정의되는 open set을 의미한다. 즉 usual topological space \\((\\mathbb{R},{\\cal U})\\)에서 \\({\\cal U}\\)의 원소를 의미한다.\n- 어떠한 수학교재에서, 아무말 없이 measurable set 이라고 하면 르벡측도로 잴 수 있는 집합을 말한다. 즉 \\((\\mathbb{R}, {\\cal R})\\) 에서의 \\({\\cal R}\\)의 원소를 의미한다. 즉 일반적으로 정의하는 “잴 수 있는 집합”에서 “잴 수 있다”는 의미는 “르벡측도로 잴 수 있다”는 의미이다.3 일반적인 \\((\\Omega, {\\cal F})\\)에서 \\({\\cal F}\\)의 원소는 ${F}-measurable set 이라고 표현해야 옳다.\n- 하지만 때에 따라서는 \\({\\cal F}\\)의 원소를 그냥 measurable set이라고 부른다.\n예시1\n여기에서 measuralbe set은 앞에서 정의한 \\({\\cal F}\\)의 원소라는 의미이다.\n\n\n\n그림1: measurable set에 대한 교재의 언급, 눈치껏 그전의 문맥에서 정의한 \\({\\cal F}\\)-measurable set을 의미함을 알아들어야 함\n\n\n- measure, measurable 등의 의미는 눈치껏 알아먹어야 한다.\n예시2\n일반적으로 measure라는 단어가 사용되면 “르벡측도로 재다”라는 의미를 지칭하는 경우가 많음\n\n\n\n그림2: 여기에서 사용되는 “measure”의 의미는 문맥상 “르벡측도로 재다”라는 의미로 해석해야함\n\n\n예시3\n\\((\\Omega, {\\cal F})\\)를 잴 수 있는 공간이라고 할 때는 meaure의 의미가 꼭 “르벡측도로 재다” 라는 것을 의미하는 건 아님\n예시4\n비탈리집합이 nonmeasurable set이라는 의미는 르벡측도로 측정불가능한 집합이라는 것을 의미함.\n\n\n\n그림3: 여기에서 \\(N\\)은 비탈리집합을 의미하며 여기에서 “nonmeasurable” 이라는 뜻은 르벡메져로 측정불가능한 집합이라는 의미"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-25-8wk.html#토폴로지와-측도론의-논리전개",
    "href": "posts/Advanved Probability Theory/2023-04-25-8wk.html#토폴로지와-측도론의-논리전개",
    "title": "8wk: 확률변수, 분포 (1)",
    "section": "토폴로지와 측도론의 논리전개",
    "text": "토폴로지와 측도론의 논리전개\n- 토폴로지와 측도론을 공부하면서 비슷한점이 있다고 느낌\n- 비슷한점1: 모두 어떠한 속성을 가지는 집합을 “일반화” 하기 위해서 생겨났다. 예를들면, 잴 수 있는 집합이라는 것은 “수직선에서 길이를 잴 수 있는 집합”의 개념을 일반화하고 싶었어서 만들었으며, 열린집합이라는 것은 “수직선에서의 열린구간”이라는 개념을 일반화하고 싶어서 만들었다.\n- 비슷한점2: 따라서 “잴 수 있는 집합들의 모임”, “열린집합들의 모임” 이라는 집합들의 집합이라는 장치를 고안하였다. 그리고 이러한 과정에서 일반적인 “길이(length)를 잴 수 있는 집합”의 속성, “열린구간”의 속성을 모아 “잴 수 있는 집합의 모임”, “열린집합의 모임” 을 정의하는 재료로 사용하였다.\n- 비슷한점3: “잴 수 있는 집합들의 모임”, “열린집합들의 모임”의 원소를 각각 \\({\\cal F}\\)-mesurable set, \\({\\cal T}\\)-open set이라고 부르는 것도 유사하다.\n- 비슷한점4: \\(\\Omega=\\mathbb{R}\\)에 대한 시그마필드와 토폴로지가 각각 \\({\\cal R}\\)이거나 \\({\\cal U}\\)이라면 그냥 잴 수 있는 집합, 열린 집합 이라고 부르는 것도 유사하다.\n- 비슷한점5: 비슷한점4의 경우를 제외하고는 \\({\\cal F}\\)-mesurable set, \\({\\cal T}\\)-open set이라고 부르는게 원칙인데 이것도 문맥에 따라서 그냥 생략하고 쓰는 것도 유사하다. (사실 매번 언급하는게 귀찮기는 해)\n- 비슷한점6: 일반화의 과정에서 발생하는 이상한 개념의 충돌이 존재한다.\n\n잴 수 있는 집합의 모임을 \\((\\mathbb{R},2^\\mathbb{R})\\)로 설정하면 비탈리집합도 잴 수 있다. (그렇지만 \\((\\mathbb{R}, {\\cal R})\\)에서는 비탈리집합이 잴 수 없는 집합이므로 보통 책에서는 “잴 수 없는 집합”이라고 배운다)\n열린집합의 모임을 \\((\\mathbb{R},2^\\mathbb{R})\\)로 설정하면 한점만 포함하는 집합 \\(\\{x\\}\\)는 열린집합이 된다. (그렇지만 \\((\\mathbb{R},{\\cal U})\\)에서는 한점만 포함하는 집합은 닫힌집합이므로 보통 책에서는 “한점만 포함하는 집합은 닫힌집합이다” 라고 배운다.\n\n\n제 생각: 사실 이는 일반화 과정이 겪는 불가피한 문제인듯 해요. “문자, 그림, 기호 따위를 쓸 수 있는 도구” 정도로 펜슬의 의미를 확장하면 손가락도 펜슬이 되고, 발가락도 펜슬이 됩니다.\n\n- 비슷한점7: 열린집합과 토폴로지, 잴수있는 집합과 시그마필드를 정의하는 두가지 루트가 존재한다.\n\n루트1: 토폴로지를 먼저 정의하고 토폴로지의 원소가 열린집합이라고 정의한다. 혹은 시그마필드를 먼저 정의하고 시그마필드의 원소가 잴 수 있는 집합이라고 정의한다.\n루트2: 열린집합을 정의하고 (집합의 모든 원소가 interior point이면 열린집합), 열린집합의 모임으로 토폴로지를 정의한다. 혹은 잴 수 있는 집합을 정의하고, 잴 수 있는 집합의 모임으로 시그마필드를 정의한다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-25-8wk.html#확률공간과-용어들",
    "href": "posts/Advanved Probability Theory/2023-04-25-8wk.html#확률공간과-용어들",
    "title": "8wk: 확률변수, 분포 (1)",
    "section": "확률공간과 용어들",
    "text": "확률공간과 용어들\n- 동전예제에서의 확률공간 \\((\\Omega,{\\cal F},P)\\)를 가정하고 용어를 정리해보자.\n\noutcomes: \\(H\\),\\(T\\)\nset of “outcomes”: \\(\\Omega=\\{H,T\\}\\)\nevent: \\(\\emptyset\\), \\(\\{H\\}\\), \\(\\{T\\}\\), \\(\\{H,T\\}\\)\nset of “events”: \\({\\cal F}\\)\nprobabilites: \\(P:{\\cal F} \\to [0,1]\\)\n\n- 교재의 언급\n\n\n\n그림4: 확률을 위한 기본용어"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html",
    "href": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html",
    "title": "6wk: 측도론 (2)",
    "section": "",
    "text": "해당 강의노트는 전북대학교 최규빈교수님 AP2023 자료임"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#약한조건-약한정리-강한조건-강한정리",
    "href": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#약한조건-약한정리-강한조건-강한정리",
    "title": "6wk: 측도론 (2)",
    "section": "약한조건, 약한정리, 강한조건, 강한정리",
    "text": "약한조건, 약한정리, 강한조건, 강한정리\n- 정리: 어떠한 조건을 만족하면, 어떠한 결론이 나온다.\n\n결론: 우리가 원하는 것.\n조건: 우리가 원하는 것을 얻기 위한 고난과정.\n\n- 결론이 동일하다면 조건이 약할 수록 유리하다.\n\n정리1: 수업에 온라인으로 참석하거나 오프라인으로 참석한다면 모두 출석으로 인정한다.\n정리2: 수업에 오프라인으로 참석할때만 출석으로 인정한다.\n\n\n정리2의 조건이 만족되면 정리1의 조건은 자동으로 만족된다. 따라서 정리2의 조건이 더 강한 조건이다. 조건이 강할수록 불리하므로 정리2가 더 불리하다.\n\n- 조건이 동일하다면 결론이 강한 쪽이 유리하다.\n\n정리1: 중간고사와 기말고사를 모두 응시한다면, B학점 이상이다.\n정리2: 중간고사와 기말고사를 모두 응시한다면, A학점 이상이다.\n\n\n정리2의 결론이 만족되면 정리1의 결론은 자동으로 만족되므로 정리2의 결론이 더 강하다. 결론은 강할수록 유리하므로 정리2가 더 유리하다.\n\n\n정리1 ->(upgrade) 정리2\n\n\n\n\n조건을 약화시킴\n\n\n\n\n\n\n결론을 강하게\n\n\n\n\n\n\n만약 정리1에서 정리3으로 가는데 조건은 약화됬는데 결론도 약화된다면? upgrade라고 볼 수 없다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#쓸모없는-측도",
    "href": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#쓸모없는-측도",
    "title": "6wk: 측도론 (2)",
    "section": "쓸모없는 측도",
    "text": "쓸모없는 측도\n- 세상엔 측도의 정의를 만족하지만 쓸모 없는 측도가 있다.\n\n예시1: \\({\\cal F}\\)의 모든 원소의 메져값은 0이다. -> m(A)=0\n예시2: \\({\\cal F}\\)의 모든 원소의 메져값은 무한대이다.\n\n- 예시2와 같은 측도를 고려하고 싶지 않음 \\(\\Rightarrow\\) 유한측도, 시그마유한측도의 개발"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#쓸모없는-가측공간",
    "href": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#쓸모없는-가측공간",
    "title": "6wk: 측도론 (2)",
    "section": "쓸모없는 가측공간",
    "text": "쓸모없는 가측공간\n- 세상엔 쓸모없는 잴 수 없는 공간이 있다. (유의미한 측도를 주는게 불가능한 잴 수 있는 공간)\n\n예시1: \\({\\cal F}= \\{\\emptyset, \\Omega\\}\\)\n예시2: \\(\\Omega =\\mathbb{R}\\) 일때 \\({\\cal F}=2^{\\mathbb{R}}\\) (르벡메져로 측정불가능함 \\(\\to\\) 비탈리집합, 모든 원소의 메져를 0으로 잡으면 무모순으로 길이를 정의할 수는 있겠으나 무슨의미?)\n\n\n위 예시 2에서 1. \\(\\Omega \\in 2^{\\mathbb{R}}\\), 2. \\(\\forall A \\in 2^{\\mathbb{R}} \\to A^c\\), 3. countable union A도 포함..\n\n- 예시2와 같은 \\({\\cal F}\\)는 고려하고 싶지 않음 \\(\\Rightarrow\\) \\(\\sigma({\\cal A})\\), 카라테오도리 확장정리의 고안.\n\nnote 귀찮아서 만든 이론1:\\(\\sigma({\\cal A})\\), 귀찮아서 만든 이론2: 카라테오도리 확장정리"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#유한측도finite-measure-시그마유한측도-finite-measure",
    "href": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#유한측도finite-measure-시그마유한측도-finite-measure",
    "title": "6wk: 측도론 (2)",
    "section": "유한측도(finite measure), 시그마유한측도(-finite measure)",
    "text": "유한측도(finite measure), 시그마유한측도(-finite measure)\n\n유한측도: 모든 measure값이 finite한 경우, 시그마유한측도는 좀더 까다로움\n\n- \\(m\\)이 잴 수 있는 공간 \\((\\Omega, {\\cal F})\\)에서의 측도라고 하자.\n\n\\(\\forall A \\in {\\cal F}\\), \\(m(A) < \\infty\\) 이면 \\(m\\)을 유한측도라고 한다.1\n\\(\\exists A_1,A_2,\\dots \\in {\\cal F}\\) such that (1) \\(\\cup_{i=1}^{\\infty}A_i = \\Omega\\) (2) \\(\\forall i \\in \\mathbb{N}:~ m(A_i)<\\infty\\) 이면 \\(m\\)을 시그마유한측도라고 한다.\n\n\n\\(m: \\cal F \\Rightarrow [0,\\infty]\\)\n\n\n\\(\\cal F\\) 안에 \\(\\Omega\\)가 들어있다. finite하면 \\(m(\\Omega)<\\infty\\)이면 \\(m(A)<\\infty\\)2일것 같다.\n\n- NOTE: 모든 확률측도는 유한측도이다. 모든 유한측도는 시그마유한측도이다.3\n\n유한측도가 더 강한 조건(시그마유한측도보다)\n확률측도라는 것은 매우 강한 조건임\n시그마유한측도라는 것은 확률측도보다 훨씬 약한 조건임\n즉, 확률측도>유한측도>시그마유한측도\n\n- 직관: 제 생각일 뿐이어요..\n\n세상엔 측도의 정의는 만족하지만 쓸모없는 측도가 있다. (모든 원소를 쟀더니 0이더라, 모든 원소를 쟀더니 무한대더라)\n그래서 모든 원소값에 무한대를 주는 측도는 인정하고 싶은 마음이 별로 없음. (하지만 측도의 정의는 만족)\n그래서 그냥 유한측도만 생각하기로 했는데…\n\n- 유한측도는 아니지만 시그마유한측도의 정의를 만족하는 경우 (엄청 중요해 보이는 예제들이 시그마유한측도잖아?)\n\n르벡메져\n카운팅메져: \\(m\\) is counting msr on \\((\\Omega, {\\cal F})\\) iff \\(m(A) = \\begin{cases} |A| & {\\tt if}~ A~{\\tt is~finite} \\\\ \\infty & {\\tt if}~A~{\\tt is~infinite}\\end{cases}\\)\n\n- 시그마유한측도의 느낌: 전체집합\\((\\Omega)\\)을 카운터블 유니온으로 커버\\((\\Omega = \\cup_{i=1}^\\infty A_i)\\)하는 메져유한인\\((m(A_i)<\\infty, \\forall i \\in \\mathbb{N})\\) 집합열이 1개만 있으면 된다.\n(기억해둘만한 예시)\n\\((\\mathbb{Z}, 2^{\\mathbb{Z}})\\) 를 잴 수 있는 공간이라고 하자. \\(m\\)을 공간 \\((\\mathbb{Z}, 2^{\\mathbb{Z}})\\)에서의 카운팅메져라고 하자.\n집합열1\n\n\\(A_1=\\mathbb{N}\\)\n\\(A_2=\\mathbb{N} \\cup \\{0\\}\\)\n\\(A_3=\\mathbb{N} \\cup \\{-1,0\\}\\)\n\\(\\dots\\)\n\n\n\\(A_1 \\cup A_2 \\cup \\dots = \\cup_{i=1}^\\infty A_i = \\mathbb{Z}\\)\n\n\n\\(m(A_1)=\\infty, m(A_2)=\\infty, m(A_3)=\\infty \\dots\\)\n\n집합열2\n\n\\(B_1=\\{0\\}\\)\n\\(B_2=\\{0,1\\}\\)\n\\(B_3=\\{-1,0,1\\}\\)\n\\(\\dots\\)\n\n\n\\(B_1 \\cup B_2 \\cup \\dots = \\cup_{i=1}^\\infty B_i = \\mathbb{Z}\\)\n\n\n\\(m(B_1)=1, m(B_2)=2, m(B_3)=3 \\dots \\forall i \\in \\mathbb{N}, m(B_i) < \\infty\\)\n\n집합열1와 집합열2는\n\n(1) \\(\\cup_{i=1}^{\\infty}A_i=\\mathbb{Z}\\), (2) \\(\\forall i \\in \\mathbb{N}:~ m(A_i)=\\infty\\)\n(1) \\(\\cup_{i=1}^{\\infty}B_i=\\mathbb{Z}\\), (2) \\(\\forall i \\in \\mathbb{N}:~ m(B_i)<\\infty\\)\n\n를 만족한다. 즉 집합열1은 전체집합\\((\\mathbb{Z})\\)을 카운터블 유니온으로 커버하지만\\((\\mathbb{Z}=\\cup_{i=1}^{\\infty}A_i)\\) 메져유한은 아니고\\((m(A_i) < \\infty\\) 를 만족하지 않음), 집합열2는 전제집합을 카운터블 유니온으로 커버하고 메져유한이다. 집합열2의 존재로 인하여 \\(m\\)은 \\((\\mathbb{Z}, 2^{\\mathbb{Z}})\\)에서의 시그마유한측도가 된다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#확률공간",
    "href": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#확률공간",
    "title": "6wk: 측도론 (2)",
    "section": "확률공간",
    "text": "확률공간\n- \\(P:{\\cal F} \\to [0,1]\\) 가 잴 수 있는 공간 \\((\\Omega, {\\cal F})\\) 에서의 확률측도라면, \\((\\Omega, {\\cal F}, P)\\) 를 확률공간이라 선언할 수 있다.\n- \\((\\Omega, {\\cal F})\\)가 잴수 있는 공간이라는 선언은 \\({\\cal F}\\)가 \\(\\Omega\\)에 대한 시그마필드라는 것이 내포되어 있다.\n- \\((\\Omega, {\\cal F}, P)\\)가 확률공간이라는 선언에는\n\n\\({\\cal F}\\)는 \\(\\Omega\\)에 대한 시그마필드이며,\n\\(P\\)는 \\((\\Omega, {\\cal F})\\)에서의 확률측도임이 내포되어 있다.\n\n- 교재의 언급 (p1) – 초록색부분\n\n\n\n그림1: 교재에 언급된 확률공간, 잴 수 있는 공간의 정의"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#시그마유한측도공간",
    "href": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#시그마유한측도공간",
    "title": "6wk: 측도론 (2)",
    "section": "시그마유한측도공간",
    "text": "시그마유한측도공간\n- \\(m:{\\cal F} \\to [0,\\infty]\\)이 잴 수 있는 공간 \\((\\Omega, {\\cal F})\\)에서의 시그마유한측도라면, \\((\\Omega, {\\cal F}, m)\\)을 시그마유한측도공간이라 부른다.\n- \\((\\Omega, {\\cal F}, m)\\)이 시그마유한측도공간이라는 선언에는\n\n\\({\\cal F}\\)는 \\(\\Omega\\)에 대한 시그마필드이며,\n\\(m\\)는 \\((\\Omega, {\\cal F})\\)에서의 시그마유한측도임이 내포되어 있다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#state",
    "href": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#state",
    "title": "6wk: 측도론 (2)",
    "section": "state",
    "text": "state\n- Thm (귀찮아서 만든 이론1): 모든 \\({\\cal A} \\subset 2^{\\Omega}\\) 에 대하여 smallest \\(\\sigma\\)-field containing \\({\\cal A}\\), 즉 \\(\\sigma({\\cal A})\\)는 존재한다.\n\n그리고 당연히 smallest 조건에 의에서 유일성이 보장됨"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#증명을-위한-준비학습",
    "href": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#증명을-위한-준비학습",
    "title": "6wk: 측도론 (2)",
    "section": "증명을 위한 준비학습",
    "text": "증명을 위한 준비학습\n- 이론: (\\(\\star\\)) 임의의 인덱스 집합 \\(I\\neq\\emptyset\\)를 고려하자. 여기에서 \\(I\\)는 uncountable set일 수도 있다. 아래의 사실이 성립함\n\n\\({\\cal F}_i\\)가 모두 시그마필드라면, \\(\\cap_{i \\in I}{\\cal F_i}\\) 역시 시그마필드이다.\n\n\n\\(\\cal F_1\\)이 \\(\\Omega\\)에 대한 \\(\\sigma\\)-field이고, \\(\\cal F_2\\)이 \\(\\Omega\\)에 대한 \\(\\sigma\\)-field이다.\n\n\n\\(\\cal F_1 \\cap \\cal F_2\\) 도 \\(\\Omega\\)에 대한 \\(\\sigma\\)-field\n\n(증명)\n편의상 \\({\\cal F}= \\cap_{i \\in I} {\\cal F}_i\\) 라고 하자. \\({\\cal F}\\)가 시그마필드임을 보이기 위해서는\n\n\\(A \\in {\\cal F} \\Rightarrow A^c \\in {\\cal F}\\)\n\\(A_1,A_2 \\dots \\in {\\cal F} \\Rightarrow \\cup_{i}A_i \\in {\\cal F}\\)\n\n만 보이면 된다. (이럴때는 전체집합 조건하나를 빼는게 유리하다)\n1번체크\n\\(A \\in {\\cal F} \\Rightarrow \\forall i: A \\in {\\cal F}_i \\Rightarrow \\forall i: A^c \\in {\\cal F}_i \\Rightarrow A^c \\in {\\cal F}\\)\n2번체크\n\\(A_1,A_2,\\dots \\in {\\cal F} \\Rightarrow \\forall i: A_1,A_2,\\dots \\in {\\cal F}_i \\Rightarrow \\forall i: \\cup_jA_j \\in {\\cal F}_i \\Rightarrow \\cup_jA_j \\in {\\cal F}\\)"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#증명",
    "href": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#증명",
    "title": "6wk: 측도론 (2)",
    "section": "증명",
    "text": "증명\n- Thm (귀찮아서 만든 이론1): 모든 \\({\\cal A} \\subset 2^{\\Omega}\\) 에 대하여 smallest \\(\\sigma\\)-field containing \\({\\cal A}\\), 즉 \\(\\sigma({\\cal A})\\)는 존재한다.\n\n그리고 당연히 smallest 조건에 의해 유일성이 보장됨\n\n(증명)\n\\({\\cal A}\\)를 포함하는 모든 시그마필드를 구하고 그걸 교집합하여 결과를 \\({\\cal F}\\)라고 하자. 아래의 사실은 자명하게 성립한다.\n\n시그마필드의 교집합은 시그마필드이므로 \\({\\cal F}\\)는 시그마필드이다.\n교집합을 하면 할수록 집합은 작아지므로 \\({\\cal F}\\)는 위에서 구한 시그마필드중에서 가장 작다.\n\\({\\cal F}\\)는 \\({\\cal A}\\)를 포함한다.\n\n따라서 \\({\\cal F}\\)는 (\\({\\cal A}\\)를 포함하는 모든 시그마필드를 교집합하여 얻은 집합) \\({\\cal A}\\)를 포함하는 가장 작은 시그마필드가 된다.\n- 아래는 교재의 언급 (p3)\n\n\n\n그림2: Durret교재에서 언급된 “귀찮아서 만든 이론1”"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#state-1",
    "href": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#state-1",
    "title": "6wk: 측도론 (2)",
    "section": "state",
    "text": "state\n- Thm: 딘킨의 \\(\\pi-\\lambda\\) 정리 ver1. (\\(\\star\\))\n\\({\\cal P}\\)가 파이시스템이면 \\(l({\\cal P})=\\sigma({\\cal P})\\)이다.\n\n\\(\\sigma({\\cal P})\\):P를 포함하는 가장 smallest한 \\(\\sigma\\)-field\n\n\n\\(l({\\cal P})\\):P를 포한하는 가장 작은 \\(\\lambda\\)-system"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#증명을-위한-준비학습-1",
    "href": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#증명을-위한-준비학습-1",
    "title": "6wk: 측도론 (2)",
    "section": "증명을 위한 준비학습",
    "text": "증명을 위한 준비학습\n- 이론: 임의의 인덱스 집합 \\(I\\neq\\emptyset\\)를 고려하자. 여기에서 \\(I\\)는 uncountable set일 수도 있다. 아래의 사실이 성립한다.\n\n\\({\\cal F}_i\\)가 모두 시그마필드라면, \\(\\cap_{i \\in I}{\\cal F_i}\\) 역시 시그마필드이다.\n\\({\\cal A}_i\\)가 모두 시그마링, \\(\\cap_{i \\in I}{\\cal A_i}\\) 역시 시그마링이다.\n\\({\\cal A}_i\\)가 모두 알지브라라면, \\(\\cap_{i \\in I}{\\cal A_i}\\) 역시 알지브라이다.\n\\({\\cal A}_i\\)가 모두 링이라면, \\(\\cap_{i \\in I}{\\cal A_i}\\) 역시 링이다.\n\\({\\cal A}_i\\)가 모두 람다시스템이라면, \\(\\cap_{i \\in I}{\\cal A_i}\\) 역시 람다시스템이다.\n\n\n세미알지브라, 세미링, 파이시스템은 성립안함.\n\n- 예제1: 아래를 고려하자.\n\n\\(\\Omega = \\{1,2,3,4\\}\\)\n\\({\\cal A}_1 = \\{\\emptyset, \\{1\\}, \\{2,3\\}, \\{4\\}, \\Omega\\}\\)\n\\({\\cal A}_2 = \\{\\emptyset, \\{1\\}, \\{2\\}, \\{3,4\\}, \\Omega\\}\\)\n\n\\({\\cal A}_1, {\\cal A}_2\\)는 모두 세미알지브라이다. 하지만 \\({\\cal A}_1 \\cap {\\cal A}_2 = \\{\\emptyset, \\Omega, \\{1\\}\\}\\)은 세미알지브라가 아니다. {2,3,4}에 partition이 없음\n\n이 예제에서 세미알지브라를 세미링으로 바꾸고 읽어도 성립함.\n\n- 예제2: 아래를 고려하자.\n\n\\(\\Omega=\\{H,T\\}\\)\n\\({\\cal A}_1 = \\{\\{H\\}\\}\\)\n\\({\\cal A}_2 = \\{\\{T\\}\\}\\)\n\n\\({\\cal A}_1, {\\cal A}_2\\)는 모두 파이시스템이다. 하지만 \\({\\cal A}_1 \\cap {\\cal A}_2 = \\emptyset\\)은 파이시스템이 아니다.\n- 이론: 임의의 \\({\\cal A}\\)에 대하여 아래는 존재한다.\n\n\\({\\cal A}\\)를 포함하는 가장 작은 시그마필드, \\(\\sigma({\\cal A})\\)\n\\({\\cal A}\\)를 포함하는 가장 작은 시그마링\n\\({\\cal A}\\)를 포함하는 가장 작은 알지브라\n\\({\\cal A}\\)를 포함하는 가장 작은 링\n\\({\\cal A}\\)를 포함하는 가장 작은 람다시스템, \\(l({\\cal A})\\)\n\n- 참고: “\\({\\cal A}\\)를 포함하는 가장 작은 세미링”, 혹은 “\\({\\cal A}\\)를 포함하는 가장 작은 세미알지브라”와 같은 것은 존재하지 않음.\n- 예제3: 아래를 고려하자.\n\n\\(\\Omega = \\{1,2,3,4\\}\\)\n\\({\\cal A} = \\{\\emptyset, \\Omega, \\{1\\}\\}\\)\n\n이때 \\({\\cal A}\\)를 포함하는 가장 작은 세미알지브라가\n\\[{\\cal A}_1 = \\{\\emptyset, \\Omega, \\{1\\}, \\{2,3,4\\}\\}\\]\n라고 주장할 수는 없음. 왜냐하면\n\\[{\\cal A}_2 = \\{\\emptyset, \\Omega, \\{1\\}, \\{2\\},\\{3\\},\\{4\\}\\}\\]\n역시 \\({\\cal A}\\)를 포함하는 세미알지브라이지만 \\({\\cal A}_1 \\not \\subset {\\cal A}_2\\)이므로.\n- 이론: \\({\\cal P}\\)가 파이시스템이라고 하자. 아래가 성립한다.\n\n\\({\\cal P}\\)를 포함하는 가장 작은 시그마필드는 그 자체로 파이시스템이다. (즉 \\(\\sigma({\\cal P})\\)는 파이시스템이다)\n\\({\\cal P}\\)를 포함하는 가장 작은 시그마링은 그 자체로 파이시스템이다.\n\\({\\cal P}\\)를 포함하는 가장 작은 알지브라는 그 자체로 파이시스템이다.\n\\({\\cal P}\\)를 포함하는 가장 작은 링은 그 자체로 파이시스템이다.\n\\({\\cal P}\\)를 포함하는 가장 작은 람다시스템은 그 자체로 파이시스템이다?? (즉 \\(l({\\cal P})\\)는 파이시스템이다?)\n\n- 1-4는 자명한데, 5는 자명하지 않다. 하지만 성립한다. (5의 증명은 복잡함. 그냥 암기하자.)\n- 이론: \\({\\cal A}\\)가 람다시스템이다. \\(\\Rightarrow\\) (\\({\\cal A}\\)는 시그마필드이다. \\(\\Leftrightarrow\\) \\({\\cal A}\\)는 파이시스템이다.)\n(증명) 아래의 표를 살펴보면 간단하게 증명가능하다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A \\cap B\\)\n\\(\\emptyset\\)\n\\(A-B\\)\n\\(\\cup_iA_i=\\uplus_i B_i\\)\n\\(\\Omega\\)\n\\(A^c\\)\n\\(A\\cup B\\)\n\\(\\cup_{i=1}^{\\infty}A_i\\)\n\\(\\uplus_{i=1}^{\\infty}B_i\\)\n\\(\\cap_{i=1}^{\\infty}A_i\\)\n\n\n\n\n\\(\\pi\\)-system\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\n\n\\(\\lambda\\)-system\n\\(X\\)\n\\(O\\)\n\\(\\Delta'\\)\n\\(X\\)\n\\(O\\)\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(O\\)\n\\(X\\)\n\n\n\\(\\sigma\\)-field\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\n\n\n\n\\(\\lambda\\)-system의 \\(A \\subset B\\)이면 \\(A-B\\)에 닫혀있다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#증명-1",
    "href": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#증명-1",
    "title": "6wk: 측도론 (2)",
    "section": "증명",
    "text": "증명\n(증명)\n\\(l(\\cal P) \\subset \\sigma({\\cal P})\\) 임을 보이고, \\(l(\\cal P) \\supset \\sigma({\\cal P})\\) 임을 보이면된다.\n“\\(\\subset\\)”: 당연하다.4\n\n3주차 예제12 참고\n\n“\\(\\supset\\)”: \\(l({\\cal P})\\)가 시그마필드임을 보이면 자동으로 \\(l({\\cal P}) \\supset \\sigma({\\cal P})\\)임이 보여진다. \\(\\sigma({\\cal P})\\)가 가장작은 \\(\\sigma\\)-field이니까\n\\(l({\\cal P})\\)이 시그마필드임은 아래를 조합하면 간단히 증명된다.\n\n파이시스템 \\({\\cal P}\\)를 포함하는 가장 작은 람다시스템 \\(l({\\cal P})\\)은 그 자체로 파이시스템이다.\n\\({\\cal A}\\)가 람다시스템이다. \\(\\Rightarrow\\) (\\({\\cal A}\\)는 시그마필드이다. \\(\\Leftrightarrow\\) \\({\\cal A}\\)는 파이시스템이다.)\n\n- 생각의 시간\n\n시그마필드(=잴 수 있는 집합의 모임)을 만들기 위해서는, 그 모임(=collection)이 파이시스템이면서 동시에 람다시스템임을 보이면 된다.\n딘킨의 정리는 적당한 파이시스템을 만들고 그것을 통하여 잴 수 있는 집합의 모임을 확률의 공리에 맞게만 설정한다면, 그것이 시그마필드가 된다는 것을 보이는 것이다.\n\n- 제 생각\n\n메져가 “선분의 길이”를 일반화 하는 개념이라 생각한다면 파이시스템에서 시작하여 시그마필드로 확장하는 것이 자연스럽다.\n메져가 “확률”을 일반화하는 개념이라 생각한다면 람다시스템에서 시작하는게 자연스럽다.5\n딘킨의 \\(\\pi-\\lambda\\) 정리는 두 흐름을 합치는 정리이다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#딘킨의-pi-lambda-정리-ver2.",
    "href": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#딘킨의-pi-lambda-정리-ver2.",
    "title": "6wk: 측도론 (2)",
    "section": "딘킨의 \\(\\pi-\\lambda\\) 정리 ver2.",
    "text": "딘킨의 \\(\\pi-\\lambda\\) 정리 ver2.\n- 이론: 딘킨의 \\(\\pi-\\lambda\\) 정리 ver2.\n\\({\\cal P}\\)가 파이시스템이고 \\({\\cal L}\\)이 \\({\\cal P}\\)를 포함하는 람다시스템이라면 \\(\\sigma({\\cal P}) \\subset {\\cal L}\\)이다.\n(설명)\nDurret에 나온 딘킨의 \\(\\pi-\\lambda\\) thm 이다. 굉장히 불친절한 편인데, ver2가 증명되면 ver1은 자명하게6 임플라이 되므로 ver2를 대신 state한 것이다.\n\nver2가 ver1를 임플라이 하는 이유: ver1의 \\(l({\\cal P}) \\subset \\sigma({\\cal P})\\)은 당연하고 \\(l({\\cal P}) \\supset \\sigma({\\cal P})\\)만 보이면 되는데, 이미 \\(\\sigma({\\cal P}) \\subset {\\cal L}\\)임을 보였으므로 \\(l({\\cal P})\\)의 정의에 의하여 \\({\\cal L} \\supset l({\\cal P}) \\supset \\sigma({\\cal P})\\)이 성립한다.\n\n- 교재의 언급 (p 456)\n\n\n\n그림2: 교재에 언급된 딘킨의 정리, 부록에 있음\n\n\n\\({\\cal L}\\):P를 포함하는 람다시스템\n\\(l({\\cal P})\\):P를 포함하는 가장 작은 람다시스템"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#state-2",
    "href": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#state-2",
    "title": "6wk: 측도론 (2)",
    "section": "state",
    "text": "state\n- 귀찮아서 만든 이론2: 운이 좋다면(특정한 조건 하에서), \\({\\cal A}\\) 에서 확률의 공리를 만족하는 적당한 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)를 \\((\\Omega, \\sigma({\\cal A}))\\) 에서의 확률측도 \\(P\\)로 업그레이드 할 수 있으며 업그레이드 결과는 유일하다.\n- 귀찮아서 만든 이론2는 (1) 업그레이드가 가능하냐 (2) 그 업그레이드가 유일하냐 를 따져야하는데 이중 유일성만을 따져보자.\n- Thm: \\((\\Omega, \\sigma({\\cal A}), P)\\)를 확률공간이라고 하자. 여기에서 \\({\\cal A}\\)는 파이시스템이라고 가정하자. 그렇다면 확률측도 \\(P:\\sigma({\\cal A}) \\to [0,1]\\)의 값은 \\(P: {\\cal A} \\to [0,1]\\)의 값에 의하여 유일하게 결정된다.\n\n\\({\\cal A}\\)가 파이시스템이라면, \\({\\cal A}\\)에서는 agree하지만 \\(\\sigma({\\cal A})\\)에서는 agree하지 않는 확률측도 \\(P:\\sigma({\\cal A}) \\to [0,1]\\)는 존재할 수 없다는 의미이다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#활용예제-star",
    "href": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#활용예제-star",
    "title": "6wk: 측도론 (2)",
    "section": "활용예제 (\\(\\star\\))",
    "text": "활용예제 (\\(\\star\\))\n- 아래의 이론을 이해하기 위한 예제들을 살펴보자.\n\n이론: \\((\\Omega, \\sigma({\\cal A}), P)\\)를 확률공간이라고 하자. 여기에서 \\({\\cal A}\\)는 파이시스템이라고 가정하자. 그렇다면 확률측도 \\(P:\\sigma({\\cal A}) \\to [0,1]\\)의 값은 \\(P: {\\cal A} \\to [0,1]\\)의 값에 의하여 유일하게 결정된다.\n\n(예제1) – 4주차에서 했던 예제에요\n- \\(\\Omega=\\{1,2,3,4\\}\\)이라고 하고 \\({\\cal A} = \\{\\emptyset, \\{1\\},\\{2\\},\\{3,4\\},\\Omega\\}\\) 라고 하자.\n- \\({\\cal A}\\)는 파이시스템이다.\n- 아래표의 왼쪽의 \\(P\\)와 같은 확률 측도를 고려하자.\n\n\n\n\n\n\n\n\n\n\\(P\\)\n\\(P'\\)\n\n\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{1\\}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\\(\\{2\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{3,4\\}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\{1,2\\}\\)\n\\(\\frac{3}{4}\\)\n\\(\\frac{3}{4}\\) 이 아닐 수 있어? ->아니 없어! 유일해야하니까\n\n\n\\(\\{1,3,4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\) 이 아닐 수 있어?\n\n\n\\(\\{2,3,4\\}\\)\n\\(\\frac{3}{4}\\)\n\\(\\frac{3}{4}\\) 이 아닐 수 있어?\n\n\n\n\\({\\cal A}\\)에서는 \\(P\\)와 그 값이 같지만 \\(\\sigma({\\cal A})-{\\cal A}\\)에서는 다른값을 가질 수도 있는 \\((\\Omega, \\sigma({\\cal A}))\\) 에서의 확률측도 \\(P'\\)는 존재하지 않는다.\n즉 \\({\\cal A}\\)가 파이시스템이라면, \\((\\Omega,\\sigma({\\cal A}))\\)에의 모든 확률측도 \\(P\\)는 \\({\\cal A}\\)에서의 값만 define하면 나머지 \\(\\sigma({\\cal A})-{\\cal A}\\)에서의 값은 유니크하게 결정된다.\n- 이 이론에 대한 짧은 생각\n\n생각1: 일단 \\((\\Omega,\\sigma({\\cal A})\\)에서의 확률측도 \\(P\\)의 존재성은 가정하고 들어간다. 즉 “존재한다면 유일하다”는 의미이지, “유일하게 존재한다”의 의미는 아니다.\n생각2: 따라서 이 정리는 “\\({\\cal A}\\)가 파이시스템일 경우, 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)가 \\((\\Omega,\\sigma({\\cal A}))\\)에서의 확률측도 \\(P\\)로 업그레이드가 가능하다면 그 결과는 유일하다” 정도로 해석할 수 있다.\n\n(예제2) – 이것도 4주차에서 했던 예제입니다.\n- \\(\\Omega=\\{1,2,3,4\\}\\) 이라고 하고 \\({\\cal A} = \\{\\emptyset, \\{1,2\\},\\{2,3\\}, \\Omega\\}\\) 라고 하자.\n- 여기에서 \\({\\cal A}\\)는 파이시스템이 아니다. 따라서 \\({\\cal A}\\)에서의 값은 agree하지만 \\((\\Omega, \\sigma({\\cal A}))\\)에서 agree하지 않는 서로 다른 확률측도가 존재할 수 있다.\n\n\n\n\n\\(P_1\\)\n\\(P_2\\)\n\n\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{1,2\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{2,3\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\{1\\}\\)\n\\(0\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{2\\}\\)\n\\(\\frac{1}{2}\\)\n\\(0\\)\n\n\n\\(\\{3\\}\\)\n\\(0\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(0\\)\n\n\n\\(\\{1,3\\}\\)\n\\(0\\)\n\\(1\\)\n\n\n\\(\\{1,4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{2,4\\}\\)\n\\(1\\)\n\\(0\\)\n\n\n\\(\\{3,4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{2,3,4\\}\\)\n\\(1\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{1,3,4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(1\\)\n\n\n\\(\\{1,2,4\\}\\)\n\\(1\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{1,2,3\\}\\)\n\\(\\frac{1}{2}\\)\n\\(1\\)\n\n\n\n- 만약에 이 예제에서 \\({\\cal A}\\)를 아래와 같이 수정한다면\n\\[{\\cal A}=\\{\\emptyset, \\{1,2\\}, \\{2,3\\}, \\{2\\}\\}\\]\n이번에는 \\({\\cal A}\\)는 파이시스템이 된다. 따라서 이 경우 \\((\\Omega, \\sigma({\\cal A}))\\)에서의 모든 확률측도 \\(P\\)는 \\({\\cal A}\\)의 값에 의하여 유일하게 결정된다.\n(예제3)\n- \\(\\Omega=\\{H,T\\}\\) 이라고 하고 \\({\\cal A} = \\{\\{H\\}\\}\\) 라고 하자.\n- 여기에서 \\({\\cal A}\\)은 파이시스템이므로 가측공간 \\((\\Omega,\\sigma({\\cal A}))\\)에서 정의가능한 모든 확률측도 \\(P\\)는 \\({\\cal A}\\)에서의 값으로만 정의해도 무방하다.7\n(예제4) – 통계학과라서 행복해\n- \\(\\Omega=\\{a,b\\}\\) 이라고 하고 \\({\\cal A} = \\{\\{a\\}\\}\\) 라고 하자.\n- 여기에서 \\({\\cal A}\\)은 파이시스템이다.\n- 가측공간 \\((\\Omega,\\sigma({\\cal A}))\\)에서 정의가능한 모든 확률측도 \\(P\\)는 \\({\\cal A}\\)에서의 값으로 유일하게 결정된다.\n- 그렇지만 가측공간 \\((\\Omega,\\sigma({\\cal A})\\)에서 정의가능한 “측도” \\(m\\)은 \\({\\cal A}\\)에서의 값으로 유일하게 결정되지 않는다.8\n\n\n\n\n\\(m_1\\)\n\\(m_2\\)\n\n\n\n\n\\(\\{a\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{b\\}\\)\n\\(\\frac{1}{2}\\)\n\\(1\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(\\frac{3}{2}\\)"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#증명-2",
    "href": "posts/Advanved Probability Theory/2023-04-11-6wk-checkpoint.html#증명-2",
    "title": "6wk: 측도론 (2)",
    "section": "증명",
    "text": "증명\n- 아래의 이론에 대한 증명\n\nThm: \\((\\Omega, \\sigma({\\cal A}), P)\\)를 확률공간이라고 하자. 여기에서 \\({\\cal A}\\)는 파이시스템이라고 가정하자. 그렇다면 확률측도 \\(P:\\sigma({\\cal A}) \\to [0,1]\\)의 값은 \\(P: {\\cal A} \\to [0,1]\\)의 값에 의하여 유일하게 결정된다.\n\n증명 (확률측도라는 가정을 추가하여 교재의 버전을 살짝 쉽게 만듬)\nLET\n\n\\((\\Omega, \\sigma({\\cal A}))\\) 는 잴 수 있는 공간임.\n\\(P_1,P_2\\)는 \\((\\Omega, \\sigma({\\cal A}))\\)에서의 확률측도임.\n\\(P_1,P_2\\)는 “\\(\\forall A \\in {\\cal A}: P_1(A)=P_2(A)\\)”를 만족함.\n\n전략: 잴 수 있는 공간 \\((\\Omega, \\sigma({\\cal A}))\\) 에서의 두 측도 \\(P_1\\), \\(P_2\\)가 \\({\\cal A}\\)에서는 일치하지만 \\(\\sigma({\\cal A})-{\\cal A}\\)에서는 일치하지 않는 경우를 찾으려 해보고, 그것이 불가능함을 보이자.\nLET: \\(\\tilde{\\cal D}=\\{B \\in \\sigma({\\cal A}): P_1(B) \\neq P_2(B)\\}\\)\nISTST: \\(\\tilde{\\cal D} =\\emptyset\\)\nISTST: \\({\\cal D} = \\{B \\in \\sigma({\\cal A}): P_1(B) = P_2(B) \\} = \\sigma({\\cal A})\\)\nISTST: (1) \\({\\cal D} \\subset \\sigma({\\cal A})\\) (2) \\({\\cal D} \\supset \\sigma({\\cal A})\\)\n(1)은 당연히 만족\nISTST: \\({\\cal D} \\supset \\sigma({\\cal A})\\)\nNOTE: IF (1) \\({\\cal D}\\) is containing \\({\\cal A}\\) (2) \\({\\cal D}\\) is \\(\\lambda\\)-system, THEN we can say \\({\\cal D} \\supset \\sigma({\\cal A})=l({\\cal A})\\)\nISTST: (1) \\({\\cal A} \\subset {\\cal D}\\) (2) \\({\\cal D}\\) is \\(\\lambda\\)-system.\nISTST: 1. \\(\\Omega \\in {\\cal D}\\) 2. \\(A,B \\in {\\cal D}, A\\subset B\\) \\(\\Rightarrow\\) \\(B-A \\in {\\cal D}\\) 3. \\(\\forall B_1,B_2,\\dots, \\in {\\cal D}\\), \\(\\uplus_{i=1}^{\\infty} B_i \\in {\\cal D}\\)\nCHECK 1: \\(P_1(\\Omega) = P_2(\\Omega)\\)\nCHECK 2: \\(P_1(B-A) = P_1(B)-P_1(A) = P_2(B) - P_2(A) = P_2(B-A)\\)\nCHECK 3: \\(P_1(\\uplus_{i=1}^{\\infty} B_i)=P_1(B_1)+P_1(B_2)\\dots = P_2(B_1)+P_2(B_2) +\\dots = P_2(\\uplus_{i=1}^\\infty B_i)\\)\n- 보충노트\n\nsupp_6wk.pdf"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_03_14_2wk_checkpoint.html",
    "href": "posts/Advanved Probability Theory/2023_03_14_2wk_checkpoint.html",
    "title": "2wk: 측도론 intro (2)",
    "section": "",
    "text": "해당 강의노트는 전북대학교 최규빈교수님 AP2023 자료임\n\n\n예비개념1: 귀류법\n- 귀류법: 니 논리 대로면… <- 인터넷 댓글에 많음..\n님 논리대로면..\n- XXX가 문제 없으면 서울 전체가 문제가 없고 (애초에 서울은 문제도 아니라는데 왜 이소리는 하고 계신지 모르겠지만)\n- 수도권 모 대학이 문제가 없으면 전체가 문제가 없겠네요?\n- 지방도 1개 대학이 문제가 없으니 전체가 문제 없겠네요?\n와우! 모든 문제가 해결되었습니다! 출산율 감소로 인한 한국대학의 위기가 해결되었.. 아니 애초에 위기가 없었군요!.\n어휴.. ㅠㅠ\nref: 하이브레인넷\n\n\n예비개념2: 일반화\n- 연필의 정의: 필기도구의 하나. 흑연과 점토의 혼합물을 구워 만든 가느다란 심을 속에 넣고, 겉은 나무로 둘러싸서 만든다. 1565년에 영국에서 처음으로 만들었다.\n- 질문: 아래는 연필인가?\n\n\n연필을 정의하는 ’속성’을 개념으로 확장하자! (기본 연필의 정의를 포함하면서) \\(\\to\\) cardinality\n\n\n\ncardinality\n\nref: https://en.wikipedia.org/wiki/Cardinality\n\n- \\(A=\\{2,4,6\\}\\) \\(\\Rightarrow\\) \\(|A|=3\\), \\(A\\) has a cardinality of 3.\n- \\(A=\\{1,2,3,4,\\dots\\}=\\mathbb{N}\\) \\(\\Rightarrow\\) \\(|A|=?\\)\n\nCardinal number: 유한집합에서의 “갯수”라는 개념을 좀 더 일반화 하여 무한집합으로 적용하고 싶다.\n유한집합: 우리가 친숙한 size 와 그 뜻이 같음\n무한집합: 무한집합의 경우는 그 동작원리가 조금 더 복잡함\n\n- 질문: \\(|\\mathbb{Q}| < |\\mathbb{Q}^c|\\) ??\nBijection, injection and surjection (예비학습)\n\nref: https://en.wikipedia.org/wiki/Bijection,_injection_and_surjection\n\n\n- 용어 정리\n\nsurjective = onto = 전사 = 위로의 함수\ninjective = one-to-one = 단사 = 일대일 함수\nbijective = one-to-one and onto, one-to-one correspondence = 전단사 = 일대일 대응\n\n- 따지는 방법:\n\n단사: 함수 \\(f\\)는 \\(X\\)에서 \\(Y\\)로 향하는 단사함수이다. \\(\\Leftrightarrow\\) \\(\\forall x_1,x_2 \\in X\\): \\(x_1\\neq x_2 \\Rightarrow f(x_1)\\neq f(x_2)\\)\n(\\(\\star\\))전사: 함수 \\(f\\)는 \\(X\\)에서 \\(Y\\)로 향하는 전사함수이다. \\(\\Leftrightarrow\\) \\(\\forall y \\in Y ~\\exists x \\in X\\) such that \\(f(x)=y\\).\n\n- 성질1: 어떤함수가 전사함수 & 단사함수 \\(\\Rightarrow\\) 전단사함수\n- 성질2:\n\n집합 \\(X\\)에서 집합 \\(Y\\)로 가는 단사함수 \\(f\\)가 존재한다. \\(\\Rightarrow\\) \\(|X| \\leq |Y|\\)\n집합 \\(X\\)에서 집합 \\(Y\\)로 가는 전사함수 \\(f\\)가 존재한다. \\(\\Rightarrow\\) \\(|X| \\geq |Y|\\)\n\n(예비학습 끝)\n- 성질1~2로 유추하면 아래와 같은 사실을 주장 할 수 있지 않을까?\n\n집합 \\(X\\)에서 집합 \\(Y\\)로 향하는 전단사함수가 존재한다 \\(\\Rightarrow\\) \\(|X|=|Y|\\)\n\n- 그렇다면 우리가 주장하고 싶은 것은 아래와 같이 된다.\n\n유리수집합의 무리수집합의 cardinality는 다르다.\n유리수집합과 무리수집합사이의 전단사함수는 존재할 수 없다.\n\n\n\n유리수집합의 카디널리티\n- 우리가 궁극적으로 궁금한 것\n\n유리수집합과 무리수집합의 카디널리티는 다를까?\n\n- 그냥 궁금한 것\n\n양의 정수의 집합, 음의 정수의 집합, 정수의 집합, 짝수의 집합, 홀수의 집합의 카디널리티는 어떠할까?\n미리보는 답 ㅎ 카디너릴티는 다 똑같다 ㅎ\n\n- (예제1)\n집합 \\(X=\\{1,2,3\\}\\), \\(Y=\\{2,4,6\\}\\)을 생각하자. 적당한 함수 \\(f\\)를 아래와 같이 정의하자.\n\n\\(f(1)=2\\)\n\\(f(2)=4\\)\n\\(f(3)=6\\)\n\n아래의 질문에 대답해보자.\n\n(단사) 함수 \\(f\\)는 정의역의 모든 값에 대해 함수값이 모두 다른가? // \\(\\forall x_1,x_2 \\in X\\), \\(x_1\\neq x_2\\) \\(\\Rightarrow\\) \\(f(x_1)\\neq f(x_2)\\)?\n(전사) 함수 \\(f\\)는 공역=치역인가? // \\(\\forall y \\in Y~ \\exists x \\in X\\) such that \\(f(x)=y\\).\n\n1의 질문과 2의 질문이 모두 맞으므로 함수 \\(f\\)는 전단사 함수이다. 집합 \\(X\\)에서 집합 \\(Y\\)로 가는 전단사 함수가 존재하므로 집합 \\(X\\)와 집합 \\(Y\\)의 카디널리티는 동일하다. 즉 위의 예제1에서 \\(Y\\)의 카디널리티는 3이다!\n- (예제2)\n집합 \\(X=\\{1,2,3,\\dots \\}\\), \\(Y=\\{2,4,6,\\dots \\}\\)을 생각하자. 적당한 함수 \\(f\\)를 아래와 같이 정의하자.\n\n\\(f(1)=2\\)\n\\(f(2)=4\\)\n\\(f(3)=6\\)\n\\(\\dots\\)\n\n아래의 질문에 대답해보자.\n\n(단사) 함수 \\(f\\)는 정의역의 모든 값에 대해 함수값이 모두 다른가? // \\(\\forall x_1,x_2 \\in X\\), \\(x_1\\neq x_2\\) \\(\\Rightarrow\\) \\(f(x_1)\\neq f(x_2)\\)?\n(전사) 함수 \\(f\\)는 공역=치역인가? // \\(\\forall y \\in Y~ \\exists x \\in X\\) such that \\(f(x)=y\\).\n\n1의 질문과 2의 질문이 모두 맞으므로 함수 \\(f\\)는 전단사함수이다. 집합 \\(X\\)에서 집합 \\(Y\\)로 가는 전단사 함수가 존재하므로 집합 \\(X\\)와 집합 \\(Y\\)의 카디널리티는 동일하다.\n- \\(\\aleph_0\\) (알레프 널, 혹은 알레프 제로라고 읽음)\n\n자연수집합 \\(\\mathbb{N}\\)의 카디널리티는 \\(\\aleph_0\\)이다. 즉 \\(|\\mathbb{N}|=\\aleph_0\\).\n짝수인 자연수 집합의 카디널리티는 \\(\\aleph_0\\)이고, 홀수인 자연수 집합의 카디널리티는 \\(\\aleph_0\\)이다.\n정수집합 \\(\\mathbb{Z}\\)의 카디널리티는 \\(\\aleph_0\\)이다. 즉 \\(|\\mathbb{Z}|=\\aleph_0\\).\n\n- 느낌: \\(\\aleph_0\\)를 2배,3배,4배 하여도 \\(\\aleph_0\\)이다.\n\n즉 무한집합의 경우, 본인과 카디널넘버가 같은 진부분집합(자기자신을 제외한 부분집합)이 존재할 수 있다. (유한집합에서는 불가능하겠지)\n무한집합의 정의: 집합 \\(A\\)가 무한집합이다. \\(\\Leftrightarrow\\) \\(A\\)와 동일한 카디널리티를 가지는 \\(A\\)의 진 부분집합이 존재한다.\n\n- (예제3)\n원소의 수가 \\(n\\)인 임의의 유한집합 \\(A\\)에 대하여 \\(|A|=n\\) 이다.\n- (예제4)\n유리수집합의 카디널리티는 얼마인가? (https://en.wikipedia.org/wiki/Rational_number)\n집합 \\(X\\)를 자연수의 집합이라고 하자. 집합 \\(Y\\)를 아래그림에 있는 숫자들의 집합이라고 하자.1\n\n예를들어 집합 \\(X\\)와 집합 \\(Y\\)를 앞의 몇개만 써보면\n\n\\(X=\\{1,2,3,4,5,6,\\dots\\}\\)\n\\(Y=\\{1,\\frac{2}{1},\\frac{1}{2},\\frac{3}{1},\\frac{2}{2},\\frac{1}{3},\\dots \\}\\)\n\n함수 \\(f\\)를 아래와 같이 정의하자.\n\n\\(f(1)=1\\)\n\\(f(2)=2/1\\)\n\\(f(3)=1/2\\)\n\\(f(4)=3/1\\)\n\\(f(5)=2/2\\)\n\\(f(6)=1/3\\)\n\\(\\dots\\)\n\n함수 \\(f\\)는 \\(X\\)에서 \\(Y\\)로 향하는 전단사함수이다. \\(\\Rightarrow\\) \\(|X|=\\aleph_0=|Y|\\)\n(관찰) 임의의 양의 유리수의 집합 \\(\\mathbb{Q}^+\\)는 모두 \\(Y\\)에 포함되어 있다. \\(\\Rightarrow\\) \\(X \\subset \\mathbb{Q}^+ \\subset Y\\) \\(\\Rightarrow\\) \\(|\\mathbb{Q}^+|=\\aleph_0\\)\n(생각) 그럼 음의 유리수의 집합 \\(\\mathbb{Q}^-\\)의 카디널넘버 역시 \\(\\aleph_0\\)이다. 즉 \\(|\\mathbb{Q}^-|=\\aleph_0\\).\n(결론) 그럼 유리수의 카디널넘버는 \\(\\aleph_0\\) 이다. [\\(\\mathbb{Q} = \\mathbb{Q}^+ \\cup \\{0\\} \\cup \\mathbb{Q}^-\\)] 좀 더 자극적으로 말하면 “자연수의 갯수와 유리수의 갯수는 같다” 라고 말할 수 있다.\n- 조금 무식하게 쓰면 아래와 같이 쓸 수 있다.\n\n\\(\\aleph_0 + 1 = \\aleph_0\\)\n\\(\\aleph_0 \\times 2 = \\aleph_0\\)\n\\(\\aleph_0 \\times \\aleph_0 = \\aleph_0^2 = \\aleph_0\\) (위의 격자를 생각..)\n\n\n\n실수집합의 카디널리티\n- 아래의 관계가 성립했다.\n\n\\(|\\mathbb{N}| = \\aleph_0\\)\n\\(|\\mathbb{N}\\cup \\{0\\}| = \\aleph_0\\)\n\\(|\\mathbb{Z}| = \\aleph_0\\)\n\\(|\\mathbb{Q}| = \\aleph_0\\)\n\n- 그렇다면 아래는 어떠할까?\n\\[|\\mathbb{R}|=??\\]\n(주장) 실수에 포함된 카디널넘버는 유리수의 카디널넘버 보다 크다.\n\n\\(\\mathbb{Q}\\)에서 \\(\\mathbb{R}\\)로 가는 단사함수는 존재하지만 전사함수는 존재할 수 없음을 보이면 된다.\n\\(\\mathbb{N}\\)에서 \\(\\mathbb{R}\\)로 가는 단사함수는 존재하지만 전사함수는 존재할 수 없음을 보여도 된다.[\\(|\\mathbb{Q}|=|\\mathbb{N}|=\\aleph_0\\)]\n\n(단사)\n자연수에서 실수로 가는 단사함수는 존재한다. (자연수는 실수의 부분집합이니까)\n(전사)\n소망: \\(\\mathbb{N}\\)에서 \\(\\mathbb{R}\\)로 향하는 전사는 존재할 수 없음을 보이고 싶음.\n소망2: 그런데 \\(\\mathbb{N}\\)에서 \\([0,1]\\)로 향하는 전사가 존재할 수 없음을 보여도 충분함.\n\\([0,1] \\subset \\mathbb{R}\\) 이므로\n전략: \\(\\mathbb{N}\\)에서 \\([0,1]\\)로 가는 전사가 존재한다고 가정하고 모순을 이끌어 내자.\n1. 아래와 같은 주장을 하는 가상의 인물을 세움:\n\n\\(\\mathbb{N}\\)에서 \\([0,1]\\)로 향하는 전사함수가 존재한다.\n\n2. 그 가상의 인물이 하는 주장을 잘 생각해보면 아래와 같음\n\\(f\\)는 정의역이 자연수이고 공역이 실수인 함수이므로 아래와 같은 형태일 것임.\n\n\\(f(1)=0.2344253456\\cdots\\)\n\\(f(2)=0.3459837981\\cdots\\)\n\\(f(3)=0.5452349871\\cdots\\)\n\\(\\dots\\)\n\n그 가상의 인물의 주장대로라면\n\\[[0,1]=\\{f(1),f(2),f(3),\\dots\\}\\]\n이라는 의미임. [다시 말하면 \\([0,1]\\) 사이의 모든 실수는 “셀수있다”라는 의미임]\n3. 전사함수의 정의에 의하여 아래가 성립해야 함\n\n\\(\\forall y\\in [0,1] ~\\exists x \\in \\mathbb{N}\\) such that \\(f(x)=y\\)\n\n아래의 원리에 따라서 \\(y=0.x_1x_2x_3\\cdots\\)를 뽑는다면?\n\n\\(y\\)의 첫번째 소수점의 값 \\(x_1\\)은 \\(f(1)\\)의 첫번째 소수점과 다르게 한다. \\(\\Rightarrow\\) \\(y\\neq f(1)\\) \\(\\Rightarrow\\) \\(y \\notin \\{f(1)\\}\\)\n\\(y\\)의 두번째 소수점의 값 \\(x_2\\)은 \\(f(2)\\)의 두번째 소수점과 다르게 한다. \\(\\Rightarrow\\) \\(y\\neq f(1)\\) and \\(y\\neq f(2)\\) \\(\\Rightarrow\\) \\(y \\notin \\{f(1), f(2)\\}\\)\n\n위의 예시에서 y=0.245……\n이러한 \\(y\\)는 분명히 실수이지만 \\(y \\notin \\{f(1),f(2),f(3),\\dots,\\}\\) 이다.2\n\n\n무리수집합의 카디널리티\n(주장) 무리수집합의 카디널리티는 \\(\\aleph_0\\)가 아니다.\n(쉐도복싱) 무리수집합의 카디널리티가 \\(\\aleph_0\\) 이라고 하자.\n\n\\(\\mathbb{R} = \\mathbb{Q} \\cup \\mathbb{Q}^c\\)\n\\(|\\mathbb{Q}|=\\aleph_0\\) 이므로 \\(\\mathbb{Q}\\)와 \\(\\mathbb{N}\\)사이에는 전단사함수가 존재함.\n\\(|\\mathbb{Q}^c|=\\aleph_0\\) 이므로 \\(\\mathbb{Q}^c\\)와 \\(\\mathbb{N}^{-}=\\{-1,-2,\\dots\\}\\)사이에는 전단사함수가 존재함.\n따라서 \\(\\mathbb{Q} \\cup \\mathbb{Q}^c\\) 와 \\(\\mathbb{N} \\cup \\mathbb{N}^-\\) 사이에는 전단사함수가 존재함. (모순)\n\n\n\n\n\n\n\nFootnotes\n\n\n그래서 일단 집합 \\(Y\\)는 양의 유리수의 집합을 포함한다↩︎\n모순이네?↩︎"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_03_07_1wk_checkpoint.html",
    "href": "posts/Advanved Probability Theory/2023_03_07_1wk_checkpoint.html",
    "title": "1wk: 측도론 intro",
    "section": "",
    "text": "해당 강의노트는 전북대학교 최규빈교수님 AP2023 자료임\n\n\n예제1: 동전\n- \\(\\Omega =\\{H,T\\}\\): sample space\n- \\(P(\\{H\\})=P(\\{T\\})=\\frac{1}{2}\\): prob\n- 질문: \\(\\Omega\\)의 임의의(=모든) 부분 집합 \\(\\Omega^*\\)에 대하여 \\(P(\\Omega^*)\\)를 모순없이 정의할 수 있을까?\n\n당연한거 아냐?\n이게 왜 안돼?\n\n- 질문에 대한 대답\n\n\\(\\Omega\\)의 부분집합: \\(\\emptyset, \\Omega, \\{H\\},\\{T\\}\\)\n\\(P(\\{H\\})=\\frac{1}{2}\\), \\(P(\\{T\\})=\\frac{1}{2}\\), \\(P(\\Omega)=P(\\{H,T\\})=1\\), \\(P(\\emptyset)=0\\)\n\n- 모순없이의 의미?\n\n우리가 상식적으로 확률에 적용가능한 어떠한 연산들이 있음. (확률의 공리 + 기본성질) // 네이버검색\n이러한 연산을 적용해도 상식적인 수준에서 납득이 가야함\n\n(상식적인 연산 적용 예시1)\n\\(\\{H\\} \\subset \\Omega \\Rightarrow P(\\{H\\})<P(\\Omega)\\)\n\n집합 \\(\\{H\\}\\)은 집합 \\(\\Omega\\)보다 작은 집합임\n상식적으로 작은집합이 일어날 확률이 큰 집합이 일어날 확률보다 클 수 없음\n동전 예제의 경우 모든 \\(A,B \\subset \\Omega\\) 에 대하여, \\(A\\subset B\\) 이라면 \\(P(A) < P(B)\\) 가 성립함\n\n(상식적인 연산 적용 예시2)\n\\(\\{H\\} \\cap \\{T\\} = \\emptyset \\Rightarrow P(\\{H\\} \\cup \\{T\\})=P(\\{H\\}) + P(\\{T\\}) =1\\)\n\n우리의 상식에 따르면 \\(A,B\\)가 서로소인 사건이라면 \\(P(A)+P(B)\\)이어야 함.1\n이 예제는 실제로 그러함.\n사실 이 예제의 경우 \\(P(\\{H\\} \\cup \\{T\\})=P(\\Omega)=1\\) 와 같이 계산할 수도 있음.\n하지만 어떠한 방식으로 계산해도 모순이 없음.\n\n\n\n예제2: 바늘이 하나만 있는 시계\n- \\(\\Omega = [0,2\\pi)\\)\n\n시계바늘을 돌려서 나오는 각도를 재는일 \\(\\Leftrightarrow\\) \\([0,2\\pi)\\)사이의 숫자중에 하나를 뽑는 일\n\n- 질문: 바늘을 랜덤으로 돌렸을때 12시-6시 사이에 바늘이 있을 확률? \\(\\frac{1}{2}\\)\n\n\\(\\Omega^* = [0,\\pi)\\)\n\\(P(\\Omega^*)= \\frac{1}{2}\\)\n\n- 계산하는 방법? 아래와 같이 계산하면 가능!!\n\\[\\forall \\Omega^* \\subset \\Omega, \\quad P(\\Omega^*)=\\frac{m(\\Omega^*)}{m(\\Omega)}\\]\n단 여기에서 \\(m\\)은 구간의 길이를 재는 함수라고 하자.\n연습: \\(m\\)의 사용\n\n\\(m(\\Omega)=m\\big([0,2\\pi)\\big)=2\\pi\\)\n\\(m(\\Omega^*) = m\\big([0,\\pi)\\big)= \\pi\\)\n\n- 위와 같은 방식으로 확률을 정의하면 잘 정의될까? 이게 쉽지 않음. 왜냐하면 확률을 잘 정의하기 위해서는\n\n\\(\\Omega\\)의 모든 부분집합 \\(\\Omega^*\\)에 대하여 \\(P(\\Omega^*)\\)를 모순없이\n\n정의할 수 있어야 하는데, 이게 쉬운일이 아님.\n(질문0) 그냥 몸풀기 용 질문\n\n\\(\\Omega^*=\\emptyset\\) 일 확률이 얼마인가?\n\n(답변)\n\n0 이야2\n\n(질문1) 첫번째 도전적인 질문\n\n\\(\\Omega^* =\\{0\\}\\)일 확률이 얼마인가?\n\n(답변)\n\n즉 바늘침이 정확하게 12시를 가르킬 확률이 얼마냐는 것\n한 점으로 이루어진 집합 \\(\\{0\\}\\)은 분명히 \\(\\Omega=[0,2\\pi)\\)의 부분집합 이므로 앞서 논의한대로라면 이러한 집합에 대한 확률을 명확하게, 모순없이 정의할 수 있어야 함\n많은 사람들이 이 질문에 대한 답은 \\(0\\) 이라고 알고 있고 그 이유를 “점의 길이는 0 이니까” 라고 이해하고 있음.3\n\n답변이 사실 좀 찝찝해. 바늘침이 정확하게 12시를 가르키는 것은 우리가 분명 하루에 한번씩은 경험하는 사건임. 그런데 그 사건이 일어날 확률은 0이다?4\n(참견질문) 생각해보니까 이런게 있었잖아?\n\\[A \\subset B \\Rightarrow P(A)<P(B)\\]\n그런데 \\(\\emptyset \\subset \\{0\\}\\) 인데 \\(P(\\emptyset)=P(\\{0\\})\\) 이다..?\n(답변)\n\n원래식 \\(A \\subset B \\Rightarrow P(A)\\leq P(B)\\) 이 성립함\n즉 \\(A\\)가 \\(B\\)의 진 부분집합이더라도 \\(P(A)=P(B)\\)인 경우가 존재함.\n\n(질문2) 두번째 질문은 아래와 같다.\n\n그렇다면 사건 \\(\\{0,\\pi\\}\\)가 일어날 확률은 얼마인가?\n\n(답변)\n\n질문을 다시 풀어쓰면 바늘침이 정확하게 12시를 가르키거나 혹은 정확하게 6시를 가르킬 확률이 얼마냐는 것\n따라서 이 질문에 대한 대답은 \\(0+0=0\\) 이므로 \\(0\\)이라고 주장할 수 있음.\n\n(질문3) 세번째 질문은 아래와 같다.\n\n구간 \\([0,2\\pi)\\)는 무수히 많은 점들이 모여서 만들어지는 집합이다. 그런데 점 하나의 길이는 0이다. 0을 무수히 더해도 0이다. 그러므로 구간 \\([0,2\\pi)\\)의 길이도 0이 되어야 한다. 이것은 모순아닌가?\n\n(답변)\n\n까다롭다.\n\\(m([0,2\\pi))=0\\) 임을 인정하면 전체확률은 1이어야 한다는 기본상식5에 어긋나 모순이 생김.\n질문의 논리는 타당해보임. 이 논리의 약점은 딱히 없어보임. 굳이 약점이 있다면 “무한”이라는 개념?\n어쩔수없이 직관에 근거한 약간의 약속을 또 다시 해야할 것 같음. 예를들면 “점들을 유한번 합치면 그냥 많은 점들이지만 무한히 합치면 이것은 선분이 된다. 따라서 길이가 생긴다.” 와 같이.\n우리는 이 약속을 “무한번의 기적”이라고 칭하자.\n\n(질문4) 그렇다면 아래의 질문은 어떻게 대답할 수 있을까?\n\n\\([0,\\pi)\\) 에서 유리수만 뽑아낸 집합이 있다고 생각하자. 편의상 이 집합을 \\(\\mathbb{Q}\\) 라고 하자. 이 집합은 분명히 무한개의 점을 포함하고 있다. 그렇다면 이 집합도 길이가 있는가? 있다면 얼마인가?\n\n(답변)\n\n이미 점들의 길이를 무한번 더하면 길이가 생긴다고 주장한 상태이므로 (무한번의 기적) 길이가 0이라고 주장할 수 없다. 따라서 길이가 있다고 주장해야 한다.\n\\(\\pi\\)말고 딱히 떠오르는 수가 없는데 단순히 길이가 \\(\\pi\\)라고 주장한다면 바로 모순에 빠짐을 알 수 있다.6\n길이는 일단 0보다 커야하고 \\(\\pi\\)보다 작아야함은 자명하므로 그 사이에 있는 어떤 값이 길이라고 주장하자.7\n따라서 (질문4)에 대한 답은 ‘’구체적으로 얼마인지는 모르겠지만 길이가 분명 존재하고 그 길이는 0 보다 크고 \\(\\pi\\) 보다는 작은 어떠한 값 \\(a\\)이다.’’ 정도로 정리할 수 있다.\n즉 \\(m(\\mathbb{Q})=a\\).\n\n(질문5) – 외통수\n질문4로부터 만들어지는 논리는 빌드업1-3으로 이어지는 콤보질문을 적절하게 대답하지 못한다. (질문이 좀 길어서 나누어서 설명합니다)\n(빌드업1) – 평행이동은 길이를 변화시키지 않아, 그렇지?\n\n\\(\\mathbb{Q}\\)의 모든점에 \\(\\sqrt{2}\\)를 더한다. 이 점들로 집합을 만들어 \\(\\mathbb{Q}_{\\sqrt{2}}\\)를 만든다.\n여기에서 \\(\\mathbb{Q}_{\\sqrt{2}}\\)는 \\(\\Omega\\)의 부분집합 \\(\\Rightarrow\\) \\(\\mathbb{Q}_{\\sqrt{2}}\\)는 길이를 명확하고 모순없이 정의할 수 있어야 함\n\\(\\mathbb{Q}_{\\sqrt{2}}\\)의 길이는 사실 쉽게 \\(a\\)라고 정의할 수 있음8. 즉, \\(m(\\mathbb{Q}_{\\sqrt{2}})=a\\).\n\n(빌드업2) – 겹치지 않게 평행이동 시킨다음에 길이를 더한다면?\n이제 \\(\\mathbb{Q}_{\\sqrt{2}},\\mathbb{Q}_{\\sqrt{2}/2},\\mathbb{Q}_{\\sqrt{2}/3}\\)를 생각하자. 아래의 성질을 관찰할 수 있다.\n\n\\(\\mathbb{Q}_{\\sqrt{2}},\\mathbb{Q}_{\\sqrt{2}/2},\\mathbb{Q}_{\\sqrt{2}/3}\\)는 모두 \\(\\Omega\\)의 부분집합 \\(\\Rightarrow\\) 따라서 길이를 명확하고 모순없이 정의할 수 있어야 함\n\\(\\mathbb{Q}_{\\sqrt{2}},\\mathbb{Q}_{\\sqrt{2}/2},\\mathbb{Q}_{\\sqrt{2}/3}\\)의 길이는 각각 \\(a\\)로 정의할 수 있다.9\n\\(P(\\mathbb{Q}_{\\sqrt{2}} \\cup \\mathbb{Q}_{\\sqrt{2}/2} \\cup \\mathbb{Q}_{\\sqrt{2}/3})=P(\\mathbb{Q}_{\\sqrt{2}}) + P(\\mathbb{Q}_{\\sqrt{2}/2})+ P(\\mathbb{Q}_{\\sqrt{2}/3})\\)10\n\n굳이 \\(P(\\mathbb{Q}_{\\sqrt{2}}) + P(\\mathbb{Q}_{\\sqrt{2}/2})+ P(\\mathbb{Q}_{\\sqrt{2}/3})\\)를 계산하면 아래와 같이 계산할 수 있겠다.\n\\[P(\\mathbb{Q}_{\\sqrt{2}}) + P(\\mathbb{Q}_{\\sqrt{2}/2})+ P(\\mathbb{Q}_{\\sqrt{2}/3})=\\frac{a}{2\\pi}+\\frac{a}{2\\pi}+\\frac{a}{2\\pi}=3 \\times \\frac{a}{2\\pi}\\]\n(빌드업3) – 그런데 난 겹치지않게 평행이동시킬 방법을 무한대로 알고 있는데?\n눈 여겨볼 점은 아래 식이 성립해야 한다는 것이다. (\\(\\because\\) 확률의 공리)11\n\\[P\\big(\\mathbb{Q}_{\\sqrt{2}} \\cup \\mathbb{Q}_{\\sqrt{2}/2} \\cup \\mathbb{Q}_{\\sqrt{2}/3}\\big) = 3 \\times \\frac{a}{2\\pi} \\leq 1 \\quad \\cdots (\\star)\\]\n\n그런데 \\((\\star)\\)에서 좌변의 값은 편의에 따라서 값을 임의로 키울 수 있다.\n이렇게 임의로 키워진 좌변의 값이라도 항상 그 값은 1보다 작아야 하는데 (확률의 공리), 이게 가능하려면 \\(\\alpha=0\\)인 경우 말고 없다.\n그런데 \\(\\alpha=0\\) 이 된다면 “무한번 더해서 일어나는 기적”은 허구가 되므로 질문3 의 대답에 모순이 된다.\n\n그런데 임의로 좌변의 값을 키워도 항상 그 값은 1보다 작아야 하는데 이러한 \\(\\alpha\\)는 0이외에 불가능하다.\n그런데 \\(\\alpha=0\\) 이 된다면 “무한번 더해서 일어나는 기적”은 허구가 되므로 질문3 의 대답에 모순이 된다.\n\n\n르벡메져\n- 예제2에서의 마지막 질문은 지금까지 제시한 논리로 방어가 불가능. 이처럼 논리적인 모순없는 체계를 만드는 것은 매우 어려운 일임.\n- 결론적으로 말하면 길이를 재는 함수 \\(m\\)을 아래와 가정하면 위의 모든 질문에 대한 대답을 논리적 모순없이 설계할 수 있다.\n\n한 점에 대한 길이는 \\(0\\) 이다.\n\\([0,2\\pi)\\) 사이의 모든 유리수를 더한 집합은 그 길이가 \\(0\\)이다.\n\\([0,2\\pi)\\) 사이의 모든 무리수를 더한 집합은 그 길이가 \\(2\\pi\\)이다.\n\n참고로 르벡측도(Lebesgue measure)를 사용하면 위의 성질을 만족한다.12 따라서 르벡측도를 활용하여 확률을 정의하는 것이 모순을 최대한 피할 수 있다.\n\n\n\n\n\nFootnotes\n\n\n확률의 공리↩︎\n이걸 좀 더 엄밀하게 따질수도 있는데 일단 직관적으로 0이라 생각하고 넘어가자↩︎\n이해 안되면 약속이라고 생각하자.↩︎\n자연어에서는 “확률=0” 와 “불가능” 은 동일하지만 여기서는 아니다.↩︎\n심지어 이건 확률의 공리↩︎\n왜 모순에 빠지냐면 \\([0,\\pi)\\)에서 무리수만 뽑아낸 집합의 길이가 뭐냐고 물을경우 0이라고 말해야함↩︎\n구체적으로 어떤값인지는 모른다고 하자.↩︎\n평행이동은 길이를 변화시킬 수 없으니까↩︎\n평행이동은 길이를 변화시키지 않으니까↩︎\n\\(\\mathbb{Q}_{\\sqrt{2}},\\mathbb{Q}_{\\sqrt{2}/2},\\mathbb{Q}_{\\sqrt{2}/3}\\)는 모두 서로소 임을 이용↩︎\n첫 등호는 서로소인 사건에 대한 공리, 그다음 부등호는 확률의 총합은 1보다 같거나 작다라는 공리↩︎\n물론 르벡측도의 정의가 위와 같지는 않다↩︎"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html",
    "href": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html",
    "title": "5wk: 측도론 (1)",
    "section": "",
    "text": "해당 강의노트는 전북대학교 최규빈교수님 AP2023 자료임"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#수학과의-기호",
    "href": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#수학과의-기호",
    "title": "5wk: 측도론 (1)",
    "section": "수학과의 기호",
    "text": "수학과의 기호\n- 아래는 기호는 몇 가지 영어단어의 축약형이다.\n\nfor all: \\(\\forall\\)\nexists: \\(\\exists\\)\nsuch that, satisfying: \\({\\sf s.t.}\\), \\({\\sf st}\\)\nif-then, implies, therefore: \\(\\Rightarrow\\)\nif and only if: \\(\\Leftrightarrow\\)\nbecause: \\(\\because\\)\ntherefore: \\(\\therefore\\)\nquod erat: \\(\\square\\), \\(\\blacksquare\\) 교재에서 증명이 끝낫을 때\n\n- 예시1: 모든 실수 \\(x\\)에 대하여, \\(x^2\\)은 양수이다.\n언어\n\nfor any \\(x\\) in \\(\\mathbb{R}\\), \\(x^2 \\geq 0\\).\nfor arbitrary \\(x \\in \\mathbb{R}\\), \\(x^2 \\geq 0\\).\nfor any choice of \\(x \\in \\mathbb{R}\\), \\(x^2 \\geq 0\\).\nfor all \\(x \\in \\mathbb{R}\\), \\(x^2 \\geq 0\\).\nif \\(x \\in \\mathbb{R}\\), then \\(x^2 \\geq 0\\).\n\n기호\n\n\\(\\forall x \\in \\mathbb{R}\\): \\(x^2\\geq 0\\).\n\\(\\forall x \\in \\mathbb{R}\\), \\(x^2\\geq 0\\).\n\\(x^2 \\geq 0\\), for all \\(x \\in \\mathbb{R}\\).\n\\(x^2 \\geq 0\\), \\(\\forall x \\in \\mathbb{R}\\).\n\\(x \\in \\mathbb{R} \\Rightarrow x^2 \\geq 0\\).\n\n\n거의 쓰는 사람 마음임, 그런데 뉘앙스가 조금씩 다름.\n\n- 예시2: \\(\\Omega\\)의 임의의 부분집합 \\(A\\),\\(B\\)에 대하여, \\(A=B\\) 일 필요충분조건은 \\(A\\subset B\\) 이고 \\(B \\subset A\\) 이어야 한다.\n언어\n\nfor all \\(A,B \\subset \\Omega\\), \\(A=B\\) if and only if (1) \\(A \\subset B\\) and (2) \\(B \\subset A\\).\n\n기호\n\n\\(A = B \\Leftrightarrow A \\subset B \\text{ and } B \\subset A, \\forall A,B \\subset \\Omega\\).\n\\(A = B \\Leftrightarrow \\big(A \\subset B \\text{ and } B \\subset A\\big), \\forall A,B \\subset \\Omega\\). 좀 더 명확하게 쓰기 위해서 가로\n\\(\\forall A,B \\subset \\Omega\\): \\(A = B \\Leftrightarrow \\big(A \\subset B \\text{ and } B \\subset A\\big)\\)\n\n\n의미가 때로는 모호할때가 있지만 눈치껏 알아먹어야 한다.\n\n- 예시3: 임의의 양수 \\(\\epsilon>0\\)에 대하여 \\(|x| \\leq \\epsilon\\)이라면 \\(x=0\\)일 수 밖에 없다.\n언어\n\nIf \\(|x|< \\epsilon\\) for all \\(\\epsilon>0\\), then \\(x=0\\).\nIf \\(|x|< \\epsilon\\), \\(\\forall \\epsilon>0\\), then \\(x=0\\).\nFor all \\(\\epsilon>0\\), \\(|x|< \\epsilon\\) implies \\(x=0\\). – 틀린표현\n\n기호\n\n\\(|x| < \\epsilon,~ \\forall \\epsilon>0 \\Rightarrow x=0\\)\n\\(\\forall \\epsilon>0: |x| < \\epsilon \\Rightarrow x=0\\) – 애매하다?\n\\(\\big(\\forall \\epsilon>0:|x| < \\epsilon\\big) \\Rightarrow x=0\\)\n\\(\\big(\\forall \\epsilon>0\\big)\\big(|x| < \\epsilon \\Rightarrow x=0\\big)\\) – 틀린표현"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#기타-약어-및-상투적인-표현",
    "href": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#기타-약어-및-상투적인-표현",
    "title": "5wk: 측도론 (1)",
    "section": "기타 약어 및 상투적인 표현",
    "text": "기타 약어 및 상투적인 표현\n- 약어\n\n\\({\\sf WLOG}\\): Without Loss Of Generality\n\\({\\sf WTS}\\): What/Want To Show\n\\({\\sf iff}\\): if and only if\n\\({\\sf Q.E.D.}\\): 증명완료 (쓰지마..)\n\\({\\sf LHS}\\): Left Hand Side\n\\({\\sf RHS}\\): Right Hand Side\n\n- 상투적인 표현\n\nIt suffices to show that, It is sufficient to show that\nthat이하의 내용을 보이면 증명이 끝난다"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#before",
    "href": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#before",
    "title": "5wk: 측도론 (1)",
    "section": "Before",
    "text": "Before\n- 아래의 기호를 약속\n\n전체집합: \\(\\Omega\\)\n관심있는 집합의 모임: \\({\\cal A} \\subset 2^{\\Omega}\\)\n\n- \\(\\Omega \\neq \\emptyset\\), \\({\\cal A} \\neq \\emptyset\\) 를 가정.\n- 약속: 집합 \\({\\cal A} \\subset 2^{\\Omega}\\)에 대하여 아래와 같은 용어를 약속하자.\n\n\\(\\cap\\)-closed (closed under intersection) or a \\(\\pi\\)-system: \\(\\forall A,B \\in {\\cal A}:~ A \\cap B \\in {\\cal A}\\)\n\\(\\sigma\\)-\\(\\cap\\)-closed (closed under countable interserction): \\(\\forall \\{A_i\\}_{i=1}^{\\infty} \\subset {\\cal A}:~ \\cap_{i=1}^{\\infty} A_i \\in {\\cal A}\\)\n\\(\\cup\\)-closed (closed under unions): \\(\\forall A,B \\in {\\cal A}:~ A\\cup B \\in {\\cal A}\\)\n\\(\\sigma\\)-\\(\\cup\\)-closed (closed under countable unois): \\(\\forall \\{A_i\\}_{i=1}^{\\infty} \\subset {\\cal A}:~ \\cup_{i=1}^{\\infty}A_i \\in {\\cal A}\\)\n＼-closed (closed under differences): \\(\\forall A,B \\in {\\cal A}:~ A-B \\in {\\cal A}\\)\n\\(^c\\)-closed (closed under complements): \\(\\forall A \\in {\\cal A}:~ A^c \\in {\\cal A}\\)\n\n- 우리만의 약속:\n\n앞으로 서로소인 집합들에 대한 합집합은 기호로 \\(\\uplus\\)라고 표현하겠다.\n따라서 앞으로 \\(B_1 \\uplus B_2\\)의 의미는 (1) \\(B_1 \\cup B_2\\) (2) \\(B_1 \\cap B_2 = \\emptyset\\) 을 의미한다고 정의하겠다. (꼭 서로소임을 명시하지 않아도)\n\\(\\sigma\\)-\\(\\uplus\\)-closed 의 의미는 \\(\\uplus_{i=1}^{\\infty}B_i \\in {\\cal A}, \\forall \\{B_i\\}_{i=1}^{\\infty} \\subset {\\cal A}:\\) 의 의미이다.\n\n- 이론: \\({\\cal A}\\subset 2^{\\Omega}\\) 가 여집합에 닫혀있다면, 아래가 성립한다.\n\n\\({\\cal A}\\)가 교집합1에 닫혀있음. \\(\\Leftrightarrow\\) \\({\\cal A}\\)가 합집합2에 닫혀있음.\n\\({\\cal A}\\)가 가산교집합3에 닫혀있음. \\(\\Leftrightarrow\\) \\({\\cal A}\\)가 가산합집합4에 닫혀있음.\n\n(증명) 생략\n- 이론: \\({\\cal A}\\subset 2^{\\Omega}\\)가 차집합에 닫혀있다면, 아래가 성립한다.\n\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\\({\\cal A}\\)가 가산합집합에 닫혀있다. \\(\\Rightarrow\\) \\({\\cal A}\\)가 가산교집합에 닫혀있다.\n\\(\\forall \\{A_i\\} \\subset {\\cal A},~ \\exists \\{B_i\\} \\subset {\\cal A}\\) such that \\(\\cup_{i=1}^{\\infty} A_i = \\uplus_{i=1}^{\\infty} B_i\\).5\n\n(증명)\n\nNote: \\(A\\cap B = A-(A-B)\\).\nNote: \\(\\cap_{i=1}^{\\infty}A_i = \\cap_{i=2}^{n}(A_1\\cap A_i)= \\cap_{i=2}^{n}(A_1 - (A_1-A_i))=A_1 - \\cup_{i=2}^{n}(A_1-A_i)\\).\nNote: \\(\\cup_{i=1}^{\\infty}A_i = A_1 \\uplus(A_2-A_1) \\uplus \\big((A_3-A_1) - A_2 \\big) \\uplus \\big(\\big((A_4-A_1)-A_2\\big)-A_3\\big)\\uplus \\cdots\\)\n\n\n차집합에 닫혀있다는 것은 매우 좋은 성질임."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#시그마필드-starstarstar",
    "href": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#시그마필드-starstarstar",
    "title": "5wk: 측도론 (1)",
    "section": "시그마필드 (\\(\\star\\star\\star\\))",
    "text": "시그마필드 (\\(\\star\\star\\star\\))\n- 정의: 시그마필드 (\\(\\sigma\\)-field, \\(\\sigma\\)-algebra)\n집합 \\({\\cal F} \\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal F}\\)를 \\(\\Omega\\)에 대한 시그마필드라고 부른다.\n\n\\(\\Omega \\in {\\cal F}\\).\n\\({\\cal F}\\)는 여집합에 닫혀있다.\n\\({\\cal F}\\)는 가산합집합에 닫혀있다.\n\n- 시그마필드의 정의에서 1을 생략하기도 한다. 이럴 경우는 특별히 \\({\\cal F}\\neq\\emptyset\\)임을 강조한다. 1을 생략할 수 있는 논리는 아래와 같다.\n\n\\({\\cal F}\\)는 공집합이 아니므로 최소한 하나의 집합 \\(A\\)는 포함해야 한다. 즉 \\(A \\in {\\cal F}\\).\n2번 원리에 의하여 \\(A^c \\in {\\cal F}\\).\n시그마필드는 합집합에 닫혀있으므로 \\(A\\cup A^c \\in {\\cal F}\\)."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#알지브라-필드-star",
    "href": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#알지브라-필드-star",
    "title": "5wk: 측도론 (1)",
    "section": "알지브라, 필드 (\\(\\star\\))",
    "text": "알지브라, 필드 (\\(\\star\\))\n- 정의1: 알지브라, 필드 (algebra, field)\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 대수라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\).\n\\({\\cal A}\\)는 차집합에 닫혀있다.\n\\({\\cal A}\\)는 합집합에 닫혀있다.\n\n- 알지브라 역시 1의 조건을 생략하기도 한다.\n- 전체집합을 포함 \\(\\Rightarrow\\) (차집합에 닫혀있음 \\(\\Rightarrow\\) 여집합에 닫혀있음) \\(\\Rightarrow\\) 따라서 대수는 여집합에 닫혀있다.\n- 차집합에 닫혀있음 \\(\\Rightarrow\\) 교집합에 닫혀있게 된다.\n\n혹은 (여집합에 닫혀있음 & 합집합에 닫혀있음) \\(\\Rightarrow\\) 교집합에 닫혀있음.\n\n- 정의2: 알지브라의 또 다른 정의\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 대수라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\).\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\\({\\cal A}\\)는 여집합에 닫혀있다.\n\n- 여집합에 닫혀있음 \\(\\Rightarrow\\) (합집합에 닫혀있음 \\(\\Rightarrow\\) 교집합에 닫혀있음) \\(\\Rightarrow\\) 2번 조건을 합집합으로 바꿔도 무방\n- 여집합에 닫혀있음 \\(\\Rightarrow\\) (합집합에 닫혀있음 \\(\\Leftrightarrow\\) 교집합에 닫혀있음) \\(\\Rightarrow\\) 2번 조건을 합집합으로 바꿔도 무방 (이예제에서는)\n- 정의3: 알지브라의 또 또 다른 정의 (교재의 정의)\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 대수라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\).\n\\({\\cal A}\\)는 여집합에 닫혀있다.\n\\({\\cal A}\\)는 합집합에 닫혀있다.\n\n드모르간의 법칙으로 정의2랑 정의3이 동치인 것을 확인할 수 있음\n- 알지브라의 예시\n\n\\(\\Omega = \\{H,T\\}\\), \\({\\cal A} = 2^\\Omega\\) 일때, \\({\\cal A}\\)는 알지브라이다. (\\(|\\Omega| <\\infty\\) 이라면 “시그마필드 = 알지브라(필드)” 이다.)"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#링",
    "href": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#링",
    "title": "5wk: 측도론 (1)",
    "section": "링",
    "text": "링\n- 정의: 링 (ring)\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 링이라고 부른다.\n\n\\(\\emptyset \\in {\\cal A}\\).\n\\({\\cal A}\\)는 차집합에 닫혀있다.\n\\({\\cal A}\\)는 합집합에 닫혀있다.\n\n- 여기에서 1의 조건을 생략할 수 있다. (이럴경우 특별히 \\({\\cal A}\\neq \\emptyset\\) 임을 강조한다.)\n\n\\({\\cal A}\\)는 공집합이 아니므로 최소한 하나의 원소 \\(A\\)는 가져야 한다.\n\n조건2에 의하여 \\(A-A\\) 역시 \\({\\cal A}\\)의 원소이다.\n\n- 링은 차집합에 닫혀있음 \\(\\Rightarrow\\) 링은 교집합에도 닫혀있음 \\(\\Rightarrow\\) 링은 교집합과 합집합 모두에 닫혀 있다.\n- 링과 알지브라의 차이는 전체집합이 포함되느냐 마느냐임 \\(\\Rightarrow\\) 그런데 이 차이로 인해 알지브라는 여집합에 닫혀있지만 링은 여집합에 닫혀있지 않게 된다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#시그마링",
    "href": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#시그마링",
    "title": "5wk: 측도론 (1)",
    "section": "시그마링",
    "text": "시그마링\n- 정의: 시그마링 (\\(\\sigma\\)-ring)\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 링이라고 부른다.\n\n\\(\\emptyset \\in {\\cal A}\\).\n\\({\\cal A}\\)는 차집합에 닫혀있다.\n\\({\\cal A}\\)는 가산합집합에 닫혀있다.\n\n- 여기에서 1의 조건을 생략할 수 있다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#세미알지브라-starstarstar",
    "href": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#세미알지브라-starstarstar",
    "title": "5wk: 측도론 (1)",
    "section": "세미알지브라 (\\(\\star\\star\\star\\))",
    "text": "세미알지브라 (\\(\\star\\star\\star\\))\n- 정의1: 세미알지브라 (semi-algebra) // ref : 위키북스\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 세미알지브라 라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\).\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\\(\\forall A,B \\in {\\cal A}, \\exists \\{B_i\\}_{i=1}^{n} \\subset {\\cal A}\\) such that \\[A-B = \\uplus_{i=1}^{n} B_i.\\]\n\n\n3번을 \\({\\cal A}\\)가 차집합에 반쯤 닫혀있다고 표현한다. 즉 차집합 자체가 \\({\\cal A}\\)에 들어가는건 아니지만 차집합의 disjoint한 조각들은 모두 \\({\\cal A}\\)에 들어간다.\n\n- 세미알지브라는 공집합을 포함한다. (이때 \\({\\cal A}\\neq \\emptyset\\)임을 강조함)\n\n\\({\\cal A}\\)는 공집합이 아니므로 최소한 하나의 집합 \\(A\\)는 포함해야 한다. 즉 \\(A \\in {\\cal A}\\).\n\\(A \\in {\\cal A}\\)이면 조건3에 의하여 \\(\\emptyset\\)6을 \\({\\cal A}\\)의 원소들의 countable union으로 만들 수 있어야 한다. 이 조건을 만족하기 위해서는 \\(\\emptyset \\in {\\cal A}\\)이어야만 한다.\n\n- 정의2: 세미알지브라의 또 다른 정의 // ref: 세미링의 위키에서 언급, Durret의 정의.\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 세미알지브라 라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\)\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\\(\\forall A \\in {\\cal A}, \\exists \\{B_i\\}_{i=1}^{n} \\subset {\\cal A}\\) such that \\[A^c = \\uplus_{i=1}^{n} B_i.\\]\n\n\n3번을 \\({\\cal A}\\)가 여집합에 반쯤 닫혀있다고 표현한다. 즉 여집합 자체가 \\({\\cal A}\\)에 들어가는건 아니지만 차집합의 disjoint한 조각들은 모두 \\({\\cal A}\\)에 들어간다.\n\n- 이 정의에서도 세미알지브라는 공집합을 포함한다. (이때 \\({\\cal A}\\neq \\emptyset\\)임을 강조함)\n\n\\({\\cal A}\\)는 공집합이 아니므로 최소한 하나의 집합 \\(A\\)는 포함해야 한다. 즉 \\(A \\in {\\cal A}\\).\n3에 의하여 \\(A^c=\\uplus_{i=1}^{n}B_i\\)를 만족하는 \\(B_1,\\dots, B_n\\) 역시 \\({\\cal A}\\)에 포함되어야 한다.\n2에 의하여 \\(A \\cap B_1=\\emptyset\\) 역시 \\({\\cal A}\\)에 포함되어야 한다.\n\n- Note: 정의2의 3번조건은 정의1의 3번조건보다 강한 조건이다. (정의2의 조건3 \\(\\Rightarrow\\) 정의1의 조건3)\n\n증명은 세미링/위키 에서 스스로 확인\n\n- 교재의 정의: 정의2에서 \\(\\Omega \\in {\\cal A}\\)이 생략되어 있음.\n\n왜 생략할 수 있는지 모르겠음. (교재가 틀렸을 수도 있음)\n\n- 세미알지브라의 예시: 아래의 \\({\\cal A}\\)는 모두 \\(\\Omega\\)에 대한 세미알지브라이다.\n\n예시1: \\(\\Omega=\\{a,b,c,d\\}\\), \\({\\cal A} = \\{\\emptyset, \\{a\\},\\{b,c,d\\}, \\Omega \\}\\)\n예시2: \\(\\Omega=\\{a,b,c,d\\}\\), \\({\\cal A} = \\{\\emptyset, \\{a\\},\\{b\\},\\{c,d\\}, \\Omega \\}\\)\n예시3: \\(\\Omega=\\{a,b,c,d\\}\\), \\({\\cal A} = \\{\\emptyset, \\{a\\},\\{b,c\\},\\{d\\}, \\Omega \\}\\)\n예시4: \\(\\Omega=\\{a,b,c,d\\}\\), \\({\\cal A} = \\{\\emptyset, \\{a\\},\\{b\\},\\{c\\},\\{d\\}, \\Omega \\}\\)\n예시5: \\(\\Omega=\\{a,b,c,d\\}\\), \\({\\cal A} = \\{\\emptyset, \\{a\\},\\{b\\},\\{c\\},\\{d\\}, \\{a,b\\},\\{b,c\\},\\Omega \\}\\)\n\n\n세미알지브라는 전체집합이 몇개의 파티션으로 쪼개져서 원소로 들어가는 느낌이 있음.\n\n- 세미알지브라의 예시\\((\\star)\\): 아래의 \\({\\cal A}\\)는 모두 \\(\\Omega=\\mathbb{R}\\)에 대한 세미알지브라이다.\n\n예시1: \\({\\cal A} = \\{(a,b]: -\\infty \\leq a < b \\leq \\infty \\}\\cup \\{\\emptyset\\}\\)\n예시2: \\({\\cal A} = \\{[a,b): -\\infty \\leq a < b \\leq \\infty \\}\\cup \\{\\emptyset\\}\\)\n\n- 세미알지브라가 아닌 예시: 아래의 \\({\\cal A}\\)는 \\(\\Omega=\\mathbb{R}\\)에 대한 세미알지브라가 아니다.\n\n예시1: \\({\\cal A} = \\{(a,b): -\\infty \\leq a < b \\leq \\infty \\}\\cup \\{\\emptyset\\}\\)\n\n\\((1,5)-(2,3) = (1,2] \\cup [3,5) \\notin \\cal A\\)\n\n예시2: \\({\\cal A} = \\{[a,b]: -\\infty \\leq a < b \\leq \\infty \\}\\cup \\{\\emptyset\\}\\)\n\n\\([1,5]-[2,3] = [1,2) \\cup (3,5] \\notin \\cal A\\)\n- 교재의 언급 (p3)\n\n\n\n그림1: 교재에서의 세미알지브라 설명"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#세미링-starstarstar",
    "href": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#세미링-starstarstar",
    "title": "5wk: 측도론 (1)",
    "section": "세미링 \\((\\star\\star\\star)\\)",
    "text": "세미링 \\((\\star\\star\\star)\\)\n- 정의: 세미링\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 세미링이라고 부른다.\n\n\\(\\emptyset \\in {\\cal A}\\).\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\\({\\cal A}\\)는 차집합에 반쯤 닫혀있다.\n\n- 세미링에서도 공집합포함 조건을 생략할 수 있다.\n- 세미링의 예시: 아래의 \\({\\cal A}\\)는 모두 \\(\\Omega\\)에 대한 세미링이다.\n\n예시1: \\(\\Omega=\\{a,b,c,d,e,f\\}\\), \\({\\cal A} = \\{\\emptyset, \\{a\\},\\{b,c,d\\}\\}\\)\n예시2: \\(\\Omega=\\{a,b,c,d,e,f\\}\\), \\({\\cal A} = \\{\\emptyset, \\{a\\},\\{b\\},\\{c,d\\}\\}\\)\n예시3: \\(\\Omega=\\{a,b,c,d,e,f\\}\\), \\({\\cal A} = \\{\\emptyset,\\{a,b,c\\},\\{b,c,d\\}, \\{a\\},\\{b,c\\},\\{d\\}\\}\\)\n\n\n전체집합이 포함될 필요가 없는 세미알지브라 느낌임.\n\n- 세미링의 예시: 아래의 \\({\\cal A}\\)는 모두 \\(\\Omega=\\mathbb{R}\\)에 대한 세미링이다.\n\n예시1: \\({\\cal A} = \\{(a,b]: -\\infty < a < b < \\infty \\}\\cup \\{\\emptyset\\}\\)\n예시2: \\({\\cal A} = \\{[a,b): -\\infty < a < b < \\infty \\}\\cup \\{\\emptyset\\}\\)\n\nequal이 빠져있음. a와 b가 +-무한대, 전체집합을 커버할 필요가 없다.\n- 세미알지브라가 아닌 예시: 아래의 \\({\\cal A}\\)는 \\(\\Omega=\\mathbb{R}\\)에 대한 세미알지브라가 아니다.\n\n예시1: \\({\\cal A} = \\{(a,b): -\\infty < a < b < \\infty \\}\\cup \\{\\emptyset\\}\\)\n예시2: \\({\\cal A} = \\{[a,b]: -\\infty < a < b < \\infty \\}\\cup \\{\\emptyset\\}\\)"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#파이시스템-starstar",
    "href": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#파이시스템-starstar",
    "title": "5wk: 측도론 (1)",
    "section": "파이시스템 (\\(\\star\\star\\))",
    "text": "파이시스템 (\\(\\star\\star\\))\n- 정의: \\(\\pi\\)-system\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 파이스시템 이라고 부른다.\n\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\n- 파이시스템임을 강조하기 위해서 \\({\\cal A}\\) 대신에 \\({\\cal P}\\) 라고 교재에서 표현하기도 한다.\n- 파이시스템의 예시: 아래는 모두 \\(\\Omega=\\mathbb{R}\\)에 대한 파이시스템이다.\n\n예시1: \\({\\cal A} = \\{(a,b]: -\\infty < a < b < \\infty \\}\\)\n예시2: \\({\\cal A} = \\{[a,b): -\\infty < a < b < \\infty \\}\\)\n예시3: \\({\\cal A} = \\{(a,b): -\\infty < a < b < \\infty \\}\\)\n예시4: \\({\\cal A} = \\{[a,b]: -\\infty < a < b < \\infty \\}\\)"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#람다시스템-starstar",
    "href": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#람다시스템-starstar",
    "title": "5wk: 측도론 (1)",
    "section": "람다시스템 (\\(\\star\\star\\))",
    "text": "람다시스템 (\\(\\star\\star\\))\n- 정의1: \\(\\lambda\\)-system\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 람다시스템 이라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\)\n\\(\\forall A,B \\in {\\cal A}:~ A\\subset B \\Rightarrow B-A \\in {\\cal A}\\)\n\\(\\forall B_1,B_2,\\dots \\in {\\cal A}\\) such that \\(B_1,B_2\\dots\\) are disjoint: \\[\\uplus_{i=1}^{\\infty} B_i \\in {\\cal A}\\]\n\n\n람다시스템은 1. 전체집합이 포함되고 2. 두 집합이 포함관계에 있는 경우 차집합에 닫혀있으며 3. 서로소인 가산합집합에 닫혀있다.\n\n- 람다시스템은 여집합에 닫혀있다. 그리고 람다시스템은 공집합을 포함한다.\n\n1,2번 조함. 2번에서 \\(B\\)에 \\(\\Omega\\)를 대입해보자\n\n- 람다시스템임을 강조하기위해서 \\({\\cal A}\\) 대신에 \\({\\cal L}\\) 이라고 교재에서 표현하기도 한다.\n- 람다시스템의 느낌: 3주차 시그마필의 motivation에서 소개한 거의 모든 예제는 사실 람다시스템이다.\n\n람다시스템의 원칙1,2,3은 사실 확률의 공리와 깊게 관련되어있음.\n내 생각: 딘킨은 확률의 공리에 착안해서 람다시스템을 만들지 않았을까?\n\n- 아래는 모두 람다시스템의 예시이다.\n\n\\(\\Omega=\\{H,T\\}\\), \\({\\cal L}=\\{\\emptyset, \\{H\\},\\{T\\},\\Omega\\}\\) – 3주차 예제1\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\), \\({\\cal L}=2^\\Omega\\) – 3주차 예제4\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\), \\({\\cal L}=\\{\\emptyset,\\{6\\},\\{1,2,3,4,5\\},\\Omega\\}\\) – 3주차 예제5\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\), \\({\\cal L}=\\{\\emptyset,\\{1,2,3\\},\\{3,4,5\\},\\Omega\\}\\) – 3주차 예제6\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\), \\({\\cal L}=\\{\\emptyset,\\Omega\\}\\) – 3주차 예제8\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\), \\({\\cal L}=\\{\\emptyset,\\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\{3,4\\}, \\{1,2\\},\\Omega\\}\\) – 3주차 예제9,10\n\\(\\Omega=(0,2\\pi]\\), \\({\\cal L}=\\sigma({\\cal A})\\) where \\({\\cal A} = \\{\\{x\\}: x\\in \\mathbb{Q} \\cap \\Omega \\}\\) – 3주차 예제11\n\\(\\Omega=\\{1,2,3,4\\}\\), \\({\\cal L}=\\{\\emptyset, \\{1,2\\}, \\{1,3\\}, \\{1,4\\}, \\{2,3\\}, \\{2,4\\}, \\{3,4\\}, \\Omega\\}\\) – 3주차 예제12에서 교집합 안넣은 버전\n\n- 정의2: \\(\\lambda\\)-system (교재의 정의)\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 람다시스템 이라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\)\n\\(\\forall A,B \\in {\\cal A}:~ A\\subset B \\Rightarrow B-A \\in {\\cal A}\\)\n\\(\\forall A_1,A_2,\\dots \\in {\\cal A}\\) such that \\(A_1 \\subset A_2 \\subset \\dots\\): \\[\\cup_{i=1}^{\\infty} A_i \\in {\\cal A}\\]\n\n- Note: 정의1의 3번조건과 정의2의 3번조건은 서로 동치관계이다.\n- 교재에서의 파이시스템, 람다시스템 설명\n\n\n\n그림2: 교재에서의 파이시스템과 람다시스템\n\n\n\n위의 정의에서 기호 \\(A_n \\uparrow A\\)의 의미는 “\\(A_1 \\subset A_2 \\subset \\dots\\) and \\(\\cup_{i}^{\\infty}A_i=A\\)”를 뜻하는 축약표현이다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#정리",
    "href": "posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html#정리",
    "title": "5wk: 측도론 (1)",
    "section": "정리",
    "text": "정리\n- 정리표 (hw): 물음표를 채워라\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A \\cap B\\)\n\\(\\emptyset\\)\n\\(A-B\\)\n\\(\\cup_i\\to\\uplus_i\\)\n\\(\\Omega\\)\n\\(A^c\\)\n\\(A\\cup B\\)\n\\(\\cup_{i=1}^{\\infty}A_i\\)\n\\(\\uplus_{i=1}^{\\infty}B_i\\)\n\\(\\cap_{i=1}^{\\infty}A_i\\)\n\n\n\n\n\\(\\pi\\)-system\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\n\nsemi-ring\n\\(O\\)\n\\(O\\)\n\\(\\Delta\\)\n\\(O\\)\n\\(X\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\n\nsemi-algebra\n\\(O\\)\n\\(O\\)\n\\(\\Delta\\)\n\\(O\\)\n\\(O\\)\n\\(\\Delta\\)\n\\(O\\)\n\\(O\\)\n\\(\\Delta\\)\n\\(O\\)\n\n\nring\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(X\\)\n\n\nalgebra\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\n\n\\(\\sigma\\)-ring\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(X\\)\n\n\n\\(\\lambda\\)-system\n\\(X\\)\n\\(O\\)\n\\(\\Delta'\\)\n\\(X\\)\n\\(O\\)\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(O\\)\n\\(X\\)\n\n\n\\(\\sigma\\)-field\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\n\n\n\\(\\Delta\\)는 반쯤 닫힌 거\n차집합에 닫혀있으면 -> \\(\\cup_i\\to\\uplus_i\\)자동으로 닫혀있다.\n\\(\\Omega\\)을 포함하고 차집합에 닫혀있다면 -> \\(A^c\\)에 닫혀있다\n- 다이어그램 (포함관계)\n\n\n\n\n\n\n\nG\n\n \n\ncluster_0\n\n RING  \n\ncluster_1\n\n ALGEBRA  \n\ncluster_2\n\n LAMBDA   \n\nσ－ring\n\n σ－ring   \n\nring\n\n ring   \n\nσ－ring->ring\n\n    \n\nsemiring\n\n semiring   \n\nring->semiring\n\n    \n\nπ－system\n\n π－system   \n\nsemiring->π－system\n\n    \n\nσ－algebra\n\n σ－algebra   \n\nσ－algebra->σ－ring\n\n    \n\nalgebra\n\n algebra   \n\nσ－algebra->algebra\n\n    \n\nλ－system\n\n λ－system   \n\nσ－algebra->λ－system\n\n    \n\nalgebra->ring\n\n    \n\nsemialgebra\n\n semialgebra   \n\nalgebra->semialgebra\n\n    \n\nsemialgebra->semiring\n\n   \n\n\n\n\n\n- 다이어그램 (이해용) – 그림은 더럽지만..\n\n\n\n\n\n\n\nG\n\n \n\ncluster_1\n\n ALGEBRA  \n\ncluster_2\n\n LAMBDA  \n\ncluster_0\n\n RING   \n\nsemiring\n\n semiring   \n\nring\n\n ring   \n\nsemiring->ring\n\n  ∪－stable   \n\nsemialgebra\n\n semialgebra   \n\nsemiring->semialgebra\n\n  Ω－contained   \n\nσ－ring\n\n σ－ring   \n\nring->σ－ring\n\n  σ－∪－stable   \n\nalgebra\n\n algebra   \n\nring->algebra\n\n  Ω－contained   \n\nσ－algebra\n\n σ－algebra   \n\nσ－ring->σ－algebra\n\n  Ω－contained   \n\nsemialgebra->algebra\n\n  ∪－stable   \n\nalgebra->σ－algebra\n\n  σ－∪－stable   \n\nλ－system\n\n λ－system   \n\nλ－system->σ－algebra\n\n  ∩－stable   \n\nπ－system\n\n π－system   \n\nπ－system->semiring\n\n  ＼－semistable"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_03_28_4wk_checkpoint.html",
    "href": "posts/Advanved Probability Theory/2023_03_28_4wk_checkpoint.html",
    "title": "4wk: 측도론 intro (4)",
    "section": "",
    "text": "해당 강의노트는 전북대학교 최규빈교수님 AP2023 자료임"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_03_28_4wk_checkpoint.html#상황1-시그마필드-구하기-귀찮아",
    "href": "posts/Advanved Probability Theory/2023_03_28_4wk_checkpoint.html#상황1-시그마필드-구하기-귀찮아",
    "title": "4wk: 측도론 intro (4)",
    "section": "상황1: 시그마필드 구하기 귀찮아",
    "text": "상황1: 시그마필드 구하기 귀찮아\n(예제1)\n- \\(\\Omega=\\{1,2,3,4\\}\\)이라고 하자. 내가 관심있는 event의 모음은 아래와 같다.\n\\[{\\cal A} = \\{\\{1\\},\\{2\\}\\}\\]\n- 당연히 이러한 이벤트에 대해서만 적절한 확률을 정의하면 좋겠는데, 이는 불가능 하다. 왜냐하면 \\({\\cal A}\\)는 시그마필드가 아니기 때문이다.\n- 따라서 할 수 없이 아래와 같은 방식으로 시그마필드를 구해야 했다.\n\\[{\\cal F} = \\big\\{\\emptyset, \\{1\\}, \\{2\\}, \\{1,2\\}, \\{3,4\\}, \\{1,3,4\\}, \\{2,3,4\\}, \\Omega \\big\\}\\]\n- 이러한 \\({\\cal F}\\)를 구하기는 것은 귀찮은 일인데, 이를 편리하게 해결하기 위해서 \\(\\sigma({\\cal A})\\)라는 기호를 도입하고 이를 “\\(\\{1\\}\\), \\(\\{2\\}\\)를 원소로 가지는 최소한의 \\({\\cal F}\\)” 라고 생각 하기로 하였다. 즉 앞으로는\n\\[\\sigma({\\cal A})\\]\n라고만 써도 위에서 명시한 \\({\\cal F}\\)를 의미한다고 알아서 생각하면 된다는 것이다.\n걱정: 문제는 이러한 논리전개가 항상 가능하냐는 것이다.\n\n귀찮아서 만든 이론1: 걱정할 필요 없다.\n\n\n언제나 \\(\\sigma({\\cal A})\\)라는 표현은 가능하다.\n\n\n즉 \\(\\Omega\\)의 임의의 부분집합에 대하여 우리가 관심있는 집합만 모은 것을 \\({\\cal A}\\)라고 할때,\n\n\n\\({\\cal A}\\)의 모든 원소를 포함하고 시그마필드의 정의를 만족하는 최소한의 시그마필드 \\(\\sigma({\\cal A})\\)는 항상 존재한다.\n\n(예제2)\n\\(\\Omega = \\mathbb{R}\\) 이라고 하자. 이중에서 우리가 관심있는 집합들은 르벡메져로 길이를 명확하게 잴 수 있는 아래와 같은 형태이다.\n\\[[a,b]\\]\n여기에서 \\(a,b \\in \\mathbb{R}\\), \\(a<b\\) 이라고 하자. 따라서 이 경우 \\({\\cal A}\\)를 아래와 같이 설정할 수 있다.\n\\[{\\cal A} = \\big\\{[a,b]: a,b \\in \\mathbb{R}, a<b \\big\\}\\]\n이제 \\(\\sigma({\\cal A})\\)를 상상하자. 이는 \\(\\Omega=\\mathbb{R}\\)에서 잴 수 있는 집합들의 모임이다. 편의상 \\(\\sigma({\\cal A}):={\\cal R}\\)로 정의하자. 여기에서 \\({\\cal R}\\) 상당히 많은 케이스를 포함하는 집합이다. 예를들면 아래와 같은 집합들은 모두 \\({\\cal R}\\)의 원소이다. (즉 아래의 집합은 \\([a,b]\\)를 잴 수 있다고 할때, 당연히 잴 수 있다고 여겨지는 집합들이다.)\n\n\\([0,2)\\)\n\n\\([0,2]\\)를 잴 수 있고 \\([0,5]\\)를 잴 수 있다면, \\([0,5] - [0,2] = [0,2)\\)도 잴수 있어야 한다.\n\n\\(\\{2\\}\\)\n\n\\([0,5]\\) 잴 수 있고, \\([0,2]\\) 잴 수 있고 \\((2,5]\\)도 잴 수 있다. 위에서 \\([0,2)\\)도 잴 수 있었는데, \\([0,2) \\cup (2,5]\\) 도 잴수 있다.\n즉, \\([0,5]\\) - \\([0,2) \\cup (2,5]\\) 를 한 \\(\\{2\\}\\)도 잴 수 있음\n\n\\((0,2)\\)\n\\([0,\\infty)\\), \\((0,\\infty)\\)\n\\((-\\infty,0)\\), \\((-\\infty,0]\\) 위의 여집합\n\\([1,2] \\cup [3,4]\\)\n\\((1,2] \\cup [3,4)\\)\n\\(\\mathbb{N}\\), \\(\\mathbb{Z}\\), \\(\\mathbb{Q}\\)\n\\([0,2] \\cap \\mathbb{Q}\\)\n\n\n사실상 \\({\\cal R}=\\sigma({\\cal A})\\)와 같은 기호가 없다면 \\(\\mathbb{R}\\)에서 잴 수 있는 집합들의 모임은 명시적으로 쓰는 것 자체가 불가능함."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_03_28_4wk_checkpoint.html#상황2-확률-정의하기-귀찮아",
    "href": "posts/Advanved Probability Theory/2023_03_28_4wk_checkpoint.html#상황2-확률-정의하기-귀찮아",
    "title": "4wk: 측도론 intro (4)",
    "section": "상황2: 확률 정의하기 귀찮아",
    "text": "상황2: 확률 정의하기 귀찮아\n(예제1) – motivating EX\n- \\(\\Omega=\\{1,2,3,4\\}\\)이라고 하자. 내가 관심있는 집합의 모음은 아래와 같다.\n\\[{\\cal A} = \\{\\emptyset, \\{1\\},\\{2\\},\\{3,4\\},\\Omega\\}\\]\n- 여기에서 \\({\\cal A}\\)는 시그마필드가 아니다. 따라서 \\({\\cal A}\\)에서는 확률을 정의할 수 없다. 확률을 정의하려면 \\(\\sigma({\\cal A})\\)에서 정의해야 한다.\n- 소망: 그래도 그냥 \\({\\cal A}\\)에서만 확률 비슷한걸5 잘 정의하면 안될까?\n- 희망: 이게 될 것 같다. 예를들면 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)를 아래와 같이 정의하자.\n\n\\(\\tilde{P}(\\emptyset) = 0\\)\n\\(\\tilde{P}(\\{1\\}) = 1/4\\)\n\\(\\tilde{P}(\\{2\\}) = 1/2\\)\n\\(\\tilde{P}(\\{3,4\\}) = 1/4\\)\n\\(\\tilde{P}(\\Omega) = 1\\)\n\n이 정도만 정의해보자. \\(\\tilde{P}\\)는 정의역이 시그마필드가 아니라는 점만 제외하면 확률의 공리 1,2,3을 따른다. 이렇게 함수 \\(\\tilde{P}\\)를 정의하게 되면\n\\[\\sigma({\\cal A}) = \\big\\{\\emptyset, \\{1\\}, \\{2\\}, \\{1,2\\}, \\{3,4\\}, \\{1,3,4\\}, \\{2,3,4\\}, \\Omega \\big\\}\\]\n에서의 확률 \\(P:\\sigma({\\cal A}) \\to [0,1]\\)는 확률 비슷한 함수 \\(\\tilde{P}\\)를 “알아서, 잘, 센스있게” 확장하여 정의할 수 있다. 구체적으로는 아래와 같이 된다.\n\n\n\n\n\\(P\\)\n\\(\\tilde{P}\\)\n\n\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{1\\}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\\(\\{2\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{3,4\\}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\{1,2\\}\\)\n\\(\\frac{3}{4}\\)\nNone\n\n\n\\(\\{1,3,4\\}\\)\n\\(\\frac{1}{2}\\)\nNone\n\n\n\\(\\{2,3,4\\}\\)\n\\(\\frac{3}{4}\\)\nNone\n\n\n\n(예제2) – motivating EX (2)\n- \\(\\Omega=\\{1,2,3,4\\}\\)이라고 하고 \\({\\cal A} = \\{\\emptyset, \\{1\\},\\{2\\}, \\{3,4\\}, \\Omega\\}\\) 라고 하자. 그리고 아래와 같은 \\(\\sigma({\\cal A})\\)를 다시 상상하자.\n\\[\\sigma({\\cal A}) = \\big\\{\\emptyset, \\{1\\}, \\{2\\}, \\{1,2\\}, \\{3,4\\}, \\{1,3,4\\}, \\{2,3,4\\}, \\Omega \\big\\}\\]\n- 위의 시그마필드에서 확률을 예제1과 다른 방식으로 정의할 수 도 있다. 예를들면 아래와 같은 방식으로 정의가능하다.\n\n\n\n\n\\(P_1\\)\n\\(\\tilde{P}_1\\)\n\n\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{1\\}\\)\n\\(\\frac{1}{3}\\)\n\\(\\frac{1}{3}\\)\n\n\n\\(\\{2\\}\\)\n\\(\\frac{1}{3}\\)\n\\(\\frac{1}{3}\\)\n\n\n\\(\\{3,4\\}\\)\n\\(\\frac{1}{3}\\)\n\\(\\frac{1}{3}\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\{1,2\\}\\)\n\\(\\frac{2}{3}\\)\nNone\n\n\n\\(\\{1,3,4\\}\\)\n\\(\\frac{2}{3}\\)\nNone\n\n\n\\(\\{2,3,4\\}\\)\n\\(\\frac{2}{3}\\)\nNone\n\n\n\n또한 아래와 같은 방식도 가능하다.\n\n\n\n\n\\(P_2\\)\n\\(\\tilde{P}_2\\)\n\n\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{1\\}\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{2\\}\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{3,4\\}\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\{1,2\\}\\)\n\\(0\\)\nNone\n\n\n\\(\\{1,3,4\\}\\)\n\\(1\\)\nNone\n\n\n\\(\\{2,3,4\\}\\)\n\\(1\\)\nNone\n\n\n\n- 어떠한 방식으로 정의하든 \\({\\cal A}\\)에서 확률 비슷한 것 \\(\\tilde{P}_1,\\tilde{P}_2\\)를 잘 정의하기만 \\(\\sigma({\\cal A})\\)에서의 확률 \\(P\\)로 적절하게 확장할 수 있다. 심지어 이런 확장은 유일한 듯 하다.\n\n귀찮아서 만든 이론2: 운이 좋다면, \\({\\cal A}\\) 에서 확률의 공리를 만족하는 적당한 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)를 \\((\\Omega, \\sigma({\\cal A}))\\) 에서의 확률측도 \\(P\\)로 업그레이드 할 수 있으며 업그레이드 결과는 유일하다.\n\n(예제3) – 운이 안 좋은 경우\n- \\(\\Omega=\\{1,2,3\\}\\) 이라고 하고 \\({\\cal A} = \\{\\emptyset, \\{1,2\\},\\{2,3\\}, \\Omega\\}\\) 라고 하자.\n- 아래와 같은 확률 비슷한 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)를 정의하자.\n\n\\(\\tilde{P}(\\emptyset) = 0\\)\n\\(\\tilde{P}(\\{1,2\\}) = 0\\)\n\\(\\tilde{P}(\\{2,3\\}) = 0\\)\n\\(\\tilde{P}(\\Omega) = 1\\)\n\n- \\(\\tilde{P}\\)는 분명히 \\({\\cal A}\\)에서 확률의 공리1-3을 만족한다.\n- 하지만 \\(\\sigma({\\cal A})\\)로의 확장은 불가능하다.\n\\(\\{1,2\\} \\cup \\{2,3\\} =\\{1,2,3\\}\\)인데 \\(0+0 \\neq 1\\)\n(예제4) – 운이 안 좋은 경우\n- \\(\\Omega=\\{1,2,3,4\\}\\) 이라고 하고 \\({\\cal A} = \\{\\emptyset, \\{1,2\\},\\{2,3\\}, \\Omega\\}\\) 라고 하자.\n- 아래와 같은 확률 비슷한 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)를 정의하자.\n\n\\(\\tilde{P}(\\emptyset) = 0\\)\n\\(\\tilde{P}(\\{1,2\\}) = 1/2\\)\n\\(\\tilde{P}(\\{2,3\\}) = 1/2\\)\n\\(\\tilde{P}(\\Omega) = 1\\)\n\n- \\(\\tilde{P}\\)는 분명히 \\({\\cal A}\\)에서 확률의 공리1-3을 만족한다.\n- \\(\\sigma({\\cal A})\\)로의 확장도 가능하다. 하지만 유일한 확장을 보장하지 않는다.\n\n\n\n\n\\(P_1\\)\n\\(P_2\\)\n\\(\\tilde{P}\\)\n\n\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{1,2\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{2,3\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\{1\\}\\)\n\\(0\\)\n\\(\\frac{1}{2}\\)\nNone\n\n\n\\(\\{2\\}\\)\n\\(\\frac{1}{2}\\)\n\\(0\\)\nNone\n\n\n\\(\\{3\\}\\)\n\\(0\\)\n\\(\\frac{1}{2}\\)\nNone\n\n\n\\(\\{4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(0\\)\nNone\n\n\n\\(\\{1,3\\}\\)\n\\(0\\)\n\\(1\\)\nNone\n\n\n\\(\\{1,4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\nNone\n\n\n\\(\\{2,4\\}\\)\n\\(1\\)\n\\(0\\)\nNone\n\n\n\\(\\{3,4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\nNone\n\n\n\\(\\{2,3,4\\}\\)\n\\(1\\)\n\\(\\frac{1}{2}\\)\nNone\n\n\n\\(\\{1,3,4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(1\\)\nNone\n\n\n\\(\\{1,2,4\\}\\)\n\\(1\\)\n\\(\\frac{1}{2}\\)\nNone\n\n\n\\(\\{1,2,3\\}\\)\n\\(\\frac{1}{2}\\)\n\\(1\\)\nNone\n\n\n\n(예제5) – 혹시…\n- \\(\\Omega=\\mathbb{R}\\), \\({\\cal A}=\\big\\{[a,b]: a,b \\in \\mathbb{R}, a<b \\big\\}\\) 라고 하자.\n- \\({\\cal A}\\)에서만 측도비슷한 함수 \\(\\tilde{m}([a,b])=b-a\\)를 잘 정의한다면 그것이 \\(\\sigma({\\cal A})\\)에서의 측도 \\(m\\)으로 업그레이드 가능하며, 그 업그레이드 결과는 유일할까?"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-18-7wk-checkpoint.html",
    "href": "posts/Advanved Probability Theory/2023-04-18-7wk-checkpoint.html",
    "title": "7wk: 측도론 (3)",
    "section": "",
    "text": "해당 강의노트는 전북대학교 최규빈교수님 AP2023 자료임"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-18-7wk-checkpoint.html#헷갈리는-표현-infty의-포함",
    "href": "posts/Advanved Probability Theory/2023-04-18-7wk-checkpoint.html#헷갈리는-표현-infty의-포함",
    "title": "7wk: 측도론 (3)",
    "section": "헷갈리는 표현: \\(\\infty\\)의 포함",
    "text": "헷갈리는 표현: \\(\\infty\\)의 포함\n- 자연수집합 \\(\\mathbb{N}\\)은 \\(\\{\\infty\\}\\)를 포함하지 않는다. 마찬가지로 실수집합 \\(\\mathbb{R}\\) 역시 \\(\\{-\\infty\\}, \\{\\infty\\}\\)를 포함하지 않는다. 만약에 이를 포함하고 싶을 경우는 아래와 같이 표현한다.\n\n\\(\\mathbb{R} \\cup \\{-\\infty\\} \\cup \\{\\infty\\} = \\bar{\\mathbb{R}}\\)\n\\(\\mathbb{N} \\cup \\{\\infty\\}\\)\n\n여기에서 \\(\\bar{\\mathbb{R}}\\)은 확장된 실수라고 부르는데 교재에따라 사용하기도 하고 사용하지 않기도 한다.\n- 만약에 \\(\\mathbb{N}\\)이 \\(\\{\\infty\\}\\)를 포함한다면\n\n\\(\\forall n \\in \\mathbb{N}:~ 0<\\frac{1}{n} \\leq 1\\)\n\n와 같은 표현은 불가능할 것이다.\n- 구간에 대한 표현들: 구간에 대한 몇가지 표현을 정리하면 아래와 같다.\n\n\\((-\\infty, b] = \\{x: x\\leq b, ~x,b \\in \\mathbb{R}\\}\\)\n\\((-\\infty, b) = \\{x: x < b,~ x,b \\in \\mathbb{R}\\}\\)\n\n- 구간에 대한 표현 응용: 아래와 같은 표현을 고려하자. (교재의 예제 1.1.8과 비슷한 표현)\n\n\\({\\cal A} = \\{(a,b]: -\\infty \\leq a < b \\leq \\infty\\}\\)\n\n\\({\\cal A}\\)의 원소의 형태는\n\n\\(\\{x: a<x\\leq b,~ a,x,b \\in \\mathbb{R}\\}\\)\n\\(\\{x: a<x,~ a,x \\in \\mathbb{R}\\}\\) 이거는 \\((a,\\infty]\\) 이고 사실상 \\((a,\\infty)\\)\n\\(\\{x: x\\leq b,~ x,b \\in \\mathbb{R}\\}\\)\n\\(\\{x: x \\in \\mathbb{R}\\}\\)\n\n이다.\n\n약간 무식하게 생각하면 \\([-\\infty, b) = (-\\infty,b)\\) 로 해석하면 된다. 즉 \\(\\{-\\infty\\} \\notin [-\\infty,b)\\) 이라는 의미! 보는것 처럼 \\([-\\infty, b)\\)와 같은 표현은 엄청난 혼란을 불러오는 표현이므로 사용을 자제한다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-18-7wk-checkpoint.html#메져의-종류와-성질",
    "href": "posts/Advanved Probability Theory/2023-04-18-7wk-checkpoint.html#메져의-종류와-성질",
    "title": "7wk: 측도론 (3)",
    "section": "메져의 종류와 성질",
    "text": "메져의 종류와 성질\n- 메져의 종류와 성질 요약\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n분류\n\\(m(\\emptyset)=0\\)\n\\(\\sigma\\)-add\n\\(A_i\\uparrow \\Omega\\), \\(m(A_i)<\\infty **\\)\n\\(m(\\Omega)<\\infty *\\)\n\\(m(\\Omega)=1\\)\n\\(.\\)\nmonotone\n\\(\\sigma\\)-subadd\nconti-below\nconti-above\n\n\n\n\nmsr\n\\(O\\)\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(.\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(\\Delta, m(A_1)<\\infty\\)\n\n\n\\(\\sigma\\)-finite-msr\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(.\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(\\Delta, m(A_1)<\\infty\\)\n\n\nfinite-msr\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(X\\)\n\\(.\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\n\nprob-msr\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(.\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\n\n\n\n(*)조건이 (**) 조건보다 세므로 *조건이 성립하면 **조건이 성립해야한다.\n\n- 용어들\n\n\\(\\sigma\\)-additive: \\(m(\\uplus_{i=1}^{\\infty} B_i) = \\sum_{i=1}^{\\infty} m(B_i)\\)\nmonotone: \\(A\\subset B \\Rightarrow m(A) \\subset m(B)\\)\n\\(\\sigma\\)-subadditive: \\(m(\\cup_{i=1}^{\\infty} A_i) \\leq \\sum_{i=1}^{\\infty} m(A_i)\\)\ncontinuous from below: \\(A_i \\uparrow A\\) \\(\\Rightarrow\\) \\(m(\\lim_{n\\to\\infty}A_i)=\\lim_{n\\to\\infty}m(A_i)\\) 측도의 연속성\n\n\\(A_i \\uparrow A\\)가 의미하는것은 \\(A_1 \\subset A_2 \\subset A_3 \\dots \\in \\cal F\\)이고 \\(\\cup_{i=1}^\\infty A_i=A\\)\n\ncontinuous from above: (1) \\(A_i \\downarrow A\\) and (2) \\(m(A_1)<\\infty\\) \\(\\Rightarrow\\) \\(m(\\lim_{n\\to\\infty}A_i)=\\lim_{n\\to\\infty}m(A_i)\\)\n\n\\(A_i \\downarrow A\\)가 의미하는것은 \\(A_1 \\supset A_2 \\supset A_3 \\dots \\in \\cal F\\)이고 \\(\\cap_{i=1}^\\infty A_i=A\\)\n\n\n\\(m\\) is msr on \\((\\Omega, \\cal F)\\), \\(m: \\cal F \\to [0,\\infty]\\) is set funcion with\n\n\n\\(m(\\emptyset)=0\\)\n\n\n\\(B_1, B_2, \\dots \\in {\\cal F}, m(\\cup_{i=1}^\\infty B_i) = \\sum_{i=1}^\\infty m(B_i)\\)\n\n2의 성질을 \\(\\sigma\\)-additivity라고 부른다.\n\n예시: m이 르벡측도이고 \\(\\Omega:[0,1]\\)이라고 정의해보자.\n\\(m({0})=m(\\cap_{i=1}^\\infty (-\\frac{1}{n},\\frac{1}{n}))=m(\\lim_{n\\to\\infty}(-\\frac{1}{n},\\frac{1}{n}))=\\lim_{n\\to\\infty}m((-\\frac{1}{n},\\frac{1}{n}))=\\lim_{n\\to\\infty} \\frac{2}{n}=0\\)\n\n한점의 길이는 0\n\n예시: \\(\\Omega=\\mathbb{N}, {\\cal F}=2^{\\mathbb{N}}\\), m:counting msr on \\((\\Omega, {\\cal F})\\)\n\\(A_n = \\mathbb{N} - \\{1,2,\\dots,n\\}\\)\n\\(A_1 = \\mathbb{N} - \\{1\\}\\)\n\\(A_2 = \\mathbb{N} - \\{1,2\\}\\)\n\\(\\dots\\)\n\\(A_1 \\supset A_2 \\supset \\dots,\\cap_{i=1}^\\infty A_i=\\lim_{n\\to\\infty}A_i = \\emptyset\\)\n\\(m(\\lim_{n\\to\\infty}A_i)=0\\)\n측도의 연속성을 적용시킬수 있다면\n밖으로 뺴가지구 \\(\\lim_{n\\to\\infty}m(A_i)=0\\)로 쓸 수 있고 \\(\\forall n \\in \\mathbb{N}, m(A_n)=\\infty\\)이므로 \\(\\lim_{n\\to\\infty}m(A_i)=0=\\infty\\)가 된다.\n즉,\n\\(m(\\lim_{n\\to\\infty}A_i) \\neq \\lim_{n\\to\\infty}m(A_i)\\)이 되어 이 예제에서는 연속성을 만족시키지 않는다.\n\\(m(A_1)=\\infty\\) 이므로\n- 교재의 언급 (p2. Thm 1.1.1)\n\n\n\n그림1: 메져의 성질 durret p2\n\n\n- \\(\\sigma\\)-finite msr 에 대한 동치조건: \\(m\\)이 \\((\\Omega, {\\cal F})\\)에서의 msr이라면, 아래는 동치이다. (ref: https://en.wikipedia.org/wiki/%CE%A3-finite_measure)\n언어버전\n\nThe set \\(\\Omega\\) can be covered with at most countably many measurable sets with finite measure.\nThe set \\(\\Omega\\) can be covered with at most countably many measurable disjoint sets with finite measure.\nThe set \\(\\Omega\\) can be covered with monotone sequence of measurable sets with finite measure.\n\n수식버전\n\nThere are sets \\(A_1,A_2,\\dots \\in {\\cal F}\\) with \\(m(A_i)<\\infty\\) such that \\(\\cup_{i=1}^{\\infty}A_i=\\Omega\\)\nThere are sets \\(B_1,B_2,\\dots \\in {\\cal F}\\) with \\(m(B_i)<\\infty\\) and \\(B_1,B_2\\dots\\) are disjoints such that \\(\\uplus_{i=1}^{\\infty}B_i=\\Omega\\)\nThere are sets \\(C_1,C_2,\\dots \\in {\\cal F}\\) with \\(m(C_i)<\\infty\\) and $C_1 C_2 $ such that \\(\\cup_{i=1}^{\\infty}C_i=\\Omega\\)"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-18-7wk-checkpoint.html#복습-motivating-ex",
    "href": "posts/Advanved Probability Theory/2023-04-18-7wk-checkpoint.html#복습-motivating-ex",
    "title": "7wk: 측도론 (3)",
    "section": "복습 & Motivating EX",
    "text": "복습 & Motivating EX\n- 귀찮아서 만든 이론2: 운이 좋다면, \\({\\cal A}\\) 에서 확률의 공리를 만족하는 적당한 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)를 \\((\\Omega, \\sigma({\\cal A}))\\) 에서의 확률측도 \\(P\\)로 업그레이드 할 수 있으며 업그레이드 결과는 유일하다.\n- 이론: \\((\\Omega, \\sigma({\\cal A}), P)\\)를 확률공간이라고 하자. 여기에서 \\({\\cal A}\\)는 파이시스템이라고 가정하자. 그렇다면 확률측도 \\(P:\\sigma({\\cal A}) \\to [0,1]\\)의 값은 \\(P: {\\cal A} \\to [0,1]\\)의 값에 의하여 유일하게 결정된다.\n- 이 이론은 확률측도일 경우만 성립하고 측도일 경우는 실패(유일하게 결정되지 않음)했었다.\n(예제1) – 통계학과라서 행복했던 예제\n\\(\\Omega=\\{a,b\\}\\) 이라고 하고 \\({\\cal A} = \\{\\{a\\}\\}\\) 라고 하자. 가측공간 \\((\\Omega,\\sigma({\\cal A}))\\)에서 정의가능한 모든 확률측도 \\(P\\)는 \\({\\cal A}\\)에서의 값으로 유일하게 결정됨을 확인하였다. 하지만 가측공간 \\((\\Omega,\\sigma({\\cal A}))\\)에서 정의가능한 측도 \\(m\\)은 \\({\\cal A}\\)에서의 값으로 유일하게 결정되지 않는다.\n\n\n\n\n\\(m_1\\)\n\\(m_2\\)\n\n\n\n\n\\(\\{a\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{b\\}\\)\n\\(\\frac{1}{2}\\)\n\\(1\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(\\frac{3}{2}\\)\n\n\n\n\\({\\cal A}\\)는 \\(\\pi\\)-system\n- 직관: 그냥 \\({\\cal A}\\)에 \\(\\Omega\\)가 있었다면 되는거 아닌가? 예를들어 아래와 같이 설정한다면?\n\n\n\n\n\\(m_1\\)\n\\(m_2\\)\n\n\n\n\n\\(\\{a\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\Omega\\)\n\\(\\frac{3}{2}\\)\n\\(\\frac{3}{2}\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{b\\}\\)\n\\(1\\)\n\\(1\\)\n\n\n\n\\(m_1(\\{b\\})=m_2(\\{b\\})=1\\) 일 수밖에 없지 않을까?\n- 혹시 아래와 같이 이론을 수정하면 되지 않을까?\n\n\\((\\Omega, \\sigma({\\cal A}))\\)을 잴 수 있는 공간이라고 하고, \\(m\\)을 이 공간에서의 메져라고 하자. 만약에 \\({\\cal A}\\)가 “전체집합을 포함하는 파이시스템” 이라면 메져 \\(m:\\sigma({\\cal A}) \\to [0,1]\\)의 값은 \\(m: {\\cal A} \\to [0,1]\\)의 값에 의하여 유일하게 결정된다. (거의 맞는데 한 조건이 빠져서 틀렸음)\n\n(예제2)\n\\(\\Omega=\\{a,b,c\\}\\) 이라고 하고 \\({\\cal A} = \\{\\{a\\},\\Omega\\}\\) 라고 하자. 여기에서 \\({\\cal A}\\)는 “\\(\\Omega\\)가 포함된 파이시스템”이다. 가측공간 \\((\\Omega,\\sigma({\\cal A}))\\)에서 정의가능한측도 \\(m\\)은 \\({\\cal A}\\)에서의 값으로 유일하게 결정될까?\n(풀이) 아래의 반례가 존재함.\n\n\n\n\n\\(m_1\\)\n\\(m_2\\)\n\n\n\n\n\\(\\{a\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\Omega\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{b\\}\\)\n\\(1\\)\n\\(\\infty\\)\n\n\n\\(\\{c\\}\\)\n\\(\\infty\\)\n\\(5\\)\n\n\n\\(\\{a,b\\}\\)\n\\(\\frac{3}{2}\\)\n\\(\\infty\\)\n\n\n\\(\\{a,c\\}\\)\n\\(\\infty\\)\n\\(\\frac{11}{2}\\)\n\n\n\\(\\{b,c\\}\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\n- 이론: \\((\\Omega, \\sigma({\\cal A}))\\)을 잴 수 있는 공간이라고 하고, \\(m\\)을 이 공간에서의 유한측도라고 하자. 그리고 \\({\\cal A}\\)는 전제집합을 포함하는 파이시스템이라고 하자. 그렇다면 메져 \\(m:\\sigma({\\cal A}) \\to [0,M]\\)의 값은 \\(m: {\\cal A} \\to [0,M]\\)의 값에 의하여 유일하게 결정된다. (단, \\(M=m(\\Omega)<\\infty\\))\n좀 더 약한 조건이 있었으면 좋겠따.\n(예제3) – \\({\\cal A}\\)가 \\(\\Omega\\)를 포함하지 않는데, 메져가 유일하게 결정될 것 같은 예제\n\\(\\Omega = \\mathbb{Z}\\) 이라고 하자. \\(\\Omega\\)의 부분집합들로 이루어진 집합열 \\(A_1,A_2,\\dots\\) 를 아래와 같이 정의하자.\n\n\\(A_{1} = [-\\frac{1}{2}, \\frac{2}{2}] \\cap \\mathbb{Z} = \\{0, 1\\}\\)\n\\(A_{2} = [-\\frac{2}{2}, \\frac{3}{2}] \\cap \\mathbb{Z} = \\{-1, 0, 1\\}\\)\n\\(A_{3} = [-\\frac{3}{2}, \\frac{4}{2}] \\cap \\mathbb{Z} = \\{-1, 0, 1, 2\\}\\)\n\\(A_{4} = [-\\frac{4}{2}, \\frac{5}{2}] \\cap \\mathbb{Z} = \\{-2, -1, 0, 1, 2\\}\\)\n\\(A_{5} = [-\\frac{5}{2}, \\frac{6}{2}] \\cap \\mathbb{Z} = \\{-2, -1, 0, 1, 2, 3\\}\\)\n\\(\\dots\\)\n\n\\(A\\)는 \\(\\pi\\)-system. 그런데 \\(\\Omega\\)는 포함하지 않는다. 하지만, \\(U_{i=1}^\\infty A_i= \\mathbb{Z}\\) 즉, \\(A_i \\uparrow \\mathbb{Z}\\)라고 볼 수 있다. 이 예제에서\n관심있는 집합들의 모임은 \\({\\cal A}=\\{A_n:n \\in \\mathbb{N}\\}\\)로 정의하자. 가측공간 \\((\\Omega,\\sigma({\\cal A}))\\)에서 정의가능한 측도 \\(m\\)은 \\({\\cal A}\\)의 값으로 유일하게 결정될까?\n(관찰)\n풀이에 앞서서 아래의 사실을 관찰해보자.\n\n\\({\\cal A}\\)는 파이시스템이다.\n집합열 \\(A_n\\)의 극한은 \\(\\Omega\\)이다. 집합열 \\(A_n\\)은 증가하는 수열이므로 이 경우 \\(A_n \\uparrow \\Omega\\)라고 표현할 수 있다.\n모든 \\(A_n\\)이 \\({\\cal A}\\)의 멤버라고 했으나 \\(A_n\\)의 극한 \\(\\Omega\\)가 \\({\\cal A}\\)의 멤버라고 한 적은 없다. 따라서 \\({\\cal A}\\)는 전체집합을 포함하지는 않는 파이시스템이다.\n\n(풀이)\n가측공간 \\((\\Omega,\\sigma({\\cal A}))\\)에서 정의가능한 측도 \\(m\\)은 \\({\\cal A}\\)의 값으로 유일하게 결정하는 것이 가능할 것 같다. (실제로 가능해) 왜냐하면\n\n\\(m(A_1),m(A_2), m(A_3) \\dots\\) 의 값이 결정 \\(\\Rightarrow\\) \\(m(\\{0,1\\})\\), \\(m(\\{-1\\})\\), \\(m(\\{2\\})\\), \\(\\dots\\) 의 값이 결정\n\n이므로, 0과 1을 제외한 \\(\\mathbb{Z}\\)의 모든 원소의 길이가 유일하게 결정되니까.\n생각의 시간\n아래의 이론을 다시 관찰하자.\n\n이론: \\((\\Omega, \\sigma({\\cal A}))\\)을 잴 수 있는 공간이라고 하고, \\(m\\)을 이 공간에서의 유한측도라고 하자. 그리고 \\({\\cal A}\\)는 전체집합을 포함하는 파이시스템이라고 하자. 그렇다면 메져 \\(m:\\sigma({\\cal A}) \\to [0,M]\\)의 값은 \\(m: {\\cal A} \\to [0,M]\\)의 값에 의하여 유일하게 결정된다. (단, \\(M=m(\\Omega)<\\infty\\))\n\n(의문1)\n\\({\\cal A}\\)가 꼭 전체집합을 포함할 필요는 없어보인다. 즉 조건 \\(\\Omega \\in {\\cal A}\\)는 굳이 필요 없어보인다. 이 조건은 더 약한 아래의 조건으로 대치가능하다.\n\n\\(\\exists A_1,A_2,\\dots \\in {\\cal A}\\) such that \\(A_i \\uparrow \\Omega\\) (*)\n위의 내용은 \\(A_1 \\subset A_2 \\subset \\dots \\cup_{i=1}^{\\infty} A_i = \\Omega\\)\n\n만약에 \\(\\Omega \\in {\\cal A}\\)인 경우는 \\(A_1=\\Omega\\)로 잡으면 위 조건이 그냥 성립한다. 따라서 위의 조건은 \\(\\Omega \\in {\\cal A}\\) 보다 약한 조건이다. 그리고 심지어 위의 조건은 다시 아래의 더 약한 조건으로 바꿀 수 있다.\n\n\\(\\exists A_1,A_2,\\dots \\in {\\cal A}\\) such that \\(\\cup_{i=1}^{\\infty} A_i = \\Omega\\) (**)\n\n\n(*)과 (**)는 동치이다.\n\n\n\\(\\exists B_1, B_2, \\dots \\in {\\cal A}\\) such that \\(\\uplus_{i=1}^\\infty B_i = \\Omega\\) 이것도 위의 (*)와 (**)와 동치\n\n(의문2)\n심지어 \\(m(\\Omega) = \\infty\\) 이어도 상관없다.1 이 예제에서\n\n\\(m(\\{0,1\\})=2\\)\n\\(m(\\{-1\\})=1\\)\n\\(m(\\{2\\})=1\\)\n\\(\\dots\\)\n\n이라고 하면 \\(m\\)은 잴 수 있는 공간 \\((\\Omega,\\sigma({\\cal A}))\\)에서의 카운팅메져가 되고, 그 \\(m\\)은 \\(A \\in {\\cal A}\\)에서의 값으로 유일하게 결정된다. 문제가 생길만한 것은\n\n\\(m(\\{0,1\\})=2\\)\n\\(m(\\{-1\\})=1\\)\n\\(m(\\{2\\})=\\infty\\) <– 이러면 곤란\n\\(\\dots\\)\n\n와 같은 경우이므로, 이 경우만 제약하면 된다. 즉 \\(m\\)이 시그마유한측도라고 제한하면 될 것 같다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-18-7wk-checkpoint.html#state",
    "href": "posts/Advanved Probability Theory/2023-04-18-7wk-checkpoint.html#state",
    "title": "7wk: 측도론 (3)",
    "section": "state",
    "text": "state\n- Thm:\n\\((\\Omega, \\sigma({\\cal A}),m)\\)을 시그마유한측도공간(\\(\\sigma\\)-finite measure space)이라고 하자. \\({\\cal A}\\)은 아래를 만족하는 파이시스템이라고 하자.\n\n\\(\\exists A_1,A_2,\\dots \\in {\\cal A}\\) such that \\(\\cup_{i=1}^{\\infty} A_i = \\Omega\\)\n\\(\\forall i \\in \\mathbb{N}:~ m(A_i) <\\infty\\)\n\n그렇다면 메져 \\(m:\\sigma({\\cal A}) \\to [0,\\infty]\\)의 값은 \\(m: {\\cal A} \\to [0,\\infty]\\)의 값에 의하여 유일하게 결정된다.\n\n조건 1,2는 결국 \\(m\\)을 시그마유한측도로 만들어주는 그 집합열이 \\(\\sigma({\\cal A})-{\\cal A}\\)가 아니라 \\({\\cal A}\\)에 있어야 한다는 의미임."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-18-7wk-checkpoint.html#증명",
    "href": "posts/Advanved Probability Theory/2023-04-18-7wk-checkpoint.html#증명",
    "title": "7wk: 측도론 (3)",
    "section": "증명",
    "text": "증명\n- 노트: supp_7wk.pdf\n- 교재의 증명: 교재의 증명은 좀 더 강한 조건에서 했음. (“\\(A_1,A_2,\\dots, {\\cal A}\\) with \\(m(A_i)<\\infty\\) such that \\(m(A_i)<\\infty\\)” 를 가정함.)\n\n\n\n그림2: 카라데오도리 확장정리의 유일성 part 증명, durret p457-8"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-18-7wk-checkpoint.html#state-1",
    "href": "posts/Advanved Probability Theory/2023-04-18-7wk-checkpoint.html#state-1",
    "title": "7wk: 측도론 (3)",
    "section": "state",
    "text": "state\n- Thm: \\({\\cal A}\\)가 \\(\\Omega\\)에 대한 semiring이라고 하자. 함수 \\(\\tilde{m}: {\\cal A} \\to [0,\\infty]\\)가\n\n\\(\\tilde{m}(\\emptyset)=0\\)\n\\(\\tilde{m}(\\uplus_{i=1}^{n} B_i)=\\sum_{i=1}^{n}\\tilde{m}(B_i)\\) additivity 조건\n\\(\\tilde{m}(\\cup_{i=1}^{\\infty} A_i) \\leq \\sum_{i=1}^{\\infty}\\tilde{m}(A_i)\\) \\(\\sigma\\)-subadd 조건\n\n\n\\(\\sigma\\)-additivity조건은 위의 2,3번으로 쪼개졌다고 생각하자.\n\n\n\\(\\exists A_1,A_2 \\dots \\in {\\cal A}\\) with \\(\\tilde{m}(A_i)<\\infty\\) such that \\(\\cup_{i=1}^{\\infty}A_i = \\Omega\\)\n\n\n1~3번은\\(\\tilde{m}\\)가 \\({\\cal A}\\)에서 msr 비슷, 4번까지하면 \\(\\tilde{m}\\)가 \\({\\cal A}\\)에서 \\(\\sigma\\)-finte msr 비슷\nadd \\(\\rightarrow\\) subadd, \\(\\sigma\\)-add \\(\\rightarrow\\) \\(\\sigma\\)-subadd\n\n를 만족한다면 \\(\\tilde{m}\\)은 \\((\\Omega,\\sigma({\\cal A})\\)에서의 측도 \\(m\\)으로 업그레이드 가능하며, 이 업그레이드 결과는 유일하다.\n\nsemiring이라는 것은 파이시스템이라는 것보다 강한 조건\n\n\n이 결과를 ver1로 생각하자.\n\n- 교재의 state (ver2, ver3)\n\n\n\n그림3: 카라데오도리 확장저정리 ver2, durret p456\n\n\n\nver1과의 비교: \\({\\cal A}\\)가 알지브라라는 것은 세미링보다 훨씬 강한 조건이다. 또한 measure on an algebra \\({\\cal A}\\)란 것은 1,2,3 조건을 다 합친것 보다 강한 조건이다. \\(\\sigma\\)-finite이라는 조건은 \\({\\cal A}\\)의 차이를 제외하면 동일하다.\n\n\n\n\n그림4: 카라데오도리 확장정리 ver3, durret p5\n\n\n\nver1과의 비교: \\({\\cal A}\\)가 세미알지브라라는 조건은 세미링보다 강한 조건이다. (i), (ii)의 \\({\\cal A}\\)의 차이만 있을 뿐 거의 동일하다. 4의 조건도 \\({\\cal A}\\)의 차이를 제외하고는 동일하다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023-04-18-7wk-checkpoint.html#예제-3월28일-4wk-예제들",
    "href": "posts/Advanved Probability Theory/2023-04-18-7wk-checkpoint.html#예제-3월28일-4wk-예제들",
    "title": "7wk: 측도론 (3)",
    "section": "예제: 3월28일 (4wk) 예제들",
    "text": "예제: 3월28일 (4wk) 예제들\n(예제1) – motivating EX\n- \\(\\Omega=\\{1,2,3,4\\}\\)이라고 하자. 내가 관심있는 집합의 모음은 아래와 같다.\n\\[{\\cal A} = \\{\\emptyset, \\{1\\},\\{2\\},\\{3,4\\},\\Omega\\}\\]\n- 소망: 그래도 그냥 \\({\\cal A}\\)에서만 확률 비슷한 함수 \\(\\tilde{P}\\)를 잘 정의하면 \\((\\Omega,\\sigma({\\cal A}))\\)에서의 확률측도로 업그레이드 가능하고 업그레이드 결과가 유일할까?\n\n\\(\\tilde{P}(\\emptyset) = 0\\)\n\\(\\tilde{P}(\\{1\\}) = 1/4\\)\n\\(\\tilde{P}(\\{2\\}) = 1/2\\)\n\\(\\tilde{P}(\\{3,4\\}) = 1/4\\)\n\\(\\tilde{P}(\\Omega) = 1\\)\n\n- 조건체크\n\n\\({\\cal A}\\)는 세미알지브라(그러므로 세미링)이다.\n\\({\\cal A}\\)는 전체집합을 포함하고 있으며 \\({\\tilde P}(\\Omega)=1\\)이다. \\(\\Rightarrow\\) 조건 (4)가 만족.\n\\({\\tilde P}\\)는 (1) \\(\\tilde{P}(\\emptyset)=0\\) 이고 (2) add 를 만족하며 (3) \\(\\sigma\\)-subadd 를 만족한다.\n\n\n참고: 이 예제의 경우 \\(|\\Omega|<\\infty\\) 이므로 \\(\\sigma\\)-subadd 는 subadd 와 같은 성질이다. 그리고 add 는 subadd를 imply 하므로 사실상 (2) 만 체크하면 끝난다.2\n\n(예제2) – motivating EX (2)\n- \\(\\Omega=\\{1,2,3,4\\}\\)이라고 하고 \\({\\cal A} = \\{\\emptyset, \\{1\\},\\{2\\}, \\{3,4\\}, \\Omega\\}\\) 라고 하자. 그리고 아래와 같은 \\(\\sigma({\\cal A})\\)를 다시 상상하자.\n\\[\\sigma({\\cal A}) = \\big\\{\\emptyset, \\{1\\}, \\{2\\}, \\{1,2\\}, \\{3,4\\}, \\{1,3,4\\}, \\{2,3,4\\}, \\Omega \\big\\}\\]\n- 위의 시그마필드에서 확률을 예제1과 다른 방식으로 정의할 수 도 있다. 예를들면 아래와 같은 방식으로 정의가능하다.\n\n\n\n\n\\(P_1\\)\n\\(\\tilde{P}_1\\)\n\n\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{1\\}\\)\n\\(\\frac{1}{3}\\)\n\\(\\frac{1}{3}\\)\n\n\n\\(\\{2\\}\\)\n\\(\\frac{1}{3}\\)\n\\(\\frac{1}{3}\\)\n\n\n\\(\\{3,4\\}\\)\n\\(\\frac{1}{3}\\)\n\\(\\frac{1}{3}\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\{1,2\\}\\)\n\\(\\frac{2}{3}\\)\nNone\n\n\n\\(\\{1,3,4\\}\\)\n\\(\\frac{2}{3}\\)\nNone\n\n\n\\(\\{2,3,4\\}\\)\n\\(\\frac{2}{3}\\)\nNone\n\n\n\n또한 아래와 같은 방식도 가능하다.\n\n\n\n\n\\(P_2\\)\n\\(\\tilde{P}_2\\)\n\n\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{1\\}\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{2\\}\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{3,4\\}\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\{1,2\\}\\)\n\\(0\\)\nNone\n\n\n\\(\\{1,3,4\\}\\)\n\\(1\\)\nNone\n\n\n\\(\\{2,3,4\\}\\)\n\\(1\\)\nNone\n\n\n\n어떠한 방식으로 정의하든 \\({\\cal A}\\)에서 확률 비슷한 것 \\(\\tilde{P}_1,\\tilde{P}_2\\)를 잘 정의하기만 \\(\\sigma({\\cal A})\\)에서의 확률 \\(P\\)로 적절하게 확장할 수 있다. 심지어 이런 확장은 유일한 듯 하다.\n- 당연함. 예제1과 동일하게 \\(\\tilde{P_1}\\)과 \\(\\tilde{P_2}\\)가 add 성질만 만족한다는 사실을 체크하면 끝난다.\n(예제3) – 운이 안 좋은 경우\n- \\(\\Omega=\\{1,2,3\\}\\) 이라고 하고 \\({\\cal A} = \\{\\emptyset, \\{1,2\\},\\{2,3\\}, \\Omega\\}\\) 라고 하자.\n- 아래와 같은 확률 비슷한 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)를 정의하자.\n\n\\(\\tilde{P}(\\emptyset) = 0\\)\n\\(\\tilde{P}(\\{1,2\\}) = 0\\)\n\\(\\tilde{P}(\\{2,3\\}) = 0\\)\n\\(\\tilde{P}(\\Omega) = 1\\)\n\n- 체크: 일단 \\({\\cal A}\\)는 세미링이 아니다. 따라서 확장 불가능. 세미링이 맞다고 하여도 subadd가 성립하지 않는다.\n(예제4) – 운이 안 좋은 경우\n- \\(\\Omega=\\{1,2,3,4\\}\\) 이라고 하고 \\({\\cal A} = \\{\\emptyset, \\{1,2\\},\\{2,3\\}, \\Omega\\}\\) 라고 하자.\n- 아래와 같은 확률 비슷한 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)를 정의하자.\n\n\\(\\tilde{P}(\\emptyset) = 0\\)\n\\(\\tilde{P}(\\{1,2\\}) = 1/2\\)\n\\(\\tilde{P}(\\{2,3\\}) = 1/2\\)\n\\(\\tilde{P}(\\Omega) = 1\\)\n\n- 체크: \\(\\tilde{P}\\)는 괜찮게 정의되었다. (1)-(4)가 모두 성립한다. (위의 예제와는 다르게 subadd 역시 성립함!!) 하지만 \\({\\cal A}\\)가 세미링이 아니어서 탈락."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html",
    "href": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html",
    "title": "3wk: 측도론 intro (3)",
    "section": "",
    "text": "해당 강의노트는 전북대학교 최규빈교수님 AP2023 자료임"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제1-잴-수-있는-집합의-모임",
    "href": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제1-잴-수-있는-집합의-모임",
    "title": "3wk: 측도론 intro (3)",
    "section": "(예제1) – 잴 수 있는 집합의 모임",
    "text": "(예제1) – 잴 수 있는 집합의 모임\n\\(\\Omega=\\{H,T\\}\\)라고 하자. 아래집합들은 모두 확률을 정의할 수 있는 집합들이다.\n\\[\\emptyset, \\{H\\}, \\{T\\}, \\Omega\\]\n따라서 \\({\\cal F}\\)을 아래와 같이 정의한다면 묶음 \\({\\cal F}\\)가 합리적일 것이다.\n\\[{\\cal F}=\\big\\{\\emptyset, \\{H\\}, \\{T\\}, \\Omega\\big\\}\\]\n\n이때 \\({\\cal F}\\)는 집합들의 집합인데, 이러한 집합을 collection 이라고 한다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제2-집합-a를-잴-수-있다면-집합-ac도-잴-수-있어",
    "href": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제2-집합-a를-잴-수-있다면-집합-ac도-잴-수-있어",
    "title": "3wk: 측도론 intro (3)",
    "section": "(예제2) – 집합 \\(A\\)를 잴 수 있다면, 집합 \\(A^c\\)도 잴 수 있어~",
    "text": "(예제2) – 집합 \\(A\\)를 잴 수 있다면, 집합 \\(A^c\\)도 잴 수 있어~\n\\(\\Omega=\\{H,T\\}\\)라고 하자. \\({\\cal F}\\)을 아래와 같이 정의한다면 묶음 \\({\\cal F}\\)는 합리적이지 않다.\n\\[{\\cal F}=\\big\\{\\emptyset, \\{H\\}, \\Omega\\big\\}\\]\n(해설1)\n이러한 묶음이 의미하는건 “앞면이 나올 확률은 모순없이 정의할 수 있지만, 뒷면이 나오는 확률은 모순없이 정의하는게 불가능해~” 라는 뜻이다. 그런데 뒷면이 나올 확률은 “1-앞면이 나올 확률” 로 모순없이 정의할 수 있으므로 “앞면이 나올 확률이 모순없이 정의되면서” 동시에 “뒷면이 나올 확률이 모순없이 정의되지 않는” 상황은 없다.\n(해설2)\n\\(\\Omega\\)의 어떠한 부분집합 \\(A\\)에 확률이 모순없이 정의된다면 그 집합의 여집합인 \\(A^c\\)에 대하여서도 확률이 모순없이 정의되어야 한다.\n\\(\\Leftrightarrow\\) \\(\\forall A \\subset {\\Omega}: ~ A \\in {\\cal F} \\Rightarrow A^c \\in {\\cal F}\\)"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제3-전체집합이-잴-수-있는-집합이니까-공집합도-잴-수-있는-집합이야",
    "href": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제3-전체집합이-잴-수-있는-집합이니까-공집합도-잴-수-있는-집합이야",
    "title": "3wk: 측도론 intro (3)",
    "section": "(예제3) – 전체집합이 잴 수 있는 집합이니까 공집합도 잴 수 있는 집합이야",
    "text": "(예제3) – 전체집합이 잴 수 있는 집합이니까 공집합도 잴 수 있는 집합이야\n\\(\\Omega=\\{H,T\\}\\)라고 하자. \\({\\cal F}\\)를 아래와 같이 정의한다면 묶음 \\({\\cal F}\\)는 합리적이지 않다.\n\\[{\\cal F}=\\big\\{ \\{H\\}, \\{T\\}, \\Omega\\big\\}\\]\n(해설)\n전체집합의 확률은 \\(P(\\Omega)=1\\)로 정의할 수 있다. 그런데 전체집합의 여집합인 공집합의 확률을 정의할 수 없는건 말이 안되므로 공집합은 \\(\\cal F\\)에 포함되어야 한다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제4-원소의-수가-유한한-경우-cal-f2omega은-잴-수-있는-집합의-모임이야.",
    "href": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제4-원소의-수가-유한한-경우-cal-f2omega은-잴-수-있는-집합의-모임이야.",
    "title": "3wk: 측도론 intro (3)",
    "section": "(예제4) – 원소의 수가 유한한 경우 \\({\\cal F}=2^\\Omega\\)은 잴 수 있는 집합의 모임이야.",
    "text": "(예제4) – 원소의 수가 유한한 경우 \\({\\cal F}=2^\\Omega\\)은 잴 수 있는 집합의 모임이야.\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\)이라고 하자. \\({\\cal F}\\)을 아래와 같이 정의한다고 하자. 이러한 묶음은 \\({\\cal F}\\)은 합리적이다.\n\\[{\\cal F}=\\text{all subset of $\\Omega$}= 2^\\Omega = \\big\\{ \\emptyset, \\{1\\}, \\{2\\}, \\dots, \\{6\\}, \\dots, \\{1,2,3,4,5\\} \\dots \\Omega\\big\\}\\]\n(해설)\n\\(\\Omega\\)의 모든 부분집합에 대하여 확률을 모순없이 정의할 수 있다. 예를들면\n\n\\(P(\\Omega)=1\\), \\(P(\\emptyset)=0\\)\n\\(P(\\{1\\})=\\frac{1}{6}\\)\n\\(P(\\{1,2,4\\})=\\frac{3}{6}\\)\n\\(P(\\{2,3,4,5,6\\})=\\frac{5}{6}\\)\n\\(\\dots\\)\n\n이런식으로 정의할 수 있다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제5-동일한-omega에-대하여-잴-수-있는-집합의-모임-cal-f는-유니크하지-않음.-하나만-있는게-아니다",
    "href": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제5-동일한-omega에-대하여-잴-수-있는-집합의-모임-cal-f는-유니크하지-않음.-하나만-있는게-아니다",
    "title": "3wk: 측도론 intro (3)",
    "section": "(예제5) – 동일한 \\(\\Omega\\)에 대하여 잴 수 있는 집합의 모임 \\({\\cal F}\\)는 유니크하지 않음. (하나만 있는게 아니다)",
    "text": "(예제5) – 동일한 \\(\\Omega\\)에 대하여 잴 수 있는 집합의 모임 \\({\\cal F}\\)는 유니크하지 않음. (하나만 있는게 아니다)\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\)이라고 하자. \\({\\cal F}\\)을 아래와 같이 정의한다고 하자. 이러한 묶음 \\({\\cal F}\\)는 합리적이다.\n\\[{\\cal F}=\\big\\{\\emptyset, \\{6\\}, \\{1,2,3,4,5\\},\\Omega \\big\\}\\]\n(해설)\n어떠한 특수한 상황을 가정하자. 주사위를 던져야하는데 6이 나오면 살수 있고 6이 나오지 않으면 죽는다고 하자. 따라서 던지는 사람 입장에서는 주사위를 던져서 6이 나오는지 안나오는지만 관심있을 것이다. 이 사람의 머리속에서 순간적으로 떠오르는 확률들은 아래와 같다.4\n\n살수있다 => 1/6\n죽는다 => 5/6\n살거나 죽는다 => 1\n살지도 죽지도 않는다 => 0\n\n이러한 확률은 합리적이다. 즉 아래의 집합들만 확률을 정의한다고 해도, 확률을 잘 정의할 수 있을 것 같다.\n\\[\\emptyset, \\{6\\}, \\{1,2,3,4,5\\}, \\Omega\\]"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제6-omega를-어떠한-사건의-집합으로-보느냐에-따라서-cal-f를-달리-구성할-수-있다.",
    "href": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제6-omega를-어떠한-사건의-집합으로-보느냐에-따라서-cal-f를-달리-구성할-수-있다.",
    "title": "3wk: 측도론 intro (3)",
    "section": "(예제6) – \\(\\Omega\\)를 어떠한 사건의 집합으로 보느냐에 따라서 \\({\\cal F}\\)를 달리 구성할 수 있다.",
    "text": "(예제6) – \\(\\Omega\\)를 어떠한 사건의 집합으로 보느냐에 따라서 \\({\\cal F}\\)를 달리 구성할 수 있다.\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\)이라고 하자. \\({\\cal F}\\)을 아래와 같이 정의한다고 하자. 이러한 묶음 \\({\\cal F}\\)는 합리적이다.\n\\[{\\cal F}=\\big\\{\\emptyset, \\{1,3,5\\}, \\{2,4,6\\},\\Omega \\big\\}\\]\n(해설)\n전체사건을 “주사위를 던져서 짝이 나오는 사건”, “주사위를 던져서 홀이 나오는 사건” 정도만 구분하겠다는 의미"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제7-ain-cal-f-rightarrow-ac-in-cal-f",
    "href": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제7-ain-cal-f-rightarrow-ac-in-cal-f",
    "title": "3wk: 측도론 intro (3)",
    "section": "(예제7) – \\(A\\in {\\cal F} \\Rightarrow A^c \\in {\\cal F}\\)",
    "text": "(예제7) – \\(A\\in {\\cal F} \\Rightarrow A^c \\in {\\cal F}\\)\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\)이라고 하자. \\({\\cal F}\\)을 아래와 같이 정의한다고 하자. 이러한 묶음 \\({\\cal F}\\)는 합리적이지 않다.\n\\[{\\cal F}=\\big\\{\\emptyset, \\{1,3,5\\}, \\Omega \\big\\}\\]\n(해설)\n“주사위를 던져서 홀수가 나올 사건”에 대한 확률을 정의할 수 있는데, 짝수가 나올 사건에 대한 확률을 정의할 수 없다는건 말이 안되는 소리임."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제8-trivial-sigma-field-공집합과-전체집합만-포함된-sigma-field",
    "href": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제8-trivial-sigma-field-공집합과-전체집합만-포함된-sigma-field",
    "title": "3wk: 측도론 intro (3)",
    "section": "(예제8) – trivial \\(\\sigma\\)-field : 공집합과 전체집합만 포함된 \\(\\sigma\\)-field",
    "text": "(예제8) – trivial \\(\\sigma\\)-field : 공집합과 전체집합만 포함된 \\(\\sigma\\)-field\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\)이라고 하자. \\({\\cal F}\\)을 아래와 같이 정의한다고 하자. 이러한 묶음 \\({\\cal F}\\)는 합리적이다.\n\\[{\\cal F}=\\{\\emptyset, \\Omega \\}\\]\n(해설)\n아예 이렇게 잡으면 모순이 일어나진 않음. (쓸모가 없겠지)"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제9-서로소인-두-집합의-합-포함관계에-있는-집합의-차",
    "href": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제9-서로소인-두-집합의-합-포함관계에-있는-집합의-차",
    "title": "3wk: 측도론 intro (3)",
    "section": "(예제9) – 서로소인 두 집합의 합, 포함관계에 있는 집합의 차",
    "text": "(예제9) – 서로소인 두 집합의 합, 포함관계에 있는 집합의 차\n\\(\\Omega=\\{1,2,3,4\\}\\)이라고 하자. 어떠한 필요에 따라서 1이 나올 확률과 2가 나올 확률에만 관심이 있고 나머지는 별로 관심이 없다고 하자. 그래서 \\({\\cal F}\\)을 아래와 같이 정의했다고 하자. 이러한 묶음 \\({\\cal F}\\)는 합리적이지 않다.\n\\[{\\cal F}=\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega \\}\\]\n(해설1)\n생각해보니까 \\(\\{2\\}\\)는 \\(\\{2,3,4\\}\\)의 부분집합이다. 그런데 \\(P(\\{2\\})\\)와 \\(P(\\{2,3,4\\})\\)를 각각 정의할 수 있는데\n\\[P(\\{2,3,4\\} - \\{2\\}) = P(\\{3,4\\})\\]\n를 정의할 수 없는건 말이 안된다. 따라서 \\({\\cal F}\\)를 아래와 같이 수정해야 한다.\n\\[{\\cal F}=\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega, \\{3,4\\}, \\{1,2\\} \\}\\]\n(해설2)\n\\({\\cal F}\\)은 전체집합과 공집합을 포함하고 여집합에 닫혀있으므로 언뜻 생각해보면 합리적인듯 보이지만 그렇지 않다. 왜냐하면 \\(\\{1,2\\}\\)이 빠졌기 때문이다. 1이 나올 확률 \\(P(\\{1\\})\\)와 2가 나올 확률 \\(P(\\{2\\})\\)를 각각 정의할 수 있는데, 1 또는 2가 나올 확률 \\(P(\\{1,2\\})\\)을 정의할 때 모순이 발생한다는 것은 합리적이지 못하다. 왜냐하면 \\(\\{1\\} \\cap \\{2\\} = \\emptyset\\) 이므로\n\\[P(\\{1\\} \\cup \\{2\\})=P(\\{1\\}) + P(\\{2\\})\\]\n와 같이 정의가능하기 때문이다. 따라서 집합이 아래와 같이 수정되어야 한다.\n\\[{\\cal F}=\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega, \\{1,2\\}, \\{3,4\\} \\}\\]\n(해설3)\n\\(\\Omega\\)의 어떠한 두 부분집합 \\(A\\), \\(B\\)가 서로소라고 상상하자. 집합 \\(A\\), \\(B\\)에 대한 확률이 각각 무모순으로 정의된다면, 집합 \\(A\\cup B\\)에 대한 확률도 무모순으로 정의되어야 한다.\n\\(\\Leftrightarrow\\) \\(\\forall A,B \\subset \\Omega\\) such that \\(A \\cap B =\\emptyset\\): \\(A,B \\in {\\cal F} \\Rightarrow A \\cup B \\in {\\cal F}\\)\n또한 \\(\\Omega\\)의 임의의 두 부분집합이 \\(A \\subset B\\)와 같은 포함관계가 성립할때, 집합 \\(A\\), \\(B\\)에 대한 확률이 각각 무모순으로 정의된다면, 집합 \\(B-A\\)에 대한 확률로 무모순으로 정의되어야 한다.\n\\(\\Leftrightarrow\\) \\(\\forall A,B \\subset \\Omega\\) such that \\(A \\subset B\\): \\(A,B \\in {\\cal F} \\Rightarrow B-A \\in {\\cal F}\\)"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제10-cal-a12-일때-sigmacal-a-를-구하는-문제",
    "href": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제10-cal-a12-일때-sigmacal-a-를-구하는-문제",
    "title": "3wk: 측도론 intro (3)",
    "section": "(예제10) – \\({\\cal A}=\\{\\{1\\},\\{2\\}\\}\\) 일때, \\(\\sigma({\\cal A})\\) 를 구하는 문제",
    "text": "(예제10) – \\({\\cal A}=\\{\\{1\\},\\{2\\}\\}\\) 일때, \\(\\sigma({\\cal A})\\) 를 구하는 문제\n\\(\\Omega=\\{1,2,3,4\\}\\)이라고 하자. 내가 관심이 있는 확률은 \\(P(\\{1\\})\\), \\(P(\\{2\\})\\) 밖에 없다고 하자. 이러한 확률들이 무모순으로 정의되기 위한 최소한의 \\({\\cal F}\\)를 정의하라.\n(해설) – 좀 귀찮네..?\n0차수정: \\({\\cal A} = \\big\\{\\{1\\}, \\{2\\}\\big\\}\\)\n1차수정: \\(\\big\\{\\emptyset, \\{1\\}, \\{2\\}, \\Omega \\big\\}\\)\n2차수정: \\(\\big\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega \\big\\}\\)\n3차수정: \\(\\big\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega, \\{1,2\\}, \\{3,4\\} \\big\\}\\)\n\n사실 우리가 관심 있는건 \\({\\cal A} = \\{ \\{1\\}, \\{2\\} \\}\\) 뿐 이었음. 그런데 뭔가 \\(P(\\{1\\})\\)와 \\(P(\\{2\\})\\)를 합리적으로 정의하기 위해서 필연적으로 발생하는 어떠한 집합들을 모두 생각하는건 매우 피곤하고 귀찮은 일임. 그래서 “아 모르겠고, \\(\\{1\\}\\) 와 \\(\\{2\\}\\)를 포함하고 확률의 뜻에 모순되지 않게 만드는 최소한의 \\({\\cal F}\\)가 있을텐데, 거기서만 확률을 정의할래!” 라고 쉽게 생각하고 싶은 사람들이 생김. 그러한 공간을 \\(\\sigma({\\cal A})\\)라는 기호로 약속하고 smallest \\(\\sigma\\)-field containing \\({\\cal A}\\) 라는 용어로 부름.\n\n\n\\(\\sigma({\\cal A})\\) 가 1. 존재하는가? -> 존재한다. 2. 유니크한가? -> smallest가 존재한다는 것은 유니크하다는 뜻"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#생각의-시간1",
    "href": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#생각의-시간1",
    "title": "3wk: 측도론 intro (3)",
    "section": "생각의 시간1",
    "text": "생각의 시간1\n우리가 잴 수 있는 집합의 모임들 \\({\\cal F}\\)라는 것은 답을 구체적으로 쓸 수는 없으나 현재까지 파악한 직관에 한정하여 아래와 같은 조건5들을 만족하는 collection이라고 “일단은” 생각할 수 있다.\n\n\\(\\Omega, \\emptyset \\in {\\cal F}\\)\n\\(\\forall A \\subset \\Omega: ~ A \\in {\\cal F} \\Rightarrow A^c \\in {\\cal F}\\)\n\\(\\forall A,B \\subset \\Omega\\) such that \\(A\\cap B =\\emptyset\\): \\(A,B \\in {\\cal F} \\Rightarrow A \\cup B \\in {\\cal F}\\)\n\\(\\forall A,B \\subset \\Omega\\) such that \\(A \\subset B\\): \\(A,B \\in {\\cal F} \\Rightarrow B-A \\in {\\cal F}\\)\n\n이것은 우리가 “확률”이라는 개념을 올바르게 정의하기 위해서 필요한 최소한의 합의6이다.\n여기에서 우리가 따져볼 것은 (1) 시그마필드의 조건으로 1~4이면 충분한지 (더 많은 조건들이 필요한건 아닌지) 그리고 (2) 우리가 있었으면 하는 조건들이 꼭 필요한 조건은 맞는지 (예를들면 한두개의 조건이 다른조건을 암시하는건 아닌지) 이다.\n(충분할까?) 조건 1,2,3,4 정도를 만족하는 집합으로 시그마필드를 정의해도 충분할까? 좀 더 많은 조건들이 필요한건 아닐까? 예를들면 아래와 같은 조건들이 필요한건 아닌가?\n\n\\(\\forall A,B \\subset \\Omega:~ A,B \\in {\\cal F} ~ \\Rightarrow A\\cap B \\in {\\cal F}\\)\n\\(\\forall A,B \\subset \\Omega:~ A,B \\in {\\cal F} ~ \\Rightarrow A\\cup B \\in {\\cal F}\\)\n\\(\\forall B_1,B_2,\\dots \\subset \\Omega\\) such that \\(B_1, B_2,\\dots\\) are disjoint: \\(B_1,B_2,\\dots \\in {\\cal F} \\Rightarrow \\cup_{i=1}^{\\infty}B_i \\in {\\cal F}\\)\n\\(\\forall A_1,A_2,\\dots \\subset \\Omega\\): \\(A_1,A_2,\\dots \\in {\\cal F} \\Rightarrow \\cup_{i=1}^{\\infty}A_i \\in {\\cal F}\\)\n\n여기에서 잠시 7의 의미를 살펴보자.\n\n3의 확장버전이라고 볼 수 있다. 3은 “각 집합을 잴 수 있다면 서로소인 집합을 유한번 더한 집합도 잴 수 있어야 한다” 라는 의미가 된다. 7은 “각 집합을 잴 수 있다면 서로소인 집합을 셀 수 있는 무한번 더한 집합도 잴 수 있어야 한다” 라는 의미가 된다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제11-람다시스템",
    "href": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제11-람다시스템",
    "title": "3wk: 측도론 intro (3)",
    "section": "(예제11) – 람다시스템",
    "text": "(예제11) – 람다시스템\n\\(\\Omega=(0,2\\pi]\\) 라고 하자. \\({\\cal A} = \\{\\{x\\}: x\\in \\mathbb{Q} \\cap \\Omega \\}\\) 이라고 할 때 아래가 성립할까?\n\\[\\mathbb{Q} \\cap \\Omega \\in \\sigma({\\cal A})\\]\n\\(\\mathbb{Q} \\cap \\Omega\\) : 구간 (0,2π] 에 있는 모든 유리수 모임\n즉 각각의 유리수 한점씩을 잴 수 있을 때7 유리수 전체의 집합 역시 잴 수 있을까?\n(해설1)\n유리수는 셀 수 있는 무한이므로 집합 \\(\\mathbb{Q} \\cap \\Omega\\)의 길이나 확률 따위는 잴 수 있다.\n(해설2)\n확률의 공리중 3을 살펴보면 이미 서로소인 집합의 countable union은 잴 수 있는 대상이라고 생각하고 있다. 이건 마치 “확률은 양수”이어야 한다든가, “전체확률은 1이어야” 한다는 사실처럼 당연한 사실이다.8\n\n\n\n그림1: 위키에서 캡쳐했어요~ 3번째 공리를 살펴보세요\n\n\n\n사실 납득이 되는건 아님. 그렇지만 일단은 “수학자들이 합의해서 이런건 잴 수 있다고 했어. 그러니까 잴 수 있어” 라고 이해하고 넘어가자."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#생각의-시간2",
    "href": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#생각의-시간2",
    "title": "3wk: 측도론 intro (3)",
    "section": "생각의 시간2",
    "text": "생각의 시간2\n이제 5,6의 성질을 살펴보자.\n\n\\(\\forall A,B \\subset \\Omega:~ A,B \\in {\\cal F} ~ \\Rightarrow A\\cap B \\in {\\cal F}\\)\n\\(\\forall A,B \\subset \\Omega:~ A,B \\in {\\cal F} ~ \\Rightarrow A\\cup B \\in {\\cal F}\\)\n\n6의 경우는 \\(A\\)와 \\(B\\)가 서로소가 아니더라고 \\(A \\cup B\\)를 잴 수 있느냐? 라는 것이다. (결국 이는 교집합을 잴 수 있느냐? 라는 물음과 같아서 5와 6은 같은 질문이다.)"
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제12-교집합을-넣을까-말까",
    "href": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#예제12-교집합을-넣을까-말까",
    "title": "3wk: 측도론 intro (3)",
    "section": "(예제12) – 교집합을 넣을까 말까",
    "text": "(예제12) – 교집합을 넣을까 말까\n\\(\\Omega=\\{1,2,3,4\\}\\)라고 하자. 아래와 같은 \\({\\cal F}\\)는 합리적일까?\n\\[{\\cal F}= \\big\\{ \\emptyset, \\{1,2\\}, \\{1,3\\}, \\{1,4\\},\\{2,3\\},\\{2,4\\},\\{3,4\\}, \\Omega\\big\\}\\]\n(해설1) – 틀린해설\n이러한 집합은 원칙 1-4,7 에 위배되지 않는다.\n1. \\(\\Omega, \\emptyset \\in {\\cal F}\\)\n2. \\(\\forall A \\subset \\Omega: ~ A \\in {\\cal F} \\Rightarrow A^c \\in {\\cal F}\\)\n3. \\(\\forall A,B \\subset \\Omega\\) such that \\(A\\cap B =\\emptyset\\): \\(A,B \\in {\\cal F} \\Rightarrow A \\cup B \\in {\\cal F}\\)\n4. \\(\\forall A,B \\subset \\Omega\\) such that \\(A \\subset B\\): \\(A,B \\in {\\cal F} \\Rightarrow B-A \\in {\\cal F}\\)\n7. \\(\\forall B_1,B_2,\\dots \\subset \\Omega\\) such that \\(B_1, B_2,\\dots\\) are disjoint: \\(B_1,B_2,\\dots \\in {\\cal F} \\Rightarrow \\cup_{i=1}^{\\infty}B_i \\in {\\cal F}\\)\n그런데 이 집합은\n\\[\\{1,2\\} \\cap \\{1,3\\} = \\{1\\}\\]\n와 같은 집합이라든가,\n\\[\\{1,2\\} \\cup \\{1,3\\} = \\{1,3,4\\}\\]\n와 같은 집합의 길이를 잴 수 없다. 따라서 아래와 같이 우리가 고등학교때 부터 써왔던 공식을 쓸 수 없다. (ref, Further consequences)\n\\[P(A\\cup B) = P(A) + P(B) - P(A\\cap B)\\]\n이것은 불편하니까 \\(A,B\\)가 잴 수 있다면, \\(A,B\\)의 교집합이나 합집합따위도 잴 수 있다고 정하자.\n(해설1의 반론)\n약속하지 않으면 “불편”하니까 약속하자라는 논리는 말이 되지 않음. 그 논리대로라면 \\(\\Omega\\)의 모든 부분 집합에 대하여 확률을 정의할 수 없다고 하면 “불편”하니까 약속하자라는 논리가 됨. 잴 수 있는 집합의 합집합이나 교집합을 잴 수 있다라는 근거는 없음.\n(해설1의 반론의 반론) – 참고용으로만..\n사실 근거가 있긴함. 즉 \\(A\\)와 \\(B\\)를 각각 잴 수 있다면 \\(A\\), \\(B\\)의 교집합도 잴 수 있음. (그렇다면 자동으로 합집합도 잴 수 있게 됨.) 이것을 지금 수준에서 엄밀하게 따지기 위해서는 “잴 수 있는 집합”의 정의를 해야하는데 지금 수준에서는 까다로움.\n(해설2) – 엄밀한 해설 X\n잴 수 있는 집합을 우리는 지금 까지 당연하게\n\n확률을 잴 수 있는 집합들\n\n로 생각했음, 그런데 원래 잴 수 있는 집합이라는 개념은 “선분의 길이” 따위를 모순없이 정의할 수 있는가? 즉 수직선 \\(\\mathbb{R}\\)의 모든 부분집합의 길이라는 개념을 정의할 수 있는가? 에서 출발하였음. 즉 원래 잴 수 있는 집합이라는 의미는\n\n수직선에서 길이를 잴 수 있는 집합들\n\n이라고 생각해야함. 그렇다면 “길이”(메저)라는 개념을 다시 추상화 해야하는데 “길이”라는 개념은 아래의 원칙에 위배되면 안될 것 같음.\n\n\n\n그림2: 위키에서 긁어온 그림. 길이는 1-4의 성질이 있어야 할 것으로 판단됨\n\n\n교집합을 잴 수 없다는 논리라면, 구간 \\([a_1,b_1]\\)의 길이는 잴 수 있고 구간 \\([a_2,b_2]\\)의 길이는 잴 수 있지만 구간 \\([a_1,b_1] \\cap [a_2,b_2]\\)의 길이는 잴 수 없다는 말인데 이는 말이되지 않음.\n\n결론 (엄밀한 해설은 아님): “잴 수 있다” 라는 개념은 확률, 길이에 모두 적용할 수 있어야 한다. 잴 수 있는 대상을 확률로 상상하면 \\(A \\in {\\cal F} \\Rightarrow A^c \\in {\\cal F}\\) 인것이 당연하듯이 잴 수 있는 대상을 길이로 상상하면 \\(A,B \\in {\\cal F} \\Rightarrow A \\cap B \\in {\\cal F}\\) 임은 당연하다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#생각의-시간3",
    "href": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#생각의-시간3",
    "title": "3wk: 측도론 intro (3)",
    "section": "생각의 시간3",
    "text": "생각의 시간3\n따라서 아래의 성질들은 모두 시그마필드가 가져아할 규칙들로 인정할 수 있다.\n\n\\(\\Omega, \\emptyset \\in {\\cal F}\\)\n\\(\\forall A \\subset \\Omega: ~ A \\in {\\cal F} \\Rightarrow A^c \\in {\\cal F}\\)\n\\(\\forall A,B \\subset \\Omega\\) such that \\(A\\cap B =\\emptyset\\): \\(A,B \\in {\\cal F} \\Rightarrow A \\cup B \\in {\\cal F}\\)\n\\(\\forall A,B \\subset \\Omega\\) such that \\(A \\subset B\\): \\(A,B \\in {\\cal F} \\Rightarrow B-A \\in {\\cal F}\\)\n\\(\\forall A,B \\subset \\Omega:~ A,B \\in {\\cal F} ~ \\Rightarrow A\\cap B \\in {\\cal F}\\)\n\\(\\forall A,B \\subset \\Omega:~ A,B \\in {\\cal F} ~ \\Rightarrow A\\cup B \\in {\\cal F}\\)\n\\(\\forall B_1,B_2,\\dots \\subset \\Omega\\) such that \\(B_1, B_2,\\dots\\) are disjoint: \\(B_1,B_2,\\dots \\in {\\cal F} \\Rightarrow \\cup_{i=1}^{\\infty}B_i \\in {\\cal F}\\)\n\n남은건 8번의 규칙이다.\n\n\\(\\forall A_1,A_2,\\dots \\subset \\Omega\\): \\(A_1,A_2,\\dots \\in {\\cal F} \\Rightarrow \\cup_{i=1}^{\\infty}A_i \\in {\\cal F}\\)\n\n이 89번 규칙은 사실 510, 711번 잘 조합하면 자동으로 이끌어진다. 즉 \\((5), (7) \\Rightarrow (8)\\). 그 외에도 “있었으면 싶은” 규칙은 모두 1-7중 적당한 것을 섞으면 만들 수 있다. 예를들어 아래와 같은 규칙을 고려하자.\n\n\\(\\forall A,B \\subset \\Omega:~ A,B \\in {\\cal F} \\Rightarrow A-B \\in {\\cal F}\\)\n\n\n\\(A-B\\) = \\(A \\cap B^c\\)\n\n\n\\(\\forall A,B,C \\subset \\Omega: A,B,C \\in {\\cal F} \\Rightarrow A\\cup B \\cup C \\in {\\cal F}\\)\n\\(\\forall A_1,A_2,\\dots \\subset \\Omega\\): \\(A_1,A_2,\\dots \\in {\\cal F} \\Rightarrow \\cap_{i=1}^{\\infty}A_i \\in {\\cal F}\\)\n\n규칙9는 규칙212와 513로 임플라이 할 수 있고, 규칙10은 규칙614의 확장으로 임플라이 할 수 있고, 규칙11은 규칙 215와 816로 임플라이 할 수 있다.\n\n결론: 규칙 1-8으로 시그마필드를 표현하기에 충분하다."
  },
  {
    "objectID": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#생각의-시간4",
    "href": "posts/Advanved Probability Theory/2023_03_21_3wk_checkpoint.html#생각의-시간4",
    "title": "3wk: 측도론 intro (3)",
    "section": "생각의 시간4",
    "text": "생각의 시간4\n규칙 1-8중 필요없는 규칙을 제거하자.\n1. 규칙217가 있다면, 규칙1에서 공집합은 빼도 될 것 같다.\n2. 규칙818이 있다면, 규칙319, 규칙620, 규칙721은 필요 없다. 즉 규칙8은 규칙3,6,7의 효과를 모두 가진다.\n3. 규칙222와 규칙623이 있다면, 규칙524는 필요없다. 따라서 규칙225와 규칙826이 있어도 규칙5는 필요없다.\n4. 규칙227와 규칙528가 있다면 규칙429는 필요없다. 그런데 규칙5는 규칙2와 규칙830이 임플라이 하므로 결국 규칙2와 규칙8이 있다면 규칙4가 필요없다.\n5. 결론: 규칙1에서 공집합을 제외한 버전, 그리고 규칙2, 규칙8만 있으면 된다."
  },
  {
    "objectID": "posts/Review/Synthetic data/synthetic data.html",
    "href": "posts/Review/Synthetic data/synthetic data.html",
    "title": "Synthetic Data",
    "section": "",
    "text": "합성데이터 연구 동향\n\n딥러닝 기술 발전 \\(\\to\\) 합성 데이터 생성 기술, 생성적 적대 신경망(GAN) 등 이미지, 음성, 자연어 등의 합성 데이터 생성\n유효성 검증 기술 \\(\\to\\) 실제 데이터와 합성 데이터의 차이 최소화, 합성 데이터가 실제 데이터와 비슷하게 생성되었는지 검증\n개인정보 보호 문제에 대한 대응책 \\(\\to\\) 개인정보 마스킹 기술이나 개인정보 사용하지 않고도 유사한 합성데이터 생성 기술 연구\n활용 분야 \\(\\to\\) 컴퓨터 비전, 자연어 처리, 의료 분야, 로봇 분야 등\n효과 입증 \\(\\to\\) 모델 학습시 성능이 개선되는 경우가 있음\n\n\n\n합성데이터 논문\n\n“딥러닝을 이용한 합성 데이터 생성 기술 연구 동향” (한국정보기술학회 논문지, 2019)\n\n\n딥러닝 기술을 이용한 합성 데이터 생성 기술의 연구 동향을 조사하고 분석한 내용\n\n\n“합성 데이터를 이용한 자율주행 차량 인식 모델의 성능 분석” (한국컴퓨터정보학회 논문지, 2019)\n\n\n합성 데이터를 이용하여 자율주행 차량 인식 모델을 학습시키고 성능을 분석한 내용\n\n\n“의료 영상 데이터를 위한 합성 데이터 생성 기술” (한국정보과학회 논문지, 2020)\n\n\n의료 영상 데이터를 위한 합성 데이터 생성 기술을 소개하고, 생성된 합성 데이터를 이용하여 의료 영상 인식 모델을 학습시키는 실험을 수행한 내용\n\n\n“합성 데이터 생성 기술을 이용한 동작 인식 성능 개선에 관한 연구” (한국지능시스템학회 논문지, 2020)\n\n\n합성 데이터 생성 기술을 이용하여 동작 인식 모델을 학습시키고 성능을 개선하는 실험을 수행한 내용\n\n\n“합성 데이터를 이용한 자연어 처리 분야에서의 문제 해결 방안 연구” (한국인터넷정보학회 논문지, 2021)\n\n\n합성 데이터를 이용하여 자연어 처리 분야에서의 문제를 해결하는 방안을 제시하고, 생성된 합성 데이터를 이용하여 자연어 처리 모델을 학습시키는 실험을 수행한 내용\n\n\n위 관련 내용은 chat GPT가 소개해준 내용\n\n\n합성 데이터 관련 논문이나 코드가 너무 없담… 어디서 찾지..\n\n\n\n예시1: Faker 데이터 사용\n\n!pip install Faker\n\nCollecting Faker\n  Downloading Faker-17.6.0-py3-none-any.whl (1.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 10.5 MB/s eta 0:00:0031m38.6 MB/s eta 0:00:01\nRequirement already satisfied: python-dateutil>=2.4 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from Faker) (2.8.2)\nRequirement already satisfied: six>=1.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from python-dateutil>=2.4->Faker) (1.16.0)\nInstalling collected packages: Faker\nSuccessfully installed Faker-17.6.0\n\n\n\nFaker: 가짜 데이터 생성하는 라이브러리\n\n\nimport numpy as np\nimport pandas as pd\n\n\nfrom faker import Faker\nfake = Faker()\n\n# 가짜 이름, 주소, 전화번호, 이메일 생성하기\nname = fake.name()\naddress = fake.address()\nphone_number = fake.phone_number()\nemail = fake.email()\n\n\ndata = np.random.rand(100)\n\n\nnames = [fake.name() for i in range(100)]\naddresses = [fake.address() for i in range(100)]\nages = [fake.random_int(min=18, max=80, step=1) for i in range(100)]\n\n\ndf = pd.DataFrame({'Name': names, 'Address': addresses, 'Age': ages})\ndf # 100개의 가짜 데이터 생성\n\n\n\n\n\n  \n    \n      \n      Name\n      Address\n      Age\n    \n  \n  \n    \n      0\n      Jason Williams\n      88899 Miller Fall Apt. 222\\nNew Eric, VA 87882\n      36\n    \n    \n      1\n      Brett Ramos\n      81526 Jacqueline Corners Suite 818\\nJessicaton...\n      73\n    \n    \n      2\n      Mario Mitchell\n      54833 Cox Lake Suite 142\\nChristianville, PW 0...\n      22\n    \n    \n      3\n      David Ryan\n      80999 Melissa Club\\nNorth Curtis, MI 28118\n      32\n    \n    \n      4\n      Marcus Adkins\n      6770 Jessica Radial\\nFloresberg, AR 35810\n      58\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      Francisco Porter\n      84916 Brown Mission\\nWest Williamborough, MI 3...\n      43\n    \n    \n      96\n      Alexander Martinez\n      3583 David View\\nPatrickfurt, IA 27730\n      50\n    \n    \n      97\n      Joshua Torres\n      4304 Macdonald Lake Suite 363\\nLake Melissashi...\n      24\n    \n    \n      98\n      Lauren Morris\n      584 Walker Squares Suite 817\\nSharonshire, MO ...\n      48\n    \n    \n      99\n      Jay Benson\n      64335 Smith Rest Suite 370\\nNorth Robertland, ...\n      54\n    \n  \n\n100 rows × 3 columns\n\n\n\n\n\n예시2: GAN 사용\n\n!pip install tensorflow\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nCollecting tensorflow\n  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 588.3/588.3 MB 8.2 MB/s eta 0:00:00m eta 0:00:01[36m0:00:01\nCollecting grpcio<2.0,>=1.24.3\n  Downloading grpcio-1.51.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 86.3 MB/s eta 0:00:0031m106.4 MB/s eta 0:00:01\nRequirement already satisfied: six>=1.12.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from tensorflow) (1.16.0)\nCollecting opt-einsum>=2.3.2\n  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 16.0 MB/s eta 0:00:00\nCollecting astunparse>=1.6.0\n  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nCollecting google-pasta>=0.1.1\n  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 13.7 MB/s eta 0:00:00\nCollecting tensorboard<2.12,>=2.11\n  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 93.8 MB/s eta 0:00:0031m124.4 MB/s eta 0:00:01\nRequirement already satisfied: numpy>=1.20 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from tensorflow) (1.24.2)\nCollecting protobuf<3.20,>=3.9.2\n  Downloading protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 95.7 MB/s eta 0:00:00\nCollecting termcolor>=1.1.0\n  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\nCollecting tensorflow-io-gcs-filesystem>=0.23.1\n  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 97.2 MB/s eta 0:00:00\nCollecting keras<2.12,>=2.11.0\n  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 100.2 MB/s eta 0:00:00\nCollecting gast<=0.4.0,>=0.2.1\n  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\nCollecting h5py>=2.9.0\n  Downloading h5py-3.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 98.9 MB/s eta 0:00:002 MB/s eta 0:00:01\nCollecting flatbuffers>=2.0\n  Downloading flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\nRequirement already satisfied: setuptools in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from tensorflow) (65.6.3)\nCollecting wrapt>=1.11.0\n  Downloading wrapt-1.15.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.5/81.5 kB 24.3 MB/s eta 0:00:00\nCollecting tensorflow-estimator<2.12,>=2.11.0\n  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 439.2/439.2 kB 76.0 MB/s eta 0:00:00\nCollecting libclang>=13.0.0\n  Downloading libclang-15.0.6.1-py2.py3-none-manylinux2010_x86_64.whl (21.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.5/21.5 MB 73.7 MB/s eta 0:00:00m eta 0:00:010:01:01\nCollecting absl-py>=1.0.0\n  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 30.3 MB/s eta 0:00:00\nRequirement already satisfied: packaging in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from tensorflow) (23.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from tensorflow) (4.4.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\nCollecting tensorboard-data-server<0.7.0,>=0.6.0\n  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 100.6 MB/s eta 0:00:001m123.6 MB/s eta 0:00:01\nCollecting google-auth-oauthlib<0.5,>=0.4.1\n  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\nCollecting google-auth<3,>=1.6.3\n  Downloading google_auth-2.16.2-py2.py3-none-any.whl (177 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.2/177.2 kB 41.2 MB/s eta 0:00:00\nCollecting markdown>=2.6.8\n  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.3/93.3 kB 22.9 MB/s eta 0:00:00\nCollecting tensorboard-plugin-wit>=1.6.0\n  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 83.9 MB/s eta 0:00:00\nRequirement already satisfied: requests<3,>=2.21.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.3)\nCollecting cachetools<6.0,>=2.0.0\n  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\nCollecting pyasn1-modules>=0.2.1\n  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 38.6 MB/s eta 0:00:00\nCollecting rsa<5,>=3.1.4\n  Downloading rsa-4.9-py3-none-any.whl (34 kB)\nCollecting requests-oauthlib>=0.7.0\n  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\nRequirement already satisfied: importlib-metadata>=4.4 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.0.0)\nRequirement already satisfied: certifi>=2017.4.17 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.14)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.15.0)\nCollecting pyasn1<0.5.0,>=0.4.6\n  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 18.9 MB/s eta 0:00:00\nCollecting oauthlib>=3.0.0\n  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 37.3 MB/s eta 0:00:00\nInstalling collected packages: tensorboard-plugin-wit, pyasn1, libclang, flatbuffers, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, pyasn1-modules, protobuf, opt-einsum, oauthlib, keras, h5py, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, markdown, google-auth, google-auth-oauthlib, tensorboard, tensorflow\nSuccessfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 flatbuffers-23.3.3 gast-0.4.0 google-auth-2.16.2 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.51.3 h5py-3.8.0 keras-2.11.0 libclang-15.0.6.1 markdown-3.4.1 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.2.0 wrapt-1.15.0\n\n\n\n# Define the generator model\ndef make_generator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(256, input_shape=(100,), use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Dense(512, use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Dense(28*28*1, use_bias=False, activation='tanh'))\n    model.add(layers.Reshape((28, 28, 1)))\n\n    return model\n\nmake_generator_model()\n- 생성기 모델 정의\n\n100차원 벡터를 입력으로 사용하고 데이터 세트의 입력 이미지와 동일한 크기의 출력 이미지 생성\nbatch normalization \\(\\to\\) leaky ReLU activation function \\(\\to\\) dense layer with a hyperbolic tangent activation function\n\n\n# Define the discriminator model\ndef make_discriminator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Flatten(input_shape=(28, 28, 1)))\n    model.add(layers.Dense(512, use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dense(256, use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dense(1))\n\n    return model\n\nmake_discriminator_model()\n- 판별기 모델\n\n이미지를 입력으로 받아 실제여부 구별하여 스칼라 값 출력(마지막 레이어가)\nbatch normalization, leaky ReLU activation function,final dense layer with no activation function\n\n\n# Define the GAN model\ndef make_gan_model(generator, discriminator):\n    discriminator.trainable = False\n\n    model = tf.keras.Sequential()\n    model.add(generator)\n    model.add(discriminator)\n\n    return model\n\nmake_gan_model( , )\n- GAN 모델\n\n생성기와 판별기 모델 단일 모델로 결합\ngenerator: discriminator가 실제로 분류하는 이미지 생성하도록 훈련\ndisciminator: generator가 생성하는 실제 이미지와 가짜 이미지 정확하게 분류하도록 훈련\n\n\n# Define the loss functions and optimizers\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\n\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n\n\nGAN 훈련시키려면 loss functions와 optimizers 정의\nbinary cross-entropy loss 사용\nAdam:0.0001\n\n\n# Train the GAN model\ndef train_gan(gan_model, dataset, epochs, batch_size):\n    generator, discriminator = gan_model.layers\n\n    for epoch in range(epochs):\n        for batch in dataset:\n            noise = tf.random.normal([batch_size, 100])\n\n            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n                generated_images = generator(noise, training=True)\n\n                real_output = discriminator(batch, training=True)\n                fake_output = discriminator(generated_images, training=True)\n\n                gen_loss = generator_loss(fake_output)\n                disc_loss = discriminator_loss(real_output, fake_output)\n\n            gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n            gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n            generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n            discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n\n        print(\"Epoch {} complete\".format(epoch))\n\n\n# Generate synthetic images using the trained generator\ndef generate_images(generator, num_images):\n    noise = tf.random.normal([num_images, 100])\n    generated_images = generator(noise, training=False)\n    generated_images = (generated_images + 1) / 2.0  # scale images to [0, 1]\n    return generated_images.numpy()\n\n\n훈련된 generator로 synthetic imges\n\n\n# Load and preprocess real images for training\n(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\nx_train = x_train.reshape(x_train.shape[0], 28,28,1)\n\n\n\n예시3: VAE사용 (개념부터 알아야할듯..)\n\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\n# load and preprocess the real images for training\n(train_images, _), (_, _) = keras.datasets.mnist.load_data()\ntrain_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\ntrain_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]\n\n\n# define the VAE model\nlatent_dim = 2\n\n\nlatent_dim = 2\n\nencoder_inputs = keras.Input(shape=(28, 28, 1))\nx = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\nx = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\nx = layers.Flatten()(x)\nx = layers.Dense(16, activation=\"relu\")(x)\nz_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\nz_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\nencoder = keras.Model(encoder_inputs, [z_mean, z_log_var], name=\"encoder\")\n\nlatent_inputs = keras.Input(shape=(latent_dim,))\nx = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\nx = layers.Reshape((7, 7, 64))(x)\nx = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\nx = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\ndecoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\ndecoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n\nclass Sampling(layers.Layer):\n    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n\n    def call(self, inputs):\n        z_mean, z_log_var = inputs\n        batch = keras.backend.shape(z_mean)[0]\n        dim = keras.backend.int_shape(z_mean)[1]\n        epsilon = keras.backend.random_normal(shape=(batch, dim))\n        return z_mean + keras.backend.exp(0.5 * z_log_var) * epsilon\n\nz = Sampling()([z_mean, z_log_var])\noutputs = decoder(z)\nvae = keras.Model(encoder_inputs, outputs, name=\"vae\")\n\n\n\n# define the loss function\nreconstruction_loss = keras.losses.binary_crossentropy(encoder_inputs, outputs)\nreconstruction_loss *= 28 * 28\nkl_loss = 1 + z_log_var - keras.backend.square(z_mean) - keras.backend.exp(z_log_var)\nkl_loss = keras.backend.sum(kl_loss, axis=-1)\nkl_loss *= -0.5\nvae_loss = keras.backend.mean(reconstruction_loss + kl_loss)\nvae.add_loss(vae_loss)\n\n\n\n# compile the model\nvae.compile(optimizer='adam')\n\n\n\n# train the model\nvae.fit(train_images, epochs=10, batch_size=128)\n\nEpoch 1/10\n\n\nInvalidArgumentError: Graph execution error:\n\nDetected at node 'gradient_tape/vae/tf.__operators__.add_1/BroadcastGradientArgs' defined at (most recent call last):\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 619, in start\n      self.io_loop.start()\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/ioloop.py\", line 688, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n      ret = callback()\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/gen.py\", line 814, in inner\n      self.ctx_run(self.run)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/gen.py\", line 775, in run\n      yielded = self.gen.send(value)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 358, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 536, in execute_request\n      self.do_execute(\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-79-41cbac8b1cbb>\", line 2, in <module>\n      vae.fit(train_images, epochs=10, batch_size=128)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/keras/engine/training.py\", line 1027, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 526, in minimize\n      grads_and_vars = self.compute_gradients(loss, var_list, tape)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 259, in compute_gradients\n      grads = tape.gradient(loss, var_list)\nNode: 'gradient_tape/vae/tf.__operators__.add_1/BroadcastGradientArgs'\nIncompatible shapes: [128,28,28] vs. [128]\n     [[{{node gradient_tape/vae/tf.__operators__.add_1/BroadcastGradientArgs}}]] [Op:__inference_train_function_3053]\n\n\n\n# generate synthetic data\nn_samples = 10\nrandom_latent_vectors = np.random.normal(size=(n_samples, latent_dim))\ngenerated_images = decoder.predict(random_latent_vectors)\n\n1/1 [==============================] - 0s 55ms/step\n\n\n\n# display the generated images\nfor i in range(n_samples):\n    plt.imshow(generated_images[i].reshape(28, 28))\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n예시4: a simple feedforward neural network\n\n!pip install scikit-learn\n\nCollecting scikit-learn\n  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.8/9.8 MB 80.6 MB/s eta 0:00:00m eta 0:00:010:01\nRequirement already satisfied: scipy>=1.3.2 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\nCollecting joblib>=1.1.1\n  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.0/298.0 kB 58.5 MB/s eta 0:00:00\nRequirement already satisfied: numpy>=1.17.3 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from scikit-learn) (1.24.2)\nCollecting threadpoolctl>=2.0.0\n  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\nInstalling collected packages: threadpoolctl, joblib, scikit-learn\nSuccessfully installed joblib-1.2.0 scikit-learn-1.2.2 threadpoolctl-3.1.0\n\n\n\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\n# generate and preprocess the real data for training\nX, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n\n# define the feedforward neural network\nmodel = keras.Sequential([\n    layers.Dense(16, activation='relu', input_shape=(X.shape[1],)),\n    layers.Dense(8, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n])\n\n\n\n# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n\n\n# train the model\nmodel.fit(X, y, epochs=100, batch_size=32)\n\nEpoch 1/100\n32/32 [==============================] - 0s 553us/step - loss: 0.5901\nEpoch 2/100\n32/32 [==============================] - 0s 472us/step - loss: 0.5224\nEpoch 3/100\n32/32 [==============================] - 0s 477us/step - loss: 0.4699\nEpoch 4/100\n32/32 [==============================] - 0s 472us/step - loss: 0.4287\nEpoch 5/100\n32/32 [==============================] - 0s 459us/step - loss: 0.3960\nEpoch 6/100\n32/32 [==============================] - 0s 461us/step - loss: 0.3715\nEpoch 7/100\n32/32 [==============================] - 0s 466us/step - loss: 0.3530\nEpoch 8/100\n32/32 [==============================] - 0s 466us/step - loss: 0.3412\nEpoch 9/100\n32/32 [==============================] - 0s 464us/step - loss: 0.3338\nEpoch 10/100\n32/32 [==============================] - 0s 470us/step - loss: 0.3267\nEpoch 11/100\n32/32 [==============================] - 0s 470us/step - loss: 0.3219\nEpoch 12/100\n32/32 [==============================] - 0s 471us/step - loss: 0.3182\nEpoch 13/100\n32/32 [==============================] - 0s 471us/step - loss: 0.3158\nEpoch 14/100\n32/32 [==============================] - 0s 461us/step - loss: 0.3131\nEpoch 15/100\n32/32 [==============================] - 0s 459us/step - loss: 0.3110\nEpoch 16/100\n32/32 [==============================] - 0s 456us/step - loss: 0.3095\nEpoch 17/100\n32/32 [==============================] - 0s 458us/step - loss: 0.3075\nEpoch 18/100\n32/32 [==============================] - 0s 454us/step - loss: 0.3059\nEpoch 19/100\n32/32 [==============================] - 0s 448us/step - loss: 0.3042\nEpoch 20/100\n32/32 [==============================] - 0s 459us/step - loss: 0.3028\nEpoch 21/100\n32/32 [==============================] - 0s 458us/step - loss: 0.3017\nEpoch 22/100\n32/32 [==============================] - 0s 457us/step - loss: 0.3018\nEpoch 23/100\n32/32 [==============================] - 0s 462us/step - loss: 0.3001\nEpoch 24/100\n32/32 [==============================] - 0s 456us/step - loss: 0.2992\nEpoch 25/100\n32/32 [==============================] - 0s 457us/step - loss: 0.2979\nEpoch 26/100\n32/32 [==============================] - 0s 467us/step - loss: 0.2966\nEpoch 27/100\n32/32 [==============================] - 0s 465us/step - loss: 0.2960\nEpoch 28/100\n32/32 [==============================] - 0s 467us/step - loss: 0.2950\nEpoch 29/100\n32/32 [==============================] - 0s 467us/step - loss: 0.2938\nEpoch 30/100\n32/32 [==============================] - 0s 467us/step - loss: 0.2929\nEpoch 31/100\n32/32 [==============================] - 0s 469us/step - loss: 0.2925\nEpoch 32/100\n32/32 [==============================] - 0s 473us/step - loss: 0.2915\nEpoch 33/100\n32/32 [==============================] - 0s 469us/step - loss: 0.2906\nEpoch 34/100\n32/32 [==============================] - 0s 472us/step - loss: 0.2896\nEpoch 35/100\n32/32 [==============================] - 0s 473us/step - loss: 0.2891\nEpoch 36/100\n32/32 [==============================] - 0s 461us/step - loss: 0.2874\nEpoch 37/100\n32/32 [==============================] - 0s 456us/step - loss: 0.2871\nEpoch 38/100\n32/32 [==============================] - 0s 455us/step - loss: 0.2865\nEpoch 39/100\n32/32 [==============================] - 0s 457us/step - loss: 0.2855\nEpoch 40/100\n32/32 [==============================] - 0s 457us/step - loss: 0.2848\nEpoch 41/100\n32/32 [==============================] - 0s 459us/step - loss: 0.2843\nEpoch 42/100\n32/32 [==============================] - 0s 454us/step - loss: 0.2835\nEpoch 43/100\n32/32 [==============================] - 0s 457us/step - loss: 0.2827\nEpoch 44/100\n32/32 [==============================] - 0s 444us/step - loss: 0.2818\nEpoch 45/100\n32/32 [==============================] - 0s 447us/step - loss: 0.2813\nEpoch 46/100\n32/32 [==============================] - 0s 457us/step - loss: 0.2801\nEpoch 47/100\n32/32 [==============================] - 0s 460us/step - loss: 0.2795\nEpoch 48/100\n32/32 [==============================] - 0s 458us/step - loss: 0.2789\nEpoch 49/100\n32/32 [==============================] - 0s 462us/step - loss: 0.2781\nEpoch 50/100\n32/32 [==============================] - 0s 459us/step - loss: 0.2775\nEpoch 51/100\n32/32 [==============================] - 0s 459us/step - loss: 0.2766\nEpoch 52/100\n32/32 [==============================] - 0s 461us/step - loss: 0.2762\nEpoch 53/100\n32/32 [==============================] - 0s 460us/step - loss: 0.2757\nEpoch 54/100\n32/32 [==============================] - 0s 462us/step - loss: 0.2750\nEpoch 55/100\n32/32 [==============================] - 0s 457us/step - loss: 0.2729\nEpoch 56/100\n32/32 [==============================] - 0s 462us/step - loss: 0.2722\nEpoch 57/100\n32/32 [==============================] - 0s 465us/step - loss: 0.2711\nEpoch 58/100\n32/32 [==============================] - 0s 447us/step - loss: 0.2708\nEpoch 59/100\n32/32 [==============================] - 0s 451us/step - loss: 0.2704\nEpoch 60/100\n32/32 [==============================] - 0s 449us/step - loss: 0.2687\nEpoch 61/100\n32/32 [==============================] - 0s 454us/step - loss: 0.2683\nEpoch 62/100\n32/32 [==============================] - 0s 464us/step - loss: 0.2673\nEpoch 63/100\n32/32 [==============================] - 0s 469us/step - loss: 0.2663\nEpoch 64/100\n32/32 [==============================] - 0s 473us/step - loss: 0.2658\nEpoch 65/100\n32/32 [==============================] - 0s 466us/step - loss: 0.2651\nEpoch 66/100\n32/32 [==============================] - 0s 467us/step - loss: 0.2641\nEpoch 67/100\n32/32 [==============================] - 0s 467us/step - loss: 0.2633\nEpoch 68/100\n32/32 [==============================] - 0s 467us/step - loss: 0.2622\nEpoch 69/100\n32/32 [==============================] - 0s 463us/step - loss: 0.2618\nEpoch 70/100\n32/32 [==============================] - 0s 463us/step - loss: 0.2611\nEpoch 71/100\n32/32 [==============================] - 0s 464us/step - loss: 0.2598\nEpoch 72/100\n32/32 [==============================] - 0s 466us/step - loss: 0.2596\nEpoch 73/100\n32/32 [==============================] - 0s 460us/step - loss: 0.2582\nEpoch 74/100\n32/32 [==============================] - 0s 448us/step - loss: 0.2583\nEpoch 75/100\n32/32 [==============================] - 0s 452us/step - loss: 0.2573\nEpoch 76/100\n32/32 [==============================] - 0s 466us/step - loss: 0.2558\nEpoch 77/100\n32/32 [==============================] - 0s 462us/step - loss: 0.2556\nEpoch 78/100\n32/32 [==============================] - 0s 462us/step - loss: 0.2543\nEpoch 79/100\n32/32 [==============================] - 0s 451us/step - loss: 0.2547\nEpoch 80/100\n32/32 [==============================] - 0s 448us/step - loss: 0.2527\nEpoch 81/100\n32/32 [==============================] - 0s 453us/step - loss: 0.2516\nEpoch 82/100\n32/32 [==============================] - 0s 450us/step - loss: 0.2505\nEpoch 83/100\n32/32 [==============================] - 0s 451us/step - loss: 0.2502\nEpoch 84/100\n32/32 [==============================] - 0s 453us/step - loss: 0.2485\nEpoch 85/100\n32/32 [==============================] - 0s 452us/step - loss: 0.2475\nEpoch 86/100\n32/32 [==============================] - 0s 448us/step - loss: 0.2468\nEpoch 87/100\n32/32 [==============================] - 0s 453us/step - loss: 0.2461\nEpoch 88/100\n32/32 [==============================] - 0s 451us/step - loss: 0.2448\nEpoch 89/100\n32/32 [==============================] - 0s 450us/step - loss: 0.2438\nEpoch 90/100\n32/32 [==============================] - 0s 447us/step - loss: 0.2431\nEpoch 91/100\n32/32 [==============================] - 0s 456us/step - loss: 0.2421\nEpoch 92/100\n32/32 [==============================] - 0s 452us/step - loss: 0.2413\nEpoch 93/100\n32/32 [==============================] - 0s 452us/step - loss: 0.2403\nEpoch 94/100\n32/32 [==============================] - 0s 454us/step - loss: 0.2395\nEpoch 95/100\n32/32 [==============================] - 0s 460us/step - loss: 0.2388\nEpoch 96/100\n32/32 [==============================] - 0s 457us/step - loss: 0.2381\nEpoch 97/100\n32/32 [==============================] - 0s 460us/step - loss: 0.2368\nEpoch 98/100\n32/32 [==============================] - 0s 459us/step - loss: 0.2362\nEpoch 99/100\n32/32 [==============================] - 0s 460us/step - loss: 0.2349\nEpoch 100/100\n32/32 [==============================] - 0s 459us/step - loss: 0.2347\n\n\n<keras.callbacks.History at 0x7f71e8106490>\n\n\n\n# generate synthetic data\nn_samples = 10\nrandom_vectors = np.random.normal(size=(n_samples, X.shape[1]))\ngenerated_data = model.predict(random_vectors)\n\n1/1 [==============================] - 0s 22ms/step\n\n\n\n# display the generated data\nprint(generated_data)\n\n[[0.00134968]\n [0.11726338]\n [0.6508618 ]\n [0.9898428 ]\n [0.9719319 ]\n [0.9829191 ]\n [0.6400001 ]\n [0.00335612]\n [0.15759172]\n [0.00645017]]\n\n\n\n\n예시5: KDE(Kernel Density Estimation)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import KernelDensity\n\n\nX, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n\n\nkde = KernelDensity(kernel='gaussian', bandwidth=0.5)\nkde.fit(X)\n\nKernelDensity(bandwidth=0.5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KernelDensityKernelDensity(bandwidth=0.5)\n\n\n\nn_samples = 10\nsynthetic_data = kde.sample(n_samples)\nsynthetic_data\n\narray([[-6.40543316e-01, -2.00816029e-01, -1.38112318e+00,\n        -5.04647026e-01, -1.16412835e-01,  2.31176462e-01,\n         2.67275989e+00, -2.43788602e+00, -1.07609433e+00,\n        -7.60376957e-01],\n       [-1.96154361e+00, -5.44035464e-01, -1.29299914e+00,\n        -1.79243149e+00, -7.91254247e-01, -1.49100460e+00,\n         2.59878119e+00, -7.00449777e-01,  2.46705029e+00,\n        -8.13271665e-01],\n       [ 4.92102583e-01,  4.61885958e-01,  9.06165151e-01,\n         7.09125038e-01,  3.62018015e-01,  1.33662227e+00,\n        -1.73166578e+00, -1.54776550e+00, -6.47684988e-01,\n         1.50384591e+00],\n       [-1.37270120e+00,  1.11711271e+00,  1.69950848e-01,\n         1.71370541e+00, -9.95787406e-01,  9.37075645e-01,\n         6.86130469e-01,  2.68568255e+00,  5.23540728e-01,\n         1.28789991e+00],\n       [-6.09739471e-01,  2.74003692e-01, -4.99186042e-01,\n         7.55177619e-01,  1.10635851e+00, -6.33249957e-01,\n         1.45340497e+00,  1.50254494e-01,  1.70527839e+00,\n        -1.68688562e+00],\n       [ 2.82712325e-01,  6.99937229e-01,  2.35281704e-01,\n        -1.38757829e+00, -3.07830479e-01, -1.66666566e+00,\n         3.49788502e-01,  1.81251770e+00,  1.41807360e+00,\n        -1.14791545e-01],\n       [ 6.89280678e-01, -2.94374178e-03,  1.00903417e+00,\n         1.42917675e+00,  9.38636180e-01,  6.54756154e-01,\n        -1.50349162e+00, -1.72798315e-01,  7.09793089e-01,\n         2.34657027e+00],\n       [ 1.40766106e+00,  1.33457643e+00,  6.06365408e-02,\n        -1.61743252e+00,  1.29343257e+00, -4.73977756e-01,\n         2.15440056e-01, -1.41240310e+00, -3.73397331e+00,\n         1.15540476e+00],\n       [-4.13886212e-02,  1.31264769e+00,  2.42505327e-01,\n        -9.10608344e-01,  1.92476492e+00,  5.73378858e-01,\n        -9.79660592e-01,  1.66989518e+00, -3.47583358e-02,\n         2.49848587e-01],\n       [ 1.73735646e-01, -8.64171575e-01, -1.98493560e-01,\n         1.64845898e-01, -8.99243472e-01, -1.51645063e-01,\n        -8.09655616e-01, -7.19465453e-01, -9.90817553e-01,\n        -1.45541083e+00]])\n\n\n\nfig, ax = plt.subplots()\nax.scatter(X[:, 0], X[:, 1], c=y)\nax.scatter(synthetic_data[:, 0], synthetic_data[:, 1], c='r')\nplt.show()\n\n\n\n\n\n\n예시6: random sampling technique\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\n\n\nX, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n\n\nn_samples = 10\nsynthetic_data = np.random.rand(n_samples, X.shape[1])\n\n\nfig, ax = plt.subplots()\nax.scatter(X[:, 0], X[:, 1], c=y)\nax.scatter(synthetic_data[:, 0], synthetic_data[:, 1], c='r')\nplt.show()\n\n\n\n\n\n\n예시7: linear regression model\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\n\n\nX, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n\n\nlr = LinearRegression()\nlr.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nn_samples = 10\nsynthetic_data = lr.predict(np.random.rand(n_samples, 1))\n\n\nfig, ax = plt.subplots()\nax.scatter(X, y)\nax.scatter(np.random.rand(n_samples, 1), synthetic_data, c='r')\nplt.show()"
  },
  {
    "objectID": "posts/Review/Synthetic data/Practical Synthetic Data Generation.html",
    "href": "posts/Review/Synthetic data/Practical Synthetic Data Generation.html",
    "title": "Practical Synthetic Data Generation",
    "section": "",
    "text": "부제: 머신러닝을 위한 실전 데이터셋\n출판사: 한빛미디어\n저자: 칼리드 엘 에맘, 루시 모스케라, 리처드 홉트로프\n옮긴이: 심상진 옮김\n소개글: 머신러닝 모델을 구축하고, 테스트를 진행하려면 크고 다양한 종류의 데이터가 필요하다. 그러나 대부분의 데이터셋은 개인 정보 문제로 사용이 제한적이라 광범위하게 사용할 수 없다. 이 책에서는 실제 데이터로 새로운 데이터를 만드는 실용적인 합성 데이터 기술을 소개한다. 합성 데이터는 이차 분석에 용이하여 데이터 연구, 고객 행동의 이해, 신제품 개발 등 다양한 목적으로 활용될 수 있다. 이 책은 실제 데이터를 합성해 다양한 산업에서 사용할 수 있는 방법을 제공하며, 개인 정보 문제를 해결하는 방법을 다룬다. 또한 실제 데이터셋에서 합성 데이터를 생성하기 위한 원칙과 단계를 배운다. 더 나아가 합성 데이터가 제품이나 솔루션 개발에 드는 시간을 어떻게 단축할 수 있는지를 학습한다.\n\n\n\n- 합성 데이터 정의\n\n실제 데이터가 아니라 실제 데이터에서 생성되어 통계속성이 동일한 데이터\n\n- 합성 데이터 유형\n\n실제 데이터로 합성하기\n실제 데이터 없이 합성하기\n두 가지 유형을 합친 하이브리드\n\n- 합성 데이터 이점\n\n식별 가능한 개인 데이터가 아님 \\(\\to\\) 개인 정보 보호 규정 적용x\n데이터 이차 목적 사용 가능\n수집이 어렵거나 비실용적, 비윤리적인 경우도 사용 가능\n초기 모델을 훈련 \\(\\to\\) 데이터 모델의 정합화 촉진\n\n- 합성 데이터 활용 사례\n\n제조/유통\n헬스케어\n금융 서비스\n교통수단\n\n- 공공 데이터는 통제되지 않는다.\n- 영국 공중보건국 합성 암 등록 데이터\n\n\n\n- 데이터 합성이 데이터에 접근하는 최선의 방식인가?\n\\(\\to\\) 구현 프로세스 고려\n- 결정기준\n\n프라이버시, 운영 비용, 데이터 효용성, 소비자 신뢰도\n\n\n\n\n- 데이터 합성 방법론과 기술\n\n분포 모델링(정규 분포/지수 분포 등 고전적인 분포)에 개별 변수를 적합시키거나 데이터 구조 모델링 사용\n\n- 과적합 해결법\n\n분포를 중립 지점에서 시작해 데이터에 더 가깝게 더 가까운 쪽으로 이동하여 각 단계별 분포의 단순성과 적합도 사이에서 균형을 이루게 하는 접근법\n최고의 절충점에 언제 도착했는지 측정해서 방지\n분포 적합성 접근법?\n\n\n\n\n- 일변량\n\n실제 데이터와 합성 데이터에서 각 변수 간의 분포 차이를 측정하기 위한 헬링거 거리 계산\n0: 분포간에 차이 없음 ~ 1: 분포 차이 많음\n\n- 이변량\n\n실제 데이터와 합성 데이터의 모든 변수 쌍 간의 상관관계의 절대적 차이 = 데이터 효용성의 척도\n\n- 다변량\n\n10겹 교차 검증\n데이터 셋을 10개의 동일한 크기의 서브셋으로 나눔\n서브셋1을 테스트셋으로 하고 나머지 9개 데이터셋 모델을 만든다.\n서브셋1에서 모델 테스트, AUROC 계산\n훈련 데이터로 서브셋2 테스트 사용, AUROC 계산\n… 10회 반복 \\(\\to\\) AUROC 10개 값 \\(\\to\\) 평균계산\n\n- 합성데이터와 실제 데이터에 대한 대응 모델에 대한 계산\n- 두 AUROC값의 절대적 차이 계산\n- 모든 절대값 차이에 대한 상자 그림 생성\n- 경향점수.. (어렵… 103p)\n\n\n\n- 방법\n\n정규분포로부터 샘플링\n샘플링 프로세스 중 상관관계 유도\n코퓰러 사용\n\n- 코퓰러?\n\n확률변수들 간의 상관관계 또는 종속성을 나타내는 함수\n흠.. 나중에 찾아보자..\n\n- 머신러닝 방법 - 의사결정 트리(CART) 사용\n- 딥러닝 방법 - 변이형 오토인코더(VAE) - 생성적 적대 신경망(GAN)\n- GAN - 생성기: 입력 무작위 데이터, 정규 분포 또는 균일 분포로부터 샘플링하며 합성 데이터 생성 - 판별기: 합성 데이터와 실제 데이터 비교하여 유사한 경향 점수 생성 차이 결과를 생성 기 훈련을 위해 다시 제공\n\n\n\n\n노출 유형\n신원 노출(정보 이득이 0이라면 신원 노출에 아무런 의미가 없음)\n속성 노출"
  },
  {
    "objectID": "posts/Review/Synthetic data/Practical Synthetic Data Generation.html#data",
    "href": "posts/Review/Synthetic data/Practical Synthetic Data Generation.html#data",
    "title": "Practical Synthetic Data Generation",
    "section": "data",
    "text": "data\n\nfrom sdmetrics import load_demo \n\nreal_data, synthetic_data, metadata = load_demo(modality='single_table')\n\n\nmetadata\n\n{'fields': {'start_date': {'type': 'datetime', 'format': '%Y-%m-%d'},\n  'end_date': {'type': 'datetime', 'format': '%Y-%m-%d'},\n  'salary': {'type': 'numerical', 'subtype': 'integer'},\n  'duration': {'type': 'numerical', 'subtype': 'integer'},\n  'student_id': {'type': 'id', 'subtype': 'integer'},\n  'high_perc': {'type': 'numerical', 'subtype': 'float'},\n  'high_spec': {'type': 'categorical'},\n  'mba_spec': {'type': 'categorical'},\n  'second_perc': {'type': 'numerical', 'subtype': 'float'},\n  'gender': {'type': 'categorical'},\n  'degree_perc': {'type': 'numerical', 'subtype': 'float'},\n  'placed': {'type': 'boolean'},\n  'experience_years': {'type': 'numerical', 'subtype': 'float'},\n  'employability_perc': {'type': 'numerical', 'subtype': 'float'},\n  'mba_perc': {'type': 'numerical', 'subtype': 'float'},\n  'work_experience': {'type': 'boolean'},\n  'degree_type': {'type': 'categorical'}},\n 'constraints': [],\n 'model_kwargs': {},\n 'name': None,\n 'primary_key': 'student_id',\n 'sequence_index': None,\n 'entity_columns': [],\n 'context_columns': []}\n\n\n\nreal_data.head()\n\n\n\n\n\n  \n    \n      \n      student_id\n      gender\n      second_perc\n      high_perc\n      high_spec\n      degree_perc\n      degree_type\n      work_experience\n      experience_years\n      employability_perc\n      mba_spec\n      mba_perc\n      salary\n      placed\n      start_date\n      end_date\n      duration\n    \n  \n  \n    \n      0\n      17264\n      M\n      67.00\n      91.00\n      Commerce\n      58.00\n      Sci&Tech\n      False\n      0\n      55.0\n      Mkt&HR\n      58.80\n      27000.0\n      True\n      2020-07-23\n      2020-10-12\n      3.0\n    \n    \n      1\n      17265\n      M\n      79.33\n      78.33\n      Science\n      77.48\n      Sci&Tech\n      True\n      1\n      86.5\n      Mkt&Fin\n      66.28\n      20000.0\n      True\n      2020-01-11\n      2020-04-09\n      3.0\n    \n    \n      2\n      17266\n      M\n      65.00\n      68.00\n      Arts\n      64.00\n      Comm&Mgmt\n      False\n      0\n      75.0\n      Mkt&Fin\n      57.80\n      25000.0\n      True\n      2020-01-26\n      2020-07-13\n      6.0\n    \n    \n      3\n      17267\n      M\n      56.00\n      52.00\n      Science\n      52.00\n      Sci&Tech\n      False\n      0\n      66.0\n      Mkt&HR\n      59.43\n      NaN\n      False\n      NaT\n      NaT\n      NaN\n    \n    \n      4\n      17268\n      M\n      85.80\n      73.60\n      Commerce\n      73.30\n      Comm&Mgmt\n      False\n      0\n      96.8\n      Mkt&Fin\n      55.50\n      42500.0\n      True\n      2020-07-04\n      2020-09-27\n      3.0\n    \n  \n\n\n\n\n\nsynthetic_data.head()\n\n\n\n\n\n  \n    \n      \n      student_id\n      gender\n      second_perc\n      high_perc\n      high_spec\n      degree_perc\n      degree_type\n      work_experience\n      experience_years\n      employability_perc\n      mba_spec\n      mba_perc\n      salary\n      placed\n      start_date\n      end_date\n      duration\n    \n  \n  \n    \n      0\n      0\n      F\n      41.361060\n      85.425072\n      Commerce\n      74.972674\n      Comm&Mgmt\n      False\n      0\n      49.986653\n      Mkt&Fin\n      57.291083\n      NaN\n      True\n      2020-02-11\n      2020-08-02\n      3.0\n    \n    \n      1\n      1\n      M\n      63.720169\n      99.059033\n      Commerce\n      62.769650\n      Others\n      False\n      0\n      78.962948\n      Mkt&HR\n      79.068319\n      NaN\n      False\n      NaT\n      NaT\n      NaN\n    \n    \n      2\n      2\n      M\n      58.473884\n      89.241528\n      Science\n      83.066328\n      Sci&Tech\n      True\n      0\n      47.980244\n      Mkt&Fin\n      77.042950\n      26727.0\n      True\n      2020-02-13\n      2020-05-27\n      3.0\n    \n    \n      3\n      3\n      F\n      77.232204\n      100.523788\n      Commerce\n      61.010445\n      Comm&Mgmt\n      True\n      0\n      61.016218\n      Mkt&HR\n      68.132991\n      22058.0\n      True\n      2020-09-24\n      2020-11-07\n      3.0\n    \n    \n      4\n      4\n      F\n      54.067830\n      109.611537\n      Commerce\n      72.846753\n      Others\n      True\n      0\n      66.949987\n      Mkt&Fin\n      66.363138\n      NaN\n      False\n      NaT\n      NaT\n      NaN\n    \n  \n\n\n\n\n\nfrom sdmetrics.reports.single_table import QualityReport\n\nreport = QualityReport()\nreport.generate(real_data, synthetic_data, metadata)\n\nCreating report: 100%|██████████| 4/4 [00:00<00:00, 12.97it/s]\n\n\n\nOverall Quality Score: 81.44%\n\nProperties:\nColumn Shapes: 81.56%\nColumn Pair Trends: 81.33%\n\n\n\nreport.get_details(property_name='Column Shapes')\n\n\n\n\n\n  \n    \n      \n      Column\n      Metric\n      Quality Score\n    \n  \n  \n    \n      0\n      second_perc\n      KSComplement\n      0.627907\n    \n    \n      1\n      high_perc\n      KSComplement\n      0.553488\n    \n    \n      2\n      degree_perc\n      KSComplement\n      0.627907\n    \n    \n      3\n      experience_years\n      KSComplement\n      0.800000\n    \n    \n      4\n      employability_perc\n      KSComplement\n      0.781395\n    \n    \n      5\n      mba_perc\n      KSComplement\n      0.841860\n    \n    \n      6\n      salary\n      KSComplement\n      0.869155\n    \n    \n      7\n      start_date\n      KSComplement\n      0.701107\n    \n    \n      8\n      end_date\n      KSComplement\n      0.768919\n    \n    \n      9\n      duration\n      KSComplement\n      0.826051\n    \n    \n      10\n      gender\n      TVComplement\n      0.939535\n    \n    \n      11\n      high_spec\n      TVComplement\n      0.902326\n    \n    \n      12\n      degree_type\n      TVComplement\n      0.925581\n    \n    \n      13\n      work_experience\n      TVComplement\n      0.972093\n    \n    \n      14\n      mba_spec\n      TVComplement\n      0.995349\n    \n    \n      15\n      placed\n      TVComplement\n      0.916279\n    \n  \n\n\n\n\n\n시각화\n\nreport.get_visualization(property_name='Column Shapes')\n\n\n                                                \n\n\n- high Quality\n\nget_column_plot?\n\n\n\nSignature: get_column_plot(real_data, synthetic_data, column_name, metadata)\nDocstring:\nReturn a plot of the real and synthetic data for a given column.\nArgs:\n    real_data (pandas.DataFrame):\n        The real table data.\n    synthetic_data (pandas.DataFrame):\n        The synthetic table data.\n    column_name (str):\n        The name of the column.\n    metadata (dict):\n        The table metadata.\nReturns:\n    plotly.graph_objects._figure.Figure\nFile:      ~/anaconda3/envs/py37/lib/python3.7/site-packages/sdmetrics/reports/utils.py\nType:      function\n\n\n\n\n\nfrom sdmetrics.reports.utils import get_column_plot\n\nfig = get_column_plot(\n    real_data=real_data,\n    synthetic_data=synthetic_data,\n    metadata=metadata,\n    column_name='mba_spec'\n)\n\nfig.show()\n\n\n                                                \n\n\n- low Quality\n\nfrom sdmetrics.reports.utils import get_column_plot\n\nfig = get_column_plot(\n    real_data=real_data,\n    synthetic_data=synthetic_data,\n    metadata=metadata,\n    column_name='second_perc'\n)\n\nfig.show()\n\n\n                                                \n\n\n\nreport.get_visualization(property_name='Column Pair Trends')\n\n\n                                                \n\n\n\nfrom sdmetrics.reports.utils import get_column_pair_plot\n\n\nget_column_pair_plot?\n\n\n\nSignature: get_column_pair_plot(real_data, synthetic_data, column_names, metadata)\nDocstring:\nReturn a plot of the real and synthetic data for a given column pair.\nArgs:\n    real_data (pandas.DataFrame):\n        The real table data.\n    synthetic_column (pandas.Dataframe):\n        The synthetic table data.\n    column_names (list[string]):\n        The names of the two columns to plot.\n    metadata (dict):\n        The table metadata.\nReturns:\n    plotly.graph_objects._figure.Figure\nFile:      ~/anaconda3/envs/py37/lib/python3.7/site-packages/sdmetrics/reports/utils.py\nType:      function\n\n\n\n\n\nfig = get_column_pair_plot(\n    real_data=real_data,\n    synthetic_data=synthetic_data,\n    metadata=metadata,\n    column_names=['start_date', 'second_perc']\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\n데이터 보고서 형태로 저장\nreport.save(filepath='sdmetrics_quality_demo.pkl')\n\n# load the report at a later time\nreport = QualityReport.load(filepath='sdmetrics_quality_demo.pkl')"
  },
  {
    "objectID": "posts/Review/Synthetic data/A Comparison of Synthetic Data Approaches Using Utility and Disclosure Risk Measures.html",
    "href": "posts/Review/Synthetic data/A Comparison of Synthetic Data Approaches Using Utility and Disclosure Risk Measures.html",
    "title": "[논문] Synthetic Data",
    "section": "",
    "text": "Deep generative model\nDiscolsure risk\nNonparametric Bayesian\nSequential regression\nSynthetic data\nUtility\n\nSeongbin An, Trang Doan, Juhee Lee, Jiwwo Kim, Yong Jae Kim, Yunji Kim, Changwon Yoon, Sungkyu Jung, Dongha Kim, Sunghoon Kwon, Hang J Kim, Jeongyoun Ahn, Cheolwoo Park\nThe Korean Journal of Applied Statistics\n\n\n- 재현자료 생성기법\n\n순차적 회귀분석\n비모수 베이지안\n인공지능 기반: CTGAN, TVAE\n\n- 유용성 지표\n. (대역 유용성):자료 전체의 분포적인 특성을 얼마나 비슷하게 유지 - Propensity Score, 거리측도, α-정밀도, β-wogusdbf\n. (특정 유용성): 특정 분석이 데이터의 적용될 것을 가정하고 해당 분석에서 원본자료와 재현자료가 얼마나 유사한 결과를 나타내는지 기반으로 유용성 판단 - 신뢰구간 중첩\n- 노출 위험도 지표\n\n신원 노출 위험도\n속성 노출 위험도\n독창성 점수\n\n\n\n\n\n2019년 전국 사업체 조사 데이터\n\n이항형, 다항형, 연속형 변수\n\n\n\n구분\n변수명\n변수설명\n\n\n\n\n범주형\nSEX\n대표자 성별(남/여)\n\n\n\nSUMMAT_CD\n매출 금액(9단계 범주)\n\n\n연속형\nWORKER_T\n총 근로자수\n\n\n\nEMP_T\n상용근로 종사자수\n\n\n\nBIS_MNTH\n영업개월수\n\n\n\n\\[WORKER_T>=EMP_T\\]\n\n\n\n\n\n변수의 순서에 따라 결합분포의 추정값이 다르다.\n\\(X_j\\) 범주형 \\(\\to\\) 분류 의사결정나무 : 지니계수\n\\(X_j\\) 수치형 \\(\\to\\) 회귀 의사결정나무 : 엔트로피\n\n\n\n\n\n\n\n\n\n\n\n\nPropensity Score: 공변량 X가 주어졌을 때 처리그룹으로 배치될 확률 \\(Pr(Treatment = 1 | X)\\)\n재현자료로 배치되는 경우를 처리 그룹으로 배치되는 경우로 생각\n\\[ pMSE = \\dfrac{1}{n_s+n_o}\\sum_{i=1}^{n_s+n_o}(p̂_i-c)^2\\]\n재현자료의 유용성이 높을수록 \\(pMSE\\)는 0에 가까움\n\n원본자료와 재현자료를 분포적으로 구분할 수 있는지 수치화\n개별적 비교 필요 없이 변수의 관계성을 고려하여 평가 가능\n분류 모델에 따라 \\(pMSE\\) 값이 달라지므로 귀무분포를 고려해야 함\n\n\n\n\n\n원본자료와 재현자료에서 각 변수의 분포를 각각 계산하여 유용성 판단 가능\n변수간의 상관성 고려 못함\n\n- KL괴리도\n\nKullback-Leibler\n\n\\[ D(f||g) = \\int_{-\\infty}^{\\infty}f(x) log \\dfrac{f(x)}{g(x)}dx\\]\n- Wasser-stein 거리\n\\[ W_r(f,g) = (\\int_{0}^{1} |F_f^{-1}(t) - F_g^{-1}(t)|^r)^{1/r} dt \\]\n\n\n\n\n\n\nNotation\n설명\n\n\n\n\nn\n원본(재현)자료 관측치 개수\n\n\n\\(f_i\\)\n원본자료의 i번째 관측치에 대해 준식별자 값이 같은 관측치 개수\n\n\n\\(X_i\\)\n원본자료의 i번째 관측치의 민감 변수 값\n\n\n\\(P_i\\)\n원본자료에서 \\(X_i\\) 와 같은 값을 갖는 관측치의 비율\n\n\n\\(d_i\\)\n1-\\(p_i\\)\n\n\n\\(Y_i\\)\n원본자료의 i번째 관측치와 연결된 재현자료 민감 변수 값\n\n\n\\((d_i)'\\)\n원본자료에서 \\(X_i\\)가 속한 군집에 있는 관측치의 비율\n\n\n\n- 민감변수: 준식별자를 제외한 나머지 변수\n\n민감변수(명목형)\n\n\\[ d_i \\times I(X_i=Y_t) > \\sqrt{p_i(1-p_i)}, i=1,2,\\dots,n\\]\n\n민감변수(연속형): k-means를 이용해 값을 군집화하고 부등식 확인\n\n\\[d'_i \\times |X_i - Y_i| < 1.48 \\times MAD , i=1,2,\\dots,n \\]\nMAD:중위절대편차\n원본자료의 i번째 관측치에서 위 부등식을 만족하는 민감벼눗의 비율이 5% 이상이면 1, 그렇지 않으면 0 \\(\\to\\) 지시함수 \\(R_i\\)\n- 신원 노출 위험도\n\\[ \\dfrac{1}{n} \\sum_{i=1}^{n}(\\dfrac{1}{f_i}\\times I_i \\times R_i)\\]\n작을수록 신원 추출 가능성이 작아짐\n\n구현 시간이 오래 걸림\n준식별자와 민감 변수로 구분시 명확한 기준이 없음\n\n\n\n\n\n공격자가 개인의 신원을 식별할 수는 없지만 특정 민감한 변수의 속성을 추론할 수 있을때 발생\n완전 재현자료여도 속성 노출 위험도 항상 존재\n\n- CAP(correct attribution probability)\n\n공격자가 원본자료의 일부 변수(K:key bariables)를 가지고 있고 하나의 특정 변수의 값에 대하여 알고자 하는(T:target variable) 상황에서 계산\n\\(K\\)와 \\(T\\) 모두 범주형이어야 계산 가능, 연속형 변수는 K-MEANS를 실시하여..\n\n\n\n\n\n원본자료와 재현자료의 토대를 추정\n테이블, 이미지 등 다양한 형태 데이터 져핸에 대한 평가 지표\n잠재공간으로 임베딩시 hyperparameter설정에 따라 결과가 다르게 나옴\n\n- α정밀도\n\n재현자료가 원본자료를 얼마나 충실하게 재현하는가\n재현자료 유용성 측정지표\nα정밀도가 높은 재현자료는 현실성이 높은 관측치를 포함\n\n원본 데이터 \\(D_O\\)의 확률분포의 서포트 안에서 α 만큼의 확률을 가지는 가장 작은 토대(α-support)를 \\(S_0^α\\)\n\\[ α정밀도:P_α\\] \\[ P_α := Pr(x_s \\in S_0^α), for α \\in [0,1]\\]\n\\[재현자료가 원본자료의 분포에서 나타날 가능성\\]\n- β재현율\n\n재현자료가 원본자료의 다양성을 충분히 반영하는가\n재현자료 유용성 측정지표\nβ재현율이 낮은 재현자료는 원본자료의 일부만을 반복적으로 재현\n\n\\[β재현율: R_β\\] \\[ R_β := Pr(x_o \\in S_0^β), for β \\in [0,1]\\]\n\\[재현자료의 분포가 원본자료를 얼마나 포함하지는지\\]\n- 독창섬점수 - 재현자료를 얼마나 원본자료에 존재하지 않는 새로운 관측치들을 만들어 내는가 - 정보노출의 위험성 측정 지표 - 재현자료가 원본자료를 과적합하여 그대로 사용하고 있는지?"
  },
  {
    "objectID": "posts/Review/Synthetic data/[R] synthpop.html",
    "href": "posts/Review/Synthetic data/[R] synthpop.html",
    "title": "[R] synthpop",
    "section": "",
    "text": "https://cran.r-project.org/web/packages/synthpop/synthpop.pdf\n\nR에서의 synthpop사용\n\n\nlibrary(synthpop)\n\nFind out more at https://www.synthpop.org.uk/\n\n\n\n\nrm(list = ls())                # to clean out workspace\n\n\ndata: SD2011\n\nhelp(SD2011)                   # this will give you information about it\n\n\n\nSD2011 {synthpop}R Documentation\n\n\nSocial Diagnosis 2011 - Objective and Subjective Quality of Life in Poland\n\n\nDescription\n\nSample of 5,000 individuals from the Social Diagnosis 2011 survey;\nselected variables only.\n\n\n\nUsage\n\nSD2011\n\n\nFormat\n\nA data frame with 5,000 observations on the following 35 variables:\n\n\n\nsexSex\n\nageAge of person, 2011\n\nagegrAge group, 2011\n\nplacesizeCategory of the place of residence\n\nregionRegion (voivodeship)\n\neduHighest educational qualification, 2011\n\neduspecDiscipline of completed qualification\n\nsocprofSocio-economic status, 2011\n\nunempdurTotal duration of unemployment in the last 2 years (in months)\n\nincomePersonal monthly net income\n\nmaritalMarital status\n\nmmarrMonth of marriage\n\nymarrYear of marriage\n\nmsepdivMonth of separation/divorce\n\nysepdivYear of separation/divorce\n\nlsPerception of life as a whole\n\ndepressDepression symptoms indicator\n\ntrustView on interpersonal trust\n\ntrustfamTrust in own family members\n\ntrustneighTrust in neighbours\n\nsportActive engagement in some form of sport or exercise\n\nnofriendNumber of friends\n\nsmokeSmoking cigarettes\n\nnocigaNumber of cigarettes smoked per day\n\nalcabuseDrinking too much alcohol\n\nalcsolStarting to use alcohol to cope with troubles\n\nworkabWorking abroad in 2007-2011\n\nwkabdurTotal time spent on working abroad\n\nwkabintPlans to go abroad to work in the next two years\n\nwkabintdurIntended duration of working abroad\n\nemccIntended destination country\n\nenglangKnowledge of English language\n\nheightHeight of person\n\nweightWeight of person\n\nbmiBody mass index\n\n\n\n\nNote\n\nPlease note that the original variable names have been changed to make them \nmore self-explanatory. Some variable labels have been adjusted as well.\n\n\nSource\n\nCouncil for Social Monitoring. Social Diagnosis 2000-2011: integrated database.\nhttp://www.diagnoza.com/index-en.html [downloaded on 13/12/2013]\n\n\n\nReferences\n\nCzapinski J. and Panek T. (Eds.) (2011). Social Diagnosis 2011. Objective and \nSubjective Quality of Life in Poland - full report. Contemporary Economics, \nVolume 5, Issue 3 (special issue) http://ce.vizja.pl/en/issues/volume/5/issue/3#art254 \n\n\n\nExamples\n\n  spineplot(englang ~ agegr, data = SD2011, xlab = \"Age group\", ylab = \"Knowledge of English\")\n  boxplot(income ~ sex, data = SD2011[SD2011$income != -8,])\n\n\n[Package synthpop version 1.8-0 ]"
  },
  {
    "objectID": "posts/Review/Synthetic data/Modeling Tabular Data using Conditional GAN.html",
    "href": "posts/Review/Synthetic data/Modeling Tabular Data using Conditional GAN.html",
    "title": "[논문] Modeling Tabular Data using Conditional GAN",
    "section": "",
    "text": "Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, Kalyan Veeramachaneni, “Modeling Tabular data using Conditional GAN” , NIPS’19, 2019\nCTGAN model open source\nbenchmark\nShine’s dev log Tstory"
  },
  {
    "objectID": "posts/Review/Synthetic data/Modeling Tabular Data using Conditional GAN.html#related-work",
    "href": "posts/Review/Synthetic data/Modeling Tabular Data using Conditional GAN.html#related-work",
    "title": "[논문] Modeling Tabular Data using Conditional GAN",
    "section": "Related Work",
    "text": "Related Work\n과거에 synthetic data는 랜덤변수, 결합 다변량 분포를 모형화하여 표본을 추출했다."
  },
  {
    "objectID": "posts/Review/Synthetic data/Modeling Tabular Data using Conditional GAN.html#notations",
    "href": "posts/Review/Synthetic data/Modeling Tabular Data using Conditional GAN.html#notations",
    "title": "[논문] Modeling Tabular Data using Conditional GAN",
    "section": "Notations",
    "text": "Notations\n– \\(x_1 \\oplus x_2 \\oplus \\dots\\): concatenate vectors \\(x_1, x_2, \\dots\\)\n– \\(gumbel_{\\gamma} (x)\\): apply Gumbel softmax with parameter \\(\\gamma\\) on a vector \\(x\\)\n– \\(leaky_{\\gamma}(x)\\): apply a leaky ReLU activation on \\(x\\) with leaky ratio \\(\\gamma\\)\n– \\(FC_{u \\to v}(x)\\): apply a linear transformation on a \\(u\\)-dim input to get a \\(v\\)-dim output.\n이 외에도 tanh, ReLU, softmax, BN, drop 등을 사용한다."
  },
  {
    "objectID": "posts/Review/Synthetic data/Modeling Tabular Data using Conditional GAN.html#mode-specific-normalization",
    "href": "posts/Review/Synthetic data/Modeling Tabular Data using Conditional GAN.html#mode-specific-normalization",
    "title": "[논문] Modeling Tabular Data using Conditional GAN",
    "section": "Mode-specific Normalization",
    "text": "Mode-specific Normalization\n\n\n연속형 변수 \\(C_i\\)는 variational Gaussian mixture model(VGM) 을 사용한다.\n\n위 예시에서 VGM은 \\(\\eta_1,\\eta_2,\\eta_3\\)인 \\(m_i=3\\)인 mode를 찾는다.\nGaussian mixture = \\(\\mathbb{P}_{C_i}(c_{i,j}) = \\sum_{k=1}^3 \\mu_k N (c_{i,j}: \\eta_k, \\phi_k)\\)\n\\(\\mu_k, \\phi_k\\) 의 가중치와 분산은 각각 구해놓는다.\n\n\\(c_{i,j}\\)의 확률밀도함수는 각각 \\(\\rho_1, \\rho_2, \\rho_3\\)이다.\n\ns.t \\(\\rho_k = \\mu_k N(c_{i,j}: \\eta_k, \\phi_k)\\)\n\n3번째 mode를 선택한다. 원핫인코딩을 통해서 \\(c_{i,j} \\to \\beta_{i,j}=[0,0,1]\\)로 바꾼다.\n\n그리고 \\(\\alpha_{i,j}= \\dfrac{c_{i,j}0 - \\eta_3}{4\\phi_3}\\) 가중치를 곱한다.\n연속형과 이산형 열들을 바꿔준다.\n\\(r_j = \\alpha_{1,j}\\oplus \\beta_{1,j}\\oplus \\dots \\oplus \\alpha_{N_{c,j}} \\oplus \\beta_{N_{c,j}} \\oplus d_{1,j} \\oplus \\dots \\oplus d_{N_{d,j}}\\)"
  },
  {
    "objectID": "posts/Review/Synthetic data/Modeling Tabular Data using Conditional GAN.html#conditional-generator-and-training-by-sampling",
    "href": "posts/Review/Synthetic data/Modeling Tabular Data using Conditional GAN.html#conditional-generator-and-training-by-sampling",
    "title": "[논문] Modeling Tabular Data using Conditional GAN",
    "section": "Conditional Generator and Training-by-Sampling",
    "text": "Conditional Generator and Training-by-Sampling\nclass imbalance\n만약 훈련 데이터가 랜덤샘플에서 훈련된다면 열은 적은카테고리를 가지고 있는 분류를 충분히 대표할수 없다.\n이 문제를 the conditional vector, the generator loss, the training-by-sampling method를 이용해 해결하자.\n\n\nConditional vector\n이산형 분포인 \\(D_{N_d}\\)를 원핫인코딩을 통해 \\(d_{N_d}\\)로 바꾼다.\nFor instance, for two discrete columns,D1 = f1, 2, 3g and D2 = f1, 2g,the condition (D2 = 1) is expressed by the mask vectors m1 = [0, 0, 0] and m2 = [1, 0]; so cond = [0, 0, 0, 1, 0].\n\n\nGenerator loss\n\n\nTraining-by-sampling\n- Figure2\n\\(m_i^{(k)} = I(if i=i^* \\  and \\ k=k^*)\\)\n\n\\(D_2\\) 열을 선택한다. 즉 \\(i^*=2\\)이다.\n\\(D_2\\)에서 첫번째를 선택한다. 즉 \\(k^*=1\\)이다.\n\\(m_1=[0,0,0], m_2=[1,0], cond=[0,0,0,1,0]\\)"
  },
  {
    "objectID": "posts/Review/Synthetic data/Modeling Tabular Data using Conditional GAN.html#network-structure",
    "href": "posts/Review/Synthetic data/Modeling Tabular Data using Conditional GAN.html#network-structure",
    "title": "[논문] Modeling Tabular Data using Conditional GAN",
    "section": "Network Structure",
    "text": "Network Structure\n\ngenerator \\(\\mathbb{g}(z,cond)\\)\n\n\\(h_0 = z \\oplus cond\\)\n\\(h_1 = h_0 \\oplus ReLU(BN(FC_{|cond|+|z| \\to 256}(h_0)))\\)\n\\(h_2 = h_1 \\oplus ReLU(BN(FC_{|cond|+|z|+256 \\to 256}(h_1)))\\)\n\\(\\widehat \\alpha_i = tanh(FC_{|cond|+|z|+512 \\to 1}(h_2)), \\ 1 \\leq i \\leq N_c\\)\n\\(\\widehat \\beta_i = gumbel_{0.2}(FC_{|cond|+|z|+512 \\to m_i} (h_2)), \\ 1 \\leq i \\leq N_c\\)\n\\(\\widehat d_i = gumbel_{0.2}(FC_{|cond|+|z|+512 \\to |D_i|}(h_2)), \\ 1 \\leq i \\leq N_c\\)\n\n\n\ncritic \\(C(r_1,\\dots,r_{10},cond_1,\\dots,cond_{10})\\)\n- use PacGAN with 10 samples in each pac to prevent mode collapse\n\n\\(h_0 = r_1 \\oplus \\dots \\oplus r_{10} \\oplus cond_1 \\oplus \\dots \\oplus cond_{10}\\)\n\\(h_1 = drop(leaky_{0.2}(FC_{10|r|+10|cond| \\to 256}(h_0)))\\)\n\\(h_2 = drop(leaky_{0.2}(FC_{256 \\to 256}(h_1)))\\)\n\\(C(\\cdot) = FC_{256 \\to 1}(h_2)\\)\n\nAdam optimaizer 사용한 학습률 \\(2·10^{-4}\\)"
  },
  {
    "objectID": "posts/Review/Synthetic data/Modeling Tabular Data using Conditional GAN.html#tvae-model",
    "href": "posts/Review/Synthetic data/Modeling Tabular Data using Conditional GAN.html#tvae-model",
    "title": "[논문] Modeling Tabular Data using Conditional GAN",
    "section": "TVAE Model",
    "text": "TVAE Model\n- TVAE generator\n\n\\(h_1 = ReLU(FC_{128 \\to 128}(z_j))\\)\n\\(h_2 = ReLU(FC_{128 \\to 128}(h_1))\\)\n\\(\\bar \\alpha _{i,j} = tanh(FC_{128 \\to 1}(h_2)) \\ , \\ 1 \\leq i \\leq N_c\\)\n\\(\\widehat \\alpha_{i,j} \\sim N(\\bar \\alpha_{i,j} , \\delta_i) \\ , \\ 1 \\leq i \\leq N_c\\)\n\\(\\widehat \\beta_{i,j} \\sim softmax(FC_{128 \\to m_i} (h_2)) \\ , \\ 1 \\leq i \\leq N_c\\)\n\\(\\widehat d_{i,j} \\sim softmax(FC_{128 \\to |D_i|}(h_2)) \\ , \\ 1 \\leq i \\leq N_d\\)\n\\(p_\\theta(r_j|z_j) = \\Pi_{i=1}^{N_c} \\mathbb{P}(\\widehat \\alpha_{i,j} = \\alpha_{i,j}) \\Pi_{i=1}^{N_c} \\mathbb{P}(\\widehat \\beta_{i,j} = \\beta_{i,j}) \\Pi_{i=1}^{N_d} \\mathbb{P}(\\widehat \\alpha_{i,j} = \\alpha_{i,j})\\)\n\n\\(\\widehat \\alpha_{i,j}, \\widehat \\beta_{i,j}, \\widehat d_{i,j}\\) : random variable\n\\(p_\\theta(r_j|z_j)\\) : joint distribuion\nTVAE를 Adam으로 학습한 학습률: \\(1e^{-3}\\)"
  },
  {
    "objectID": "posts/Review/Synthetic data/Modeling Tabular Data using Conditional GAN.html#baselines-and-datasets",
    "href": "posts/Review/Synthetic data/Modeling Tabular Data using Conditional GAN.html#baselines-and-datasets",
    "title": "[논문] Modeling Tabular Data using Conditional GAN",
    "section": "Baselines and Datasets",
    "text": "Baselines and Datasets\n\nsimulated data\n\n오라클 S로부터 \\(T_{train}, T_{test}\\)을 만든다.\n이 오라클은 가우시안 혼합 모델 또는 베이지안 네트워크이다.\nGridR:각 모드에 랜덤 오프셋을 추가\n베이지안 네트워크: alarm, child, asia, insurance를 사용\n\n\n\nreal datasets\n\nUCI머신러닝에서 사용되는 6개 사용\nadult, census, covertype, intrusion,news\nMNIST 사용"
  },
  {
    "objectID": "posts/Review/Synthetic data/Modeling Tabular Data using Conditional GAN.html#evaluation-metrics-and-framework",
    "href": "posts/Review/Synthetic data/Modeling Tabular Data using Conditional GAN.html#evaluation-metrics-and-framework",
    "title": "[논문] Modeling Tabular Data using Conditional GAN",
    "section": "Evaluation Metrics and Framework",
    "text": "Evaluation Metrics and Framework\n\n\nLikelihood fitness metric\n\nsimulated data 사용\n$T_{syn} L_{syn},T_{test} L_{test}, $\n\\(L_{syn}\\)이 과적합되는 문제를 해결하기 위하 \\(L_{test}\\)를 사용\n\n\n\nMachine learning efficacy\n\nreal dataset 사용\naccruracy와 F1, \\(\\mathbb{R^2}\\) 측정"
  },
  {
    "objectID": "posts/Review/Synthetic data/Modeling Tabular Data using Conditional GAN.html#benchmarking-results",
    "href": "posts/Review/Synthetic data/Modeling Tabular Data using Conditional GAN.html#benchmarking-results",
    "title": "[논문] Modeling Tabular Data using Conditional GAN",
    "section": "Benchmarking Results",
    "text": "Benchmarking Results\n\n\nGM Sim : Gaussian mixture\nBN Sim : Bayesian networks\nTVAE가 CTGAN보다 우수한 편이지만 privacy해결은 못하므로 privacy생각하면 CTGAN사용.."
  },
  {
    "objectID": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html",
    "href": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "",
    "text": "import tensorflow as tf"
  },
  {
    "objectID": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#mlp",
    "href": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#mlp",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "MLP",
    "text": "MLP\n\n멀티레이어 퍼셉트론(Multilayer Perceptrom)\n\n: 완전 연결 네트워크, 심층 피드-포워드망, 피드-포워드 신경망"
  },
  {
    "objectID": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#cnn",
    "href": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#cnn",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "CNN",
    "text": "CNN"
  },
  {
    "objectID": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#rnn",
    "href": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#rnn",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "RNN",
    "text": "RNN"
  },
  {
    "objectID": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#오토인코더-구축",
    "href": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#오토인코더-구축",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "오토인코더 구축",
    "text": "오토인코더 구축\n\n'''Example of autoencoder model on MNIST dataset\n\nThis autoencoder has modular design. The encoder, decoder and autoencoder\nare 3 models that share weights. For example, after training the\nautoencoder, the encoder can be used to  generate latent vectors\nof input data for low-dim visualization like PCA or TSNE.\n'''\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.layers import Conv2D, Flatten\nfrom tensorflow.keras.layers import Reshape, Conv2DTranspose\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras import backend as K\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# load MNIST dataset\n(x_train, _), (x_test, _) = mnist.load_data()\n\n# reshape to (28, 28, 1) and normalize input images\nimage_size = x_train.shape[1]\nx_train = np.reshape(x_train, [-1, image_size, image_size, 1])\nx_test = np.reshape(x_test, [-1, image_size, image_size, 1])\nx_train = x_train.astype('float32') / 255\nx_test = x_test.astype('float32') / 255\n\n# network parameters\ninput_shape = (image_size, image_size, 1)\nbatch_size = 32\nkernel_size = 3\nlatent_dim = 16\n# encoder/decoder number of CNN layers and filters per layer\nlayer_filters = [32, 64]\n\n# build the autoencoder model\n# first build the encoder model\ninputs = Input(shape=input_shape, name='encoder_input')\nx = inputs\n# stack of Conv2D(32)-Conv2D(64)\nfor filters in layer_filters:\n    x = Conv2D(filters=filters,\n               kernel_size=kernel_size,\n               activation='relu',\n               strides=2,\n               padding='same')(x)\n\n# shape info needed to build decoder model\n# so we don't do hand computation\n# the input to the decoder's first\n# Conv2DTranspose will have this shape\n# shape is (7, 7, 64) which is processed by\n# the decoder back to (28, 28, 1)\nshape = K.int_shape(x)\n\n# generate latent vector\nx = Flatten()(x)\nlatent = Dense(latent_dim, name='latent_vector')(x)\n\n# instantiate encoder model\nencoder = Model(inputs,\n                latent,\n                name='encoder')\nencoder.summary()\nplot_model(encoder,\n           to_file='encoder.png',\n           show_shapes=True)\n\n# build the decoder model\nlatent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n# use the shape (7, 7, 64) that was earlier saved\nx = Dense(shape[1] * shape[2] * shape[3])(latent_inputs)\n# from vector to suitable shape for transposed conv\nx = Reshape((shape[1], shape[2], shape[3]))(x)\n\n# stack of Conv2DTranspose(64)-Conv2DTranspose(32)\nfor filters in layer_filters[::-1]:\n    x = Conv2DTranspose(filters=filters,\n                        kernel_size=kernel_size,\n                        activation='relu',\n                        strides=2,\n                        padding='same')(x)\n\n# reconstruct the input\noutputs = Conv2DTranspose(filters=1,\n                          kernel_size=kernel_size,\n                          activation='sigmoid',\n                          padding='same',\n                          name='decoder_output')(x)\n\n# instantiate decoder model\ndecoder = Model(latent_inputs, outputs, name='decoder')\ndecoder.summary()\nplot_model(decoder, to_file='decoder.png', show_shapes=True)\n\n# autoencoder = encoder + decoder\n# instantiate autoencoder model\nautoencoder = Model(inputs,\n                    decoder(encoder(inputs)),\n                    name='autoencoder')\nautoencoder.summary()\nplot_model(autoencoder,\n           to_file='autoencoder.png',\n           show_shapes=True)\n\n# Mean Square Error (MSE) loss function, Adam optimizer\nautoencoder.compile(loss='mse', optimizer='adam')\n\n# train the autoencoder\nautoencoder.fit(x_train,\n                x_train,\n                validation_data=(x_test, x_test),\n                epochs=1,\n                batch_size=batch_size)\n\n# predict the autoencoder output from test data\nx_decoded = autoencoder.predict(x_test)\n\n# display the 1st 8 test input and decoded images\nimgs = np.concatenate([x_test[:8], x_decoded[:8]])\nimgs = imgs.reshape((4, 4, image_size, image_size))\nimgs = np.vstack([np.hstack(i) for i in imgs])\nplt.figure()\nplt.axis('off')\nplt.title('Input: 1st 2 rows, Decoded: last 2 rows')\nplt.imshow(imgs, interpolation='none', cmap='gray')\nplt.savefig('input_and_decoded.png')\nplt.show()\n\nModel: \"encoder\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n encoder_input (InputLayer)  [(None, 28, 28, 1)]       0         \n                                                                 \n conv2d (Conv2D)             (None, 14, 14, 32)        320       \n                                                                 \n conv2d_1 (Conv2D)           (None, 7, 7, 64)          18496     \n                                                                 \n flatten (Flatten)           (None, 3136)              0         \n                                                                 \n latent_vector (Dense)       (None, 16)                50192     \n                                                                 \n=================================================================\nTotal params: 69,008\nTrainable params: 69,008\nNon-trainable params: 0\n_________________________________________________________________\nYou must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\nModel: \"decoder\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n decoder_input (InputLayer)  [(None, 16)]              0         \n                                                                 \n dense (Dense)               (None, 3136)              53312     \n                                                                 \n reshape (Reshape)           (None, 7, 7, 64)          0         \n                                                                 \n conv2d_transpose (Conv2DTra  (None, 14, 14, 64)       36928     \n nspose)                                                         \n                                                                 \n conv2d_transpose_1 (Conv2DT  (None, 28, 28, 32)       18464     \n ranspose)                                                       \n                                                                 \n decoder_output (Conv2DTrans  (None, 28, 28, 1)        289       \n pose)                                                           \n                                                                 \n=================================================================\nTotal params: 108,993\nTrainable params: 108,993\nNon-trainable params: 0\n_________________________________________________________________\nYou must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\nModel: \"autoencoder\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n encoder_input (InputLayer)  [(None, 28, 28, 1)]       0         \n                                                                 \n encoder (Functional)        (None, 16)                69008     \n                                                                 \n decoder (Functional)        (None, 28, 28, 1)         108993    \n                                                                 \n=================================================================\nTotal params: 178,001\nTrainable params: 178,001\nNon-trainable params: 0\n_________________________________________________________________\nYou must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0212 - val_loss: 0.0104\n313/313 [==============================] - 1s 2ms/step\n\n\n\n\n\n\n인코더 모델은 낮은 차원의 잠재 벡터를 생성하기 위해서 Conv2D(32)-Conv2D(64)-Dense(16)으로 구성\n디코더 모델은 Dense(16)-Conv2DTranspose(64)-Conv2DTranspose(32)-Conv2DTranspose(1)으로 구성\n입력은 원본 입력을 복원하기 위한 디코딩된 잠재 벡터"
  },
  {
    "objectID": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#잠재벡터-시각화",
    "href": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#잠재벡터-시각화",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "잠재벡터 시각화",
    "text": "잠재벡터 시각화\n\n- 잠재 코드 차원\n\n숫자 0: 왼쪽 아래 사분면\n숫자 1: 오른쪽 위 사분면"
  },
  {
    "objectID": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#노이즈-제거-오토인코더dae",
    "href": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#노이즈-제거-오토인코더dae",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "노이즈 제거 오토인코더(DAE)",
    "text": "노이즈 제거 오토인코더(DAE)\n노이즈 제거 Denoising\n\\[ x = x_{orig} + noise \\]\n인코더의 목적: 잠재벡터인 \\(z\\)를 생성하는 방법을 찾는 것 \\(\\to\\) 잠재 벡터를 디코더가 MSE와 같은 손실 함수의 비 유사성을 최소화하여 \\(x_{orig}\\)로 복원\n\\[ L(x_{orig}, \\tilde{x}) = MSE = \\frac{1}{m} \\sum _{i=1} ^{i=m} (x _{origi} - {\\tilde{x _{i}}} ) ^{2}\\]\n\n'''Trains a denoising autoencoder on MNIST dataset.\n\nDenoising is one of the classic applications of autoencoders.\nThe denoising process removes unwanted noise that corrupted the\ntrue data.\n\nNoise + Data ---> Denoising Autoencoder ---> Data\n\nGiven a training dataset of corrupted data as input and\ntrue data as output, a denoising autoencoder can recover the\nhidden structure to generate clean data.\n\nThis example has modular design. The encoder, decoder and autoencoder\nare 3 models that share weights. For example, after training the\nautoencoder, the encoder can be used to  generate latent vectors\nof input data for low-dim visualization like PCA or TSNE.\n'''\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.layers import Conv2D, Flatten\nfrom tensorflow.keras.layers import Reshape, Conv2DTranspose\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.datasets import mnist\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nnp.random.seed(1337)\n\n# load MNIST dataset\n(x_train, _), (x_test, _) = mnist.load_data()\n\n# reshape to (28, 28, 1) and normalize input images\nimage_size = x_train.shape[1]\nx_train = np.reshape(x_train, [-1, image_size, image_size, 1])\nx_test = np.reshape(x_test, [-1, image_size, image_size, 1])\nx_train = x_train.astype('float32') / 255\nx_test = x_test.astype('float32') / 255\n\n# generate corrupted MNIST images by adding noise with normal dist\n# centered at 0.5 and std=0.5\nnoise = np.random.normal(loc=0.5, scale=0.5, size=x_train.shape)\nx_train_noisy = x_train + noise\nnoise = np.random.normal(loc=0.5, scale=0.5, size=x_test.shape)\nx_test_noisy = x_test + noise\n\n# adding noise may exceed normalized pixel values>1.0 or <0.0\n# clip pixel values >1.0 to 1.0 and <0.0 to 0.0\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)\n\n# network parameters\ninput_shape = (image_size, image_size, 1)\nbatch_size = 32\nkernel_size = 3\nlatent_dim = 16\n# encoder/decoder number of CNN layers and filters per layer\nlayer_filters = [32, 64]\n\n# build the autoencoder model\n# first build the encoder model\ninputs = Input(shape=input_shape, name='encoder_input')\nx = inputs\n\n# stack of Conv2D(32)-Conv2D(64)\nfor filters in layer_filters:\n    x = Conv2D(filters=filters,\n               kernel_size=kernel_size,\n               strides=2,\n               activation='relu',\n               padding='same')(x)\n\n# shape info needed to build decoder model so we don't do hand computation\n# the input to the decoder's first Conv2DTranspose will have this shape\n# shape is (7, 7, 64) which can be processed by the decoder back to (28, 28, 1)\nshape = K.int_shape(x)\n\n# generate the latent vector\nx = Flatten()(x)\nlatent = Dense(latent_dim, name='latent_vector')(x)\n\n# instantiate encoder model\nencoder = Model(inputs, latent, name='encoder')\nencoder.summary()\n\n# build the decoder model\nlatent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n# use the shape (7, 7, 64) that was earlier saved\nx = Dense(shape[1] * shape[2] * shape[3])(latent_inputs)\n# from vector to suitable shape for transposed conv\nx = Reshape((shape[1], shape[2], shape[3]))(x)\n\n# stack of Conv2DTranspose(64)-Conv2DTranspose(32)\nfor filters in layer_filters[::-1]:\n    x = Conv2DTranspose(filters=filters,\n                        kernel_size=kernel_size,\n                        strides=2,\n                        activation='relu',\n                        padding='same')(x)\n\n# reconstruct the denoised input\noutputs = Conv2DTranspose(filters=1,\n                          kernel_size=kernel_size,\n                          padding='same',\n                          activation='sigmoid',\n                          name='decoder_output')(x)\n\n# instantiate decoder model\ndecoder = Model(latent_inputs, outputs, name='decoder')\ndecoder.summary()\n\n# autoencoder = encoder + decoder\n# instantiate autoencoder model\nautoencoder = Model(inputs, decoder(encoder(inputs)), name='autoencoder')\nautoencoder.summary()\n\n# Mean Square Error (MSE) loss function, Adam optimizer\nautoencoder.compile(loss='mse', optimizer='adam')\n\n# train the autoencoder\nautoencoder.fit(x_train_noisy,\n                x_train,\n                validation_data=(x_test_noisy, x_test),\n                epochs=10,\n                batch_size=batch_size)\n\n# predict the autoencoder output from corrupted test images\nx_decoded = autoencoder.predict(x_test_noisy)\n\n# 3 sets of images with 9 MNIST digits\n# 1st rows - original images\n# 2nd rows - images corrupted by noise\n# 3rd rows - denoised images\nrows, cols = 3, 9\nnum = rows * cols\nimgs = np.concatenate([x_test[:num], x_test_noisy[:num], x_decoded[:num]])\nimgs = imgs.reshape((rows * 3, cols, image_size, image_size))\nimgs = np.vstack(np.split(imgs, rows, axis=1))\nimgs = imgs.reshape((rows * 3, -1, image_size, image_size))\nimgs = np.vstack([np.hstack(i) for i in imgs])\nimgs = (imgs * 255).astype(np.uint8)\nplt.figure()\nplt.axis('off')\nplt.title('Original images: top rows, '\n          'Corrupted Input: middle rows, '\n          'Denoised Input:  third rows')\nplt.imshow(imgs, interpolation='none', cmap='gray')\nImage.fromarray(imgs).save('corrupted_and_denoised.png')\nplt.show()\n\nModel: \"encoder\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n encoder_input (InputLayer)  [(None, 28, 28, 1)]       0         \n                                                                 \n conv2d_4 (Conv2D)           (None, 14, 14, 32)        320       \n                                                                 \n conv2d_5 (Conv2D)           (None, 7, 7, 64)          18496     \n                                                                 \n flatten_2 (Flatten)         (None, 3136)              0         \n                                                                 \n latent_vector (Dense)       (None, 16)                50192     \n                                                                 \n=================================================================\nTotal params: 69,008\nTrainable params: 69,008\nNon-trainable params: 0\n_________________________________________________________________\nModel: \"decoder\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n decoder_input (InputLayer)  [(None, 16)]              0         \n                                                                 \n dense_2 (Dense)             (None, 3136)              53312     \n                                                                 \n reshape_2 (Reshape)         (None, 7, 7, 64)          0         \n                                                                 \n conv2d_transpose_4 (Conv2DT  (None, 14, 14, 64)       36928     \n ranspose)                                                       \n                                                                 \n conv2d_transpose_5 (Conv2DT  (None, 28, 28, 32)       18464     \n ranspose)                                                       \n                                                                 \n decoder_output (Conv2DTrans  (None, 28, 28, 1)        289       \n pose)                                                           \n                                                                 \n=================================================================\nTotal params: 108,993\nTrainable params: 108,993\nNon-trainable params: 0\n_________________________________________________________________\nModel: \"autoencoder\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n encoder_input (InputLayer)  [(None, 28, 28, 1)]       0         \n                                                                 \n encoder (Functional)        (None, 16)                69008     \n                                                                 \n decoder (Functional)        (None, 28, 28, 1)         108993    \n                                                                 \n=================================================================\nTotal params: 178,001\nTrainable params: 178,001\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/10\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0367 - val_loss: 0.0205\nEpoch 2/10\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0193 - val_loss: 0.0180\nEpoch 3/10\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0176 - val_loss: 0.0172\nEpoch 4/10\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0166\nEpoch 5/10\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0163 - val_loss: 0.0163\nEpoch 6/10\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0160 - val_loss: 0.0161\nEpoch 7/10\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0157 - val_loss: 0.0160\nEpoch 8/10\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0154 - val_loss: 0.0160\nEpoch 9/10\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0153 - val_loss: 0.0157\nEpoch 10/10\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0151 - val_loss: 0.0156\n313/313 [==============================] - 1s 2ms/step"
  },
  {
    "objectID": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#자동-채색-오토인코더",
    "href": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#자동-채색-오토인코더",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "자동 채색 오토인코더",
    "text": "자동 채색 오토인코더\n\n해당 코딩은 너무 길어서 생략. 자세한 것은 여기 링크 참고 \\(\\to\\) 자동 채색 오토인코더\n입력: 회색도 사진, 출력: 해당하는 채색된 사진들로 오토인코더를 훈련"
  },
  {
    "objectID": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#요약",
    "href": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#요약",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "요약",
    "text": "요약\n\n노이즈 제거, 채색 등 구조적인 변환을 효율적으로 하기 위하여 데이터를 낮은 차원의 표현으로 압축하는 신경망"
  },
  {
    "objectID": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#생성기-개발을-위한-클래스",
    "href": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#생성기-개발을-위한-클래스",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "생성기 개발을 위한 클래스",
    "text": "생성기 개발을 위한 클래스\nclass Generator:\n    \n    def __init__(self):\n        self.initVariable = 1\n        \n    def lossFunction(self):\n        \n        return\n    \n    def buldModel(self):\n        \n        return\n    \n    def trainModel(self, inputX, inputY):\n        \n        return"
  },
  {
    "objectID": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#판별기-개발을-위한-클래스",
    "href": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#판별기-개발을-위한-클래스",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "판별기 개발을 위한 클래스",
    "text": "판별기 개발을 위한 클래스\n\nclass Discriminator:\n    \n    def __init__(self):\n        self.initVariable = 1\n        \n    def lossFunction(self):\n        \n        return\n    \n    def buildModel(self):\n        \n        return\n    \n    def trainModel(self,inputX,inputY):\n        \n        return"
  },
  {
    "objectID": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#손실-함수",
    "href": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#손실-함수",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "손실 함수",
    "text": "손실 함수\n\nclass Loss:\n    \n    def __init__(self):\n        self.initVariable = 1\n        \n    def lossBaseFunction1(self):\n        \n        return\n    \n    def lossBaseFunction2(self):\n        \n        return\n    \n    \n    def lossBaseFunction3(self):\n        \n        return\n- 적대적 훈련을 할 때 생성기에서 사용하는 손실함수\n\\[\\nabla \\theta_g \\sum_{i=1}^{m} log(1-D(G(z^{(i)}))) \\]\n- GAN에서 적용되는 표준 교차 엔트로피 구현\n\\[ \\nabla \\theta_d \\dfrac{1}{m} \\sum_{i=1}^{m} [logD(x^{(i)})+log(1-D(G(z^{(i)})))] \\]\n\n굿펠로우 논문에 나오는 함수\n\n\nnote: 교수님이 설명해주신 코드 보는게 더 나을듯 하다. 교수님 코드"
  },
  {
    "objectID": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#dcgan",
    "href": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#dcgan",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "DCGAN",
    "text": "DCGAN\n\n심층 CNN을 이용하여 초기 GAN를 성공적으로 구현"
  },
  {
    "objectID": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#조건부conditional-gan",
    "href": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#조건부conditional-gan",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "조건부(Conditional) GAN",
    "text": "조건부(Conditional) GAN\n\n원-핫 벡터를 제외하면 DCGAN과 유사\n생성자와 판별자의 출력에 조건을 부여하기 위해 원-핫 벡터 사용"
  },
  {
    "objectID": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#wasserstein-gan",
    "href": "posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.html#wasserstein-gan",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "Wasserstein GAN",
    "text": "Wasserstein GAN\n\nGAN의 불안정성은 Jensen-Shannon (JS) 거리에 기초한 손실함수 때문이라고 주장\nGAN의 최적화에 더 알맞게 JS거리 함수를 대체하기에 적합한 것을 찾아야함\n\nref: https://lilianweng.github.io/posts/2017-08-20-gan/\n\n책 개념이 너무 어렵당.. 관련 수식에 대해 이해하고 싶은뎀 수식에 대한 내용이 자세하진 않음.. 일단 수식에 대한 내용이해 먼저 하고 추후에 다시 책 읽어보는 걸로~~"
  },
  {
    "objectID": "posts/Theoretical statistics/TS5.html",
    "href": "posts/Theoretical statistics/TS5.html",
    "title": "TS HW5",
    "section": "",
    "text": "\\((X_1, X_2, X_3)\\) ∼ \\(MULT(n, p_1, p_2, p_3)\\)일 때 \\(X_1|X_2 = x_2\\)의 조건부 확률밀도함수를 구하시오.\n\\(f_{X_1,X_2,X_3}(x_1,x_2,x_3)=\\begin{pmatrix} n \\\\ x_1 \\ x_2\\  x_3 \\end{pmatrix} p_1^{x_1} p_2^{x_2} p_3^{x_3}\\) , 단 \\(\\begin{pmatrix} 0\\leq x_{1}\\leq 1 & \\\\ 0\\leq x_{2}\\leq 1 & \\\\ 0\\leq x_{3}\\leq 1 & \\\\ x_{1}+x_{2}+x_{3}= & 1 \\end{pmatrix}\\)\n\\(X_1\\) ~ \\(B(n,p_1)\\)이므로 \\(E(X_1)=np_1, E(X_2)=np_2\\)\n\\(f_{X_1,X_2}(x_1,x_2)=P(X_1=x_1,X_2=x_2)\\) 이고 \n이 실험에서는,\n\\(=P(X_1=x_1,X_2=x_2,X_3=n-x_1-x_2)\\)\n\\(=\\dfrac{n!}{x_1! x_2! (n-x_1 -x_2)!}p_1^{x_1} p_2^{x_2} p_3^{n-x_1-x_2}\\)\n분모는, 이항분포를 따르므로\n\\(f_{X_2}(x_2) = \\dfrac{n!}{x_2!(n-x_2)!}p_2^{x_2}(1-p_2)^{n-x_2}\\)\n\\(f_{X_1|X_2}(x_1|x_2)=\\dfrac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)}\\)\n\\(=\\dfrac{\\dfrac{n!}{x_1! x_2! (n-x_1 -x_2)!}p_1^{x_1} p_2^{x_2} p_3^{n-x_1-x_2}}{\\dfrac{n!}{x_2!(n-x_2)!}p_2^{x_2}(1-p_2)^{n-x_2}}\\)\n\\(=\\dfrac{(n-x_2)!}{x_1!(n-x_2-x_1)!} \\left( \\dfrac{p_{1}}{1-p_{2}}\\right) ^{x_{1}} \\left( \\dfrac{p_{3}}{1-p_{2}}\\right) ^{n-x_{2}-x_{1}}\\)\n\\(=\\begin{pmatrix} n-x_{2} \\\\ x_{1} \\end{pmatrix} \\left( \\dfrac{p_{1}}{1-p_{2}}\\right) ^{x_{1}} \\left( 1-\\dfrac{p_{1}}{1-p_{2}}\\right) ^{n-x_{2}-x_{1}}\\)\n즉, \\(B(n-x_2, \\dfrac{p_1}{1-p_2})\\)를 따른다."
  },
  {
    "objectID": "posts/Theoretical statistics/TS5.html#section",
    "href": "posts/Theoretical statistics/TS5.html#section",
    "title": "TS HW5",
    "section": "(1)",
    "text": "(1)\n\\(Y = |X − µ|\\)의 확률밀도함수\n\\(f_X(x)=\\dfrac{1}{\\sqrt{2\\pi \\sigma^2}}e^ {-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\n\\(F_Y(y) = P(Y \\le y) = P(|X-\\mu| \\le y) = P(\\mu-y \\le X \\le \\mu+y) = F_X(\\mu + y) - F_X(\\mu - y)\\) 이다.\n\\(f_y(y) = \\frac{d}{dy}\\{F_X(\\mu + y) - F_X(\\mu - y) \\} = f_X (\\mu + y) + f_X(\\mu - y) = \\frac{2}{\\sqrt{2\\pi}\\sigma} exp(-\\frac{y^2}{2\\sigma^2}) I(y>0)\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS5.html#section-1",
    "href": "posts/Theoretical statistics/TS5.html#section-1",
    "title": "TS HW5",
    "section": "(2)",
    "text": "(2)\n\\(Y = exp(X)\\)의 확률밀도함수\n\\(g(y) = e^X\\) 라 놓자.\n\\(g^{-1}(y) = lny\\)\n\\(\\dfrac{d}{dy}(g^{-1}(y))= \\dfrac{1}{y}\\)\n\\(f_Y(y) = f_X(g^{-1}(y))\\left| \\dfrac{d g^{-1}(y) }{dy} \\right| = \\dfrac{1}{y\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(lny - \\mu)^2}{2\\sigma^2}}\\)\n\\(= \\dfrac{1}{y\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(lny - \\mu)^2}{2\\sigma^2}}I(y>0)\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS5.html#section-2",
    "href": "posts/Theoretical statistics/TS5.html#section-2",
    "title": "TS HW5",
    "section": "(1)",
    "text": "(1)\n\\(Z = Y_1 + Y_2\\)의 확률분포\n\\(Y_1\\)과 \\(Y_2\\)는 서로 독립이므로 합성곱 이용해보면, 범위는 \\(0<y_1<z\\)이고\n\\(f_Z(z)=\\int_{-\\infty}^\\infty f_{Y_2}(z-y_1) f_{Y_1}(y_1) dy_1=\\dfrac{e^{-2/z}}{8}\\int_0^z y_1 dy_1=\\dfrac{ze^{-2/z}}{8}I(z>0)\\)\n 교수님 풀이\n\\(f_Z(z) = \\dfrac{d}{dz}F_Z(z)\\)\n\\(F_Z(z) = P(Z \\leq z) = P(Y_1 + Y_2 \\leq z)\\) 이므로 \\((Y_1, Y_2)\\)의 사건\n\\(\\int\\int_{y_1+y_2 \\leq z} f_{Y_1,Y_2}(y_1,y_2)dy_1dy_2\\)\n=\\(\\int\\int_{y_1+y_2 \\leq z} \\dfrac{1}{8}y_1e^{-(y_1+y_2)/2}I(y_1>0)I(y_2>0)dy_1dy_2\\)\n=\\(\\int_0^z \\dfrac{1}{8}y_1 e^{-\\frac{y_1}{2}}\\int_0^{z-y_1}e^{-\\frac{y_2}{2}}dy_2dy_1\\)\n=\\(\\int_0^z \\dfrac{1}{8}y_1 e^{-\\frac{y_1}{2}} \\left[ -2e^{-\\frac{y^2}{2}} \\right]_0^{z-y_1}dy_1\\)\n=\\(\\int_0^z \\dfrac{1}{8}y_1 e^{-\\frac{y_1}{2}} 2(1-e^{-\\frac{z-y_1}{2}})dy_1\\)\n=\\(\\int_0^z \\dfrac{1}{4}y_1 \\left( e^{-\\frac{y_1}{2}}-e^{-\\frac{z}{2}} \\right)dy_1\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS5.html#section-3",
    "href": "posts/Theoretical statistics/TS5.html#section-3",
    "title": "TS HW5",
    "section": "(2)",
    "text": "(2)\n\\(U = Y_2/Y_1\\)와 \\(V = Y_1\\)의 결합확률밀도함수\n\\(U.V\\)를 연립하면 \\(Y_1=V, Y_2=UV\\) 이다.\n\\(|J|=\\begin{vmatrix} 0 & 1 \\\\ v & u \\end{vmatrix}=|-v|=v\\)\n\\(f_{U,V}(u,v) = f_{Y_1,Y_2}(y_1,y_2)|J| = f_{Y_1,Y_2}(v,uv)v =\\dfrac{1}{8} v^2 e^{-(v+uv)/2}I(u>0,v>0)\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS5.html#section-4",
    "href": "posts/Theoretical statistics/TS5.html#section-4",
    "title": "TS HW5",
    "section": "(3)",
    "text": "(3)\n\\((2)\\)의 결과를 이용하여 \\(U\\)의 주변확률밀도함수\n\\(f_U(u)=\\int_{-\\infty}^{\\infty} f_{U,V}(u,v) dv = \\int_{0}^{\\infty} \\dfrac{1}{8} v^2 e^{-(v+uv)/2}I(u>0)dv\\)\n \\(\\dfrac{v(u+1)}{2}=t\\)치환, \\(v=\\dfrac{2t}{u+1}, dv=\\dfrac{2}{u+1}dt\\)\n=\\(\\int_{-\\infty}^\\infty \\dfrac{1}{8} (\\dfrac{2}{u+1})^3 t^2 e^{-t} I(u>0)I(t>0)dt\\)\n=\\(\\dfrac{1}{8}(\\dfrac{2}{u+1})^3I(u>0) \\int_0^\\infty t^2e^{-t}dt\\)\n\\(\\because \\int_0^\\infty t^2e^{-t}dt=\\Gamma(3)=2!=2\\)\n=\\(\\dfrac{2}{(u+1)^3}I(u>0)\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS5.html#section-5",
    "href": "posts/Theoretical statistics/TS5.html#section-5",
    "title": "TS HW5",
    "section": "(3)",
    "text": "(3)\n\\(U = X − Y\\) 라고 할 때 \\(U\\)의 확률밀도함수\n\\(U=X-Y, V=Y\\)이면 \\(X=U+V, Y=V, |J|=1\\)\n\\(f_{U,V}(u,v)=p^2(1-p)^{u+2v-2}\\)\n\\(f_U(u)=\\sum_v f_{U,V}(u,v) = \\sum_{v=1}^\\infty (1-p)^u [p(1-p)^{v-1}]^2 = \\dfrac{p}{2-p}(1-p)^u\\)\n \\(u\\)의 범위가 \\((-\\infty, \\infty)\\)이고, 위 식은 \\(u>0\\)인 경우에만 성립한다.\n만약 \\(u\\leq0\\)인 경우를 생각하면\n\\(\\begin{aligned}\\sum \\sum \\\\ \\begin{pmatrix} x=y+n \\\\ y=1.2,\\dots \\\\ x\\geq 1 \\end{pmatrix}\\end{aligned} = \\sum_{y=1-u}^\\infty p^2 (1-p)^{2y+u-2} = p^2 \\dfrac{(1-p)^{-u}}{1-(1-p)^2}=\\dfrac{p}{(2-p)(1-p)^u}\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS5.html#section-6",
    "href": "posts/Theoretical statistics/TS5.html#section-6",
    "title": "TS HW5",
    "section": "(1)",
    "text": "(1)\n\\(P (X = Y )\\)\n\\(P (X = Y )=P(X-Y=0)=P(U=0)=\\dfrac{p}{2-p}\\)\n교수님 풀이\n\\(f_{X,Y}(x,y) = \\sum \\sum_{x=y}f_{X,Y}(x,y)= \\begin{aligned}\\sum \\sum \\\\ \\begin{pmatrix} x=y \\\\ x=1,2,\\dots \\\\ y=1,2,\\dots \\end{pmatrix}\\end{aligned}p^2(1-p)^{x+y-2}=\\sum_{k=1}^\\infty (1-p)^{2k-2}=\\dfrac{p}{2-p}\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS5.html#section-7",
    "href": "posts/Theoretical statistics/TS5.html#section-7",
    "title": "TS HW5",
    "section": "(2)",
    "text": "(2)\n\\(P (X − Y = 1)\\)\n\\(P (X − Y = 1)=P(U=1)=\\dfrac{p(1-p)}{2-p}\\)\n교수님 풀이\n\\(f_{X,Y}(x,y) = \\sum \\sum_{x-y=1}f_{X,Y}(x,y)= \\begin{aligned}\\sum \\sum \\\\ \\begin{pmatrix} x-y=1 \\\\ x=1,2,\\dots \\\\ y=1,2,\\dots \\end{pmatrix}\\end{aligned}p^2(1-p)^{x+y-2}=\\sum_{y=1}^\\infty (1-p)^{2y-1}=\\dfrac{p(1-p)}{2-p}\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS4.html",
    "href": "posts/Theoretical statistics/TS4.html",
    "title": "TS HW4",
    "section": "",
    "text": "1번\n음이항분포 \\(NB(r, p)\\)의 확률밀도함수 \\(f(x)\\)가 \\(\\sum_{x=r}^\\infty f(x) = 1\\)을 만족함을 음이항정리를 이용하여 증명하시오\n음이항정리\n\\(\\dfrac{1}{1-q} = \\sum_{x=0}^{\\infty} \\begin{pmatrix} x+1-1 \\\\ 1-1 \\end{pmatrix} q^x (|x|<1)\\)\n\\(\\dfrac{1}{(1-q)^2} = \\dfrac{d}{dq} (\\dfrac{1}{1-q}) = \\sum_{x=1}^\\infty xq^{x-1} = \\sum_{x=0}^\\infty (x+1)q^x = \\sum_{x=0}^{\\infty} \\begin{pmatrix} x+2-1 \\\\ 2-1 \\end{pmatrix} q^x (|x|<1)\\)\n수학적 귀납법을 통해\n\\((1-q)^{-r} = \\sum_{x=0}^\\infty \\begin{pmatrix} x+r-1 \\\\ r-1 \\end{pmatrix} q^x\\) 임을 구할 수 있다. -(A)\n\\(\\sum_{x=r}^\\infty f(x)\\)\n= \\(\\sum_{x=r}^\\infty \\begin{pmatrix} x-r \\\\ r-1 \\end{pmatrix} p^r (1-p)^{x-r} (x=r, r+1, \\dots)\\)\n= \\(p^r \\sum_{x=r}^\\infty \\begin{pmatrix} x-r \\\\ r-1 \\end{pmatrix} (1-p)^{x-r}\\) 이고\n\\(x-r=t\\)로 치환하면\n= \\(p^r \\sum_{t=0}^\\infty \\begin{pmatrix} t+r-1 \\\\ r-1 \\end{pmatrix} (1-p)^t\\) 이다.\n(A)식을 이용해\n= \\(p^r(1-q)^{-r} = p^r p^{-r} = 1\\) 을 증명할 수 있다.\n이항정리 \\((x+y)^n = \\sum_{k=0}^n \\begin{pmatrix} n \\\\ k \\end{pmatrix} x^k y^{n-k}\\)\n일반화된 이항정리 \\((x+y)^a = \\sum_{k=0}^\\infty \\begin{pmatrix} a \\\\ k \\end{pmatrix} a^k y^{n-k}, a\\in \\mathbb{R}, -1<x<1\\)\n즉, \\((1-w)^{-r}= \\sum_{k=0}^\\infty \\begin{pmatrix} -r \\\\ k \\end{pmatrix} (-w)^k\\)\n=\\(\\sum_{k=0}^\\infty \\dfrac{-r \\cdot (-r-1) \\dots \\cdot (-r-k+1) }{k!}(-1)^k w^k\\)\n=\\(\\sum_{k=0}^\\infty \\dfrac{r \\cdot (r+1) \\dots \\cdot (r+k-1) }{k!}(-1)^{2k} w^k\\)\n\\(\\sum_{k=0}^\\infty \\dfrac{-r \\cdot (-r-1) \\dots \\cdot (-r-k+1) }{k!}(-1)^k w^k\\)\n\\(\\sum_{k=0}^\\infty\\begin{pmatrix} r+k-1 \\\\ k \\end{pmatrix} w^k\\)\n\\(x=r+k\\)로 치환하면\n=\\(\\sum_{x=r}^\\infty \\begin{pmatrix} x-1 \\\\ r-1 \\end{pmatrix} w^{x-r}\\)\n\\(s.t \\sum \\begin{pmatrix} x-1 \\\\ r-1 \\end{pmatrix} p^r (1-p)^{x-r} = p^r \\sum \\begin{pmatrix} x-1 \\\\ r-1 \\end{pmatrix} (1-p)^{x-r} = p^r {1-(1-p)}^{-r} = 1\\)\n\n\n2번\n음이항분포 \\(NB(r, p)\\)의 적률생성함수를 구하시오\n\\(Y_i\\)가 기하분포를 따르면 \\(\\sum_{i=1}^r Y_i = X~NB(r,p)\\)이다.\n\\(M_X(t)=E(e^{tX})=E(e^{tY_1+\\dots+tY_r})\\)\n\\(Y_i\\)는 각각 독립이므로,\n=\\(E(e^{tY_1}) \\dots E(e^{tY_r})\\)\n=\\((\\dfrac{pe^t}{1-qe^t})^r\\)\n\n\n3번\n베타분포 \\(BETA(a, b)\\)의 \\(k\\)차적률이 \\(E(X^k) = \\dfrac{Γ(a + k)Γ(a + b)}{Γ(a)Γ(a + b + k)}\\)임을 보이시오.\n\\(E(X^k)=\\int_{-\\infty}^{\\infty}x^k \\dfrac{1}{B(a,b)}x^{a-1}(1-x)^{b-1}I(0<x<1)dx\\)\n= \\(\\dfrac{1}{B(a,b)}\\int_0^1 x^{a+k-1}(1-x)^{b-1}dx\\)\n= \\(\\dfrac{B(a+k,b)}{B(a,b)}= \\dfrac{\\Gamma(a+k)\\Gamma(b)}{\\Gamma(a+k+b)} \\dfrac{\\Gamma(a+B)}{\\Gamma(a)\\Gamma(b)}=\\dfrac{\\Gamma(a+k) \\Gamma(a+b)}{\\Gamma(a) \\Gamma(a+b+k)}\\)\n과제 3의 9번 문제도 이 공식을 이용해 구할 수 있다.\n\n\n4번\n감마분포 \\(GAM(α, β)\\)의 평균과 분산의 기댓값 정의를 이용하여 구하시오.\n\\(E(X)=\\dfrac{1}{\\Gamma(\\alpha) \\beta^\\alpha}\\int_0^\\infty x x^{\\alpha -1} e^{-x/\\beta}\\)\n\\(=\\dfrac{\\Gamma(\\alpha+1) \\beta^{\\alpha+1}}{\\Gamma(\\alpha) \\beta^\\alpha} \\int_0^\\infty \\dfrac{1}{\\Gamma(\\alpha+1)\\beta^{\\alpha+1}} x^{(a+1)-1}e^{-x/\\beta}\\)\n\\(\\because \\int_0^\\infty \\dfrac{1}{\\Gamma(\\alpha+1)\\beta^{\\alpha+1}} x^{(a+1)-1}e^{-x/\\beta} =1\\)\n\\(=\\dfrac{\\Gamma(\\alpha+1) \\beta^{\\alpha+1}}{\\Gamma(\\alpha) \\beta^\\alpha} = \\dfrac{\\alpha \\Gamma(\\alpha) \\beta^{\\alpha +1}}{\\Gamma(\\alpha) \\beta^\\alpha} = \\alpha\\beta\\)\n\\(E(X^2)=\\dfrac{1}{\\Gamma(\\alpha) \\beta^\\alpha}\\int_0^\\infty x^2 x^{\\alpha -1} e^{-x/\\beta}\\)\n\\(=\\dfrac{\\Gamma(\\alpha+2) \\beta^{\\alpha+2}}{\\Gamma(\\alpha) \\beta^\\alpha} \\int_0^\\infty \\dfrac{1}{\\Gamma(\\alpha+2)\\beta^{\\alpha+2}} x^{(a+2)-1}e^{-x/\\beta}\\)\n\\(=\\dfrac{\\Gamma(\\alpha+2) \\beta^{\\alpha+2}}{\\Gamma(\\alpha) \\beta^\\alpha} = \\dfrac{\\Gamma(\\alpha+1)\\alpha \\beta^{\\alpha +2} \\Gamma(\\alpha)}{\\Gamma(\\alpha) \\beta^\\alpha} = (\\alpha+1)\\alpha\\beta^2\\)\n\\(Var(X)=E(X^2)-[E(X)]^2= (\\alpha+1)\\alpha\\beta^2 - (\\alpha\\beta)^2 = \\alpha\\beta^2\\)\n\n\n5번\n정규분포 \\(N(µ, σ^2)\\)의 적률생성함수를 구하시오.\n\\(M_X(t)=E(e^{tX})=\\int_{-\\infty}^\\infty e^{tx} \\dfrac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\dfrac{(x-\\mu)^2}{2\\sigma^2}}dx\\)\n(\\(\\dfrac{x-\\mu}{\\sigma}=u\\) 치환하면, \\(dx=\\sigma du\\), \\(x=\\mu+\\sigma u\\))\n= \\(\\int_{-\\infty}^\\infty e^{t(\\mu+\\sigma u)} \\dfrac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-u^2/2} \\sigma du\\)\n= \\(\\dfrac{e^{t\\mu}}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty}e^{-\\dfrac{1}{2}(u-t \\mu)^2 + -\\dfrac{1}{2}t^2\\sigma^2}du\\)\n=\\(\\dfrac{e^{t\\mu+ \\dfrac{1}{2}t^2 \\sigma^2}}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty e ^{-\\dfrac{(u-t\\mu)^2}{2}}du\\)\n(\\(u-t\\sigma=v\\)로 치환하면, \\(du=dv, u=v+t\\sigma\\))\n=\\(\\dfrac{e^{t\\mu+ \\dfrac{1}{2}t^2 \\sigma^2}}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty e^{-\\dfrac{v}{2}}dv\\)\n\\(\\because \\int_{-\\infty}^\\infty e^{-\\dfrac{v}{2}}dv = \\sqrt{2\\pi}\\)\n=\\(e^{t\\mu+ \\dfrac{1}{2}t^2 \\sigma^2}\\)\n\n\n6번\n이변량정규분포 \\(BVN(µ_X, µ_Y , σ_X, σ_Y , ρ)\\)를 따르는 확률벡터 \\((X,Y)\\)의 확률밀도함수를 다변량정규분포의 확률밀도함수의 2차원형임을 이용하여 구하시오.\n\\(\\mu =\\begin{pmatrix} \\mu _{x} \\\\ \\mu _{y} \\end{pmatrix}\\)\n\\(\\sum =\\begin{pmatrix} \\sigma _{x}^{2} & \\sigma _{xy} \\\\ \\sigma _{xy} & \\sigma _{y}^{2} \\end{pmatrix}= \\begin{pmatrix} \\sigma _{x}^{2} & \\rho \\sigma _{x}\\sigma _{y} \\\\ \\rho \\sigma _{x}\\sigma _{y} & \\sigma _{y}^{2} \\end{pmatrix}\\)\n\\(f_{X,Y}(x,y)=\\dfrac{1}{|2\\pi \\sum|^{1/2}} exp\\left[ -\\dfrac{1}{2}\\left( \\begin{pmatrix} x \\\\ y \\end{pmatrix}-\\begin{pmatrix} u_{x} \\\\ u_{y} \\end{pmatrix}\\right) ^{t}\\Sigma ^{-1}\\left( \\begin{pmatrix} x \\\\ y \\end{pmatrix}-\\begin{pmatrix} \\mu _{x} \\\\ \\mu _{y} \\end{pmatrix}\\right) \\right]\\)\n먼저, \\(\\dfrac{1}{|2\\pi\\sum|^{1/2}}=\\dfrac{1}{2\\pi|\\sum|^{1/2}}=\\dfrac{1}{2\\pi(\\sigma_x^2 \\sigma_y^2 - \\rho^2 \\sigma_x^2 \\sigma_y^2)^{1/2}}=\\dfrac{1}{2\\pi \\sigma_x \\sigma_y \\sqrt{1-\\rho^2}}\\)\n\\(exp\\left[ -\\dfrac{1}{2}\\left( \\begin{pmatrix} x \\\\ y \\end{pmatrix}-\\begin{pmatrix} \\mu_{x} \\\\ \\mu_{y} \\end{pmatrix}\\right) ^{t}\\Sigma ^{-1}\\left( \\begin{pmatrix} x \\\\ y \\end{pmatrix}-\\begin{pmatrix} \\mu _{x} \\\\ \\mu _{y} \\end{pmatrix}\\right) \\right]\\)\n= \\(exp \\left[-\\dfrac{1}{2(\\sigma_x^2 \\sigma_y^2 - \\rho^2 \\sigma_x^2 \\sigma_y^2)} \\begin{pmatrix} x-\\mu_x & y-\\mu_y \\end{pmatrix} \\begin{pmatrix} \\sigma _{y}^{2} & - \\rho \\sigma _{x}\\sigma _{y} \\\\ - \\rho \\sigma _{x}\\sigma _{y} & \\sigma _{x}^{2} \\end{pmatrix} \\begin{pmatrix} x-\\mu_x \\\\ y-\\mu_y \\end{pmatrix} \\right]\\)\n= \\(exp \\left[ -\\dfrac{1}{2(\\sigma_x^2 \\sigma_y^2 - \\rho^2 \\sigma_x^2 \\sigma_y^2)} ( (x-\\mu_x)^2 \\sigma_y^2 + (y-\\mu_y)^2 \\sigma_x^2 - 2\\rho \\sigma_x \\sigma_y (x-\\mu_x)(y-\\mu_y)) \\right]\\)\n=\\(exp \\left[-\\dfrac{\\sigma_x^2 \\sigma_y^2}{2\\sigma_x^2 \\sigma_y^2(1- \\rho^2)} \\left( \\left(\\dfrac{x-\\mu_x}{\\sigma_x}\\right)^2 + \\left(\\dfrac{y-\\mu_y}{\\sigma_y}\\right)^2 - 2\\rho \\left(\\dfrac{x-\\mu_x}{\\sigma_x}\\right) \\left(\\dfrac{y-\\mu_y}{\\sigma_y}\\right) \\right) \\right]\\)\n=\\(exp \\left[-\\dfrac{1}{2(1- \\rho^2)} \\left( \\left(\\dfrac{x-\\mu_x}{\\sigma_x}\\right)^2 + \\left(\\dfrac{y-\\mu_y}{\\sigma_y}\\right)^2 - 2\\rho \\left(\\dfrac{x-\\mu_x}{\\sigma_x}\\right) \\left(\\dfrac{y-\\mu_y}{\\sigma_y}\\right) \\right) \\right]\\)\n즉,\n\\(f_{X,Y}(x,y)=\\dfrac{1}{2\\pi \\sigma_x \\sigma_y \\sqrt{1-\\rho^2}}exp \\left[-\\dfrac{1}{2(1- \\rho^2)} \\left( \\left(\\dfrac{x-\\mu_x}{\\sigma_x}\\right)^2 + \\left(\\dfrac{y-\\mu_y}{\\sigma_y}\\right)^2 - 2\\rho \\left(\\dfrac{x-\\mu_x}{\\sigma_x}\\right) \\left(\\dfrac{y-\\mu_y}{\\sigma_y}\\right) \\right) \\right]\\)\n\n\n7번\n이변량정규분포 \\(BVN(µ_X, µ_Y , σ_X, σ_Y , ρ)\\)의 확률밀도함수로부터 \\(X\\)의 주변확률밀도함수를 구하시오.\n\\(f_X(x)=\\int_{-\\infty}^\\infty f_{X,Y}(x,y)dy\\)\n\\(= \\int_{-\\infty}^\\infty \\dfrac{1}{2\\pi \\sigma_x \\sigma_y \\sqrt{1-\\rho^2}}exp \\left[-\\dfrac{1}{2(1- \\rho^2)} \\left( \\left(\\dfrac{x-\\mu_x}{\\sigma_x}\\right)^2 + \\left(\\dfrac{y-\\mu_y}{\\sigma_y}\\right)^2 - 2\\rho \\left(\\dfrac{x-\\mu_x}{\\sigma_x}\\right) \\left(\\dfrac{y-\\mu_y}{\\sigma_y}\\right) \\right) \\right]dy\\)\n\\(v=\\dfrac{y-\\mu_y}{\\sigma_y}\\)치환하면, \\(dv=\\dfrac{1}{\\sigma_y}dy\\)\n\\(=\\int_{-\\infty}^\\infty \\dfrac{1}{2 \\pi \\sigma_x \\sqrt{1-\\rho^2}} exp \\left[ -\\dfrac{1}{2} \\left(\\dfrac{x-\\mu_x}{\\sigma_x}\\right)^2 - \\dfrac{1}{2(1-\\rho^2)} \\left( v-\\rho \\left( \\dfrac{x-\\mu_x}{\\sigma_x}\\right) \\right)^2 \\right]dy\\)\n\\(u=\\left( v-\\rho \\left( \\dfrac{x-\\mu_x}{\\sigma_x}\\right) \\right)\\) 치환하면, \\(du=\\dfrac{dv}{\\sqrt{1-\\rho^2}}\\)\n\\(=\\int_{-\\infty}^\\infty \\dfrac{1}{\\sqrt{2\\pi}\\sigma_x} exp \\left[ -\\dfrac{1}{2} \\left( \\dfrac{x-\\mu_x}{\\sigma_x}\\right)^2 - \\dfrac{u^2}{2}\\right]du\\)\n\\(=\\dfrac{1}{\\sqrt{2\\pi}\\sigma_x} exp \\left[ -\\dfrac{1}{2} \\left( \\dfrac{x-\\mu_x}{\\sigma_x}\\right)^2 \\right] \\int_{-\\infty}^\\infty e^{-u^2/2}du\\)\n\\(\\because \\int_{-\\infty}^\\infty e^{-u^2/2}du=1\\)\n\\(=\\dfrac{1}{\\sqrt{2\\pi}\\sigma_x} exp \\left[ -\\dfrac{1}{2} \\left( \\dfrac{x-\\mu_x}{\\sigma_x}\\right)^2 \\right]\\)\n\n\n8번\n\\((X_1, X_2, X_3) ∼ MULT(n, p_1, p_2, p_3)\\)일 때 \\(Cov(X_1, X_2)\\)를 구하시오\n\n\\(Var(X_1+X_2)=n(p_1+p_2)(1-(p_1+p_2))\\)\n\\(Var(X_1+X_2)=Var(X_1)+Var(X_2)+2Cov(X_1,X_2)=np_1(1-p_1)+np_2(1-p_2)+2Cov(X_1,X_2)\\)\n\n1과 2를 연립하면,\n\\(Cov(X_1,X_2)=-np_1p_2\\)\n\\(f_{X_1,X_2,X_3}(x_1,x_2,x_3)\\begin{pmatrix} n \\\\ x_1 \\ x_2\\  x_3 \\end{pmatrix} p_1^{x_1} p_2^{x_2} p_3^{x_3}\\) , 단 \\(\\begin{pmatrix} 0\\leq x_{1}\\leq 1 & \\\\ 0\\leq x_{2}\\leq 1 & \\\\ 0\\leq x_{3}\\leq 1 & \\\\ x_{1}+x_{2}+x_{3}= & 1 \\end{pmatrix}\\)\n\\(Cov(X_1, X_2) = E(X_1X_2) - E(X_1)E(X_2)\\)\n\\(X_1\\) ~ \\(B(n,p_1)\\)이므로 \\(E(X_1)=np_1, E(X_2)=np_2\\)\n\\(E(X_1X_2)=E(E(X_1X_2|X_2))\\) 이고 \\(X_2\\)가 주어졌을 때의 조건부서식이므로 \\(X_2\\)는 밖으로 나갈 수 있다.\n=\\(E(X_2 E(X_1|X_2))\\)를 구하자.\n먼저, \\(E(X_1|X_2=x_2)\\)는 \\(f_{X_1|X_2}(x_1|x_2)=\\dfrac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)}\\)이다.\n분자먼저 구하자! \n\\(f_{X_1,X_2}(x_1,x_2)=P(X_1=x_1,X_2=x_2)\\) 이고 \n이 실험에서는,\n\\(=P(X_1=x_1,X_2=x_2,X_3=n-x_1-x_2)\\)\n\\(=\\dfrac{n!}{x_1! x_2! (n-x_1 -x_2)!}p_1^{x_1} p_2^{x_2} p_3^{n-x_1-x_2}\\)\n분모는, 이항분포를 따르므로\n\\(f_{X_2}(x_2) = \\dfrac{n!}{x_2!(n-x_2)!}p_2^{x_2}(1-p_2)^{n-x_2}\\)\n\\(f_{X_1|X_2}(x_1|x_2)=\\dfrac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)}\\)\n\\(=\\dfrac{\\dfrac{n!}{x_1! x_2! (n-x_1 -x_2)!}p_1^{x_1} p_2^{x_2} p_3^{n-x_1-x_2}}{\\dfrac{n!}{x_2!(n-x_2)!}p_2^{x_2}(1-p_2)^{n-x_2}}\\)\n\\(=\\dfrac{(n-x_2)!}{x_1!(n-x_2-x_1)!} \\left( \\dfrac{p_{1}}{1-p_{2}}\\right) ^{x_{1}} \\left( \\dfrac{p_{3}}{1-p_{2}}\\right) ^{n-x_{2}-x_{1}}\\)\n\\(=\\begin{pmatrix} n-x_{2} \\\\ x_{1} \\end{pmatrix} \\left( \\dfrac{p_{1}}{1-p_{2}}\\right) ^{x_{1}} \\left( 1-\\dfrac{p_{1}}{1-p_{2}}\\right) ^{n-x_{2}-x_{1}}\\)\n즉, \\(B(n-x_2, \\dfrac{p_1}{1-p_2})\\)를 따른다.\n즉, \\(E(X_1X_2)=\\dfrac{(n-x_2)p_1}{1-p_2}\\)\n다시 이중기댓값 정의로 돌아가서..\n구하려는 값 : \\(E(X_2 E(X_1|X_2))\\)\n\\(E(X_2 E(X_1|X_2))=E(X_2 \\cdot \\dfrac{(n-x_2)p_1}{1-p_2})\\)\n\\(=\\dfrac{p_1}{1-p_2}[E(n_2X_2)-E(X_2^2)]\\)\n\\(\\because E(X_2^2) = Var(X_2) + [E(X_2)]^2 = np_2(1-p_2) + n^2 p_2^2\\)\n\\(=\\dfrac{p_1}{1-p_2}(n^2p_2 - np_2(1-p_2) - n^2 p_2^2)\\)\n\\(=\\dfrac{np_1p_2}{1-p_2}(n-1+p_2-np_2)\\)\n\\(=\\dfrac{np_1p_2(1-p_2)(n-1)}{1-p_2}\\)\n\\(=n(n-1)p_1p_2\\)\n돌고돌아 구하고자 하는 공분산을 구해보자.\n\\(Cov(X_1, X_2) = E(X_1X_2) - E(X_1)E(X_2)\\)\n\\(=n(n-1)p_1p_2 - n^2p_1p_2\\)\n\\(=-np_1p_2\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS3.html",
    "href": "posts/Theoretical statistics/TS3.html",
    "title": "TS HW3",
    "section": "",
    "text": "모든 풀이는 틀릴 수 있움"
  },
  {
    "objectID": "posts/Theoretical statistics/TS3.html#section",
    "href": "posts/Theoretical statistics/TS3.html#section",
    "title": "TS HW3",
    "section": "(1)",
    "text": "(1)\n\\(E(U)= μ\\)를 만족하기 위한 상수 \\(a_1, \\dots, a_n\\)에 대한 조건은?\n\\(E(U)=E(\\sum_{i=1}^n a_i X_i)= E(a_1X_1)+ \\dots + E(a_nX_n)= μ(a_1 + \\dots + a_n)=μ\\)\ns.t \\((a_1 + \\dots + a_n)=1\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS3.html#section-1",
    "href": "posts/Theoretical statistics/TS3.html#section-1",
    "title": "TS HW3",
    "section": "(2)",
    "text": "(2)\n\\(Var(U)=\\sum_{i=1}^n a_i^2 Var(X_i) + 2 \\sum\\sum_{i<j} a_i a_j Cov(X_i, X_j)\\)임을 보이시오.\n만약 \\(X_i\\) 가 랜덤분포라면..\n\\(Var(U)=Var(\\sum_{i=1}^n a_i X_i)= Var(a_1X_1)+ \\dots + Var(a_nX_n)= a_1^2Var(X_1)+\\dots+a_n^2Var(X_n)=\\sum a_i^2 Var(X_i)\\)\n\\(n=2\\)인 경우를 고려\n\\(Var(a_1X_1+a_2X_2)=E(a_1X_1+a_2X_2)^2 - [E(a_1X_1)+E(a_2X_2)]^2\\)\n= \\(E(a_1^2X_1^2+ a_2^2X_2^2 + 2a_1a_2X_1X_2) - [E(a_1X_1)]^2 -[E(a_2X_2)]^2 -2E(a_1X_1)E(a_2X_2)\\)\n= \\(E(a_1^2X_1^2)+E(a_2^2X_2^2)+ 2a_1a_2E(X_1X_2) - [E(a_1X_1)]^2 -[E(a_2X_2)]^2 -2E(a_1X_1)E(a_2X_2)\\)\n= \\(E(a_1^2X_1^2)- [E(a_1X_1)]^2+E(a_2^2X_2^2)-[E(a_2X_2)]^2 + 2a_1a_2E(X_1X_2)-2a_1a_2E(X_1)E(X_2)\\)\n=\\(Var(a_1X_1)+Var(a_2X_2)+2a_1a_2Cov(X_1,X_2)\\)\n수학적 귀납법을 통해 그 이후도 성립\n교수님 풀이\n\\(Var(U)=E[(u-E(u))^2]\\)\n\\(E(u)=\\sum a_i \\mu\\) 이므로\n\\(u-E(u) = \\sum a_iX_i - \\sum a_i \\mu = \\sum a_i(X_i - \\mu)\\)\n\\(E[(u-E(u))^2] = E(\\sum a_i(X_i - \\mu))^2 = E(\\sum a_i^2(X_i-\\mu)^2 + \\sum \\sum a_i (X_i-\\mu) a_j (X_j-\\mu))\\)\n\\(=\\sum a_i^2 E(X_i-\\mu)^2 + 2\\sum \\sum a_i a_j E(X_i-\\mu)E(X_j-\\mu)\\)\n\\(= \\sum_{i=1}^n a_i^2 Var(X_i) + 2 \\sum\\sum_{i<j} a_i a_j Cov(X_i, X_j)\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS3.html#확인해보쟈",
    "href": "posts/Theoretical statistics/TS3.html#확인해보쟈",
    "title": "TS HW3",
    "section": "(3) 확인해보쟈",
    "text": "(3) 확인해보쟈\n\\(X_1, \\dots, X_n\\)이 서로 독립인 확률변수라고 할 때 \\(E(U)=μ\\)을 만족하면서 \\(Var(U)\\)를 최소로 하는 상수 \\(a_1,\\dots,a_n\\)에 대한 조건은?\n\\(X_i\\)가 서로 독립이면 \\(Cov(X_i,X_j)=0\\)이다.\n만약 \\(X_i\\) 가 랜덤분포라면..\n\\(Var(U)=Var(\\sum_{i=1}^n a_i X_i)= Var(a_1X_1)+ \\dots + Var(a_nX_n)= a_1^2Var(X_1)+\\dots+a_n^2Var(X_n)=\\sum a_i^2 Var(X_i)\\)\n\\((σ^2 - μ^2) \\sum a_i^2\\)\n위식이 최소가 되기 위해서는 \\(\\sum a_i^2\\)이 최소가 되어야 한다.\n코시 슈바츠르 부등식을 이용하여,\n\n코시 슈차르츠 부등식: \\((a^2+b^2)(x^2+y^2) > (ax+by)^2\\)\n\n\\((a_1^2 + \\dots + a_n^2)(1^2+ \\dots + 1^2) \\geq (a_1+ \\dots a_n)^2\\)\n\\(n \\sum a_i^2 \\geq μ^2\\)\n\\(\\sum a_i^2 \\geq \\dfrac{μ^2}{n}\\)\n문제오류!! \\(E(X_i^2)=\\sigma^2\\)이 아니라 \\(Var(X_i)=\\sigma^2\\)\n\\(Var(U)=\\sum a_i^2 \\sigma^2\\)을 최소로 하는 \\(a_1, \\dots, a_n\\)을 구하자.\n라그랑쥬(Lagrange)승수법 이용\n\\(g(a_1,\\dots,a_n,\\lambda) = \\sum_{i=1}^n a_i^2 \\sigma^2 + \\lambda(\\sum a_i - 1)\\)\n제약조건을 \\(\\lambda\\)로 넣는다.\n\\(\\begin{cases}\\dfrac{\\partial g}{\\partial a_{i}}=0(i=1,2,\\dots)\\\\ \\dfrac{\\partial g}{\\partial \\lambda}=0\\end{cases}\\)인 것을 찾자\n\\(2a_i \\sigma^2 + \\lambda=0 (i=1,\\dots,n)\\) \n\\(a_i=-\\dfrac{\\lambda}{2\\sigma^2}\\)\n\\(\\sum a_i - 1 = 0\\)\n\\(1=-\\dfrac{\\lambda}{2\\sigma^2}\\)\n\\(\\lambda = -\\dfrac{2\\sigma^2}{n}\\)\n\\(\\therefore a_i=\\dfrac{1}{n}\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS3.html#section-2",
    "href": "posts/Theoretical statistics/TS3.html#section-2",
    "title": "TS HW3",
    "section": "(1)",
    "text": "(1)\n\\(E(X-Y)=E(X)-E(Y)\\)\nT"
  },
  {
    "objectID": "posts/Theoretical statistics/TS3.html#section-3",
    "href": "posts/Theoretical statistics/TS3.html#section-3",
    "title": "TS HW3",
    "section": "(2)",
    "text": "(2)\n\\(Var(X-Y) = Var(X) - Var(Y)\\)\nF(\\(Var(X-Y) = Var(X) + Var(Y)\\))"
  },
  {
    "objectID": "posts/Theoretical statistics/TS3.html#section-4",
    "href": "posts/Theoretical statistics/TS3.html#section-4",
    "title": "TS HW3",
    "section": "(3)",
    "text": "(3)\n\\(Cov(X+Y,X) = Var(X)\\)\nT\n\\(Cov(X+Y,X)=E((X+Y)X)-E(X+Y)E(X)=E(X^2+YX)-(E(X)+E(Y))E(X)=E(X^2)+E(YX)-[E(X)]^2-E(Y)E(X)=E(X^2)-[E(X)]^2=Var(X)\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS3.html#section-5",
    "href": "posts/Theoretical statistics/TS3.html#section-5",
    "title": "TS HW3",
    "section": "(4)",
    "text": "(4)\n\\(Cov(X+Y,X-Y) = Var(X)+Var(Y)\\)\nF(\\(Var(X)-Var(Y)\\))\n\\(Cov(X+Y,X-Y)=E((X+Y)(X-Y))-E(X+Y)E(X-Y)=E(X^2+YX-XY-Y^2)-(E(X)+E(Y))(E(X)-E(Y))=Var(X)-Var(Y)\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS3.html#section-6",
    "href": "posts/Theoretical statistics/TS3.html#section-6",
    "title": "TS HW3",
    "section": "(5)",
    "text": "(5)\n\\(Corr(X+Y,Y)=\\dfrac{SD(Y)}{SD(X)}\\)\nF\n\\(Corr(X+Y,Y)=\\dfrac{Cov(X+Y,Y)}{SD(X+Y)SD(Y)}=\\dfrac{Var(Y)}{\\sqrt{Var(X)+Var(Y)}SD(Y)}=\\dfrac{SD(Y)}{\\sqrt{Var(X)+Var(Y)}}\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS3.html#section-7",
    "href": "posts/Theoretical statistics/TS3.html#section-7",
    "title": "TS HW3",
    "section": "(1)",
    "text": "(1)\n확률변수 \\(X_i\\)의 적률생성함수를 구하라.\n\\(M_X(t)=E(e^{tX})=\\int_{-\\infty}^\\infty e^{tx}xe^{-x}I(x>0)dx=\\int_0^\\infty xe^{x(t-1)}dx=\\dfrac{1}{(t-1)^2}\\)\n\\(\\dfrac{1}{(1-t)^2} \\ for (1-t)>0\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS3.html#section-8",
    "href": "posts/Theoretical statistics/TS3.html#section-8",
    "title": "TS HW3",
    "section": "(2)",
    "text": "(2)\n(1)의 결과를 이용하여 확률변수 \\(X_i\\)의 분산을 구하라\n\\(Var(X)=E(X^2)-[E(X)]^2\\)\n\\(M_X^{(1)}(t=0)=E(X), M_X^{(2)}(t=0)=E(X^2)\\) 이용\n\\(M_X^{(1)}(t)=\\dfrac{-2}{(t-1)^3}, M_X^{(1)}(t=0)=2\\)\n\\(\\dfrac{-2}{(1-t)^3}\\)\n\\(M_X^{(2)}(t)=\\dfrac{6}{(t-1)^4}, M_X^{(2)}(t=0)=6\\)\n\\(Var(X)=E(X^2)-[E(X)]^2=6-4=2\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS3.html#section-9",
    "href": "posts/Theoretical statistics/TS3.html#section-9",
    "title": "TS HW3",
    "section": "(3)",
    "text": "(3)\n(1)의 결과를 이용하여 확률변수 \\(\\sum_{i=1}^n X_i\\)의 적률생성함수를 구하라.\n확률변수 \\(X_i\\)가 랜덤분포이므로 \\(U=\\sum X_i\\)일때 적률생성함수는\n\\(M_U(t)=M_{X_1}(t)M_{X_2}(t) \\dots M_{X_n}(t)\\) 이다.\n\\(M_U(t)= \\dfrac{1}{(t-1)^{2n}}\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS3.html#section-10",
    "href": "posts/Theoretical statistics/TS3.html#section-10",
    "title": "TS HW3",
    "section": "(4)",
    "text": "(4)\n(3)의 결과를 이용하여 확률변수 \\(Y=\\sum_{i=1}^n 2X_i\\)의 적률생성함수를 구하라.\n\\(M_Y(t)=E(e^{tY})=E(e^{2tu})=M_u(2t)=(\\dfrac{1}{1-2t})^{2n}\\) 이다.\n위 결과는 \\(GAM(2n,2)\\)의 mgf이다, 즉 \\(Y\\)~\\(GAM(2n,2)\\)이다.\n\\(GAM(2n,2)\\)은 자유도가 4n인 카이제곱을 따른다."
  },
  {
    "objectID": "posts/Theoretical statistics/TS3.html#section-11",
    "href": "posts/Theoretical statistics/TS3.html#section-11",
    "title": "TS HW3",
    "section": "(2)",
    "text": "(2)\n\\(Var(Y|X=x)\\)\n\\(Var(Y|X=x) = E(Y^2|x)-[E(Y|x)]^2\\)\n\\(E(Y^2|x)=\\sum y^2 f_{X|Y}(x|y) = \\sum_0^n y^2 \\begin{pmatrix} n \\\\ y \\end{pmatrix} x^y \\ (1-x)^{n-y}\\)\n\\(f_{X|Y}(x|y)\\)는 이항분포이며 기댓값은 \\(nx\\), 분산은 \\(nx(1-x)\\)이다.\n\\(E(Y|X)=nx, Var(Y|X)=nx(1-x)\\)\n계산이 너무 복잡해.."
  },
  {
    "objectID": "posts/Theoretical statistics/TS3.html#section-12",
    "href": "posts/Theoretical statistics/TS3.html#section-12",
    "title": "TS HW3",
    "section": "(1)",
    "text": "(1)\n이중기댓값 정의를 이용하여 \\(E(Y)=E[E(Y|X)]\\) 풀이\n\\(Var(Y)=Var(E(Y|X))+E(Var(Y|X))\\)\n\\(=Var(nX)+E(nX(1-X))\\)\n\\(=n^2 Var(X) + nE(X) - nE(X^2)\\)\n\\(=n^2/20 + n/2 - 3n/10\\)\n\\(=n^2/20 + n/5\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS1.html",
    "href": "posts/Theoretical statistics/TS1.html",
    "title": "TS HW1",
    "section": "",
    "text": "모든 풀이는 틀릴 수 있음 하하하 ㅇ_<"
  },
  {
    "objectID": "posts/Theoretical statistics/TS1.html#a_1과-a_2c",
    "href": "posts/Theoretical statistics/TS1.html#a_1과-a_2c",
    "title": "TS HW1",
    "section": "(1) \\(A_{1}\\)과 \\(A_{2}^{c}\\)",
    "text": "(1) \\(A_{1}\\)과 \\(A_{2}^{c}\\)\n\n\\(P(A_{1} \\cap A_{2}^c)\\)\n\n= \\(P(A_1) - P(A_1 \\cap A_2)\\)\n= \\(P(A_1) - P(A_1) \\cdot P(A_2)\\)\n= \\(P(A_1) \\cdot (1 - P(A_2))\\)\n\\(= P(A_{1})P(A_{2}^c)\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS1.html#a_1c과-a_2",
    "href": "posts/Theoretical statistics/TS1.html#a_1c과-a_2",
    "title": "TS HW1",
    "section": "(2) \\(A_{1}^{c}\\)과 \\(A_{2}\\)",
    "text": "(2) \\(A_{1}^{c}\\)과 \\(A_{2}\\)\n\n\\(P(A_{2} \\cap A_{1}^c)\\)\n\n= \\(P(A_2) - P(A_1 \\cap A_2)\\)\n= \\(P(A_2) - P(A_1) \\cdot P(A_2)\\)\n= \\(P(A_2) \\cdot (1-P(A_1))\\)\n= \\(P(A_2) \\cdot P(A_1^c)\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS1.html#a_1c과-a_2c",
    "href": "posts/Theoretical statistics/TS1.html#a_1c과-a_2c",
    "title": "TS HW1",
    "section": "(3) \\(A_{1}^{c}\\)과 \\(A_{2}^{c}\\)",
    "text": "(3) \\(A_{1}^{c}\\)과 \\(A_{2}^{c}\\)\n\n\\(P(A_1^c \\cap A_2^c)\\)\n\n= \\(P((A_1 \\cup A_2)^c)\\)\n= \\(1-P(A_1 \\cup A_2)\\)\n= \\(1 - (P(A_1) + P(A_2) - P(A_1 \\cap A_2))\\)\n= \\(1 - P(A_1) - P(A_2) + P(A_1 \\cap A_2)\\)\n= \\(1 - P(A_1) - P(A_2) + P(A_1)P(A_2)\\)\n= \\(1 - P(A_1) - P(A_2)(1-P_{A_1})\\)\n= \\((1-P(A_1))(1-P(A_2))\\)\n= \\(P(A_1^c)P(A_2^c)\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS1.html#pa_1-cap-a_2",
    "href": "posts/Theoretical statistics/TS1.html#pa_1-cap-a_2",
    "title": "TS HW1",
    "section": "(1) \\(P(A_{1} \\cap A_{2})\\)",
    "text": "(1) \\(P(A_{1} \\cap A_{2})\\)\n\\(A_{1}\\)과 \\(A_{2}\\)가 서로 독립이므로 \\(P(A_{1} \\cap A_{2}) = P(A_{1}) P(A_{2})\\) 가 성립\n\\(P(A_1 \\cap A_2) = P(A_1)P(A_2) = 0.6 \\times 0.3 = 0.18\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS1.html#pleft-a_1cup-a_2right",
    "href": "posts/Theoretical statistics/TS1.html#pleft-a_1cup-a_2right",
    "title": "TS HW1",
    "section": "(2) \\(P\\left( A_{1}\\cup A_{2}\\right)\\)",
    "text": "(2) \\(P\\left( A_{1}\\cup A_{2}\\right)\\)\n\\(P(A_1 \\cup A_2) = P(A_1) + P(A_2) - P(A_1 \\cap A_2) = 0.6 + 0.3 - 0.18 = 0.72\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS1.html#pa_1-cup-a_2c",
    "href": "posts/Theoretical statistics/TS1.html#pa_1-cup-a_2c",
    "title": "TS HW1",
    "section": "(3) \\(P(A_{1} \\cup A_{2}^{c})\\)",
    "text": "(3) \\(P(A_{1} \\cup A_{2}^{c})\\)\n\\(P(A_{1} \\cup A_{2}^c)\\)\n= \\(P(A_1) + P(A_2^c) - P(A_1 \\cap A_2^c)\\)\n= \\(P(A_1) + 1 - P(A_2) - ( P(A_1) - P(A_1 \\cap A_2))\\)\n= \\(1 - P(A_2) + P(A_1 \\cap A_2)\\)\n= \\(1 - 0.3 + 0.18\\)\n= \\(0.88\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS1.html#fleft-xright-c-dfrac23x-ileft-xin-left-012ldots-right-right",
    "href": "posts/Theoretical statistics/TS1.html#fleft-xright-c-dfrac23x-ileft-xin-left-012ldots-right-right",
    "title": "TS HW1",
    "section": "(1) \\(f\\left( x\\right) = c( \\dfrac{2}{3})^x\\) \\(I\\left( x\\in \\left\\{ 0,1,2,\\ldots \\right\\} \\right)\\)",
    "text": "(1) \\(f\\left( x\\right) = c( \\dfrac{2}{3})^x\\) \\(I\\left( x\\in \\left\\{ 0,1,2,\\ldots \\right\\} \\right)\\)\n이럴수가 연속형이 아니라 이산형이였담 두둥 이산형인 경우 저렇게 0,1,2,… 표시되고 연속형인 경우에는 \\(x\\geq0\\) 이렇게 표시됨..ㅠㅠ\n각 \\(f(x) \\geq 0\\) 이므로 \\(c \\geq 0\\)\n\\(\\sum _{\\forall x_{i}}f\\left( x_{i}\\right) = 1\\) 을 만족하는 상수 c값 찾기\n\\(\\sum ^{\\infty}_{0}c( \\dfrac{2}{3})^x =\\dfrac{\\dfrac{2}{3}c}{1-\\dfrac{2}{3}}= 2c = 1\\)\ns.t \\(c= \\dfrac{1}{2}\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS1.html#fx-cx-ileft-xin-left-123456-right-right",
    "href": "posts/Theoretical statistics/TS1.html#fx-cx-ileft-xin-left-123456-right-right",
    "title": "TS HW1",
    "section": "(2) \\(f(x) = cx\\) \\(I\\left( x\\in \\left\\{ 1,2,3,4,5,6 \\right\\} \\right)\\)",
    "text": "(2) \\(f(x) = cx\\) \\(I\\left( x\\in \\left\\{ 1,2,3,4,5,6 \\right\\} \\right)\\)\n각 \\(f(x) \\geq 0\\) 이므로 \\(c \\geq 0\\)\n\\(\\sum _{\\forall x_{i}}f\\left( x_{i}\\right) = 1\\) 을 만족하는 상수 c값 찾기\n\\(\\sum ^{6}_{x=1}f\\left( x_{i}\\right) = c (1+2+3+4+5+6) = 21 c =1\\)\ns.t \\(c= \\dfrac{1}{21}\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS1.html#p0-x_1-dfrac12-dfrac14-x_2-1",
    "href": "posts/Theoretical statistics/TS1.html#p0-x_1-dfrac12-dfrac14-x_2-1",
    "title": "TS HW1",
    "section": "(1) \\(P(0 < X_{1} < \\dfrac{1}{2}, \\dfrac{1}{4} < X_{2} < 1)\\)",
    "text": "(1) \\(P(0 < X_{1} < \\dfrac{1}{2}, \\dfrac{1}{4} < X_{2} < 1)\\)\n\\(\\int \\int _{A}f\\left( x_{1},x_{2}\\right) dx_{1}dx_{2} = \\int _{0}^{1/2}\\int _{1/4}^{1}4x_{1}x_{2}dx_{2}dx_{1} = \\dfrac{15}{64}\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS1.html#px_1-x_2",
    "href": "posts/Theoretical statistics/TS1.html#px_1-x_2",
    "title": "TS HW1",
    "section": "(2) \\(P(X_{1} = X_{2})\\)",
    "text": "(2) \\(P(X_{1} = X_{2})\\)\n\\(\\int _{0}^{.1}\\int ^{x_{1}}_{x_{1}}4x_{1}x_{2}dx_{2}dx_{1} = 0\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS1.html#px_1-x_2-1",
    "href": "posts/Theoretical statistics/TS1.html#px_1-x_2-1",
    "title": "TS HW1",
    "section": "(3) \\(P(X_{1} < X_{2})\\)",
    "text": "(3) \\(P(X_{1} < X_{2})\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS1.html#p-x_1leq-x_2",
    "href": "posts/Theoretical statistics/TS1.html#p-x_1leq-x_2",
    "title": "TS HW1",
    "section": "(4) \\(P( X_{1}\\leq X_{2})\\)",
    "text": "(4) \\(P( X_{1}\\leq X_{2})\\)\n(2)에서 \\(P(X_{1} = X_{2}) = 0\\) 이므로 (3)식과 (4)식의 확률값은 동일하다.\n\\(\\int \\int _{A}f\\left( x_{1},x_{2}\\right) dx_{1}dx_{2} = \\int _{0}^{1}\\int _{0}^{x_2}4x_{1}x_{2}dx_{1}dx_{2} = \\int _{0}^{1}2x_{2}^{3}dx_{2} = \\dfrac{1}{2}\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS2.html",
    "href": "posts/Theoretical statistics/TS2.html",
    "title": "TS HW2",
    "section": "",
    "text": "모든 풀이는 틀릴 수 있움..ㅡ"
  },
  {
    "objectID": "posts/Theoretical statistics/TS2.html#section",
    "href": "posts/Theoretical statistics/TS2.html#section",
    "title": "TS HW2",
    "section": "1(1)",
    "text": "1(1)\n\n\\(X_1\\)과 \\(X_2\\)의 결합확률밀도함수를 구하시오.\n\n\\(f_{X_1,X_2}(x_1,x_2) = \\sum_{X_3}f_{X_1,X_2,X_3}(x_1,x_2,x_3) = \\int_{-\\infty}^{\\infty} f_{X_1,X_2,X_3}(x_1,x_2,x_3) dx_3\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS2.html#section-1",
    "href": "posts/Theoretical statistics/TS2.html#section-1",
    "title": "TS HW2",
    "section": "1(2)",
    "text": "1(2)\n\n\\(X_3\\)의 주변확률밀도함수를 구하시오.\n\n\\(f_{X_3}(x_3) = \\sum_{X_1} \\sum_{X_2}f_{X_1,X_2,X_3}(x_1,x_2,x_3) = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} f_{X_1,X_2,X_3}(x_1,x_2,x_3) dx_1 dx_2\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS2.html#section-2",
    "href": "posts/Theoretical statistics/TS2.html#section-2",
    "title": "TS HW2",
    "section": "1(3)",
    "text": "1(3)\n\n\\(X_3 = x_3\\) 으로 주어져 있을 때 \\(X_1\\)과 \\(X_2\\)의 결합조건부확률밀도함수를 구하시오.\n\n\\(f_{X_1,X_2|X_3=x_3}(x_1,x_2|x_3) = \\dfrac{f_{X_1,X_2,X_3}(x_1,x_2,x_3)}{f_{X_3}(x_3)}\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS2.html#section-3",
    "href": "posts/Theoretical statistics/TS2.html#section-3",
    "title": "TS HW2",
    "section": "2(1)",
    "text": "2(1)\n\n\\(Y\\)의 주변확률밀도함수를 구하시오.\n\n\\(f_X(x)\\)는 기하분포 \\(f_{Y|X}(y|x)\\)는 이항분포이다.\n\\(f_Y(y)=\\int_0^1 6\\begin{pmatrix} n \\\\ y \\end{pmatrix} x^{y+1} \\ (1-x)^{n-y+1}dx, y=0,1,\\dots,n\\)\n\\(B(y+2,n-y+2)=\\int_0^1x^{y+1}(1-x)^{n-y+1}dx=\\dfrac{(y+2-1)!(n-y+2-1)!}{(y+2+n-y+2-1)!}=\\dfrac{(y+1)!(n-y+1)!}{(n+3)!}\\)\n\\(f_Y(y)=6\\begin{pmatrix} n \\\\ y \\end{pmatrix}\\int_0^1 x^{y+1} \\ (1-x)^{n-y+1}dx, y=0,1,\\dots,n\\)\n= \\(\\dfrac{6n!}{(n-y)!y!}\\dfrac{(y+1)!(n-y+1)!}{(n+3)!},y=0,1,\\dots,n\\)\n= \\(\\dfrac{6(y+1)(n-y+1)}{(n+3)(n+2)(n+1)},y=0,1,\\dots,n\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS2.html#section-4",
    "href": "posts/Theoretical statistics/TS2.html#section-4",
    "title": "TS HW2",
    "section": "2(2)",
    "text": "2(2)\n\n\\(Y=y\\)일때 \\(X\\)의 조건부 확률밀도함수를 구하시오.\n\n\\(f_{X|Y}(x|y=y)=\\dfrac{f_{X,Y}(x,y)}{f_Y(y)}=\\dfrac{f_{Y|X}(y|x)f_X(x)}{f_Y(y)}=\\dfrac{(n+3)!}{(y+1)!(n-y+1)!}x^{y+1}(1-x)^{n-y+1}, 단(0<x<1, y=0,1,\\dots, n)\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS2.html#section-5",
    "href": "posts/Theoretical statistics/TS2.html#section-5",
    "title": "TS HW2",
    "section": "2(3)",
    "text": "2(3)\n\n\\(P(0<X<0.1|Y=n)\\)을 구하시오.\n\n=\\(\\dfrac{P(0<X<0.1,Y=n)}{P(Y=n)}=\\dfrac{\\int_0^{0.1}f_{X,Y}(x,y=n)dx}{P(Y=n)}=(n+3)(0.1)^{n+2}-(n+2)(0.1)^{n+3}\\)\n\\(\\int_0^{0.1}f_{X,Y}(x,y=n)dx= \\int_0^{0.1} 6x^{n+1}(1-x)dx=6[\\dfrac{(0.1)^{n+2}}{n+2} - \\dfrac{(0.1)^{n+3}}{n+3}]\\)\n\\(P(Y=n) = \\dfrac{6}{(n+3)(n+2)}\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS2.html#section-6",
    "href": "posts/Theoretical statistics/TS2.html#section-6",
    "title": "TS HW2",
    "section": "3(1)",
    "text": "3(1)\n\n\\(f(x,y) = (\\dfrac{1}{α+1} + \\dfrac{1}{β+1})(x^α + y^β)I(0<x<1, 0<y<1)\\)\n\n쉽게 생각해서 인수분해 안되서 독립아님\n\\((\\dfrac{1}{α+1} + \\dfrac{1}{β+1}) = A\\) 라고 놓고 계산하자.\n\\(f_X(x) = \\int_{0}^{1}(Ax^α + Ay^β)I(0<y<1)dy = Ax^α + \\dfrac{A}{β+1}\\)\n\\(f_Y(y) = \\int_{0}^{1}(Ax^α + Ay^β)I(0<x<1)dx = Ay^β + \\dfrac{A}{α+1}\\)\n\\(f_X(x)f_Y(y) \\neq f(x,y)\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS2.html#section-7",
    "href": "posts/Theoretical statistics/TS2.html#section-7",
    "title": "TS HW2",
    "section": "3(2)",
    "text": "3(2)\n\n\\(f(x,y) = 4y(x-y)e^{-(x+y)}I(0 \\leq y \\leq x)\\)\n\n곱으로 분리 안되서 독립아님. 범위도 분리가 안된당.."
  },
  {
    "objectID": "posts/Theoretical statistics/TS2.html#section-8",
    "href": "posts/Theoretical statistics/TS2.html#section-8",
    "title": "TS HW2",
    "section": "3(3)",
    "text": "3(3)\n\n\\(f(x,y) = \\dfrac{1}{2x^2y} I(1 \\leq x, \\dfrac{1}{x} \\leq y \\leq x)\\)\n\n\\(f_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dy = \\int_{1/x}^{x} \\dfrac{1}{2x^2y} I(1 \\leq x)dy = \\dfrac{1}{2x^2} I(1 \\leq x) \\int_{1/x}^{x} \\dfrac{1}{y}dy = \\dfrac{lnx}{x^2}I(1 \\leq x)\\)\n\\(f_Y(y) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dx = \\int_{1}^{\\infty} \\dfrac{1}{2x^2y} I( \\dfrac{1}{x}\\leq y \\leq x)dx = \\dfrac{1}{2y} I( \\dfrac{1}{x}\\leq y \\leq x) \\int_{1}^{\\infty} \\dfrac{1}{x^2}dX = \\dfrac{1}{2y} I( \\dfrac{1}{x}\\leq y \\leq x)\\)\n\\(f_X(x)f_Y(y) = \\dfrac{lnx}{2x^2y} I(1 \\leq x, \\dfrac{1}{x} \\leq y \\leq x) \\neq f(x,y)\\)\n\nnote: 범위가 나누어지지 않기 때문에 이것도 독립이 아니다."
  },
  {
    "objectID": "posts/Theoretical statistics/TS2.html#section-9",
    "href": "posts/Theoretical statistics/TS2.html#section-9",
    "title": "TS HW2",
    "section": "3(4)",
    "text": "3(4)\n\n\\(f(x,y) = xyI(0 \\leq x \\leq 2, 0 \\leq y \\leq 1)\\)\n\n쓰기 귀찮으니까 밑에 생략\n\\(f(x) = \\int f(x,y) dy = \\int_{0}^{1} xyI(0 \\leq x \\leq 2) dy = \\dfrac{x}{2}I(0 \\leq x \\leq 2)\\)\n\\(f(y) = \\int f(x,y) dx = \\int_{0}^{2} xyI(0 \\leq y \\leq 1) dx = 2y I(0 \\leq y \\leq 1)\\)\n\\(f(x)f(y) = f(x,y)\\) 이므로 독립이다."
  },
  {
    "objectID": "posts/Theoretical statistics/TS2.html#section-10",
    "href": "posts/Theoretical statistics/TS2.html#section-10",
    "title": "TS HW2",
    "section": "4(1)",
    "text": "4(1)\n\n\\(f(x) = \\dfrac{1}{λ} e^{-x/λ}I(x>0)\\)\n\n\\(E(X)= \\int_{-\\infty}^{\\infty} xf(x)dx = \\int_{0}^{\\infty}\\dfrac{x}{λ} e^{-x/λ}dx =λ\\)\n\\(E(X^2)= \\int_{-\\infty}^{\\infty} x^2f(x)dx = \\int_{0}^{\\infty}\\dfrac{x^2}{λ} e^{-x/λ}dx = 2λ^2\\)\n\nnote: 위는 지수분포이며 GAM(1, λ)로 표현할 수 있다."
  },
  {
    "objectID": "posts/Theoretical statistics/TS2.html#section-11",
    "href": "posts/Theoretical statistics/TS2.html#section-11",
    "title": "TS HW2",
    "section": "4(2)",
    "text": "4(2)\n\n\\(f(x) = \\dfrac{1}{π(1+x^2)}\\)\n\n\nnote: 위는 cauchy분포라고 한다.\n\n\\(E(|X|)<\\infty\\) 이면 기댓값이 존재한다.\n코시분포는 기댓값이 존재하지 않는다."
  },
  {
    "objectID": "posts/Theoretical statistics/TS2.html#section-12",
    "href": "posts/Theoretical statistics/TS2.html#section-12",
    "title": "TS HW2",
    "section": "4(3)",
    "text": "4(3)\n\n\\(f(x) = p(1-p)^{x-1}, x=1,2,\\dots, 0<p<1\\)\n\n위 식은 기하분포의 확률밀도함수이며 \\(E(X)= \\dfrac{1}{p}, Var(X)= \\dfrac{1-p}{p^2}\\) 임\n\\(E(X) = \\sum_{x=1}^{\\infty}xf(x) = \\sum_{x=1}^{\\infty}xp(1-p)^{x-1}\\) 는 \\((\\sum_{x=0}^{\\infty} (1-p)^x)'\\)을 이용해서 풀이\n\\(E(X)=\\sum_{x=1}^{\\infty}xpq^{x-1}\\),단 (\\(p+q=1\\))\n=\\(\\sum_{x=1}^{\\infty} p \\dfrac{d}{dq}q^x = p \\dfrac{d}{dq} \\sum_{x=1}^{\\infty} q^x= p \\dfrac{d}{dq} \\dfrac{q}{1-q}= \\dfrac{p}{(1-q)^2} = \\dfrac{1}{p}\\)\n\\(E(X(X+1))=E(X^2+X)=E(X^2)+E(X)\\) 를 이용해서 \\(E(X^2)\\)을 구하자.\n\\(E(X(X+1))=\\sum_{x=1}^{\\infty}x(x+1)pq^{x-1}=p\\sum_{x=1}^{\\infty} \\dfrac{d^2}{dq^2}q^{x+1}=p \\dfrac{d^2}{dq^2} \\sum_{x=1}^{\\infty} q^{x+1} = p \\dfrac{d^2}{dq^2} \\dfrac{q^2}{1-q} = \\dfrac{2}{p^2}\\)\n\\(E(X^2)=E(X(X+1))-E(X)=\\dfrac{1+q}{p^2}\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS2.html#section-13",
    "href": "posts/Theoretical statistics/TS2.html#section-13",
    "title": "TS HW2",
    "section": "5(1)",
    "text": "5(1)\n\n\\(E(\\sum_{i=1}^{n}X_i)\\)\n\n\\(E(\\sum_{i=1}^{n}X_i) = \\sum_{i=1}^{n} E(X_i) = n\\)\n\\(E(X) = \\int_{0}^{\\infty} xe^{-x} dx = 1\\)\n\\(X_i\\)는 랜덤표본이고 기댓값이 모두 1이다."
  },
  {
    "objectID": "posts/Theoretical statistics/TS2.html#section-14",
    "href": "posts/Theoretical statistics/TS2.html#section-14",
    "title": "TS HW2",
    "section": "5(2)",
    "text": "5(2)\n\n\\(P(max_{i=1,\\dots,n} X_i \\leq x)\\)\n\n(Hint: \\((max_{i=1,\\dots,n} X_i \\leq x) = (X_1 \\leq x, \\dots, X_n \\leq x)\\)\n\\(P(max_{i=1,\\dots,n} X_i \\leq x)=P(X_1\\leq x, \\dots, X_n \\leq x)\\)\n\\(X_i\\)는 랜덤표본이므로,\n= \\(P(X_1 \\leq x) \\dots P(X_n \\leq x)= [P(X_i \\leq x)]^n\\)\n\\(P(X_i \\leq x)= \\int_0^x e^{-x}dx=1-e^{-x}\\) 이므로\n=\\([P(X_i \\leq x)]^n=(1-e^{-x})^n\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS2.html#section-15",
    "href": "posts/Theoretical statistics/TS2.html#section-15",
    "title": "TS HW2",
    "section": "5(3)",
    "text": "5(3)\n\n\\(Y_i\\)는 \\(X_i > 1\\) 일 때 1, 아니면 0의 값을 가진다고 하자. (\\(i\\)=1,\\(\\dots\\),\\(n\\))\n\n\\(E(\\sum_{i=1}^{n}Y_i)\\)을 구하시오.\n\\(Y_i=I(X_i>1)\\)\n\\(E(\\sum_{i=1}^{n}Y_i)=\\sum_{i=1}^{n}E(Y_i)\\) 이다.\n\\(X_i\\)가 랜덤표본이므로 \\(Y_i\\)도 랜덤표본\n\\(E(Y_i)=0f_{Y_i}(0)+1f_{Y_i}(1)=\\int_1^{\\infty}e^{-x}dx=\\dfrac{1}{e}\\)\n\\(\\sum_{i=1}^{n}E(Y_i)=\\dfrac{n}{e}\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS2.html#section-16",
    "href": "posts/Theoretical statistics/TS2.html#section-16",
    "title": "TS HW2",
    "section": "6(1)",
    "text": "6(1)\n\n\\(U\\)는 \\(Y=0\\)일 때 0, 아니면 1의 값을 가지는 확률변수로 정의하자. \\(U\\)와 \\(Y\\)의 결합확률밀도함수를 구하시오.\n\n\\[f_{U,Y}(u,y)\\]\n\n\n\n\nu=0\nu=1\n\n\n\n\ny=0\n35/84\n0\n\n\ny=1\n0\n42/84\n\n\ny=2\n0\n7/84"
  },
  {
    "objectID": "posts/Theoretical statistics/TS2.html#section-17",
    "href": "posts/Theoretical statistics/TS2.html#section-17",
    "title": "TS HW2",
    "section": "6(2)",
    "text": "6(2)\n\n\\(P(U=1)\\)\n\n\\(P(U=1) = \\sum_{y}f_{U,Y}(1,y) = f_{U,Y}(1,0) + f_{U,Y}(1,1) + f_{U,Y}(1,2) = \\dfrac{49}{84}\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS2.html#section-18",
    "href": "posts/Theoretical statistics/TS2.html#section-18",
    "title": "TS HW2",
    "section": "6(3)",
    "text": "6(3)\n\n\\(P(X>2|U=1)\\)\n\n\\(P(X>2|U=1) = \\dfrac{f_{X,U}(x>2,u=1)}{f_U(u=1)}\\)\n\\(P(X>1, U=1) \\to y \\neq 0\\) 일때 \\(X=3\\)의 값이므로 \\(f_{X,Y}(x=3,y=1)+f_{X,Y}(x=3,y=2)= \\dfrac{12}{84} + \\dfrac{4}{84} = \\dfrac{16}{84}\\)\n즉, \\(\\dfrac{f_{X,U}(x>2,u=1)}{f_U(u=1)}=\\dfrac{16}{49}\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS2.html#section-19",
    "href": "posts/Theoretical statistics/TS2.html#section-19",
    "title": "TS HW2",
    "section": "6(4)",
    "text": "6(4)\n\n\\(P(X>2|U=0)\\)\n\n\\(P(X>2|U=0) = \\dfrac{f_{X,U}(x>2,u=0)}{f_U(u=0)}\\)\n\\(P(X>2, U=0) \\to y = 0\\) 일때 \\(X=3\\)의 값이므로 \\(f_{X,Y}(x=3,y=0)= \\dfrac{1}{84}\\)\n즉, \\(\\dfrac{f_{X,U}(x>2,u=0)}{f_U(u=0)}=\\dfrac{1}{35}\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS6.html",
    "href": "posts/Theoretical statistics/TS6.html",
    "title": "TS HW6",
    "section": "",
    "text": "사건 A와 B가 상호배반일 때 \\(P(A|A ∪ B) = \\dfrac{P(A)}{P(A) + P(B)}\\)임을 보이시오\n상호배반이라는 것은 \\(P(A \\cap B)=\\emptyset\\)이므로, \\(P(A|A\\cup B)=\\dfrac{P(A)\\cap P(A \\cup B)}{P(A \\cup B) }=\\dfrac{P(A)}{P(A)+P(B)}\\)\n\n\n\n\\(A ∪ B = S\\)이고 \\(P(A) = 0.8, P(B) = 0.5\\)일 때 \\(P(A ∩ B)\\)를 구하시오.\n\\(P(A \\cup B) = P(S) = 1 = P(A) + P(B) - P(A \\cap B)\\)\n\\(P(A \\cap B) = 0.3\\)\n\n\n\n\\(P(A|B) = P(A|B^c)\\)이면 사건 A와 B는 서로 독립임을 보이시오.\n좌변 \\(P(A|B) = \\dfrac{P(A) \\cap P(B)}{P(B)}\\) 이고\n우변 \\(P(A|B^c) = \\dfrac{P(A)}{P(A \\cap B^C}\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS6.html#a-1",
    "href": "posts/Theoretical statistics/TS6.html#a-1",
    "title": "TS HW6",
    "section": "(a)",
    "text": "(a)\n\\(X\\)가 \\(1\\)의 값을 가지는 사건을 실험 결과들의 집합으로 기술하시오. (단, 앞면은 \\(H\\), 뒷면은 \\(T\\)로 나타내시오.)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS6.html#b-1",
    "href": "posts/Theoretical statistics/TS6.html#b-1",
    "title": "TS HW6",
    "section": "(b)",
    "text": "(b)\n\\(X\\)의 확률밀도함수 \\(f_X(x)\\)를 구하시오. (주의: \\(x\\)의 범위도 구체적으로 기술 )"
  },
  {
    "objectID": "posts/Theoretical statistics/TS6.html#a-2",
    "href": "posts/Theoretical statistics/TS6.html#a-2",
    "title": "TS HW6",
    "section": "(a)",
    "text": "(a)\n\\(E[(Y_2)^2]\\)을 구하시오."
  },
  {
    "objectID": "posts/Theoretical statistics/TS6.html#b-2",
    "href": "posts/Theoretical statistics/TS6.html#b-2",
    "title": "TS HW6",
    "section": "(b)",
    "text": "(b)\n\\(Y_1 + 2Y_2 − Y_3\\)의 기대값과 분산을 구하시오."
  },
  {
    "objectID": "posts/Theoretical statistics/TS6.html#c-1",
    "href": "posts/Theoretical statistics/TS6.html#c-1",
    "title": "TS HW6",
    "section": "(c)",
    "text": "(c)\n\\(Corr(Y_1 + Y_2, Y_1 − Y_2)\\)을 구하고, 그 값을 해석하시오."
  },
  {
    "objectID": "posts/Theoretical statistics/TS6.html#a-3",
    "href": "posts/Theoretical statistics/TS6.html#a-3",
    "title": "TS HW6",
    "section": "(a)",
    "text": "(a)\n\\(X\\)의 주변확률밀도함수 \\(f_X(x)\\)를 구하시오. (주의 : \\(x\\)의 범위도 명확히 기술)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS6.html#b-3",
    "href": "posts/Theoretical statistics/TS6.html#b-3",
    "title": "TS HW6",
    "section": "(b)",
    "text": "(b)\n\\(E(X)\\)를 구하시오."
  },
  {
    "objectID": "posts/Theoretical statistics/TS6.html#c-2",
    "href": "posts/Theoretical statistics/TS6.html#c-2",
    "title": "TS HW6",
    "section": "(c)",
    "text": "(c)\n\\(Var(Y|X=2)\\)을 구하시오."
  },
  {
    "objectID": "posts/Theoretical statistics/TS6.html#a-4",
    "href": "posts/Theoretical statistics/TS6.html#a-4",
    "title": "TS HW6",
    "section": "(a)",
    "text": "(a)\n\\(X_1\\)의 적률생성함수"
  },
  {
    "objectID": "posts/Theoretical statistics/TS6.html#b-4",
    "href": "posts/Theoretical statistics/TS6.html#b-4",
    "title": "TS HW6",
    "section": "(b)",
    "text": "(b)\n\\(P(X_1 + X_2 + · · · + X_5 ≥ 2)\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS6.html#c-3",
    "href": "posts/Theoretical statistics/TS6.html#c-3",
    "title": "TS HW6",
    "section": "(c)",
    "text": "(c)\n\\(E(X_1|X_1 + X_2 + · · · + X_5 = 3)\\)"
  },
  {
    "objectID": "posts/Theoretical statistics/TS6.html#section",
    "href": "posts/Theoretical statistics/TS6.html#section",
    "title": "TS HW6",
    "section": "(1)",
    "text": "(1)\n\\(X_{(1)}\\)과 \\(X_{(n)}\\)의 결합확률밀도함수"
  },
  {
    "objectID": "posts/Theoretical statistics/TS6.html#section-1",
    "href": "posts/Theoretical statistics/TS6.html#section-1",
    "title": "TS HW6",
    "section": "(2)",
    "text": "(2)\n표본범위 \\(X_{(n)} - X_{(1)}\\)의 확률밀도함수"
  },
  {
    "objectID": "posts/Theoretical statistics/TS6.html#section-2",
    "href": "posts/Theoretical statistics/TS6.html#section-2",
    "title": "TS HW6",
    "section": "(1)",
    "text": "(1)\n\\(X_{(1)}\\)의 확률밀도함수"
  },
  {
    "objectID": "posts/Theoretical statistics/TS6.html#section-3",
    "href": "posts/Theoretical statistics/TS6.html#section-3",
    "title": "TS HW6",
    "section": "(2)",
    "text": "(2)\n\\(X_{(n)}\\)의 확률밀도함수"
  },
  {
    "objectID": "posts/Theoretical statistics/TS6.html#section-4",
    "href": "posts/Theoretical statistics/TS6.html#section-4",
    "title": "TS HW6",
    "section": "(3)",
    "text": "(3)\n\\(n=2r+1\\)이라고 가정하고, 표본중위수 \\(X_{(r)}\\)의 확률밀도함수"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Boram-coco",
    "section": "",
    "text": "Everyday with Coco"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "coco",
    "section": "",
    "text": "coco 블로그는 대학원 강의노트, 논문 리뷰, 공부 등으로 이루어진 블로그입니다!\n\nData Visualization 강의는 데이터 크기 문제로 DV블로그 따로 개설\n낙서장\n\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n\n\nimports\n\n\n\n\n\n\nMay 3, 2023\n\n\n8wk: 확률변수, 분포 (1)\n\n\n김보람\n\n\n\n\nMay 2, 2023\n\n\nTS HW6\n\n\n김보람\n\n\n\n\nApr 27, 2023\n\n\nCH8. 신용카드 거래 분석(로지스틱amt+time-f1:0.009370)\n\n\n김보람\n\n\n\n\nApr 27, 2023\n\n\nCH8. 신용카드 거래 분석(로지스틱amt+time+lat+merch_lat-f1:0.98538)\n\n\n김보람\n\n\n\n\nApr 27, 2023\n\n\nCH8. 신용카드 거래 분석(로지스틱amt+time+city_pop-f1:0.986655)\n\n\n김보람\n\n\n\n\nApr 27, 2023\n\n\nCH8. 신용카드 거래 분석(로지스틱amt+time+city_pop+lat+merch_lat-f1:0.985323)\n\n\n김보람\n\n\n\n\nApr 27, 2023\n\n\nCH8. 신용카드 거래 분석(로지스틱+그래프)\n\n\n김보람\n\n\n\n\nApr 27, 2023\n\n\nCH8. 신용카드 거래 분석(df원본데이터)\n\n\n김보람\n\n\n\n\nApr 25, 2023\n\n\n5. 가변수 실습\n\n\n김보람\n\n\n\n\nApr 24, 2023\n\n\n인공신경망과 퍼셉트론\n\n\n김보람\n\n\n\n\nApr 24, 2023\n\n\n다층 퍼셉트론과 딥러닝\n\n\n김보람\n\n\n\n\nApr 24, 2023\n\n\n경사하강법\n\n\n김보람\n\n\n\n\nApr 23, 2023\n\n\n7wk: 측도론 (3)\n\n\n김보람\n\n\n\n\nApr 20, 2023\n\n\n딥러닝: 회귀분석\n\n\n김보람\n\n\n\n\nApr 19, 2023\n\n\nTS HW5\n\n\n김보람\n\n\n\n\nApr 19, 2023\n\n\nAS HW2\n\n\n김보람\n\n\n\n\nApr 18, 2023\n\n\n6wk: 측도론 (2)\n\n\n김보람\n\n\n\n\nApr 18, 2023\n\n\n4. MLR실습\n\n\n김보람\n\n\n\n\nApr 16, 2023\n\n\nCH8. 신용카드 거래 분석(사기거래=0제외:_df2)\n\n\n김보람\n\n\n\n\nApr 15, 2023\n\n\nTS HW4\n\n\n김보람\n\n\n\n\nApr 14, 2023\n\n\n파이썬:사이킷런\n\n\n김보람\n\n\n\n\nApr 14, 2023\n\n\nplot_comparison_under_sampling\n\n\n김보람\n\n\n\n\nApr 14, 2023\n\n\nplot_comparison_over_sampling\n\n\n김보람\n\n\n\n\nApr 12, 2023\n\n\nimbalanced data\n\n\n김보람\n\n\n\n\nApr 12, 2023\n\n\nRef\n\n\n김보람\n\n\n\n\nApr 10, 2023\n\n\n5wk: 측도론 (1)\n\n\n김보람\n\n\n\n\nApr 7, 2023\n\n\nCH5. 그래프에서의 머신러닝 문제(커뮤니티와 같은 의미 있는 구조 감지)\n\n\n김보람\n\n\n\n\nApr 6, 2023\n\n\nCH5. 그래프에서의 머신러닝 문제(링크예측)\n\n\n김보람\n\n\n\n\nApr 6, 2023\n\n\nCH4. 지도 그래프 학습(특징기반방법)\n\n\n김보람\n\n\n\n\nApr 6, 2023\n\n\nCH4. 지도 그래프 학습(얕은 임베딩 방법)\n\n\n김보람\n\n\n\n\nApr 6, 2023\n\n\nCH3. 비지도 그래프 학습(오토인코더)\n\n\n김보람\n\n\n\n\nApr 6, 2023\n\n\nCH3. 비지도 그래프 학습(얕은 임베딩 방법)\n\n\n김보람\n\n\n\n\nApr 6, 2023\n\n\nCH3. 비지도 그래프 학습(그래프신경망)\n\n\n김보람\n\n\n\n\nApr 6, 2023\n\n\nCH2. 그래프 머신러닝\n\n\n김보람\n\n\n\n\nApr 5, 2023\n\n\n데이터 개념공부\n\n\n김보람\n\n\n\n\nApr 4, 2023\n\n\nalpha\n\n\n김보람\n\n\n\n\nApr 4, 2023\n\n\nCH8. 신용카드 거래에 대한 그래프 분석(basic)\n\n\n김보람\n\n\n\n\nApr 4, 2023\n\n\nCH8. 신용카드 거래에 대한 그래프 분석(basic)\n\n\n김보람\n\n\n\n\nApr 4, 2023\n\n\nAS HW1_\n\n\n김보람\n\n\n\n\nApr 4, 2023\n\n\n3. SLR실습_simulation\n\n\n김보람\n\n\n\n\nApr 1, 2023\n\n\nCH1. graph basic\n\n\n김보람\n\n\n\n\nApr 1, 2023\n\n\n4wk: 측도론 intro (4)\n\n\n김보람\n\n\n\n\nMar 31, 2023\n\n\n[논문] Modeling Tabular Data using Conditional GAN\n\n\n김보람\n\n\n\n\nMar 30, 2023\n\n\n2. SLR실습\n\n\n김보람\n\n\n\n\nMar 30, 2023\n\n\n1. SLR(Simple Linear Regression)\n\n\n김보람\n\n\n\n\nMar 29, 2023\n\n\n[R] synthpop\n\n\n김보람\n\n\n\n\nMar 29, 2023\n\n\nTS HW3\n\n\n김보람\n\n\n\n\nMar 29, 2023\n\n\nAS HW1\n\n\n김보람\n\n\n\n\nMar 28, 2023\n\n\nTS HW2\n\n\n김보람\n\n\n\n\nMar 24, 2023\n\n\n[논문] Synthetic Data\n\n\n김보람\n\n\n\n\nMar 22, 2023\n\n\nAdvanced Deep Learning with TensorFlow 2 and Keras(-ing)\n\n\n김보람\n\n\n\n\nMar 21, 2023\n\n\nTS HW1\n\n\n김보람\n\n\n\n\nMar 21, 2023\n\n\n3wk: 측도론 intro (3)\n\n\n김보람\n\n\n\n\nMar 14, 2023\n\n\n2wk: 측도론 intro (2)\n\n\n김보람\n\n\n\n\nMar 13, 2023\n\n\n콰트로 블로그\n\n\n김보람\n\n\n\n\nMar 13, 2023\n\n\nSynthetic Data\n\n\n김보람\n\n\n\n\nMar 10, 2023\n\n\nPractical Synthetic Data Generation\n\n\n김보람\n\n\n\n\nMar 7, 2023\n\n\n1wk: 측도론 intro\n\n\n김보람\n\n\n\n\nDec 21, 2022\n\n\n기계학습 (1221)\n\n\n김보람\n\n\n\n\nDec 14, 2022\n\n\n기계학습 final\n\n\n김보람\n\n\n\n\nDec 1, 2022\n\n\n기계학습 (1201)\n\n\n김보람\n\n\n\n\nNov 30, 2022\n\n\n기계학습 (1130) 12주차\n\n\n김보람\n\n\n\n\nNov 29, 2022\n\n\n기계학습 final(교수님)\n\n\n최규빈\n\n\n\n\nNov 24, 2022\n\n\n6: K-beauty\n\n\n김보람\n\n\n\n\nNov 24, 2022\n\n\n5: 건강검진 데이터로 가설검정\n\n\n김보람\n\n\n\n\nNov 24, 2022\n\n\n4: 서울 종합병원 분포 확인하기\n\n\n김보람\n\n\n\n\nNov 24, 2022\n\n\n3: 데이터 분석 준비하기\n\n\n김보람\n\n\n\n\nNov 24, 2022\n\n\n2: file-path-setting\n\n\n김보람\n\n\n\n\nNov 24, 2022\n\n\n1: 주피터 노트북 사용법\n\n\n김보람\n\n\n\n\nNov 16, 2022\n\n\n기계학습 (1116) 11주차\n\n\n김보람\n\n\n\n\nNov 9, 2022\n\n\n기계학습 (1109) 10주차\n\n\n김보람\n\n\n\n\nOct 31, 2022\n\n\n기계학습 (1031) 9주차\n\n\n김보람\n\n\n\n\nOct 26, 2022\n\n\n기계학습 midterm\n\n\n김보람\n\n\n\n\nOct 26, 2022\n\n\n기계학습 (1026) 8주차\n\n\n김보람\n\n\n\n\nOct 19, 2022\n\n\n기계학습 (1019) 7주차\n\n\n김보람\n\n\n\n\nOct 12, 2022\n\n\n기계학습 (1012) 6주차\n\n\n김보람\n\n\n\n\nOct 5, 2022\n\n\n기계학습 (1005) 5주차\n\n\n김보람\n\n\n\n\nSep 28, 2022\n\n\n기계학습 (0928) 4주차\n\n\n김보람\n\n\n\n\nSep 21, 2022\n\n\n기계학습 (0921) 3주차\n\n\n김보람\n\n\n\n\nSep 14, 2022\n\n\n기계학습 (0914) 2주차\n\n\n김보람\n\n\n\n\nSep 7, 2022\n\n\n기계학습 (0907) 1주차\n\n\n김보람\n\n\n\n\nJun 6, 2022\n\n\n파이썬 (0606) 14주차\n\n\n김보람\n\n\n\n\nMay 30, 2022\n\n\n파이썬 (0530) 13주차\n\n\n김보람\n\n\n\n\nMay 25, 2022\n\n\n파이썬 (0525) 13주차\n\n\n김보람\n\n\n\n\nMay 23, 2022\n\n\n파이썬 (0523) 12주차\n\n\n김보람\n\n\n\n\nMay 18, 2022\n\n\n파이썬 (0518) 12주차\n\n\n김보람\n\n\n\n\nMay 16, 2022\n\n\n파이썬 (0516) 11주차\n\n\n김보람\n\n\n\n\nMay 11, 2022\n\n\n파이썬 (0511) 11주차\n\n\n김보람\n\n\n\n\nMay 9, 2022\n\n\n파이썬 (0509) 10주차\n\n\n김보람\n\n\n\n\nMay 6, 2022\n\n\n파이썬 (0506) 10주차\n\n\n김보람\n\n\n\n\nApr 18, 2022\n\n\n파이썬 (0418) 7주차\n\n\n김보람\n\n\n\n\nApr 13, 2022\n\n\n파이썬 (0413) 7주차\n\n\n김보람\n\n\n\n\nApr 11, 2022\n\n\n파이썬 (0411) 6주차\n\n\n김보람\n\n\n\n\nApr 6, 2022\n\n\n파이썬 (0406) 5주차\n\n\n김보람\n\n\n\n\nApr 4, 2022\n\n\n파이썬 (0404) 5주차\n\n\n김보람\n\n\n\n\nMar 28, 2022\n\n\n파이썬 (0328) 4주차\n\n\n김보람\n\n\n\n\nMar 23, 2022\n\n\n파이썬 (0323) 4주차\n\n\n김보람\n\n\n\n\nMar 21, 2022\n\n\n파이썬 (0321) 3주차\n\n\n김보람\n\n\n\n\nMar 16, 2022\n\n\n파이썬 (0316) 3주차\n\n\n김보람\n\n\n\n\nMar 14, 2022\n\n\n파이썬 (0314) 2주차\n\n\n김보람\n\n\n\n\nMar 7, 2022\n\n\n파이썬 (0307) 1주차\n\n\n김보람\n\n\n\n\n\n\nNo matching items"
  }
]