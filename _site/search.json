[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "coco",
    "section": "",
    "text": "Coco’s blog\nThis is Boram’s study blog, and it’s a summary of what she’s been studying, so there may be some mistakes. For blogs related to his graduate classes, click on the links below! I want to go for a walk. I want a snack. woof!\n\nlinks\n\nStudyJBNU Class\n\n\n\nGraph Machine Learning\nGNN paper (poster)\n\n\n\n\nApplied Statistics\nData Visualization\nDeep Learning\nMachine Learning\nProbabaility Theory\nPython\nRegression Analysis\nStatistical Computing\nSpecial Topics in Big Data Analysis(2022)\nSpecial Topics in Big Data Analysis(2023)\nTheoretical Statistics\nTime Series\n\n\n\n\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Study.html",
    "href": "Study.html",
    "title": "Study",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nOct 12, 2023\n\n\nAutogluon fraud\n\n\n김보람 \n\n\n\n\nJul 14, 2023\n\n\nTensor Basic\n\n\n김보람 \n\n\n\n\nJun 29, 2023\n\n\n[한국통계학회] 통계계산연구회 튜토리얼\n\n\n김보람 \n\n\n\n\nJun 27, 2023\n\n\nTDA tutorial\n\n\n김보람 \n\n\n\n\nJun 11, 2023\n\n\n주성분 분석(PCA)\n\n\n김보람 \n\n\n\n\nJun 1, 2023\n\n\n앙상블 학습\n\n\n김보람 \n\n\n\n\nMay 18, 2023\n\n\n미래 예측 데이터 분석(프로야구 선수의 다음 해 연봉 예측)\n\n\n김보람 \n\n\n\n\nMay 11, 2023\n\n\n모형 평가\n\n\n김보람 \n\n\n\n\nApr 24, 2023\n\n\n경사하강법\n\n\n김보람 \n\n\n\n\nApr 24, 2023\n\n\n다층 퍼셉트론과 딥러닝\n\n\n김보람 \n\n\n\n\nApr 24, 2023\n\n\n인공신경망과 퍼셉트론\n\n\n김보람 \n\n\n\n\nApr 20, 2023\n\n\n딥러닝: 회귀분석\n\n\n김보람 \n\n\n\n\nApr 14, 2023\n\n\nplot_comparison_under_sampling\n\n\n김보람 \n\n\n\n\nApr 14, 2023\n\n\nplot_comparison_over_sampling\n\n\n김보람 \n\n\n\n\nApr 14, 2023\n\n\n파이썬:사이킷런\n\n\n김보람 \n\n\n\n\nApr 12, 2023\n\n\nimbalanced data\n\n\n김보람 \n\n\n\n\nApr 5, 2023\n\n\n데이터 개념공부\n\n\n김보람 \n\n\n\n\nNov 24, 2022\n\n\n3: 데이터 분석 준비하기\n\n\n김보람 \n\n\n\n\nNov 24, 2022\n\n\n1: 주피터 노트북 사용법\n\n\n김보람 \n\n\n\n\nNov 24, 2022\n\n\n4: 서울 종합병원 분포 확인하기\n\n\n김보람 \n\n\n\n\nNov 24, 2022\n\n\n2: file-path-setting\n\n\n김보람 \n\n\n\n\nNov 24, 2022\n\n\n6: K-beauty\n\n\n김보람 \n\n\n\n\nNov 24, 2022\n\n\n5: 건강검진 데이터로 가설검정\n\n\n김보람 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Boram-coco",
    "section": "",
    "text": "Everyday with Coco"
  },
  {
    "objectID": "posts/Study/sklearn.html",
    "href": "posts/Study/sklearn.html",
    "title": "파이썬:사이킷런",
    "section": "",
    "text": "ref:https://losskatsu.github.io/machine-learning/sklearn/#train-test-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%84%ED%95%A0%ED%95%98%EA%B8%B0\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\n# 참고: 분류용 가상 데이터 만들기\nfrom sklearn.datasets import make_classification\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB #나이브 베이즈\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport matplotlib.pyplot as plt\n\n\n# 분류용 가상 데이터 만들기\nfrom sklearn.datasets import make_classification\n\n\nX, Y = make_classification(n_samples=1000, n_features=4,\n                        n_informative=2, n_redundant=0,\n                        random_state=0, shuffle=False)\n\n\n# n_informative: 종속변수와 상관관계가 존재하는 독립변수 수(default=2)\n# n_redundant: 독립변수끼리 종속관계에 있는 독립변수 수\n\n\nraw = datasets.load_breast_cancer()         ## sklearn에 내장된 원본 데이터 불러오기\nprint(raw.feature_names)                    ## 열(column) 이름 확인\n['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n 'mean smoothness' 'mean compactness' 'mean concavity'\n 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n 'radius error' 'texture error' 'perimeter error' 'area error'\n 'smoothness error' 'compactness error' 'concavity error'\n 'concave points error' 'symmetry error' 'fractal dimension error'\n 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n 'worst smoothness' 'worst compactness' 'worst concavity'\n 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n\ndata = pd.DataFrame(raw.data)               ## 독립변수 데이터 모음  \ntarget = pd.DataFrame(raw.target)           ## 종속변수 데이터 모음\nrawData = pd.concat([data,target], axis=1)  ## 독립변수 + 종속변수 열 결합\n\n## 열(column)이름 설정\nrawData.columns=['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n 'mean smoothness', 'mean compactness', 'mean concavity',\n 'mean concave points', 'mean symmetry', 'mean fractal dimension',\n 'radius error', 'texture error', 'perimeter error', 'area error',\n 'smoothness error', 'compactness error', 'concavity error',\n 'concave points error', 'symmetry error', 'fractal dimension error',\n 'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n 'worst smoothness', 'worst compactness', 'worst concavity',\n 'worst concave points', 'worst symmetry', 'worst fractal dimension'\n , 'cancer']\n\nrawData.head(10)                                ## 데이터 확인 \n\n['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n 'mean smoothness' 'mean compactness' 'mean concavity'\n 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n 'radius error' 'texture error' 'perimeter error' 'area error'\n 'smoothness error' 'compactness error' 'concavity error'\n 'concave points error' 'symmetry error' 'fractal dimension error'\n 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n 'worst smoothness' 'worst compactness' 'worst concavity'\n 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ncancer\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.30010\n0.14710\n0.2419\n0.07871\n...\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n0\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.08690\n0.07017\n0.1812\n0.05667\n...\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n0\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.19740\n0.12790\n0.2069\n0.05999\n...\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n0\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.24140\n0.10520\n0.2597\n0.09744\n...\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n0\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.19800\n0.10430\n0.1809\n0.05883\n...\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n0\n\n\n5\n12.45\n15.70\n82.57\n477.1\n0.12780\n0.17000\n0.15780\n0.08089\n0.2087\n0.07613\n...\n23.75\n103.40\n741.6\n0.1791\n0.5249\n0.5355\n0.1741\n0.3985\n0.12440\n0\n\n\n6\n18.25\n19.98\n119.60\n1040.0\n0.09463\n0.10900\n0.11270\n0.07400\n0.1794\n0.05742\n...\n27.66\n153.20\n1606.0\n0.1442\n0.2576\n0.3784\n0.1932\n0.3063\n0.08368\n0\n\n\n7\n13.71\n20.83\n90.20\n577.9\n0.11890\n0.16450\n0.09366\n0.05985\n0.2196\n0.07451\n...\n28.14\n110.60\n897.0\n0.1654\n0.3682\n0.2678\n0.1556\n0.3196\n0.11510\n0\n\n\n8\n13.00\n21.82\n87.50\n519.8\n0.12730\n0.19320\n0.18590\n0.09353\n0.2350\n0.07389\n...\n30.73\n106.20\n739.3\n0.1703\n0.5401\n0.5390\n0.2060\n0.4378\n0.10720\n0\n\n\n9\n12.46\n24.04\n83.97\n475.9\n0.11860\n0.23960\n0.22730\n0.08543\n0.2030\n0.08243\n...\n40.68\n97.65\n711.4\n0.1853\n1.0580\n1.1050\n0.2210\n0.4366\n0.20750\n0\n\n\n\n\n10 rows × 31 columns\n\n\n\n\nx = rawData[['mean radius', 'mean texture']]\ny = rawData['cancer']\n\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25, random_state=0)\n\nfit(x_train, y_train)     ## 모수 추정(estimat)\nget_params()              ## 추정된 모수 확인\npredict(x_test)           ## x_test로부터 라벨 예측\npredict_log_proba(x_test) ## 로그 취한 확률 예측\npredict_proba(x_test)     ## 각 라벨로 예측될 확률\nscore(x_test, y_test)     ## 모델 정확도 평가를 위한 mean accuracy\n- 선형회귀\n\nclf = LinearRegression()\nclf.fit(x_train,y_train)  # 모수 추정\nclf.coef_                 # 추정 된 모수 확인(상수항 제외)\nclf.intercept_            # 추정 된 상수항 확인\nclf.predict(x_test)\n#clf.predic(x_test)        # 예측\nclf.score(x_test, y_test) # 모형 성능 평가\n\n0.6092200214592733\n\n\n- 로지스틱\n\nclf = LogisticRegression(solver='lbfgs').fit(x_train,y_train)\nclf.predict(x_test)\nclf.predict_proba(x_test)\nclf.score(x_test,y_test)\n\n0.9020979020979021\n\n\n- 나이브베이즈\n\ngnb = GaussianNB()\ngnb.fit(x_train, y_train)\ngnb.predict(x_test)\ngnb.score(x_test, y_test)\n\n0.8951048951048951\n\n\n- 의사결정나무\n\nclf = tree.DecisionTreeClassifier()\nclf.fit(x_train, y_train)\nclf.predict(x_test)\nclf.predict_proba(x_test)\nclf.score(x_test, y_test)\n\n0.8601398601398601\n\n\n- svm\n\nclf = svm.SVC(kernel='linear')\nclf.fit(x_train, y_train)\nclf.predict(x_test)\nclf.score(x_test, y_test)\n\n0.9020979020979021\n\n\n- 랜덤포레스트\n\nclf = RandomForestClassifier(max_depth=2, random_state=0)\nclf.fit(x_train, y_train)\nclf.feature_importances_\nclf.predict(x_test)\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n       1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n       1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n       0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n       0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n       1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n       1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0])"
  },
  {
    "objectID": "posts/Study/baseball-salary-prediction.html",
    "href": "posts/Study/baseball-salary-prediction.html",
    "title": "미래 예측 데이터 분석(프로야구 선수의 다음 해 연봉 예측)",
    "section": "",
    "text": "python-data-analysis data\nData Source\n이것이 데이터 분석이다 with 파이썬\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nimport matplotlib as mpl"
  },
  {
    "objectID": "posts/Study/baseball-salary-prediction.html#데이터-분리",
    "href": "posts/Study/baseball-salary-prediction.html#데이터-분리",
    "title": "미래 예측 데이터 분석(프로야구 선수의 다음 해 연봉 예측)",
    "section": "데이터 분리",
    "text": "데이터 분리\n\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n\nX=picher_df[picher_df.columns.difference(['선수명','y'])]\ny=picher_df['y']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=19)"
  },
  {
    "objectID": "posts/Study/baseball-salary-prediction.html#lrnr",
    "href": "posts/Study/baseball-salary-prediction.html#lrnr",
    "title": "미래 예측 데이터 분석(프로야구 선수의 다음 해 연봉 예측)",
    "section": "lrnr",
    "text": "lrnr\n\nlr = linear_model.LinearRegression()"
  },
  {
    "objectID": "posts/Study/baseball-salary-prediction.html#fit",
    "href": "posts/Study/baseball-salary-prediction.html#fit",
    "title": "미래 예측 데이터 분석(프로야구 선수의 다음 해 연봉 예측)",
    "section": "fit",
    "text": "fit\n\nmodel = lr.fit(X_train, y_train)\n\n\nprint(lr.coef_)\n\n[-1.66433649e+04 -1.00005486e+02 -5.03120671e+04 -1.56086205e+03\n  1.57200472e+03 -7.47049524e+02 -1.18666920e+02 -5.23546876e+02\n  2.14736071e+03  8.98376390e+02  7.76920737e+03  3.91969663e+04\n -1.25668595e+02 -8.92118013e+02  4.49911172e+02  3.22164149e+03\n  7.77935715e+02 -2.81055645e+03  1.90870829e+03 -4.92026373e+02\n -5.21324705e+01  2.36025301e+02  7.63803603e-01  1.69794484e+01\n  4.15729514e+02  3.61384063e+03 -2.07081858e+02  1.78400390e+04]\n\n\n\n학습된 계수"
  },
  {
    "objectID": "posts/Study/baseball-salary-prediction.html#evaluate",
    "href": "posts/Study/baseball-salary-prediction.html#evaluate",
    "title": "미래 예측 데이터 분석(프로야구 선수의 다음 해 연봉 예측)",
    "section": "evaluate",
    "text": "evaluate\n\nimport statsmodels.api as sm\n\n\nX_train = sm.add_constant(X_train)   # X_train데이터에 상수열 추가 \nmodel = sm.OLS(y_train, X_train).fit()\nmodel.summary()\n\n\n\n\n\nDep. Variable:\ny\nR-squared:\n0.928\n\n\nModel:\nOLS\nAdj. R-squared:\n0.907\n\n\nMethod:\nLeast Squares\nF-statistic:\n44.19\n\n\nDate:\nThu, 18 May 2023\nProb (F-statistic):\n7.70e-42\n\n\nTime:\n13:58:48\nLog-Likelihood:\n-1247.8\n\n\nNo. Observations:\n121\nAIC:\n2552.\n\n\nDf Residuals:\n93\nBIC:\n2630.\n\n\nDf Model:\n27\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\nOLS Regression Results\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n2.591e+04\n2.08e+04\n1.246\n0.216\n-1.54e+04\n6.72e+04\n\n\nBABIP\n-1.664e+04\n1.45e+04\n-1.145\n0.255\n-4.55e+04\n1.22e+04\n\n\nERA\n-100.0055\n557.379\n-0.179\n0.858\n-1206.850\n1006.839\n\n\nFIP\n-5.031e+04\n5.04e+04\n-0.998\n0.321\n-1.5e+05\n4.98e+04\n\n\nKIA\n1317.9412\n2954.415\n0.446\n0.657\n-4548.941\n7184.823\n\n\nKT\n4450.8079\n3178.624\n1.400\n0.165\n-1861.310\n1.08e+04\n\n\nLG\n2131.7537\n2842.243\n0.750\n0.455\n-3512.378\n7775.885\n\n\nLOB%\n-118.6669\n134.995\n-0.879\n0.382\n-386.740\n149.406\n\n\nNC\n2355.2563\n3713.995\n0.634\n0.528\n-5020.002\n9730.514\n\n\nRA9-WAR\n2147.3607\n1589.996\n1.351\n0.180\n-1010.055\n5304.777\n\n\nSK\n3777.1796\n2799.284\n1.349\n0.181\n-1781.644\n9336.004\n\n\nWAR\n7769.2074\n1915.216\n4.057\n0.000\n3965.967\n1.16e+04\n\n\nkFIP\n3.92e+04\n4.01e+04\n0.977\n0.331\n-4.05e+04\n1.19e+05\n\n\n경기\n-125.6686\n152.468\n-0.824\n0.412\n-428.439\n177.102\n\n\n두산\n1986.6852\n3665.539\n0.542\n0.589\n-5292.349\n9265.719\n\n\n롯데\n3328.7144\n3332.488\n0.999\n0.320\n-3288.946\n9946.374\n\n\n볼넷/9\n3221.6415\n2655.869\n1.213\n0.228\n-2052.388\n8495.671\n\n\n블론\n777.9357\n760.237\n1.023\n0.309\n-731.745\n2287.616\n\n\n삼성\n68.2468\n3734.016\n0.018\n0.985\n-7346.768\n7483.262\n\n\n삼진/9\n1908.7083\n2576.865\n0.741\n0.461\n-3208.435\n7025.852\n\n\n선발\n-492.0264\n563.005\n-0.874\n0.384\n-1610.043\n625.990\n\n\n세\n-52.1325\n269.373\n-0.194\n0.847\n-587.054\n482.789\n\n\n승\n236.0253\n2215.264\n0.107\n0.915\n-4163.049\n4635.100\n\n\n연봉(2017)\n0.7638\n0.051\n15.055\n0.000\n0.663\n0.865\n\n\n이닝\n16.9794\n131.695\n0.129\n0.898\n-244.540\n278.499\n\n\n패\n415.7295\n618.326\n0.672\n0.503\n-812.143\n1643.602\n\n\n한화\n6492.6438\n3541.464\n1.833\n0.070\n-540.002\n1.35e+04\n\n\n홀드\n-207.0819\n362.736\n-0.571\n0.569\n-927.403\n513.239\n\n\n홈런/9\n1.784e+04\n1.65e+04\n1.082\n0.282\n-1.49e+04\n5.06e+04\n\n\n\n\n\n\nOmnibus:\n28.069\nDurbin-Watson:\n2.025\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n194.274\n\n\nSkew:\n-0.405\nProb(JB):\n6.52e-43\n\n\nKurtosis:\n9.155\nCond. No.\n2.39e+15\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The smallest eigenvalue is 1.72e-20. This might indicate that there arestrong multicollinearity problems or that the design matrix is singular.\n\n\n\nOLS : Ordinary Least Squares방법을 사용한 회귀 모델 정의 작업\npvalue가 0.05미만인 데이터: WAR, 연봉(2017), 한화"
  },
  {
    "objectID": "posts/Study/baseball-salary-prediction.html#r2-score",
    "href": "posts/Study/baseball-salary-prediction.html#r2-score",
    "title": "미래 예측 데이터 분석(프로야구 선수의 다음 해 연봉 예측)",
    "section": "\\(R^2\\) SCORE",
    "text": "\\(R^2\\) SCORE\n\nX = picher_df[picher_df.columns.difference(['선수명','y'])]\ny = picher_df['y']\nX_trian, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=19)\n\n\nlr=  linear_model.LinearRegression()\nmodel=lr.fit(X_train, y_train)\nprint(model.score(X_train, y_train))\n\n\n0.9129870505440939\n\n\n\nX_test = sm.add_constant(X_test)\n\n\nprint(model.score(X_test, y_test))\n\n0.87323834388485\n\n\n이ㅏ거 왜 안대"
  },
  {
    "objectID": "posts/Study/baseball-salary-prediction.html#rmse-score",
    "href": "posts/Study/baseball-salary-prediction.html#rmse-score",
    "title": "미래 예측 데이터 분석(프로야구 선수의 다음 해 연봉 예측)",
    "section": "RMSE score",
    "text": "RMSE score\n\ny_predictions = lr.predict(X_train)\nprint(sqrt(mean_squared_error(y_train, y_predictions)))\n\n7989.160309407914\n\n\n\ny_predictions = lr.predict(X_test)\nprint(sqrt(mean_squared_error(y_test,y_predictions)))\n\n15091.589840279374"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html",
    "href": "posts/Study/데이터 개념 공부.html",
    "title": "데이터 개념공부",
    "section": "",
    "text": "자격증 공부하면서 헷갈렸던 개념 공부"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html#k-익명성",
    "href": "posts/Study/데이터 개념 공부.html#k-익명성",
    "title": "데이터 개념공부",
    "section": "k-익명성",
    "text": "k-익명성\n\n주어진 데이터 집합에서 같은 값이 적어도 k개 이상 존재\n연결 공격 취약점 방어"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html#l-다양성",
    "href": "posts/Study/데이터 개념 공부.html#l-다양성",
    "title": "데이터 개념공부",
    "section": "l-다양성",
    "text": "l-다양성\n\n주어진 데이터 집합에서 비식별 되는 레코드들은 적어도 l개의 서로 다른 민감한 정보를 가짐\nk-익명성에 두가지 취약점인 동질성 공격, 배경 지식에 의한 공격 방어"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html#t-근접성",
    "href": "posts/Study/데이터 개념 공부.html#t-근접성",
    "title": "데이터 개념공부",
    "section": "t-근접성",
    "text": "t-근접성\n\n정보의 분포가 t이하의 차이를 보여야 함\nl-다양성의 쏠림 공격, 유사성 공격 보완"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html#m-유일성",
    "href": "posts/Study/데이터 개념 공부.html#m-유일성",
    "title": "데이터 개념공부",
    "section": "m-유일성",
    "text": "m-유일성\n\n원본 데이터와 동일한 속성 값의 조합이 비식별 결과 데이터에 최소 m개 이상 존재하여 재식별 가능성 낮춤"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html#분류-모델",
    "href": "posts/Study/데이터 개념 공부.html#분류-모델",
    "title": "데이터 개념공부",
    "section": "분류 모델",
    "text": "분류 모델"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html#회귀모델",
    "href": "posts/Study/데이터 개념 공부.html#회귀모델",
    "title": "데이터 개념공부",
    "section": "회귀모델",
    "text": "회귀모델"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html#svm",
    "href": "posts/Study/데이터 개념 공부.html#svm",
    "title": "데이터 개념공부",
    "section": "SVM",
    "text": "SVM\n\nMargin : 서포트 벡터를 지나는 초평면 사이 거리"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html#군집-분석",
    "href": "posts/Study/데이터 개념 공부.html#군집-분석",
    "title": "데이터 개념공부",
    "section": "군집 분석",
    "text": "군집 분석\nex) 미국 주별 강력 범죄율 군집분석: 가까운 거리에 있는 유사 특징을 가진 도시들을 묶어서 보여줌\n\n군집내 유사성, 군집간 상이성"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html#연관성분석",
    "href": "posts/Study/데이터 개념 공부.html#연관성분석",
    "title": "데이터 개념공부",
    "section": "연관성분석",
    "text": "연관성분석\n\n유사 개체들을 그룹화하여 각 집단의 특성을 파악하고 사건의 연관규칙을 찾는다.\n\n- 지지도\n- 신뢰도\n- 향상도"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html#인공신경망",
    "href": "posts/Study/데이터 개념 공부.html#인공신경망",
    "title": "데이터 개념공부",
    "section": "인공신경망",
    "text": "인공신경망"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html#오토인코더",
    "href": "posts/Study/데이터 개념 공부.html#오토인코더",
    "title": "데이터 개념공부",
    "section": "오토인코더",
    "text": "오토인코더\n\n다차원데이터를 저차원, 고차원 데이터로 바꾸면서 특징을 찾는다.\n다차원 데이터를 입력하면 encoder 통해 차원을 줄이는 은닉충으로 이동하고 decoder 통해 차원을 늘리는 출력층으로 내보낸 뒤 출력값을 입력값과 비슷하게 만드는 가중치를 찾는다.\n데이터를 압축하고 배경의 잡음을 억제"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html#gan",
    "href": "posts/Study/데이터 개념 공부.html#gan",
    "title": "데이터 개념공부",
    "section": "GAN",
    "text": "GAN\n\n학습 데이터 패턴과 유사한 것을 만드는 생성자(generator) 네트워크와 패턴 진위 여부를 판별하는 판별자(discriminator) 네트워크로 구성\n기계학습에서 교수님이 설명해주신 내용중에.. 그 경찰이랑 범죄자 관련 내용 참고하자"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html#독립변수x연속형-종속변수y연속형",
    "href": "posts/Study/데이터 개념 공부.html#독립변수x연속형-종속변수y연속형",
    "title": "데이터 개념공부",
    "section": "독립변수(X:연속형)-종속변수(Y:연속형)",
    "text": "독립변수(X:연속형)-종속변수(Y:연속형)\n\n회귀 분석\n인공신경망 모델\nk-최근접 이웃기법\n의사결정나무(회귀 나무)"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html#독립변수x연속형-종속변수y이산형범주형",
    "href": "posts/Study/데이터 개념 공부.html#독립변수x연속형-종속변수y이산형범주형",
    "title": "데이터 개념공부",
    "section": "독립변수(X:연속형)-종속변수(Y:이산형/범주형)",
    "text": "독립변수(X:연속형)-종속변수(Y:이산형/범주형)\n\n로지스틱 회귀 분석\n판별 분석\nK-최근접 이웃기법\n의사결정나무(분류나무)"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html#독립변수x이산형범주형-종속변수y연속형",
    "href": "posts/Study/데이터 개념 공부.html#독립변수x이산형범주형-종속변수y연속형",
    "title": "데이터 개념공부",
    "section": "독립변수(X:이산형/범주형)-종속변수(Y:연속형)",
    "text": "독립변수(X:이산형/범주형)-종속변수(Y:연속형)\n\n회귀 분석\n인공신경망 모델\n의사결정나무(회귀 나무)"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html#독립변수x이산형범주형-종속변수y이산형범주형",
    "href": "posts/Study/데이터 개념 공부.html#독립변수x이산형범주형-종속변수y이산형범주형",
    "title": "데이터 개념공부",
    "section": "독립변수(X:이산형/범주형)-종속변수(Y:이산형/범주형)",
    "text": "독립변수(X:이산형/범주형)-종속변수(Y:이산형/범주형)\n\n인공신경망 모델\n의사결정나무(분류 나무)\n로지스틱 회귀 분석"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html#필터-기법",
    "href": "posts/Study/데이터 개념 공부.html#필터-기법",
    "title": "데이터 개념공부",
    "section": "필터 기법",
    "text": "필터 기법"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html#래퍼-기법",
    "href": "posts/Study/데이터 개념 공부.html#래퍼-기법",
    "title": "데이터 개념공부",
    "section": "래퍼 기법",
    "text": "래퍼 기법\n\n예측 정확도 측면에서 가장 좋은 성능을 보이는 하위집합 선택\n그리디 알고리즘(문제를 해결하는 과정에서 그 순간 최적이라고 생각하는 결정)\n전진 선택법, 후진 소거법, 단계적 방법\n변수의 일부를 모델링에 사용하고 그 결과를 확인하는 작업 반복"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html#임베디드-기법",
    "href": "posts/Study/데이터 개념 공부.html#임베디드-기법",
    "title": "데이터 개념공부",
    "section": "임베디드 기법",
    "text": "임베디드 기법\n\n모델의 정확도에 기여 하는 변수 학습\n좀 더 적은 계수를 가지는 회귀식 찾기\n\n- 사례\n\n라쏘(LASSO): 가중치의 절댓값의 합을 최소화, L1-norm을 통해 제약을 줌\n릿지(Ridge): 가중치들의 제곱합을 최소화, L2-norm\n엘라스틱넷: 라쏘 + 릿지"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html#자기회귀모형ar",
    "href": "posts/Study/데이터 개념 공부.html#자기회귀모형ar",
    "title": "데이터 개념공부",
    "section": "자기회귀모형(AR)",
    "text": "자기회귀모형(AR)\n\n현 시점의 자료가 p시점 전의 유한개의 과거 자료로 설명될 수 있는 모형"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html#이동평균모형ma",
    "href": "posts/Study/데이터 개념 공부.html#이동평균모형ma",
    "title": "데이터 개념공부",
    "section": "이동평균모형(MA)",
    "text": "이동평균모형(MA)\n\n시간이 지날수록 관측치의 평균값이 지속적으로 증가하거나 감소하는 시계열 모형\n유한개의 백색잡음의 선형결합으로 정상성 만족"
  },
  {
    "objectID": "posts/Study/데이터 개념 공부.html#자기-회귀-누적-이동평균-모형arima",
    "href": "posts/Study/데이터 개념 공부.html#자기-회귀-누적-이동평균-모형arima",
    "title": "데이터 개념공부",
    "section": "자기 회귀 누적 이동평균 모형(ARIMA)",
    "text": "자기 회귀 누적 이동평균 모형(ARIMA)\n\n분기/반기/연간 단위로 다음 지표를 예측하거나 주간/월간 단위로 지표 리뷰\n비정상 시계열 모형. 차분이나 변환으로 AR,MA,ARMA모형으로 정상화\nARIMA(p,d,q)\np: AR과 관련, q: MA와 관련, d:ARIMA에서 ARMA로 정상화 할때 차분 횟수\nARIMA(0,0,0): 백색잡음 모형\nARIMA(p,0,0): AR모형\nARIMA(0,0,q): MA모형"
  },
  {
    "objectID": "posts/Study/boostcourse/5. K-beauty.html",
    "href": "posts/Study/boostcourse/5. K-beauty.html",
    "title": "6: K-beauty",
    "section": "",
    "text": "e:추정지, p:잠정치, -:자료없음, …:미상자료, x: 비밀번호"
  },
  {
    "objectID": "posts/Study/boostcourse/5. K-beauty.html#기간에서-연도를-분리하기",
    "href": "posts/Study/boostcourse/5. K-beauty.html#기간에서-연도를-분리하기",
    "title": "6: K-beauty",
    "section": "기간에서 연도를 분리하기",
    "text": "기간에서 연도를 분리하기\n\ndf[\"기간\"]\n# object : string데이터를 의미\n\n0        2014.1/4\n1        2014.1/4\n2        2014.1/4\n3        2014.1/4\n4        2014.1/4\n           ...   \n10795    2019.4/4\n10796    2019.4/4\n10797    2019.4/4\n10798    2019.4/4\n10799    2019.4/4\nName: 기간, Length: 10800, dtype: object\n\n\ndf[“기간”].map?\n\n\ndf[\"연도\"] = list(map(lambda x : int(x.split(\".\")[0]), df[\"기간\"])) \n\n\n# 기간에서 분기만 분리하기\ndf[\"분기\"] = list(map(lambda x : int(x.split(\".\")[1].split()[0].split(\"/\")[0]), df[\"기간\"])) \n\n\n# (1) \".\" 을 기준으로 split하고 ([\"2022\", \"1/4 p\"]) 1번째 인덱스를 취함 -&gt; \"1/4 p\"\n\n# (2) \" \" 을 기준으로 split하고 ([\"1/4\", \"p\"]) 0번째 인덱스를 취함 -&gt; \"1/4\"\n\n# (3) \"/\"을 기준으로 split하고 ([\"1\", \"4\"]) 0번째 인덱스를 취함\n\n# (4) \"1\"에 int() 형변환 함수를 씌워 int64형으로 변환\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\n국가(대륙)별\n상품군별\n판매유형별\n기간\n백만원\n연도\n분기\n\n\n\n\n0\n합계\n합계\n계\n2014.1/4\n148272\n2014\n1\n\n\n1\n합계\n합계\n면세점\n2014.1/4\n-\n2014\n1\n\n\n2\n합계\n합계\n면세점 이외\n2014.1/4\n-\n2014\n1\n\n\n3\n합계\n컴퓨터 및 주변기기\n계\n2014.1/4\n4915\n2014\n1\n\n\n4\n합계\n컴퓨터 및 주변기기\n면세점\n2014.1/4\n-\n2014\n1"
  },
  {
    "objectID": "posts/Study/boostcourse/5. K-beauty.html#금액을-수치데이터로-표현하기-위해-데이터-타입-변경하기",
    "href": "posts/Study/boostcourse/5. K-beauty.html#금액을-수치데이터로-표현하기-위해-데이터-타입-변경하기",
    "title": "6: K-beauty",
    "section": "금액을 수치데이터로 표현하기 위해 데이터 타입 변경하기",
    "text": "금액을 수치데이터로 표현하기 위해 데이터 타입 변경하기\n\n# - 문자를 결측치로 변경하고 float타입으로 변경하기\ndf[\"백만원\"] = df[\"백만원\"].replace(\"-\",pd.np.nan).astype(float)\ndf[\"백만원\"]\n\nC:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_117660\\99655999.py:2: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n  df[\"백만원\"] = df[\"백만원\"].replace(\"-\",pd.np.nan).astype(float)\n\n\n0        148272.0\n1             NaN\n2             NaN\n3          4915.0\n4             NaN\n           ...   \n10795         0.0\n10796       531.0\n10797      1094.0\n10798         1.0\n10799      1093.0\nName: 백만원, Length: 10800, dtype: float64\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\n국가(대륙)별\n상품군별\n판매유형별\n기간\n백만원\n연도\n분기\n\n\n\n\n0\n합계\n합계\n계\n2014.1/4\n148272.0\n2014\n1\n\n\n1\n합계\n합계\n면세점\n2014.1/4\nNaN\n2014\n1\n\n\n2\n합계\n합계\n면세점 이외\n2014.1/4\nNaN\n2014\n1\n\n\n3\n합계\n컴퓨터 및 주변기기\n계\n2014.1/4\n4915.0\n2014\n1\n\n\n4\n합계\n컴퓨터 및 주변기기\n면세점\n2014.1/4\nNaN\n2014\n1"
  },
  {
    "objectID": "posts/Study/boostcourse/5. K-beauty.html#필요없는-데이터-제거하기",
    "href": "posts/Study/boostcourse/5. K-beauty.html#필요없는-데이터-제거하기",
    "title": "6: K-beauty",
    "section": "필요없는 데이터 제거하기",
    "text": "필요없는 데이터 제거하기\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10800 entries, 0 to 10799\nData columns (total 7 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   국가(대륙)별  10800 non-null  object \n 1   상품군별     10800 non-null  object \n 2   판매유형별    10800 non-null  object \n 3   기간       10800 non-null  object \n 4   백만원      7200 non-null   float64\n 5   연도       10800 non-null  int64  \n 6   분기       10800 non-null  int64  \ndtypes: float64(1), int64(2), object(4)\nmemory usage: 590.8+ KB\n\n\n\n# 합계 데이터는 따로 구할 수 있으므로 전체 데이터에서 제거한다.\n\ndf = df[(df[\"국가(대륙)별\"] != \"합계\") & (df[\"상품군별\"] != \"합계\")].copy()\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 9072 entries, 48 to 10799\nData columns (total 7 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   국가(대륙)별  9072 non-null   object \n 1   상품군별     9072 non-null   object \n 2   판매유형별    9072 non-null   object \n 3   기간       9072 non-null   object \n 4   백만원      6048 non-null   float64\n 5   연도       9072 non-null   int64  \n 6   분기       9072 non-null   int64  \ndtypes: float64(1), int64(2), object(4)\nmemory usage: 567.0+ KB\n\n\n\n# 결측치 보기\ndf.isnull().sum()\n\n국가(대륙)별       0\n상품군별          0\n판매유형별         0\n기간            0\n백만원        3024\n연도            0\n분기            0\ndtype: int64"
  },
  {
    "objectID": "posts/Study/boostcourse/5. K-beauty.html#전체-상품군-판매액",
    "href": "posts/Study/boostcourse/5. K-beauty.html#전체-상품군-판매액",
    "title": "6: K-beauty",
    "section": "전체 상품군 판매액",
    "text": "전체 상품군 판매액\n\n# 판매유형별 데이터는 일부 기간에는 \"계\"만 존재하기 때문에\n# 판매유형별 == \"계\" 데이터만 가져와서 봐야지\n# 평균값을 구하는 그래프에서 올바른 값을 표현할 수 있다.\n\ndf_total=df[df[\"판매유형별\"] == \"계\"].copy()\ndf_total.head()\n\n\n\n\n\n\n\n\n국가(대륙)별\n상품군별\n판매유형별\n기간\n백만원\n연도\n분기\n\n\n\n\n48\n미국\n컴퓨터 및 주변기기\n계\n2014.1/4\n2216.0\n2014\n1\n\n\n51\n미국\n가전·전자·통신기기\n계\n2014.1/4\n2875.0\n2014\n1\n\n\n54\n미국\n소프트웨어\n계\n2014.1/4\n47.0\n2014\n1\n\n\n57\n미국\n서 적\n계\n2014.1/4\n962.0\n2014\n1\n\n\n60\n미국\n사무·문구\n계\n2014.1/4\n25.0\n2014\n1\n\n\n\n\n\n\n\n\n# 연도, 판매액 lineplot으로 그리기\n\nsns.lineplot(data=df_total, x=\"연도\", y=\"백만원\")\n\n&lt;AxesSubplot:xlabel='연도', ylabel='백만원'&gt;\n\n\n\n\n\n\n# 연도, 판매액 lineplot으로 그리고 상품군별로 다른 색상으로 표시하기\nsns.lineplot(data=df_total, x=\"연도\", y=\"백만원\", hue=\"상품군별\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n# 오른쪽으로 범례 옮기는거 \n\n&lt;matplotlib.legend.Legend at 0x15f495c10a0&gt;\n\n\n\n\n\n\n# 위에 그린 그래프를 자세히 보기 위해 서브플롯으로 표시하기\nsns.relplot(data=df_total, x=\"연도\", y=\"백만원\", hue=\"상품군별\", kind=\"line\", col=\"상품군별\", col_wrap=4)\n\n\n\n\n\n# isin을 사용해 화장품만 제외하고 df_sb이라는 변수에 담기\n\ndf_sub=df_total[~df_total[\"상품군별\"].isin([\"화장품\",\"의류 및 패션 관련상품\"])].copy()\n\n# 앞에 ~ 물결 표시해ㅜㅈ면 화장품만 빼고...\n\n\n# 연도별 판매액을 상품군별로 relplot을 활용해 서브플롯으로 그려보기\n\nsns.relplot(data=df_sub, x=\"연도\", y=\"백만원\", hue=\"상품군별\", col=\"상품군별\", col_wrap=4, kind=\"line\")\n#kind기본값은 scatter"
  },
  {
    "objectID": "posts/Study/boostcourse/5. K-beauty.html#화장품의-온라인-쇼핑-해외직접판매액",
    "href": "posts/Study/boostcourse/5. K-beauty.html#화장품의-온라인-쇼핑-해외직접판매액",
    "title": "6: K-beauty",
    "section": "화장품의 온라인 쇼핑 해외직접판매액",
    "text": "화장품의 온라인 쇼핑 해외직접판매액\n\n# df_cosmetic이라는 변수에 상품군별이 화장품인 데이터만 가져오기\n\ndf_cosmetic = df_total[df_total[\"상품군별\"]==\"화장품\"].copy()\ndf_cosmetic.head()\n\n\n\n\n\n\n\n\n국가(대륙)별\n상품군별\n판매유형별\n기간\n백만원\n연도\n분기\n\n\n\n\n72\n미국\n화장품\n계\n2014.1/4\n3740.0\n2014\n1\n\n\n117\n중국\n화장품\n계\n2014.1/4\n32235.0\n2014\n1\n\n\n162\n일본\n화장품\n계\n2014.1/4\n1034.0\n2014\n1\n\n\n207\n아세안(ASEAN)\n화장품\n계\n2014.1/4\n398.0\n2014\n1\n\n\n252\n유럽연합(EU)\n화장품\n계\n2014.1/4\n937.0\n2014\n1\n\n\n\n\n\n\n\n\ndf_cosmetic[\"상품군별\"].unique()\n\narray(['화장품'], dtype=object)\n\n\n\n# 연도와 판매액을 lineplot으로 그리고 분기별로 다른 색상으로 표현해 보기\nplt.figure(figsize=(15,4))\nsns.lineplot(data=df_cosmetic, x=\"연도\", y=\"백만원\", hue=\"분기\")\n\n&lt;AxesSubplot:xlabel='연도', ylabel='백만원'&gt;\n\n\n\n\n\n\ndf_cosmetic.head()\n\n\n\n\n\n\n\n\n국가(대륙)별\n상품군별\n판매유형별\n기간\n백만원\n연도\n분기\n\n\n\n\n72\n미국\n화장품\n계\n2014.1/4\n3740.0\n2014\n1\n\n\n117\n중국\n화장품\n계\n2014.1/4\n32235.0\n2014\n1\n\n\n162\n일본\n화장품\n계\n2014.1/4\n1034.0\n2014\n1\n\n\n207\n아세안(ASEAN)\n화장품\n계\n2014.1/4\n398.0\n2014\n1\n\n\n252\n유럽연합(EU)\n화장품\n계\n2014.1/4\n937.0\n2014\n1\n\n\n\n\n\n\n\n\n# 화장품 판매액에 대한 기간별 금액 데이터 시각화 하기\nplt.figure(figsize=(15,4))\nplt.xticks(rotation=30) #x축 기울기\nsns.lineplot(data=df_cosmetic, x=\"기간\", y=\"백만원\")\n\n&lt;AxesSubplot:xlabel='기간', ylabel='백만원'&gt;\n\n\n\n\n\n\n# 화장품 판매액에 대한 기간별 금액 데이터 시각화하고 \"국가(대륙)별\"로 다른 색상으로 표시하기\n\nplt.figure(figsize=(15,4))\nplt.xticks(rotation=30) #x축 기울기\nsns.lineplot(data=df_cosmetic, x=\"기간\", y=\"백만원\", hue=\"국가(대륙)별\")\n\n&lt;AxesSubplot:xlabel='기간', ylabel='백만원'&gt;\n\n\n\n\n\n\n# 중국빼고 보기\n\nplt.figure(figsize=(15,4))\nplt.xticks(rotation=30) #x축 기울기\nsns.lineplot(data=df_cosmetic[df_cosmetic[\"국가(대륙)별\"]!=\"중국\"], x=\"기간\", y=\"백만원\", hue=\"국가(대륙)별\")\n\n&lt;AxesSubplot:xlabel='기간', ylabel='백만원'&gt;\n\n\n\n\n\n\n# 화장품 판매액에 대한 기간별 금액 데이터를 시각화하고 \"판매유형별\"로 다른색상으로 표현하기\nplt.figure(figsize=(15,4))\nplt.xticks(rotation=30) #x축 기울기\ndf_sub = df[df[\"판매유형별\"] != \"계\"].copy()\nsns.lineplot(data=df_sub, x=\"기간\", y=\"백만원\", hue=\"판매유형별\")\n\n&lt;AxesSubplot:xlabel='기간', ylabel='백만원'&gt;\n\n\n\n\n\n\n# 화장품 판매액에 대한 기간별 금액 데이터를 시각화하고 \"판매유형별\"로 다른색상으로 표현하기\nplt.figure(figsize=(15,4))\nplt.xticks(rotation=30) #x축 기울기\ndf_sub = df[(df[\"판매유형별\"] != \"계\") & (df[\"판매유형별\"]!=\"면세점\")].copy()\nsns.lineplot(data=df_sub, x=\"기간\", y=\"백만원\", hue=\"판매유형별\", ci=None)\n\n&lt;AxesSubplot:xlabel='기간', ylabel='백만원'&gt;"
  },
  {
    "objectID": "posts/Study/boostcourse/5. K-beauty.html#의류-및-패션관련-상품-온라인쇼핑-해외직접판매액",
    "href": "posts/Study/boostcourse/5. K-beauty.html#의류-및-패션관련-상품-온라인쇼핑-해외직접판매액",
    "title": "6: K-beauty",
    "section": "의류 및 패션관련 상품 온라인쇼핑 해외직접판매액",
    "text": "의류 및 패션관련 상품 온라인쇼핑 해외직접판매액\n\n# df_fashion 이라는 변수에 의류 데이터만 가져와 따로 담아두기\n\n\ndf_fashion = df[(df[\"상품군별\"] == \"의류 및 패션 관련상품\") & (df[\"판매유형별\"]==\"계\")].copy()\ndf_fashion.head()\n\n\n\n\n\n\n\n\n국가(대륙)별\n상품군별\n판매유형별\n기간\n백만원\n연도\n분기\n\n\n\n\n66\n미국\n의류 및 패션 관련상품\n계\n2014.1/4\n9810.0\n2014\n1\n\n\n111\n중국\n의류 및 패션 관련상품\n계\n2014.1/4\n12206.0\n2014\n1\n\n\n156\n일본\n의류 및 패션 관련상품\n계\n2014.1/4\n13534.0\n2014\n1\n\n\n201\n아세안(ASEAN)\n의류 및 패션 관련상품\n계\n2014.1/4\n3473.0\n2014\n1\n\n\n246\n유럽연합(EU)\n의류 및 패션 관련상품\n계\n2014.1/4\n1364.0\n2014\n1\n\n\n\n\n\n\n\n\n# 의류 및 패션 관련상품 판매액에 대한 기간별 금액 데이터를 시각화하고\n# 국가별로 다른색상으로 표시하기\n\nplt.figure(figsize=(15,4))\nplt.xticks(rotation=30) #x축 기울기\nsns.lineplot(data=df_fashion, x=\"기간\", y=\"백만원\", hue=\"국가(대륙)별\")\n\n&lt;AxesSubplot:xlabel='기간', ylabel='백만원'&gt;\n\n\n\n\n\n\n# 의류 및 패션관련 상품 판매엑에 대한 기간별 금액 데이터 시각화\n# 판매유형별로 다른 색상 표시\n\ndf_fashion2 = df[(df[\"상품군별\"] == \"의류 및 패션 관련상품\") & (df[\"판매유형별\"] != \"계\")].copy()\n\nplt.figure(figsize=(15, 4))\nplt.xticks(rotation=30)\nsns.lineplot(data=df_fashion2, x=\"기간\", y=\"백만원\", hue=\"판매유형별\", ci=None)\n\n&lt;AxesSubplot:xlabel='기간', ylabel='백만원'&gt;"
  },
  {
    "objectID": "posts/Study/boostcourse/5. K-beauty.html#데이터-집계하기",
    "href": "posts/Study/boostcourse/5. K-beauty.html#데이터-집계하기",
    "title": "6: K-beauty",
    "section": "데이터 집계하기",
    "text": "데이터 집계하기\n\n# 피봇테이블로 \"국가(대륙)별\", \"연도\"별로 합계 금액을 표 형ㅇ태로 구하기\n\ndf_fashion.pivot_table?\n\n\ndf_fashion[\"판매유형별\"].value_counts()\n\n\ndf_fashion.head()\n\n\n\n\n\n\n\n\n국가(대륙)별\n상품군별\n판매유형별\n기간\n백만원\n연도\n분기\n\n\n\n\n66\n미국\n의류 및 패션 관련상품\n계\n2014.1/4\n9810.0\n2014\n1\n\n\n111\n중국\n의류 및 패션 관련상품\n계\n2014.1/4\n12206.0\n2014\n1\n\n\n156\n일본\n의류 및 패션 관련상품\n계\n2014.1/4\n13534.0\n2014\n1\n\n\n201\n아세안(ASEAN)\n의류 및 패션 관련상품\n계\n2014.1/4\n3473.0\n2014\n1\n\n\n246\n유럽연합(EU)\n의류 및 패션 관련상품\n계\n2014.1/4\n1364.0\n2014\n1\n\n\n\n\n\n\n\n\nresult = df_fashion.pivot_table(index=\"국가(대륙)별\", columns=\"연도\", values=\"백만원\", aggfunc=\"sum\")\n# 기본은 평균으로 되어있음.. aggfunc\n\nresult\n\n\n\n\n\n\n\n연도\n2014\n2015\n2016\n2017\n2018\n2019\n\n\n국가(대륙)별\n\n\n\n\n\n\n\n\n\n\n기타\n9683.0\n7248.0\n5918.0\n14387.0\n23901.0\n6475.0\n\n\n대양주\n3392.0\n2349.0\n3401.0\n2266.0\n2725.0\n2489.0\n\n\n미국\n33223.0\n38066.0\n48451.0\n50353.0\n47875.0\n55536.0\n\n\n아세안(ASEAN)\n14936.0\n19639.0\n24478.0\n22671.0\n23068.0\n31247.0\n\n\n유럽연합(EU)\n4485.0\n3374.0\n4899.0\n3736.0\n4114.0\n3694.0\n\n\n일본\n48960.0\n57594.0\n79905.0\n90584.0\n136800.0\n134637.0\n\n\n중국\n57531.0\n142339.0\n190932.0\n225407.0\n288848.0\n330267.0\n\n\n중남미\n975.0\n616.0\n649.0\n762.0\n576.0\n544.0\n\n\n중동\n1172.0\n1018.0\n968.0\n772.0\n879.0\n951.0"
  },
  {
    "objectID": "posts/Study/boostcourse/5. K-beauty.html#연산결과를-시각적으로-보기",
    "href": "posts/Study/boostcourse/5. K-beauty.html#연산결과를-시각적으로-보기",
    "title": "6: K-beauty",
    "section": "연산결과를 시각적으로 보기",
    "text": "연산결과를 시각적으로 보기\n\n# 피봇테이블로 구한 결과를 값의 많고 적음에 따라 시각적으로 표현\n\nplt.figure(figsize=(15,4)\nsns.heatmap(result, cmap=\"Blues\", annot= True, fmt=\".0f\")\n# annot=true 숫자값 표시 \n\n&lt;AxesSubplot:xlabel='연도', ylabel='국가(대륙)별'&gt;\n\n\n\n\n\n\nsns.heatmap(result, cmap=\"Blues_r\")\n# _r 하면 위에랑 반대로.. \n\n&lt;AxesSubplot:xlabel='연도', ylabel='국가(대륙)별'&gt;"
  },
  {
    "objectID": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html",
    "href": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html",
    "title": "4: 서울 종합병원 분포 확인하기",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n\n\n\n\n# ctrl(cmd) + / : 주석처리를 풀었다 했다 \n\nimport matplotlib.pyplot as plt\n # window의 한글 폰트 설정\nplt.rc('font',family='Malgun Gothic') #윈도우의 경우\n\n# plt.rc('font', family='AppleGothic') #맥의 경우\n\nplt.rc('axes', unicode_minus=False) #마이너스 폰트 깨지는 것 대비\n\n# 그래프가 노트북 안에 보이게 하기 위해\n%matplotlib inline\n\n\nfrom IPython.display import set_matplotlib_formats\n#폰트가 선명하게 보이기 위해\n\n%config InlineBackend.figure_format = 'retina'\n\n\n\n\n\n!move \"C:\\Users\\user\\Downloads\\소상공인시장진흥공단_상가업소정보_의료기관_201909.csv\"\n\n지정된 파일을 찾을 수 없습니다.\n\n\n\ndf = pd.read_csv(\"data/소상공인시장진흥공단_상가업소정보_의료기관_201909.csv\", low_memory=False)\n# low_memory=False로 설정이 되어야 한다. 안그럼 오류남.\ndf.shape # 데이터의 행과 열 크기를 찍어볼 수 있따\n\n(91335, 39)\n\n\n\n\n\n\nhead, tail을 통해 데이터를 미리 볼수 있따\n\n\n# shift + tab 키를 누르면 docstring을 볼 수 있다\n# head: 데이터 미리보기\ndf.head(1)\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n지점명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n표준산업분류코드\n...\n건물관리번호\n건물명\n도로명주소\n구우편번호\n신우편번호\n동정보\n층정보\n호정보\n경도\n위도\n\n\n\n\n0\n19956873\n하나산부인과\nNaN\nS\n의료\nS01\n병원\nS01B10\n산부인과\nQ86201\n...\n4127310900110810000010857\n산호한양아파트\n경기도 안산시 단원구 달미로 10\n425764.0\n15236.0\nNaN\nNaN\nNaN\n126.814295\n37.336344\n\n\n\n\n1 rows × 39 columns\n\n\n\n\n# tail: 마지막 데이터 불러오기\ndf.tail(1)\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n지점명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n표준산업분류코드\n...\n건물관리번호\n건물명\n도로명주소\n구우편번호\n신우편번호\n동정보\n층정보\n호정보\n경도\n위도\n\n\n\n\n91334\n16109073\n천안김안과천안역본점의원\nNaN\nS\n의료\nS01\n병원\nS01B13\n안과의원\nQ86201\n...\n4413110700102660017016314\n김안과\n충청남도 천안시 동남구 중앙로 92\n330952.0\n31127.0\nNaN\nNaN\nNaN\n127.152651\n36.80664\n\n\n\n\n1 rows × 39 columns\n\n\n\n\n\n\n\n\n\n# 요약정보가 나타난다\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 91335 entries, 0 to 91334\nData columns (total 39 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   상가업소번호     91335 non-null  int64  \n 1   상호명        91335 non-null  object \n 2   지점명        1346 non-null   object \n 3   상권업종대분류코드  91335 non-null  object \n 4   상권업종대분류명   91335 non-null  object \n 5   상권업종중분류코드  91335 non-null  object \n 6   상권업종중분류명   91335 non-null  object \n 7   상권업종소분류코드  91335 non-null  object \n 8   상권업종소분류명   91335 non-null  object \n 9   표준산업분류코드   86413 non-null  object \n 10  표준산업분류명    86413 non-null  object \n 11  시도코드       90956 non-null  float64\n 12  시도명        90956 non-null  object \n 13  시군구코드      90956 non-null  float64\n 14  시군구명       90956 non-null  object \n 15  행정동코드      91335 non-null  int64  \n 16  행정동명       90956 non-null  object \n 17  법정동코드      91280 non-null  float64\n 18  법정동명       91280 non-null  object \n 19  지번코드       91335 non-null  int64  \n 20  대지구분코드     91335 non-null  int64  \n 21  대지구분명      91335 non-null  object \n 22  지번본번지      91335 non-null  int64  \n 23  지번부번지      72079 non-null  float64\n 24  지번주소       91335 non-null  object \n 25  도로명코드      91335 non-null  int64  \n 26  도로명        91335 non-null  object \n 27  건물본번지      91335 non-null  int64  \n 28  건물부번지      10604 non-null  float64\n 29  건물관리번호     91335 non-null  object \n 30  건물명        46453 non-null  object \n 31  도로명주소      91335 non-null  object \n 32  구우편번호      91323 non-null  float64\n 33  신우편번호      91333 non-null  float64\n 34  동정보        7406 non-null   object \n 35  층정보        44044 non-null  object \n 36  호정보        15551 non-null  object \n 37  경도         91335 non-null  float64\n 38  위도         91335 non-null  float64\ndtypes: float64(9), int64(7), object(23)\nmemory usage: 27.2+ MB\n\n\n\n# object : 문자열로 된 데이터 타입\n# int: 정수형\n# float : 실수형\n\n\n\n\n\n# 컬럼명만 추출해보기\ndf.columns\n\nIndex(['상가업소번호', '상호명', '지점명', '상권업종대분류코드', '상권업종대분류명', '상권업종중분류코드',\n       '상권업종중분류명', '상권업종소분류코드', '상권업종소분류명', '표준산업분류코드', '표준산업분류명', '시도코드',\n       '시도명', '시군구코드', '시군구명', '행정동코드', '행정동명', '법정동코드', '법정동명', '지번코드',\n       '대지구분코드', '대지구분명', '지번본번지', '지번부번지', '지번주소', '도로명코드', '도로명', '건물본번지',\n       '건물부번지', '건물관리번호', '건물명', '도로명주소', '구우편번호', '신우편번호', '동정보', '층정보',\n       '호정보', '경도', '위도'],\n      dtype='object')\n\n\n\n\n\n\n# 데이터 타입만 출력\ndf.dtypes\n\n상가업소번호         int64\n상호명           object\n지점명           object\n상권업종대분류코드     object\n상권업종대분류명      object\n상권업종중분류코드     object\n상권업종중분류명      object\n상권업종소분류코드     object\n상권업종소분류명      object\n표준산업분류코드      object\n표준산업분류명       object\n시도코드         float64\n시도명           object\n시군구코드        float64\n시군구명          object\n행정동코드          int64\n행정동명          object\n법정동코드        float64\n법정동명          object\n지번코드           int64\n대지구분코드         int64\n대지구분명         object\n지번본번지          int64\n지번부번지        float64\n지번주소          object\n도로명코드          int64\n도로명           object\n건물본번지          int64\n건물부번지        float64\n건물관리번호        object\n건물명           object\n도로명주소         object\n구우편번호        float64\n신우편번호        float64\n동정보           object\n층정보           object\n호정보           object\n경도           float64\n위도           float64\ndtype: object\n\n\n\n\n\n\n\ndf.isnull()\n# true로 표시되는 값은 null 값! \n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n지점명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n표준산업분류코드\n...\n건물관리번호\n건물명\n도로명주소\n구우편번호\n신우편번호\n동정보\n층정보\n호정보\n경도\n위도\n\n\n\n\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\n\n\n1\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\n\n\n3\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n91330\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\n\n\n91331\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\n\n\n91332\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n91333\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\n\n\n91334\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\n\n\n\n\n91335 rows × 39 columns\n\n\n\n\nnull_count = df.isnull().sum()\nnull_count\n\n상가업소번호           0\n상호명              0\n지점명          89989\n상권업종대분류코드        0\n상권업종대분류명         0\n상권업종중분류코드        0\n상권업종중분류명         0\n상권업종소분류코드        0\n상권업종소분류명         0\n표준산업분류코드      4922\n표준산업분류명       4922\n시도코드           379\n시도명            379\n시군구코드          379\n시군구명           379\n행정동코드            0\n행정동명           379\n법정동코드           55\n법정동명            55\n지번코드             0\n대지구분코드           0\n대지구분명            0\n지번본번지            0\n지번부번지        19256\n지번주소             0\n도로명코드            0\n도로명              0\n건물본번지            0\n건물부번지        80731\n건물관리번호           0\n건물명          44882\n도로명주소            0\n구우편번호           12\n신우편번호            2\n동정보          83929\n층정보          47291\n호정보          75784\n경도               0\n위도               0\ndtype: int64\n\n\n\n# 결측치를 막대그래프로 표현\nnull_count.plot() # 선그래프로 표시되는데.. 적합하지 않은 거 같아!\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nnull_count.plot.bar(rot=60)\n# rot = 글자 돌려보는것\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nnull_count.plot.barh()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nnull_count.plot.barh(figsize=(5,7)) # 그래프 사이즈 지정\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\n# 위에서 계산한 결측치 수를 reset_index 통해 데이터 프레임으로 만들기\n# df_null_coount 변수에 결과를 담아 head로 미리보기 해보기\n\ndf_null_count = null_count.reset_index()\ndf_null_count.head()\n\n\n\n\n\n\n\n\nindex\n0\n\n\n\n\n0\n상가업소번호\n0\n\n\n1\n상호명\n0\n\n\n2\n지점명\n89989\n\n\n3\n상권업종대분류코드\n0\n\n\n4\n상권업종대분류명\n0\n\n\n\n\n\n\n\n\n\n\n\n# 변수에 담겨있는 컬럼이름 변경\n\ndf_null_count.columns = [\"컬럼명\", \"결측치수\"]\ndf_null_count.head()\n\n\n\n\n\n\n\n\n컬럼명\n결측치수\n\n\n\n\n0\n상가업소번호\n0\n\n\n1\n상호명\n0\n\n\n2\n지점명\n89989\n\n\n3\n상권업종대분류코드\n0\n\n\n4\n상권업종대분류명\n0\n\n\n\n\n\n\n\n\n\n\n\n# sort_values 통해 정렬\n# 결측치가 많은 순으로 상위 10개 출력\n\ndf_null_count_top = df_null_count.sort_values(by=\"결측치수\", ascending=False).head(10)\ndf_null_count_top \n\n\n\n\n\n\n\n\n컬럼명\n결측치수\n\n\n\n\n2\n지점명\n89989\n\n\n34\n동정보\n83929\n\n\n28\n건물부번지\n80731\n\n\n36\n호정보\n75784\n\n\n35\n층정보\n47291\n\n\n30\n건물명\n44882\n\n\n23\n지번부번지\n19256\n\n\n9\n표준산업분류코드\n4922\n\n\n10\n표준산업분류명\n4922\n\n\n11\n시도코드\n379\n\n\n\n\n\n\n\n\n\n\n\n# 지점명 컬럼 불러오기\n# NaN == Not a Number의 약자로 결측치를 의미한다.\n\ndf[\"지점명\"].head()\n\n0    NaN\n1    NaN\n2    NaN\n3    NaN\n4    수지점\nName: 지점명, dtype: object\n\n\n\n# \"컬럼명\" 이라는 컬럼 값만 가져와서 drop_columns 라는 변수에 담기\n\ndrop_columns = df_null_count_top[\"컬럼명\"].tolist()    # tolist : list로 만들어줌\ndrop_columns \n\n['지점명',\n '동정보',\n '건물부번지',\n '호정보',\n '층정보',\n '건물명',\n '지번부번지',\n '표준산업분류코드',\n '표준산업분류명',\n '시도코드']\n\n\n\n# drop_columns 변수로 해당 컬럼 정보만 데이터프레임에서 가져오기\ndf[drop_columns].head()\n\n\n\n\n\n\n\n\n지점명\n동정보\n건물부번지\n호정보\n층정보\n건물명\n지번부번지\n표준산업분류코드\n표준산업분류명\n시도코드\n\n\n\n\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n산호한양아파트\nNaN\nQ86201\n일반 의원\n41.0\n\n\n1\nNaN\nNaN\nNaN\nNaN\n4\nNaN\n14.0\nQ86201\n일반 의원\n11.0\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\n한라프라자\n1.0\nQ86201\n일반 의원\n41.0\n\n\n3\nNaN\nNaN\nNaN\nNaN\n5\nNaN\n1.0\nNaN\nNaN\n26.0\n\n\n4\n수지점\nNaN\nNaN\nNaN\n1\nNaN\n2.0\nG47811\n의약품 및 의료용품 소매업\n41.0\n\n\n\n\n\n\n\n\n\n\n\nprint(df.shape)\n\ndf = df.drop(drop_columns, axis=1) \n# axis=1 컬럼기준으로 drop axis &gt; 행(0), 열 (1)\n\nprint(df.shape)\n\n(91335, 39)\n(91335, 29)\n\n\n\n# 제거 결과 info로 확인\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 91335 entries, 0 to 91334\nData columns (total 29 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   상가업소번호     91335 non-null  int64  \n 1   상호명        91335 non-null  object \n 2   상권업종대분류코드  91335 non-null  object \n 3   상권업종대분류명   91335 non-null  object \n 4   상권업종중분류코드  91335 non-null  object \n 5   상권업종중분류명   91335 non-null  object \n 6   상권업종소분류코드  91335 non-null  object \n 7   상권업종소분류명   91335 non-null  object \n 8   시도명        90956 non-null  object \n 9   시군구코드      90956 non-null  float64\n 10  시군구명       90956 non-null  object \n 11  행정동코드      91335 non-null  int64  \n 12  행정동명       90956 non-null  object \n 13  법정동코드      91280 non-null  float64\n 14  법정동명       91280 non-null  object \n 15  지번코드       91335 non-null  int64  \n 16  대지구분코드     91335 non-null  int64  \n 17  대지구분명      91335 non-null  object \n 18  지번본번지      91335 non-null  int64  \n 19  지번주소       91335 non-null  object \n 20  도로명코드      91335 non-null  int64  \n 21  도로명        91335 non-null  object \n 22  건물본번지      91335 non-null  int64  \n 23  건물관리번호     91335 non-null  object \n 24  도로명주소      91335 non-null  object \n 25  구우편번호      91323 non-null  float64\n 26  신우편번호      91333 non-null  float64\n 27  경도         91335 non-null  float64\n 28  위도         91335 non-null  float64\ndtypes: float64(6), int64(7), object(16)\nmemory usage: 20.2+ MB\n\n\n\n\n\n\n\n\n\ndf.dtypes\n\n상가업소번호         int64\n상호명           object\n상권업종대분류코드     object\n상권업종대분류명      object\n상권업종중분류코드     object\n상권업종중분류명      object\n상권업종소분류코드     object\n상권업종소분류명      object\n시도명           object\n시군구코드        float64\n시군구명          object\n행정동코드          int64\n행정동명          object\n법정동코드        float64\n법정동명          object\n지번코드           int64\n대지구분코드         int64\n대지구분명         object\n지번본번지          int64\n지번주소          object\n도로명코드          int64\n도로명           object\n건물본번지          int64\n건물관리번호        object\n도로명주소         object\n구우편번호        float64\n신우편번호        float64\n경도           float64\n위도           float64\ndtype: object\n\n\n\n#평균값\ndf[\"위도\"].mean()\n\n36.62471119236673\n\n\n\n# 중앙값\ndf[\"위도\"].median()\n\n37.2346523177033\n\n\n\n# 최댓값\ndf[\"위도\"].max()\n\n38.4996585705598\n\n\n\n# 최솟값\ndf[\"위도\"].min()\n\n33.2192896688307\n\n\n\n# 갯수\ndf[\"위도\"].count()\n\n91335\n\n\n\n\n\n데이터를 요약해서 볼 수 있음\n\n# 위도를 descibe로 요약\n\ndf[\"위도\"].describe()\n\ncount    91335.000000\nmean        36.624711\nstd          1.041361\nmin         33.219290\n25%         35.811830\n50%         37.234652\n75%         37.507463\nmax         38.499659\nName: 위도, dtype: float64\n\n\n\n# 2개의 컬럼을 describe로 요약\ndf[\"위도\", \"경도\"] \n\n# pandas에서는 리스트 형태로 가져와야한다. 위와 같이 하면 오류남\n\nKeyError: ('위도', '경도')\n\n\n\ndf[[\"위도\", \"경도\"]]\n\n\n\n\n\n\n\n\n위도\n경도\n\n\n\n\n0\n37.336344\n126.814295\n\n\n1\n37.488742\n127.053198\n\n\n2\n37.344955\n126.734841\n\n\n3\n35.166872\n129.115438\n\n\n4\n37.323528\n127.095522\n\n\n...\n...\n...\n\n\n91330\n36.352728\n127.389865\n\n\n91331\n37.627530\n126.830144\n\n\n91332\n35.227138\n129.082790\n\n\n91333\n37.540993\n127.143958\n\n\n91334\n36.806640\n127.152651\n\n\n\n\n91335 rows × 2 columns\n\n\n\n\ndf[[\"위도\", \"경도\"]].describe()\n\n\n\n\n\n\n\n\n위도\n경도\n\n\n\n\ncount\n91335.000000\n91335.000000\n\n\nmean\n36.624711\n127.487524\n\n\nstd\n1.041361\n0.842877\n\n\nmin\n33.219290\n124.717632\n\n\n25%\n35.811830\n126.914297\n\n\n50%\n37.234652\n127.084550\n\n\n75%\n37.507463\n128.108919\n\n\nmax\n38.499659\n130.909912\n\n\n\n\n\n\n\n\n# describe로 문자열 데이터타입 요약\n\ndf.describe() # 기본값은 수치형으로 되어있음!\n\n\n\n\n\n\n\n\n상가업소번호\n시군구코드\n행정동코드\n법정동코드\n지번코드\n대지구분코드\n지번본번지\n도로명코드\n건물본번지\n구우편번호\n신우편번호\n경도\n위도\n\n\n\n\ncount\n9.133500e+04\n90956.000000\n9.133500e+04\n9.128000e+04\n9.133500e+04\n91335.000000\n91335.000000\n9.133500e+04\n91335.000000\n91323.000000\n91333.00000\n91335.000000\n91335.000000\n\n\nmean\n2.121818e+07\n32898.381877\n3.293232e+09\n3.293385e+09\n3.293191e+18\n1.001336\n587.534549\n3.293207e+11\n251.200482\n428432.911085\n28085.47698\n127.487524\n36.624711\n\n\nstd\n5.042828e+06\n12985.393171\n1.297387e+09\n1.297706e+09\n1.297393e+18\n0.036524\n582.519364\n1.297391e+11\n477.456487\n193292.339066\n18909.01455\n0.842877\n1.041361\n\n\nmin\n2.901108e+06\n11110.000000\n1.111052e+09\n1.111010e+09\n1.111010e+18\n1.000000\n1.000000\n1.111020e+11\n0.000000\n100011.000000\n1000.00000\n124.717632\n33.219290\n\n\n25%\n2.001931e+07\n26350.000000\n2.635065e+09\n2.635011e+09\n2.635011e+18\n1.000000\n162.000000\n2.635042e+11\n29.000000\n302120.000000\n11681.00000\n126.914297\n35.811830\n\n\n50%\n2.211900e+07\n41117.000000\n4.111758e+09\n4.111710e+09\n4.111711e+18\n1.000000\n462.000000\n4.111743e+11\n92.000000\n440300.000000\n24353.00000\n127.084550\n37.234652\n\n\n75%\n2.480984e+07\n43113.000000\n4.311370e+09\n4.311311e+09\n4.311311e+18\n1.000000\n858.000000\n4.311332e+11\n257.000000\n602811.000000\n46044.00000\n128.108919\n37.507463\n\n\nmax\n2.852470e+07\n50130.000000\n5.013061e+09\n5.013032e+09\n5.013061e+18\n2.000000\n7338.000000\n5.013049e+11\n8795.000000\n799801.000000\n63643.00000\n130.909912\n38.499659\n\n\n\n\n\n\n\n\ndf.describe(include=\"object\")\n\n# top : 가장 많이 나타난걸 보여줌\n# freq : frequency : 빈도수.. 리원이라는 상호명이 152번 등장한다.\n# 결측치는 제외하고 보여줌! \n\n\n\n\n\n\n\n\n상호명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n시도명\n시군구명\n행정동명\n법정동명\n대지구분명\n지번주소\n도로명\n건물관리번호\n도로명주소\n\n\n\n\ncount\n91335\n91335\n91335\n91335\n91335\n91335\n91335\n90956\n90956\n90956\n91280\n91335\n91335\n91335\n91335\n91335\n\n\nunique\n56910\n1\n1\n5\n5\n34\n34\n17\n228\n2791\n2822\n2\n53118\n16610\n54142\n54031\n\n\ntop\n리원\nS\n의료\nS01\n병원\nS02A01\n약국\n경기도\n서구\n중앙동\n중동\n대지\n서울특별시 동대문구 제기동 965-1\n서울특별시 강남구 강남대로\n1123010300109650001031604\n서울특별시 동대문구 약령중앙로8길 10\n\n\nfreq\n152\n91335\n91335\n60774\n60774\n18964\n18964\n21374\n3165\n1856\n874\n91213\n198\n326\n198\n198\n\n\n\n\n\n\n\n\n# 모든 데이터 요약\ndf.describe(include=\"all\")\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n시도명\n시군구코드\n...\n지번주소\n도로명코드\n도로명\n건물본번지\n건물관리번호\n도로명주소\n구우편번호\n신우편번호\n경도\n위도\n\n\n\n\ncount\n9.133500e+04\n91335\n91335\n91335\n91335\n91335\n91335\n91335\n90956\n90956.000000\n...\n91335\n9.133500e+04\n91335\n91335.000000\n91335\n91335\n91323.000000\n91333.00000\n91335.000000\n91335.000000\n\n\nunique\nNaN\n56910\n1\n1\n5\n5\n34\n34\n17\nNaN\n...\n53118\nNaN\n16610\nNaN\n54142\n54031\nNaN\nNaN\nNaN\nNaN\n\n\ntop\nNaN\n리원\nS\n의료\nS01\n병원\nS02A01\n약국\n경기도\nNaN\n...\n서울특별시 동대문구 제기동 965-1\nNaN\n서울특별시 강남구 강남대로\nNaN\n1123010300109650001031604\n서울특별시 동대문구 약령중앙로8길 10\nNaN\nNaN\nNaN\nNaN\n\n\nfreq\nNaN\n152\n91335\n91335\n60774\n60774\n18964\n18964\n21374\nNaN\n...\n198\nNaN\n326\nNaN\n198\n198\nNaN\nNaN\nNaN\nNaN\n\n\nmean\n2.121818e+07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n32898.381877\n...\nNaN\n3.293207e+11\nNaN\n251.200482\nNaN\nNaN\n428432.911085\n28085.47698\n127.487524\n36.624711\n\n\nstd\n5.042828e+06\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n12985.393171\n...\nNaN\n1.297391e+11\nNaN\n477.456487\nNaN\nNaN\n193292.339066\n18909.01455\n0.842877\n1.041361\n\n\nmin\n2.901108e+06\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n11110.000000\n...\nNaN\n1.111020e+11\nNaN\n0.000000\nNaN\nNaN\n100011.000000\n1000.00000\n124.717632\n33.219290\n\n\n25%\n2.001931e+07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n26350.000000\n...\nNaN\n2.635042e+11\nNaN\n29.000000\nNaN\nNaN\n302120.000000\n11681.00000\n126.914297\n35.811830\n\n\n50%\n2.211900e+07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n41117.000000\n...\nNaN\n4.111743e+11\nNaN\n92.000000\nNaN\nNaN\n440300.000000\n24353.00000\n127.084550\n37.234652\n\n\n75%\n2.480984e+07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n43113.000000\n...\nNaN\n4.311332e+11\nNaN\n257.000000\nNaN\nNaN\n602811.000000\n46044.00000\n128.108919\n37.507463\n\n\nmax\n2.852470e+07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n50130.000000\n...\nNaN\n5.013049e+11\nNaN\n8795.000000\nNaN\nNaN\n799801.000000\n63643.00000\n130.909912\n38.499659\n\n\n\n\n11 rows × 29 columns\n\n\n\n\n\n\n\nunique로 중복 제거 nuique 갯수 세기\n\n\n# 상권업종대분류명\n\ndf[\"상권업종대분류명\"].unique()\n\narray(['의료'], dtype=object)\n\n\n\ndf[\"상권업종대분류명\"].nunique()\n\n1\n\n\n\n# 상권업종중분류명\ndf[\"상권업종중분류명\"].unique()\n\narray(['병원', '약국/한약방', '수의업', '유사의료업', '의료관련서비스업'], dtype=object)\n\n\n\ndf[\"상권업종중분류명\"].nunique()\n\n5\n\n\n\n# 상권업종소분류명\ndf[\"상권업종소분류명\"].unique()\n\narray(['산부인과', '내과/외과', '신경외과', '기타병원', '약국', '동물병원', '한약방', '탕제원',\n       '정형/성형외과', '소아과', '이비인후과의원', '노인/치매병원', '언어치료', '수의업-종합', '한의원',\n       '치과의원', '침구원', '일반병원', '안과의원', '조산원', '한방병원', '종합병원', '유사의료업기타',\n       '응급구조대', '혈액원', '치과병원', '척추교정치료', '피부과', '비뇨기과', '치과기공소', '산후조리원',\n       '접골원', '수의업-기타', '제대혈'], dtype=object)\n\n\n\ndf[\"상권업종소분류명\"].nunique()\n\n34\n\n\n\nlen(df[\"상권업종소분류명\"].unique())\n\n34\n\n\n\n\n\n\n카테고리 형태의 데이터 갯수를 셀 수 있다.\n\n\n# 시도코드 세어보기 -&gt; 결측치...\ndf[\"시도명\"].head()\n\n0      경기도\n1    서울특별시\n2      경기도\n3    부산광역시\n4      경기도\nName: 시도명, dtype: object\n\n\n\n# 시도명 세보면\ncity = df[\"시도명\"].value_counts()\ncity\n\n경기도        21374\n서울특별시      18943\n부산광역시       6473\n경상남도        4973\n인천광역시       4722\n대구광역시       4597\n경상북도        4141\n전라북도        3894\n충청남도        3578\n전라남도        3224\n광주광역시       3214\n대전광역시       3067\n충청북도        2677\n강원도         2634\n울산광역시       1997\n제주특별자치도     1095\n세종특별자치시      353\nName: 시도명, dtype: int64\n\n\n\n# normalize=True 옵션 사용시 비율을 구할 수 있다.\n\ncity_normalize= df[\"시도명\"].value_counts(normalize=True)\ncity_normalize\n\n경기도        0.234993\n서울특별시      0.208266\n부산광역시      0.071166\n경상남도       0.054675\n인천광역시      0.051915\n대구광역시      0.050541\n경상북도       0.045528\n전라북도       0.042812\n충청남도       0.039338\n전라남도       0.035446\n광주광역시      0.035336\n대전광역시      0.033720\n충청북도       0.029432\n강원도        0.028959\n울산광역시      0.021956\n제주특별자치도    0.012039\n세종특별자치시    0.003881\nName: 시도명, dtype: float64\n\n\n\ncity.plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\n # 막대그래프 표현\ncity.plot.barh()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\n# plto.pie() 사용하여 파이그래프 그리기\ncity_normalize.plot.pie(figsize=(7,7))\n\n&lt;AxesSubplot:ylabel='시도명'&gt;\n\n\n\n\n\n\n# seaborn의 countplot 그리기\nsns.countplot(data=df, y=\"시도명\" )\n\n&lt;AxesSubplot:xlabel='count', ylabel='시도명'&gt;\n\n\n\n\n\n\nc=sns.countplot(data=df, y=\"시도명\" ) \n# 변수명에 담아주면 밑에 글씨가 없어진당 \n\n\n\n\n\ndf[\"상권업종대분류명\"].value_counts()\n\n의료    91335\nName: 상권업종대분류명, dtype: int64\n\n\n\nd= df[\"상권업종중분류명\"].value_counts()\nd\n\n병원          60774\n약국/한약방      20923\n수의업          5323\n유사의료업        3774\n의료관련서비스업      541\nName: 상권업종중분류명, dtype: int64\n\n\n\nn =df[\"상권업종중분류명\"].value_counts(normalize=True)\nn\n\n병원          0.665397\n약국/한약방      0.229080\n수의업         0.058280\n유사의료업       0.041320\n의료관련서비스업    0.005923\nName: 상권업종중분류명, dtype: float64\n\n\n\nd.plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nd.plot.bar()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nd.plot.bar(rot=0) # x축 레이블 값 회전\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nn.plot.pie()\n\n&lt;AxesSubplot:ylabel='상권업종중분류명'&gt;\n\n\n\n\n\n\nc = df[\"상권업종소분류명\"].value_counts()\nc\n\n약국         18964\n치과의원       13731\n한의원        13211\n내과/외과      11374\n기타병원        4922\n일반병원        3385\n동물병원        3098\n정형/성형외과     2562\n소아과         2472\n수의업-종합      2216\n치과기공소       1724\n이비인후과의원     1486\n한약방         1442\n피부과         1273\n산부인과        1116\n노인/치매병원     1055\n안과의원        1042\n비뇨기과         809\n종합병원         762\n치과병원         756\n언어치료         664\n유사의료업기타      629\n탕제원          517\n산후조리원        511\n신경외과         421\n한방병원         397\n척추교정치료       338\n침구원          154\n혈액원          130\n응급구조대        125\n조산원           30\n접골원            9\n수의업-기타         9\n제대혈            1\nName: 상권업종소분류명, dtype: int64\n\n\n\nc.plot.barh(figsize=(7, 8), grid= True) #gird: 격자 표시\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\n\n\n\n\n# 상권업종분류명이 약국/한약방인 데이터만 가져와서\n# df_medical이라는 변수에 담고\n# head()통해 미리보기\n\ndf_medical = df[df[\"상권업종중분류명\"] == \"약국/한약방\"].copy()\ndf_medical.head(1)\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n시도명\n시군구코드\n...\n지번주소\n도로명코드\n도로명\n건물본번지\n건물관리번호\n도로명주소\n구우편번호\n신우편번호\n경도\n위도\n\n\n\n\n4\n20364049\n더블유스토어수지점\nS\n의료\nS02\n약국/한약방\nS02A01\n약국\n경기도\n41465.0\n...\n경기도 용인시 수지구 풍덕천동 712-2\n414653205024\n경기도 용인시 수지구 문정로\n32\n4146510100107120002026238\n경기도 용인시 수지구 문정로 32\n448170.0\n16837.0\n127.095522\n37.323528\n\n\n\n\n1 rows × 29 columns\n\n\n\n\n# 상권업종대분류명이 의료만 가져오기\n# df.loc 사용하면 행, 열을 함께 가져온다.\n# 이 기능을 통해 상권업종중뷴려망만 가져온다\n# 가져온 결과를 value_counts를 통해 중분류의 갯수를 세본다.\n\ndf.loc[df[\"상권업종대분류명\"] == \"의료\", \"상권업종중분류명\"].value_counts()\n\n병원          60774\n약국/한약방      20923\n수의업          5323\n유사의료업        3774\n의료관련서비스업      541\nName: 상권업종중분류명, dtype: int64\n\n\n\n# 위와 같은 기능을 수행하는 코드\n# df.loc[df[\"상권업종대분류명\"] == \"의료\"][\"상권업종중분류명\"] 근데 이건 느리다!! \n\n\nm = df[\"상권업종대분류명\"] == \"의료\"\ndf.loc[m, \"상권업종중분류명\"].value_counts()\n\n병원          60774\n약국/한약방      20923\n수의업          5323\n유사의료업        3774\n의료관련서비스업      541\nName: 상권업종중분류명, dtype: int64\n\n\n\n# 유사의료업\ndf[df[\"상권업종중분류명\"] == \"유사의료업\"]\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n시도명\n시군구코드\n...\n지번주소\n도로명코드\n도로명\n건물본번지\n건물관리번호\n도로명주소\n구우편번호\n신우편번호\n경도\n위도\n\n\n\n\n22\n21013731\n세종언어치료센터\nS\n의료\nS03\n유사의료업\nS03B07\n언어치료\n부산광역시\n26410.0\n...\n부산광역시 금정구 구서동 84-1\n264102000010\n부산광역시 금정구 중앙대로\n1817\n2641010700100840001017686\n부산광역시 금정구 중앙대로 1817-11\n609310.0\n46273.0\n129.091662\n35.246528\n\n\n40\n20933900\n고려수지침학회\nS\n의료\nS03\n유사의료업\nS03B03\n침구원\n경상남도\n48123.0\n...\n경상남도 창원시 성산구 상남동 5-2\n481234784088\n경상남도 창원시 성산구 마디미로4번길\n9\n4812312700100050002026799\n경상남도 창원시 성산구 마디미로4번길 9\n642832.0\n51495.0\n128.684678\n35.224113\n\n\n97\n21717820\n청명원\nS\n의료\nS03\n유사의료업\nS03B09\n유사의료업기타\n충청북도\n43760.0\n...\n충청북도 괴산군 청안면 금신리 241\n437604538132\n충청북도 괴산군 청안면 금신로1길\n93\n4376037022102410000007293\n충청북도 괴산군 청안면 금신로1길 93\n367831.0\n28050.0\n127.635740\n36.768935\n\n\n102\n21865854\n응급환자이송센터\nS\n의료\nS03\n유사의료업\nS03B01\n응급구조대\n대전광역시\n30140.0\n...\n대전광역시 중구 대사동 248-237\n301404295026\n대전광역시 중구 계룡로921번길\n40\n3014011000102480237013097\n대전광역시 중구 계룡로921번길 40\n301846.0\n34946.0\n127.417693\n36.321801\n\n\n108\n21914637\n태화아동발달지원센터\nS\n의료\nS03\n유사의료업\nS03B07\n언어치료\n대전광역시\n30140.0\n...\n대전광역시 중구 문화동 27\n301404295402\n대전광역시 중구 보문산로333번길\n29\n3014011600100270000008172\n대전광역시 중구 보문산로333번길 29\n301130.0\n35020.0\n127.412725\n36.312953\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n91300\n16131218\n으뜸치과기공소\nS\n의료\nS03\n유사의료업\nS03B06\n치과기공소\n경상남도\n48170.0\n...\n경상남도 진주시 수정동 39-11\n481704797625\n경상남도 진주시 향교로18번길\n8\n4817011600100390011004490\n경상남도 진주시 향교로18번길 8\n660180.0\n52753.0\n128.084600\n35.197029\n\n\n91310\n16199325\n보령치과기공소\nS\n의료\nS03\n유사의료업\nS03B06\n치과기공소\n서울특별시\n11290.0\n...\n서울특별시 성북구 동소문동4가 103-11\n112903107003\n서울특별시 성북구 동소문로\n47\n1129010700101030014050661\n서울특별시 성북구 동소문로 47-15\n136821.0\n2832.0\n127.010602\n37.591455\n\n\n91311\n16199088\n점프셈교실\nS\n의료\nS03\n유사의료업\nS03B09\n유사의료업기타\n경상북도\n47130.0\n...\n경상북도 경주시 황성동 446\n471304715895\n경상북도 경주시 용담로104번길\n16\n4713012400104460000024894\n경상북도 경주시 용담로104번길 16\n780954.0\n38084.0\n129.211755\n35.865600\n\n\n91319\n16108560\n씨앤디자인치과기공소\nS\n의료\nS03\n유사의료업\nS03B06\n치과기공소\n서울특별시\n11545.0\n...\n서울특별시 금천구 가산동 60-25\n115453116013\n서울특별시 금천구 벚꽃로\n234\n1154510100100600025000001\n서울특별시 금천구 벚꽃로 234\n153798.0\n8513.0\n126.886122\n37.475986\n\n\n91327\n16190388\n오피스알파\nS\n의료\nS03\n유사의료업\nS03B06\n치과기공소\n경기도\n41173.0\n...\n경기도 안양시 동안구 호계동 970-24\n411734349013\n경기도 안양시 동안구 경수대로507번길\n28\n4117310400109700024005182\n경기도 안양시 동안구 경수대로507번길 28\n431849.0\n14120.0\n126.956365\n37.367779\n\n\n\n\n3774 rows × 29 columns\n\n\n\n\n\ndf[df[\"상권업종중분류명\"] == \"유사의료업\"].shape\n\n(3774, 29)\n\n\n\n# 상호명 그룹화해서 갯수\n# value_counts를 사용해 상위 10개 출력\n\ndf[\"상호명\"].value_counts().head(10)\n\n리원       152\n온누리약국    149\n경희한의원    141\n우리약국     119\n중앙약국     111\n전자담배      98\n조은약국      95\n건강약국      87\n제일약국      79\n사랑약국      73\nName: 상호명, dtype: int64\n\n\n\n\ndf[\"상호명\"].value_counts().tail()\n\n메리디언치과          1\n이엘피부과성형외과       1\n금오중국한의원         1\n오케이연합의원         1\n천안김안과천안역본점의원    1\nName: 상호명, dtype: int64\n\n\n\n\ndf_medi = df[df[\"상권업종중분류명\"] == \"유사의료업\"]\ndf_medi\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n시도명\n시군구코드\n...\n지번주소\n도로명코드\n도로명\n건물본번지\n건물관리번호\n도로명주소\n구우편번호\n신우편번호\n경도\n위도\n\n\n\n\n22\n21013731\n세종언어치료센터\nS\n의료\nS03\n유사의료업\nS03B07\n언어치료\n부산광역시\n26410.0\n...\n부산광역시 금정구 구서동 84-1\n264102000010\n부산광역시 금정구 중앙대로\n1817\n2641010700100840001017686\n부산광역시 금정구 중앙대로 1817-11\n609310.0\n46273.0\n129.091662\n35.246528\n\n\n40\n20933900\n고려수지침학회\nS\n의료\nS03\n유사의료업\nS03B03\n침구원\n경상남도\n48123.0\n...\n경상남도 창원시 성산구 상남동 5-2\n481234784088\n경상남도 창원시 성산구 마디미로4번길\n9\n4812312700100050002026799\n경상남도 창원시 성산구 마디미로4번길 9\n642832.0\n51495.0\n128.684678\n35.224113\n\n\n97\n21717820\n청명원\nS\n의료\nS03\n유사의료업\nS03B09\n유사의료업기타\n충청북도\n43760.0\n...\n충청북도 괴산군 청안면 금신리 241\n437604538132\n충청북도 괴산군 청안면 금신로1길\n93\n4376037022102410000007293\n충청북도 괴산군 청안면 금신로1길 93\n367831.0\n28050.0\n127.635740\n36.768935\n\n\n102\n21865854\n응급환자이송센터\nS\n의료\nS03\n유사의료업\nS03B01\n응급구조대\n대전광역시\n30140.0\n...\n대전광역시 중구 대사동 248-237\n301404295026\n대전광역시 중구 계룡로921번길\n40\n3014011000102480237013097\n대전광역시 중구 계룡로921번길 40\n301846.0\n34946.0\n127.417693\n36.321801\n\n\n108\n21914637\n태화아동발달지원센터\nS\n의료\nS03\n유사의료업\nS03B07\n언어치료\n대전광역시\n30140.0\n...\n대전광역시 중구 문화동 27\n301404295402\n대전광역시 중구 보문산로333번길\n29\n3014011600100270000008172\n대전광역시 중구 보문산로333번길 29\n301130.0\n35020.0\n127.412725\n36.312953\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n91300\n16131218\n으뜸치과기공소\nS\n의료\nS03\n유사의료업\nS03B06\n치과기공소\n경상남도\n48170.0\n...\n경상남도 진주시 수정동 39-11\n481704797625\n경상남도 진주시 향교로18번길\n8\n4817011600100390011004490\n경상남도 진주시 향교로18번길 8\n660180.0\n52753.0\n128.084600\n35.197029\n\n\n91310\n16199325\n보령치과기공소\nS\n의료\nS03\n유사의료업\nS03B06\n치과기공소\n서울특별시\n11290.0\n...\n서울특별시 성북구 동소문동4가 103-11\n112903107003\n서울특별시 성북구 동소문로\n47\n1129010700101030014050661\n서울특별시 성북구 동소문로 47-15\n136821.0\n2832.0\n127.010602\n37.591455\n\n\n91311\n16199088\n점프셈교실\nS\n의료\nS03\n유사의료업\nS03B09\n유사의료업기타\n경상북도\n47130.0\n...\n경상북도 경주시 황성동 446\n471304715895\n경상북도 경주시 용담로104번길\n16\n4713012400104460000024894\n경상북도 경주시 용담로104번길 16\n780954.0\n38084.0\n129.211755\n35.865600\n\n\n91319\n16108560\n씨앤디자인치과기공소\nS\n의료\nS03\n유사의료업\nS03B06\n치과기공소\n서울특별시\n11545.0\n...\n서울특별시 금천구 가산동 60-25\n115453116013\n서울특별시 금천구 벚꽃로\n234\n1154510100100600025000001\n서울특별시 금천구 벚꽃로 234\n153798.0\n8513.0\n126.886122\n37.475986\n\n\n91327\n16190388\n오피스알파\nS\n의료\nS03\n유사의료업\nS03B06\n치과기공소\n경기도\n41173.0\n...\n경기도 안양시 동안구 호계동 970-24\n411734349013\n경기도 안양시 동안구 경수대로507번길\n28\n4117310400109700024005182\n경기도 안양시 동안구 경수대로507번길 28\n431849.0\n14120.0\n126.956365\n37.367779\n\n\n\n\n3774 rows × 29 columns\n\n\n\n\ndf_medi[\"상호명\"].value_counts().head(10)\n\n리원          32\n고려수지침       22\n대한적십자사      17\n헌혈의집        12\n고려수지침학회     10\n수치과기공소      10\n제일치과기공소      9\n미소치과기공소      8\n아트치과기공소      8\n이사랑치과기공소     8\nName: 상호명, dtype: int64\n\n\n\n\n\n# 상권업종소분류명이 약국이고 시도명이 서울특별시인 데이터\ndf[\"상권업종소분류명\"] == \"약국\"  and df[\"시도명\"] == \"서울특별시\"\n# 오류! 판다스에서는 & 써야함. \n\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\n\n\n\ndf[\"상권업종소분류명\"] == \"약국\"  & df[\"시도명\"] == \"서울특별시\"\n# 오류! 연산자 우선순위 때문에 오류가 났다.\n\nTypeError: Cannot perform 'rand_' with a dtyped [object] array and scalar of type [bool]\n\n\n\n\ndf_seoul_drug = df[(df[\"상권업종소분류명\"] == \"약국\")  & (df[\"시도명\"] == \"서울특별시\")]\nprint(df_seoul_drug.shape)\ndf_seoul_drug.head(1)\n\n(3579, 29)\n\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n시도명\n시군구코드\n...\n지번주소\n도로명코드\n도로명\n건물본번지\n건물관리번호\n도로명주소\n구우편번호\n신우편번호\n경도\n위도\n\n\n\n\n33\n20816709\n이즈타워약\nS\n의료\nS02\n약국/한약방\nS02A01\n약국\n서울특별시\n11680.0\n...\n서울특별시 강남구 역삼동 821\n116803122010\n서울특별시 강남구 테헤란로\n101\n1168010100108210001000001\n서울특별시 강남구 테헤란로 101\n135080.0\n6134.0\n127.028023\n37.498656\n\n\n\n\n1 rows × 29 columns\n\n\n\n\n\n\n\n# 시군구명으로 그룹화해서 갯수 세어보기\n# 구별로 약국이 몇개가 있는지 확인\nc = df_seoul_drug[\"시군구명\"].value_counts()\nc.head()\n\n강남구     374\n동대문구    261\n광진구     212\n서초구     191\n송파구     188\nName: 시군구명, dtype: int64\n\n\n\nn = df_seoul_drug[\"시군구명\"].value_counts(normalize=True)\nn.head()\n\n강남구     0.104498\n동대문구    0.072925\n광진구     0.059234\n서초구     0.053367\n송파구     0.052529\nName: 시군구명, dtype: float64\n\n\n\nc.plot.bar(rot=60) #rot:글씨를 기울인다\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\n# 상권업종소분류명이 종합병원\n# 시도명이 서울특별시인 데이터\n\ndf_seoul_hospital = df[(df[\"상권업종소분류명\"] == \"종합병원\") & (df[\"시도명\"] == \"서울특별시\")].copy()\n# copy를 해줘야 df_soeul_hospital 이 바뀐다. df까지 바뀌지 않는다. \ndf_seoul_hospital\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n시도명\n시군구코드\n...\n지번주소\n도로명코드\n도로명\n건물본번지\n건물관리번호\n도로명주소\n구우편번호\n신우편번호\n경도\n위도\n\n\n\n\n305\n25155642\n대진의료재단\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11215.0\n...\n서울특별시 광진구 중곡동 58-25\n112153104006\n서울특별시 광진구 긴고랑로\n119\n1121510100100580025000733\n서울특별시 광진구 긴고랑로 119\n143220.0\n4944.0\n127.088279\n37.559048\n\n\n353\n20471487\n홍익병원별관\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11470.0\n...\n서울특별시 양천구 신정동 897-13\n114702005008\n서울특별시 양천구 국회대로\n250\n1147010100108970013001044\n서울특별시 양천구 국회대로 250\n158070.0\n7937.0\n126.862805\n37.529213\n\n\n385\n20737057\nSNUH\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11680.0\n...\n서울특별시 강남구 역삼동 736-55\n116804166727\n서울특별시 강남구 테헤란로26길\n10\n1168010100107360055027688\n서울특별시 강남구 테헤란로26길 10\n135080.0\n6236.0\n127.035825\n37.499630\n\n\n1917\n23210677\n평화드림여의도성모병원의료기매장\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11560.0\n...\n서울특별시 영등포구 여의도동 62\n115603118001\n서울특별시 영등포구 63로\n10\n1156011000100620000031477\n서울특별시 영등포구 63로 10\n150713.0\n7345.0\n126.936693\n37.518296\n\n\n2461\n20024045\n한양\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11200.0\n...\n서울특별시 성동구 행당동 15-1\n112003103002\n서울특별시 성동구 마조로\n22\n1120010700100150001019623\n서울특별시 성동구 마조로 22-2\n133070.0\n4763.0\n127.041325\n37.559469\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n71991\n28505952\n서울성모병원응급의료센터\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11650.0\n...\n서울특별시 서초구 반포동 505\n116502121003\n서울특별시 서초구 반포대로\n222\n1165010700101230000017226\n서울특별시 서초구 반포대로 222\n137701.0\n6591.0\n127.005841\n37.502382\n\n\n76508\n12292992\n라마르의원\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11740.0\n...\n서울특별시 강동구 천호동 453-8\n117404172367\n서울특별시 강동구 천호대로157길\n18\n1174010900104530021010314\n서울특별시 강동구 천호대로157길 18\n134864.0\n5335.0\n127.127466\n37.538485\n\n\n90492\n16031909\n가톨릭대학교여의도성모병원\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11140.0\n...\n서울특별시 중구 명동2가 1-1\n111404103165\n서울특별시 중구 명동길\n74\n1114012700100010001019574\n서울특별시 중구 명동길 74\n100809.0\n4537.0\n126.986758\n37.563662\n\n\n90581\n16332576\n씨엠병원\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11560.0\n...\n서울특별시 영등포구 영등포동4가 90\n115604154717\n서울특별시 영등포구 영등포로36길\n13\n1156010500100900000035097\n서울특별시 영등포구 영등포로36길 13\n150030.0\n7301.0\n126.903857\n37.518807\n\n\n90788\n16162338\n성베드로병원\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11680.0\n...\n서울특별시 강남구 도곡동 910-27\n116802000003\n서울특별시 강남구 남부순환로\n2649\n1168011800109100027000895\n서울특별시 강남구 남부순환로 2649\n135859.0\n6271.0\n127.039567\n37.485604\n\n\n\n\n91 rows × 29 columns\n\n\n\n\ndf_seoul_hospital[\"시군구명\"].value_counts()\n\n강남구     15\n영등포구     8\n광진구      6\n서초구      6\n강동구      5\n중구       5\n송파구      5\n강북구      4\n도봉구      4\n서대문구     4\n양천구      4\n성북구      3\n강서구      2\n중랑구      2\n종로구      2\n동대문구     2\n구로구      2\n노원구      2\n금천구      2\n성동구      2\n관악구      2\n동작구      1\n마포구      1\n용산구      1\n은평구      1\nName: 시군구명, dtype: int64\n\n\n\n\n\n\n# 색인 전 상호명 중에 종합병원이 아닌 데이터 찾기\ndf_seoul_hospital.loc[~df_seoul_hospital[\"상호명\"].str.contains(\"종합병원\"),\"상호명\"].unique()\n\n# str.contains하면 특정 값만 찾을 수 있다. \n# 앞에 물결 표시를 하게 되면 종합병원이 안들어간 것만 찾을 수 있다. \n\narray(['대진의료재단', '홍익병원별관', 'SNUH', '평화드림여의도성모병원의료기매장', '한양', '백산의료재단친구병원',\n       '서울보훈병원', '서울성모병원장례식장꽃배달', '서울대학교병원', '알콜중독및정신질환상담소',\n       '강남성모병원장례식장꽃배달', '제일병원', '이랜드클리닉', '사랑나눔의료재단', '우울증센터', '성심의료재단',\n       '다나의료재단', '서울아산병원신관', '원자력병원장례식장', '국민의원', '고려대학교구로병원', '학교법인일송학원',\n       '삼성의료원장례식장', '희명스포츠의학센터인공신장실', '연세대학교의과대학강남세브란스', '국립정신병원',\n       '코아클리닉', '수서제일의원', '사랑의의원', '한국전력공사부속한일병원', '신촌연세병원', '창동제일의원',\n       '영동세브란스병원', '제일성심의원', '삼성의료재단강북삼성태', '서울시립보라매병원', '서울이의원',\n       '서울대학교병원비상계획외래', '평화드림서울성모병원의료', '홍익병원', '사랑나눔의료재단서', '독일의원',\n       '서울연합의원', '우신향병원', '동부제일병원', '아산재단금강병원', '명곡안연구소', '아산재단서울중앙병원',\n       '메디힐특수여객', '삼성생명공익재단삼성서', '성광의료재단차병원', '한국건강관리협회서울특',\n       '정해복지부설한신메디피아', '성베드로병원', '성애의료재단', '실로암의원', 'Y&T성모마취과', '광진성모의원',\n       '서울현대의원', '이노신경과의원', '송정훼밀리의원', '서울중앙의원', '영남의료재단', '인제대학교서울백병원',\n       '한국필의료재단', '세브란스의원', '가톨릭대학교성바오로병원장례식장', '서울연세의원', '사랑의병원',\n       '성삼의료재단미즈메디병원', '씨엠충무병원', '성신의원', '원진재단부설녹색병원', '송파제일의원',\n       '카톨릭성모의원', '한양성심의원', '관악성모의원', '강남센트럴병원', '우이한솔의원', '우리들병원',\n       '서울성모병원어린이집', '건국대학교병원', '서울적십자병원', '북부성모의원', '한림대학교부속한강성심병원장례식장',\n       '서울성모병원응급의료센터', '라마르의원', '가톨릭대학교여의도성모병원', '씨엠병원'], dtype=object)\n\n\n\ndf_seoul_hospital[df_seoul_hospital[\"상호명\"].str.contains(\"꽃배달\")]\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n시도명\n시군구코드\n...\n지번주소\n도로명코드\n도로명\n건물본번지\n건물관리번호\n도로명주소\n구우편번호\n신우편번호\n경도\n위도\n\n\n\n\n2803\n20895655\n서울성모병원장례식장꽃배달\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11650.0\n...\n서울특별시 서초구 반포동 551\n116504163330\n서울특별시 서초구 사평대로28길\n55\n1165010700105510000017194\n서울특별시 서초구 사평대로28길 55\n137040.0\n6578.0\n127.000682\n37.498257\n\n\n4644\n22020310\n강남성모병원장례식장꽃배달\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11650.0\n...\n서울특별시 서초구 반포동 547-6\n116504163242\n서울특별시 서초구 반포대로39길\n56\n1165010700105470006016762\n서울특별시 서초구 반포대로39길 56-24\n137040.0\n6578.0\n127.001756\n37.499095\n\n\n\n\n2 rows × 29 columns\n\n\n\n\ndf_seoul_hospital[df_seoul_hospital[\"상호명\"].str.contains(\"의료기\")]\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n시도명\n시군구코드\n...\n지번주소\n도로명코드\n도로명\n건물본번지\n건물관리번호\n도로명주소\n구우편번호\n신우편번호\n경도\n위도\n\n\n\n\n1917\n23210677\n평화드림여의도성모병원의료기매장\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11560.0\n...\n서울특별시 영등포구 여의도동 62\n115603118001\n서울특별시 영등포구 63로\n10\n1156011000100620000031477\n서울특별시 영등포구 63로 10\n150713.0\n7345.0\n126.936693\n37.518296\n\n\n\n\n1 rows × 29 columns\n\n\n\n\ndf_seoul_hospital[df_seoul_hospital[\"상호명\"].str.contains(\"꽃배달|의료기|장례식장|상담소|어린이집\")]\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n시도명\n시군구코드\n...\n지번주소\n도로명코드\n도로명\n건물본번지\n건물관리번호\n도로명주소\n구우편번호\n신우편번호\n경도\n위도\n\n\n\n\n1917\n23210677\n평화드림여의도성모병원의료기매장\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11560.0\n...\n서울특별시 영등포구 여의도동 62\n115603118001\n서울특별시 영등포구 63로\n10\n1156011000100620000031477\n서울특별시 영등포구 63로 10\n150713.0\n7345.0\n126.936693\n37.518296\n\n\n2803\n20895655\n서울성모병원장례식장꽃배달\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11650.0\n...\n서울특별시 서초구 반포동 551\n116504163330\n서울특별시 서초구 사평대로28길\n55\n1165010700105510000017194\n서울특별시 서초구 사평대로28길 55\n137040.0\n6578.0\n127.000682\n37.498257\n\n\n4431\n21781516\n알콜중독및정신질환상담소\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11320.0\n...\n서울특별시 도봉구 창동 181-52\n113204127202\n서울특별시 도봉구 마들로13길\n153\n1132010700101810052014414\n서울특별시 도봉구 마들로13길 153\n132040.0\n1411.0\n127.046203\n37.657046\n\n\n4644\n22020310\n강남성모병원장례식장꽃배달\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11650.0\n...\n서울특별시 서초구 반포동 547-6\n116504163242\n서울특별시 서초구 반포대로39길\n56\n1165010700105470006016762\n서울특별시 서초구 반포대로39길 56-24\n137040.0\n6578.0\n127.001756\n37.499095\n\n\n7938\n20625484\n원자력병원장례식장\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11350.0\n...\n서울특별시 노원구 공릉동 215-4\n113503110002\n서울특별시 노원구 노원로\n75\n1135010300102150004014400\n서울특별시 노원구 노원로 75\n139706.0\n1812.0\n127.082670\n37.628808\n\n\n10283\n20024377\n삼성의료원장례식장\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11680.0\n...\n서울특별시 강남구 일원동 50\n116803122009\n서울특별시 강남구 일원로\n81\n1168011400100500000002609\n서울특별시 강남구 일원로 81\n135710.0\n6351.0\n127.089579\n37.490334\n\n\n47008\n21738670\n가톨릭대학교성바오로병원장례식장\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11230.0\n...\n서울특별시 동대문구 전농동 620-56\n112303105008\n서울특별시 동대문구 왕산로\n180\n1123010400106200056027814\n서울특별시 동대문구 왕산로 180\n130709.0\n2559.0\n127.043471\n37.579246\n\n\n60645\n27670796\n서울성모병원어린이집\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11650.0\n...\n서울특별시 서초구 반포동 505\n116502121003\n서울특별시 서초구 반포대로\n222\n1165010700101230000017226\n서울특별시 서초구 반포대로 222\n137701.0\n6591.0\n127.005841\n37.502382\n\n\n70177\n11537223\n한림대학교부속한강성심병원장례식장\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11560.0\n...\n서울특별시 영등포구 영등포동7가 94-200\n115604154428\n서울특별시 영등포구 버드나루로7길\n12\n1156010800100940200033663\n서울특별시 영등포구 버드나루로7길 12\n150030.0\n7247.0\n126.909676\n37.523168\n\n\n\n\n9 rows × 29 columns\n\n\n\n\ndf_seoul_hospital[df_seoul_hospital[\"상호명\"].str.contains(\"꽃배달|의료기|장례식장|상담소|어린이집\")].index\n\nInt64Index([1917, 2803, 4431, 4644, 7938, 10283, 47008, 60645, 70177], dtype='int64')\n\n\n\n# 종합병원과 무관한 데이터를 전처리를 위해 해당 텍스트 한번에 검색\n# 제거할 데이터의 인덱스만 drop_row에 담아주고 list 형태로 변환\n\ndrop_row = df_seoul_hospital[df_seoul_hospital[\"상호명\"].str.contains(\"꽃배달|의료기|장례식장|상담소|어린이집\")].index\ndrop_row = drop_row.tolist() \ndrop_row\n\n[1917, 2803, 4431, 4644, 7938, 10283, 47008, 60645, 70177]\n\n\n\n# 의원으로 끝나는 데이터 인덱스 찾기\n# drop_row2 에 담고 list 변환\n# str.endswith() : ~로 끝나는거\n\ndrop_row2 = df_seoul_hospital[df_seoul_hospital[\"상호명\"].str.endswith(\"의원\")].index\ndrop_row2 = drop_row2.tolist()\ndrop_row2\n\n[8479,\n 12854,\n 13715,\n 14966,\n 16091,\n 18047,\n 20200,\n 20415,\n 30706,\n 32889,\n 34459,\n 34720,\n 35696,\n 37251,\n 45120,\n 49626,\n 51575,\n 55133,\n 56320,\n 56404,\n 56688,\n 57551,\n 62113,\n 76508]\n\n\n\n# 삭제할 행을 drop_row에 합치기\ndrop_row = drop_row + drop_row2\nlen(drop_row)\n\n33\n\n\n\n# 해당 셀을 삭제하고 삭제 전 후의 행의 갯수 비교\nprint(df_seoul_hospital.shape)\ndf_seoul_hospital = df_seoul_hospital.drop(drop_row, axis=0)\nprint(df_seoul_hospital.shape)\n\n(91, 29)\n(58, 29)\n\n\n\n# 시군구명에 따라 종합병원의 숫자\ndf_seoul_hospital[\"시군구명\"].value_counts().plot.bar()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.countplot(data=df_seoul_hospital, x=\"시군구명\", order=df_seoul_hospital[\"시군구명\"].value_counts().index)\n\n&lt;AxesSubplot:xlabel='시군구명', ylabel='count'&gt;\n\n\n\n\n\n\ndf_seoul_hospital[\"상호명\"].unique()\n\narray(['대진의료재단', '홍익병원별관', 'SNUH', '한양', '백산의료재단친구병원', '서울보훈병원',\n       '서울대학교병원', '제일병원', '이랜드클리닉', '사랑나눔의료재단', '우울증센터', '성심의료재단',\n       '다나의료재단', '서울아산병원신관', '고려대학교구로병원', '학교법인일송학원', '희명스포츠의학센터인공신장실',\n       '연세대학교의과대학강남세브란스', '국립정신병원', '코아클리닉', '한국전력공사부속한일병원', '신촌연세병원',\n       '영동세브란스병원', '삼성의료재단강북삼성태', '서울시립보라매병원', '서울대학교병원비상계획외래',\n       '평화드림서울성모병원의료', '홍익병원', '사랑나눔의료재단서', '우신향병원', '동부제일병원', '아산재단금강병원',\n       '명곡안연구소', '아산재단서울중앙병원', '메디힐특수여객', '삼성생명공익재단삼성서', '성광의료재단차병원',\n       '한국건강관리협회서울특', '정해복지부설한신메디피아', '성베드로병원', '성애의료재단', 'Y&T성모마취과',\n       '영남의료재단', '인제대학교서울백병원', '한국필의료재단', '사랑의병원', '성삼의료재단미즈메디병원',\n       '씨엠충무병원', '원진재단부설녹색병원', '강남센트럴병원', '우리들병원', '건국대학교병원', '서울적십자병원',\n       '서울성모병원응급의료센터', '가톨릭대학교여의도성모병원', '씨엠병원'], dtype=object)\n\n\n\n\n\n\n# 서울에 있는 데이터의 위도와 경도 보기\n# 결과를 df_seoul 이라는 df에 저장\n# 새로운 변수에 데이터프레임 저장시 copy()를 사용\n\ndf_seoul= df[df[\"시도명\"] == \"서울특별시\"].copy()\ndf_seoul.shape\n\n(18943, 29)\n\n\n\n# seaborn 의 countplot를 사용해 위에서 만든 데이터프레ㅣㅁ의 시군구명 시각화\ndf_seoul[\"시군구명\"].value_counts().head()\ndf_seoul[\"시군구명\"].value_counts().plot.bar(figsize=(10,4), rot=30)\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.countplot(data=df_seoul, x=\"시군구명\")\n# x축 y축 두개 중 하나만 지정해주면 된다.\n\n&lt;AxesSubplot:xlabel='시군구명', ylabel='count'&gt;\n\n\n\n\n\n\n# pandas의 plot.scatter를 통해 경도와 위도 표시\ndf_seoul[[\"경도\", \"위도\", \"시군구명\"]].plot.scatter()\n# scatter는 x축과 y축이 꼭 들어가야 한다!!\n\nTypeError: scatter() missing 2 required positional arguments: 'x' and 'y'\n\n\n\ndf_seoul[[\"경도\", \"위도\", \"시군구명\"]].plot.scatter(x=\"경도\", y=\"위도\", figsize=(8,7), grid=True)\n\n&lt;AxesSubplot:xlabel='경도', ylabel='위도'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(9,8))\nsns.scatterplot(data=df_seoul,x=\"경도\", y=\"위도\", hue=\"시군구명\") \n# hue: 색상 다르게\n\n&lt;AxesSubplot:xlabel='경도', ylabel='위도'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(9,8))\nsns.scatterplot(data=df_seoul,x=\"경도\", y=\"위도\", hue=\"상권업종중분류명\") \n\n&lt;AxesSubplot:xlabel='경도', ylabel='위도'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(16,12))\nsns.scatterplot(data=df,x=\"경도\", y=\"위도\", hue=\"시도명\") \n\n&lt;AxesSubplot:xlabel='경도', ylabel='위도'&gt;\n\n\n\n\n\n\n\n\n\n\nimport folium\n# 아나콘다에서 folium 별도 설치해야함\n# conda install -c conda-forge folium\n# 지도 시각화를 위한 라이브러리\n\nm= folium.Map(location=[45.5236, -122.6750])\n\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nfolium.Map()\n# 세계 지도 출력! \n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nfolium.Map()\n\n\n# 지도의 중심을 지정하기 위해 위도와 경도의 평균을 구한다.\n\ndf_seoul_hospital[\"위도\"].mean()\ndf_seoul_hospital[\"경도\"].mean()\n\n126.9963589356625\n\n\n\nmap = folium.Map(location=[df_seoul_hospital[\"위도\"].mean(),df_seoul_hospital[\"경도\"].mean()], zoom_start=12)\n\n\ndf_seoul_hospital.head(1)\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n시도명\n시군구코드\n...\n지번주소\n도로명코드\n도로명\n건물본번지\n건물관리번호\n도로명주소\n구우편번호\n신우편번호\n경도\n위도\n\n\n\n\n305\n25155642\n대진의료재단\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11215.0\n...\n서울특별시 광진구 중곡동 58-25\n112153104006\n서울특별시 광진구 긴고랑로\n119\n1121510100100580025000733\n서울특별시 광진구 긴고랑로 119\n143220.0\n4944.0\n127.088279\n37.559048\n\n\n\n\n1 rows × 29 columns\n\n\n\n\nfor n in df_seoul_hospital.index:\n    name = df_seoul_hospital.loc[n, \"상호명\"]\n    address = df_seoul_hospital.loc[n, \"도로명주소\"]\n    popup = f\"{name}-{address}\"\n    location = [df_seoul_hospital.loc[n, \"위도\"], df_seoul_hospital.loc[n, \"경도\"]]\n    folium.Marker(\n        location = location,\n        popup = popup,\n    ).add_to(map)\nmap\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#필요한-라이브러리-불러오기",
    "href": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#필요한-라이브러리-불러오기",
    "title": "4: 서울 종합병원 분포 확인하기",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#시각화를-위한-폰트-설정",
    "href": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#시각화를-위한-폰트-설정",
    "title": "4: 서울 종합병원 분포 확인하기",
    "section": "",
    "text": "# ctrl(cmd) + / : 주석처리를 풀었다 했다 \n\nimport matplotlib.pyplot as plt\n # window의 한글 폰트 설정\nplt.rc('font',family='Malgun Gothic') #윈도우의 경우\n\n# plt.rc('font', family='AppleGothic') #맥의 경우\n\nplt.rc('axes', unicode_minus=False) #마이너스 폰트 깨지는 것 대비\n\n# 그래프가 노트북 안에 보이게 하기 위해\n%matplotlib inline\n\n\nfrom IPython.display import set_matplotlib_formats\n#폰트가 선명하게 보이기 위해\n\n%config InlineBackend.figure_format = 'retina'"
  },
  {
    "objectID": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#데이터-로드하기",
    "href": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#데이터-로드하기",
    "title": "4: 서울 종합병원 분포 확인하기",
    "section": "",
    "text": "!move \"C:\\Users\\user\\Downloads\\소상공인시장진흥공단_상가업소정보_의료기관_201909.csv\"\n\n지정된 파일을 찾을 수 없습니다.\n\n\n\ndf = pd.read_csv(\"data/소상공인시장진흥공단_상가업소정보_의료기관_201909.csv\", low_memory=False)\n# low_memory=False로 설정이 되어야 한다. 안그럼 오류남.\ndf.shape # 데이터의 행과 열 크기를 찍어볼 수 있따\n\n(91335, 39)"
  },
  {
    "objectID": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#데이터-미리보기",
    "href": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#데이터-미리보기",
    "title": "4: 서울 종합병원 분포 확인하기",
    "section": "",
    "text": "head, tail을 통해 데이터를 미리 볼수 있따\n\n\n# shift + tab 키를 누르면 docstring을 볼 수 있다\n# head: 데이터 미리보기\ndf.head(1)\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n지점명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n표준산업분류코드\n...\n건물관리번호\n건물명\n도로명주소\n구우편번호\n신우편번호\n동정보\n층정보\n호정보\n경도\n위도\n\n\n\n\n0\n19956873\n하나산부인과\nNaN\nS\n의료\nS01\n병원\nS01B10\n산부인과\nQ86201\n...\n4127310900110810000010857\n산호한양아파트\n경기도 안산시 단원구 달미로 10\n425764.0\n15236.0\nNaN\nNaN\nNaN\n126.814295\n37.336344\n\n\n\n\n1 rows × 39 columns\n\n\n\n\n# tail: 마지막 데이터 불러오기\ndf.tail(1)\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n지점명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n표준산업분류코드\n...\n건물관리번호\n건물명\n도로명주소\n구우편번호\n신우편번호\n동정보\n층정보\n호정보\n경도\n위도\n\n\n\n\n91334\n16109073\n천안김안과천안역본점의원\nNaN\nS\n의료\nS01\n병원\nS01B13\n안과의원\nQ86201\n...\n4413110700102660017016314\n김안과\n충청남도 천안시 동남구 중앙로 92\n330952.0\n31127.0\nNaN\nNaN\nNaN\n127.152651\n36.80664\n\n\n\n\n1 rows × 39 columns"
  },
  {
    "objectID": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#데이터-요약하기",
    "href": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#데이터-요약하기",
    "title": "4: 서울 종합병원 분포 확인하기",
    "section": "",
    "text": "# 요약정보가 나타난다\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 91335 entries, 0 to 91334\nData columns (total 39 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   상가업소번호     91335 non-null  int64  \n 1   상호명        91335 non-null  object \n 2   지점명        1346 non-null   object \n 3   상권업종대분류코드  91335 non-null  object \n 4   상권업종대분류명   91335 non-null  object \n 5   상권업종중분류코드  91335 non-null  object \n 6   상권업종중분류명   91335 non-null  object \n 7   상권업종소분류코드  91335 non-null  object \n 8   상권업종소분류명   91335 non-null  object \n 9   표준산업분류코드   86413 non-null  object \n 10  표준산업분류명    86413 non-null  object \n 11  시도코드       90956 non-null  float64\n 12  시도명        90956 non-null  object \n 13  시군구코드      90956 non-null  float64\n 14  시군구명       90956 non-null  object \n 15  행정동코드      91335 non-null  int64  \n 16  행정동명       90956 non-null  object \n 17  법정동코드      91280 non-null  float64\n 18  법정동명       91280 non-null  object \n 19  지번코드       91335 non-null  int64  \n 20  대지구분코드     91335 non-null  int64  \n 21  대지구분명      91335 non-null  object \n 22  지번본번지      91335 non-null  int64  \n 23  지번부번지      72079 non-null  float64\n 24  지번주소       91335 non-null  object \n 25  도로명코드      91335 non-null  int64  \n 26  도로명        91335 non-null  object \n 27  건물본번지      91335 non-null  int64  \n 28  건물부번지      10604 non-null  float64\n 29  건물관리번호     91335 non-null  object \n 30  건물명        46453 non-null  object \n 31  도로명주소      91335 non-null  object \n 32  구우편번호      91323 non-null  float64\n 33  신우편번호      91333 non-null  float64\n 34  동정보        7406 non-null   object \n 35  층정보        44044 non-null  object \n 36  호정보        15551 non-null  object \n 37  경도         91335 non-null  float64\n 38  위도         91335 non-null  float64\ndtypes: float64(9), int64(7), object(23)\nmemory usage: 27.2+ MB\n\n\n\n# object : 문자열로 된 데이터 타입\n# int: 정수형\n# float : 실수형\n\n\n\n\n\n# 컬럼명만 추출해보기\ndf.columns\n\nIndex(['상가업소번호', '상호명', '지점명', '상권업종대분류코드', '상권업종대분류명', '상권업종중분류코드',\n       '상권업종중분류명', '상권업종소분류코드', '상권업종소분류명', '표준산업분류코드', '표준산업분류명', '시도코드',\n       '시도명', '시군구코드', '시군구명', '행정동코드', '행정동명', '법정동코드', '법정동명', '지번코드',\n       '대지구분코드', '대지구분명', '지번본번지', '지번부번지', '지번주소', '도로명코드', '도로명', '건물본번지',\n       '건물부번지', '건물관리번호', '건물명', '도로명주소', '구우편번호', '신우편번호', '동정보', '층정보',\n       '호정보', '경도', '위도'],\n      dtype='object')\n\n\n\n\n\n\n# 데이터 타입만 출력\ndf.dtypes\n\n상가업소번호         int64\n상호명           object\n지점명           object\n상권업종대분류코드     object\n상권업종대분류명      object\n상권업종중분류코드     object\n상권업종중분류명      object\n상권업종소분류코드     object\n상권업종소분류명      object\n표준산업분류코드      object\n표준산업분류명       object\n시도코드         float64\n시도명           object\n시군구코드        float64\n시군구명          object\n행정동코드          int64\n행정동명          object\n법정동코드        float64\n법정동명          object\n지번코드           int64\n대지구분코드         int64\n대지구분명         object\n지번본번지          int64\n지번부번지        float64\n지번주소          object\n도로명코드          int64\n도로명           object\n건물본번지          int64\n건물부번지        float64\n건물관리번호        object\n건물명           object\n도로명주소         object\n구우편번호        float64\n신우편번호        float64\n동정보           object\n층정보           object\n호정보           object\n경도           float64\n위도           float64\ndtype: object"
  },
  {
    "objectID": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#결측치",
    "href": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#결측치",
    "title": "4: 서울 종합병원 분포 확인하기",
    "section": "",
    "text": "df.isnull()\n# true로 표시되는 값은 null 값! \n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n지점명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n표준산업분류코드\n...\n건물관리번호\n건물명\n도로명주소\n구우편번호\n신우편번호\n동정보\n층정보\n호정보\n경도\n위도\n\n\n\n\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\n\n\n1\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\n\n\n3\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n91330\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\n\n\n91331\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\n\n\n91332\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n91333\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\n\n\n91334\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\n\n\n\n\n91335 rows × 39 columns\n\n\n\n\nnull_count = df.isnull().sum()\nnull_count\n\n상가업소번호           0\n상호명              0\n지점명          89989\n상권업종대분류코드        0\n상권업종대분류명         0\n상권업종중분류코드        0\n상권업종중분류명         0\n상권업종소분류코드        0\n상권업종소분류명         0\n표준산업분류코드      4922\n표준산업분류명       4922\n시도코드           379\n시도명            379\n시군구코드          379\n시군구명           379\n행정동코드            0\n행정동명           379\n법정동코드           55\n법정동명            55\n지번코드             0\n대지구분코드           0\n대지구분명            0\n지번본번지            0\n지번부번지        19256\n지번주소             0\n도로명코드            0\n도로명              0\n건물본번지            0\n건물부번지        80731\n건물관리번호           0\n건물명          44882\n도로명주소            0\n구우편번호           12\n신우편번호            2\n동정보          83929\n층정보          47291\n호정보          75784\n경도               0\n위도               0\ndtype: int64\n\n\n\n# 결측치를 막대그래프로 표현\nnull_count.plot() # 선그래프로 표시되는데.. 적합하지 않은 거 같아!\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nnull_count.plot.bar(rot=60)\n# rot = 글자 돌려보는것\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nnull_count.plot.barh()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nnull_count.plot.barh(figsize=(5,7)) # 그래프 사이즈 지정\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\n# 위에서 계산한 결측치 수를 reset_index 통해 데이터 프레임으로 만들기\n# df_null_coount 변수에 결과를 담아 head로 미리보기 해보기\n\ndf_null_count = null_count.reset_index()\ndf_null_count.head()\n\n\n\n\n\n\n\n\nindex\n0\n\n\n\n\n0\n상가업소번호\n0\n\n\n1\n상호명\n0\n\n\n2\n지점명\n89989\n\n\n3\n상권업종대분류코드\n0\n\n\n4\n상권업종대분류명\n0"
  },
  {
    "objectID": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#컬럼명-변경하기",
    "href": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#컬럼명-변경하기",
    "title": "4: 서울 종합병원 분포 확인하기",
    "section": "",
    "text": "# 변수에 담겨있는 컬럼이름 변경\n\ndf_null_count.columns = [\"컬럼명\", \"결측치수\"]\ndf_null_count.head()\n\n\n\n\n\n\n\n\n컬럼명\n결측치수\n\n\n\n\n0\n상가업소번호\n0\n\n\n1\n상호명\n0\n\n\n2\n지점명\n89989\n\n\n3\n상권업종대분류코드\n0\n\n\n4\n상권업종대분류명\n0"
  },
  {
    "objectID": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#정렬하기",
    "href": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#정렬하기",
    "title": "4: 서울 종합병원 분포 확인하기",
    "section": "",
    "text": "# sort_values 통해 정렬\n# 결측치가 많은 순으로 상위 10개 출력\n\ndf_null_count_top = df_null_count.sort_values(by=\"결측치수\", ascending=False).head(10)\ndf_null_count_top \n\n\n\n\n\n\n\n\n컬럼명\n결측치수\n\n\n\n\n2\n지점명\n89989\n\n\n34\n동정보\n83929\n\n\n28\n건물부번지\n80731\n\n\n36\n호정보\n75784\n\n\n35\n층정보\n47291\n\n\n30\n건물명\n44882\n\n\n23\n지번부번지\n19256\n\n\n9\n표준산업분류코드\n4922\n\n\n10\n표준산업분류명\n4922\n\n\n11\n시도코드\n379"
  },
  {
    "objectID": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#특정-컬럼만-불러오기",
    "href": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#특정-컬럼만-불러오기",
    "title": "4: 서울 종합병원 분포 확인하기",
    "section": "",
    "text": "# 지점명 컬럼 불러오기\n# NaN == Not a Number의 약자로 결측치를 의미한다.\n\ndf[\"지점명\"].head()\n\n0    NaN\n1    NaN\n2    NaN\n3    NaN\n4    수지점\nName: 지점명, dtype: object\n\n\n\n# \"컬럼명\" 이라는 컬럼 값만 가져와서 drop_columns 라는 변수에 담기\n\ndrop_columns = df_null_count_top[\"컬럼명\"].tolist()    # tolist : list로 만들어줌\ndrop_columns \n\n['지점명',\n '동정보',\n '건물부번지',\n '호정보',\n '층정보',\n '건물명',\n '지번부번지',\n '표준산업분류코드',\n '표준산업분류명',\n '시도코드']\n\n\n\n# drop_columns 변수로 해당 컬럼 정보만 데이터프레임에서 가져오기\ndf[drop_columns].head()\n\n\n\n\n\n\n\n\n지점명\n동정보\n건물부번지\n호정보\n층정보\n건물명\n지번부번지\n표준산업분류코드\n표준산업분류명\n시도코드\n\n\n\n\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n산호한양아파트\nNaN\nQ86201\n일반 의원\n41.0\n\n\n1\nNaN\nNaN\nNaN\nNaN\n4\nNaN\n14.0\nQ86201\n일반 의원\n11.0\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\n한라프라자\n1.0\nQ86201\n일반 의원\n41.0\n\n\n3\nNaN\nNaN\nNaN\nNaN\n5\nNaN\n1.0\nNaN\nNaN\n26.0\n\n\n4\n수지점\nNaN\nNaN\nNaN\n1\nNaN\n2.0\nG47811\n의약품 및 의료용품 소매업\n41.0"
  },
  {
    "objectID": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#제거하기",
    "href": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#제거하기",
    "title": "4: 서울 종합병원 분포 확인하기",
    "section": "",
    "text": "print(df.shape)\n\ndf = df.drop(drop_columns, axis=1) \n# axis=1 컬럼기준으로 drop axis &gt; 행(0), 열 (1)\n\nprint(df.shape)\n\n(91335, 39)\n(91335, 29)\n\n\n\n# 제거 결과 info로 확인\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 91335 entries, 0 to 91334\nData columns (total 29 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   상가업소번호     91335 non-null  int64  \n 1   상호명        91335 non-null  object \n 2   상권업종대분류코드  91335 non-null  object \n 3   상권업종대분류명   91335 non-null  object \n 4   상권업종중분류코드  91335 non-null  object \n 5   상권업종중분류명   91335 non-null  object \n 6   상권업종소분류코드  91335 non-null  object \n 7   상권업종소분류명   91335 non-null  object \n 8   시도명        90956 non-null  object \n 9   시군구코드      90956 non-null  float64\n 10  시군구명       90956 non-null  object \n 11  행정동코드      91335 non-null  int64  \n 12  행정동명       90956 non-null  object \n 13  법정동코드      91280 non-null  float64\n 14  법정동명       91280 non-null  object \n 15  지번코드       91335 non-null  int64  \n 16  대지구분코드     91335 non-null  int64  \n 17  대지구분명      91335 non-null  object \n 18  지번본번지      91335 non-null  int64  \n 19  지번주소       91335 non-null  object \n 20  도로명코드      91335 non-null  int64  \n 21  도로명        91335 non-null  object \n 22  건물본번지      91335 non-null  int64  \n 23  건물관리번호     91335 non-null  object \n 24  도로명주소      91335 non-null  object \n 25  구우편번호      91323 non-null  float64\n 26  신우편번호      91333 non-null  float64\n 27  경도         91335 non-null  float64\n 28  위도         91335 non-null  float64\ndtypes: float64(6), int64(7), object(16)\nmemory usage: 20.2+ MB"
  },
  {
    "objectID": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#기초-통계값-보기",
    "href": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#기초-통계값-보기",
    "title": "4: 서울 종합병원 분포 확인하기",
    "section": "",
    "text": "df.dtypes\n\n상가업소번호         int64\n상호명           object\n상권업종대분류코드     object\n상권업종대분류명      object\n상권업종중분류코드     object\n상권업종중분류명      object\n상권업종소분류코드     object\n상권업종소분류명      object\n시도명           object\n시군구코드        float64\n시군구명          object\n행정동코드          int64\n행정동명          object\n법정동코드        float64\n법정동명          object\n지번코드           int64\n대지구분코드         int64\n대지구분명         object\n지번본번지          int64\n지번주소          object\n도로명코드          int64\n도로명           object\n건물본번지          int64\n건물관리번호        object\n도로명주소         object\n구우편번호        float64\n신우편번호        float64\n경도           float64\n위도           float64\ndtype: object\n\n\n\n#평균값\ndf[\"위도\"].mean()\n\n36.62471119236673\n\n\n\n# 중앙값\ndf[\"위도\"].median()\n\n37.2346523177033\n\n\n\n# 최댓값\ndf[\"위도\"].max()\n\n38.4996585705598\n\n\n\n# 최솟값\ndf[\"위도\"].min()\n\n33.2192896688307\n\n\n\n# 갯수\ndf[\"위도\"].count()\n\n91335\n\n\n\n\n\n데이터를 요약해서 볼 수 있음\n\n# 위도를 descibe로 요약\n\ndf[\"위도\"].describe()\n\ncount    91335.000000\nmean        36.624711\nstd          1.041361\nmin         33.219290\n25%         35.811830\n50%         37.234652\n75%         37.507463\nmax         38.499659\nName: 위도, dtype: float64\n\n\n\n# 2개의 컬럼을 describe로 요약\ndf[\"위도\", \"경도\"] \n\n# pandas에서는 리스트 형태로 가져와야한다. 위와 같이 하면 오류남\n\nKeyError: ('위도', '경도')\n\n\n\ndf[[\"위도\", \"경도\"]]\n\n\n\n\n\n\n\n\n위도\n경도\n\n\n\n\n0\n37.336344\n126.814295\n\n\n1\n37.488742\n127.053198\n\n\n2\n37.344955\n126.734841\n\n\n3\n35.166872\n129.115438\n\n\n4\n37.323528\n127.095522\n\n\n...\n...\n...\n\n\n91330\n36.352728\n127.389865\n\n\n91331\n37.627530\n126.830144\n\n\n91332\n35.227138\n129.082790\n\n\n91333\n37.540993\n127.143958\n\n\n91334\n36.806640\n127.152651\n\n\n\n\n91335 rows × 2 columns\n\n\n\n\ndf[[\"위도\", \"경도\"]].describe()\n\n\n\n\n\n\n\n\n위도\n경도\n\n\n\n\ncount\n91335.000000\n91335.000000\n\n\nmean\n36.624711\n127.487524\n\n\nstd\n1.041361\n0.842877\n\n\nmin\n33.219290\n124.717632\n\n\n25%\n35.811830\n126.914297\n\n\n50%\n37.234652\n127.084550\n\n\n75%\n37.507463\n128.108919\n\n\nmax\n38.499659\n130.909912\n\n\n\n\n\n\n\n\n# describe로 문자열 데이터타입 요약\n\ndf.describe() # 기본값은 수치형으로 되어있음!\n\n\n\n\n\n\n\n\n상가업소번호\n시군구코드\n행정동코드\n법정동코드\n지번코드\n대지구분코드\n지번본번지\n도로명코드\n건물본번지\n구우편번호\n신우편번호\n경도\n위도\n\n\n\n\ncount\n9.133500e+04\n90956.000000\n9.133500e+04\n9.128000e+04\n9.133500e+04\n91335.000000\n91335.000000\n9.133500e+04\n91335.000000\n91323.000000\n91333.00000\n91335.000000\n91335.000000\n\n\nmean\n2.121818e+07\n32898.381877\n3.293232e+09\n3.293385e+09\n3.293191e+18\n1.001336\n587.534549\n3.293207e+11\n251.200482\n428432.911085\n28085.47698\n127.487524\n36.624711\n\n\nstd\n5.042828e+06\n12985.393171\n1.297387e+09\n1.297706e+09\n1.297393e+18\n0.036524\n582.519364\n1.297391e+11\n477.456487\n193292.339066\n18909.01455\n0.842877\n1.041361\n\n\nmin\n2.901108e+06\n11110.000000\n1.111052e+09\n1.111010e+09\n1.111010e+18\n1.000000\n1.000000\n1.111020e+11\n0.000000\n100011.000000\n1000.00000\n124.717632\n33.219290\n\n\n25%\n2.001931e+07\n26350.000000\n2.635065e+09\n2.635011e+09\n2.635011e+18\n1.000000\n162.000000\n2.635042e+11\n29.000000\n302120.000000\n11681.00000\n126.914297\n35.811830\n\n\n50%\n2.211900e+07\n41117.000000\n4.111758e+09\n4.111710e+09\n4.111711e+18\n1.000000\n462.000000\n4.111743e+11\n92.000000\n440300.000000\n24353.00000\n127.084550\n37.234652\n\n\n75%\n2.480984e+07\n43113.000000\n4.311370e+09\n4.311311e+09\n4.311311e+18\n1.000000\n858.000000\n4.311332e+11\n257.000000\n602811.000000\n46044.00000\n128.108919\n37.507463\n\n\nmax\n2.852470e+07\n50130.000000\n5.013061e+09\n5.013032e+09\n5.013061e+18\n2.000000\n7338.000000\n5.013049e+11\n8795.000000\n799801.000000\n63643.00000\n130.909912\n38.499659\n\n\n\n\n\n\n\n\ndf.describe(include=\"object\")\n\n# top : 가장 많이 나타난걸 보여줌\n# freq : frequency : 빈도수.. 리원이라는 상호명이 152번 등장한다.\n# 결측치는 제외하고 보여줌! \n\n\n\n\n\n\n\n\n상호명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n시도명\n시군구명\n행정동명\n법정동명\n대지구분명\n지번주소\n도로명\n건물관리번호\n도로명주소\n\n\n\n\ncount\n91335\n91335\n91335\n91335\n91335\n91335\n91335\n90956\n90956\n90956\n91280\n91335\n91335\n91335\n91335\n91335\n\n\nunique\n56910\n1\n1\n5\n5\n34\n34\n17\n228\n2791\n2822\n2\n53118\n16610\n54142\n54031\n\n\ntop\n리원\nS\n의료\nS01\n병원\nS02A01\n약국\n경기도\n서구\n중앙동\n중동\n대지\n서울특별시 동대문구 제기동 965-1\n서울특별시 강남구 강남대로\n1123010300109650001031604\n서울특별시 동대문구 약령중앙로8길 10\n\n\nfreq\n152\n91335\n91335\n60774\n60774\n18964\n18964\n21374\n3165\n1856\n874\n91213\n198\n326\n198\n198\n\n\n\n\n\n\n\n\n# 모든 데이터 요약\ndf.describe(include=\"all\")\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n시도명\n시군구코드\n...\n지번주소\n도로명코드\n도로명\n건물본번지\n건물관리번호\n도로명주소\n구우편번호\n신우편번호\n경도\n위도\n\n\n\n\ncount\n9.133500e+04\n91335\n91335\n91335\n91335\n91335\n91335\n91335\n90956\n90956.000000\n...\n91335\n9.133500e+04\n91335\n91335.000000\n91335\n91335\n91323.000000\n91333.00000\n91335.000000\n91335.000000\n\n\nunique\nNaN\n56910\n1\n1\n5\n5\n34\n34\n17\nNaN\n...\n53118\nNaN\n16610\nNaN\n54142\n54031\nNaN\nNaN\nNaN\nNaN\n\n\ntop\nNaN\n리원\nS\n의료\nS01\n병원\nS02A01\n약국\n경기도\nNaN\n...\n서울특별시 동대문구 제기동 965-1\nNaN\n서울특별시 강남구 강남대로\nNaN\n1123010300109650001031604\n서울특별시 동대문구 약령중앙로8길 10\nNaN\nNaN\nNaN\nNaN\n\n\nfreq\nNaN\n152\n91335\n91335\n60774\n60774\n18964\n18964\n21374\nNaN\n...\n198\nNaN\n326\nNaN\n198\n198\nNaN\nNaN\nNaN\nNaN\n\n\nmean\n2.121818e+07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n32898.381877\n...\nNaN\n3.293207e+11\nNaN\n251.200482\nNaN\nNaN\n428432.911085\n28085.47698\n127.487524\n36.624711\n\n\nstd\n5.042828e+06\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n12985.393171\n...\nNaN\n1.297391e+11\nNaN\n477.456487\nNaN\nNaN\n193292.339066\n18909.01455\n0.842877\n1.041361\n\n\nmin\n2.901108e+06\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n11110.000000\n...\nNaN\n1.111020e+11\nNaN\n0.000000\nNaN\nNaN\n100011.000000\n1000.00000\n124.717632\n33.219290\n\n\n25%\n2.001931e+07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n26350.000000\n...\nNaN\n2.635042e+11\nNaN\n29.000000\nNaN\nNaN\n302120.000000\n11681.00000\n126.914297\n35.811830\n\n\n50%\n2.211900e+07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n41117.000000\n...\nNaN\n4.111743e+11\nNaN\n92.000000\nNaN\nNaN\n440300.000000\n24353.00000\n127.084550\n37.234652\n\n\n75%\n2.480984e+07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n43113.000000\n...\nNaN\n4.311332e+11\nNaN\n257.000000\nNaN\nNaN\n602811.000000\n46044.00000\n128.108919\n37.507463\n\n\nmax\n2.852470e+07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n50130.000000\n...\nNaN\n5.013049e+11\nNaN\n8795.000000\nNaN\nNaN\n799801.000000\n63643.00000\n130.909912\n38.499659\n\n\n\n\n11 rows × 29 columns\n\n\n\n\n\n\n\nunique로 중복 제거 nuique 갯수 세기\n\n\n# 상권업종대분류명\n\ndf[\"상권업종대분류명\"].unique()\n\narray(['의료'], dtype=object)\n\n\n\ndf[\"상권업종대분류명\"].nunique()\n\n1\n\n\n\n# 상권업종중분류명\ndf[\"상권업종중분류명\"].unique()\n\narray(['병원', '약국/한약방', '수의업', '유사의료업', '의료관련서비스업'], dtype=object)\n\n\n\ndf[\"상권업종중분류명\"].nunique()\n\n5\n\n\n\n# 상권업종소분류명\ndf[\"상권업종소분류명\"].unique()\n\narray(['산부인과', '내과/외과', '신경외과', '기타병원', '약국', '동물병원', '한약방', '탕제원',\n       '정형/성형외과', '소아과', '이비인후과의원', '노인/치매병원', '언어치료', '수의업-종합', '한의원',\n       '치과의원', '침구원', '일반병원', '안과의원', '조산원', '한방병원', '종합병원', '유사의료업기타',\n       '응급구조대', '혈액원', '치과병원', '척추교정치료', '피부과', '비뇨기과', '치과기공소', '산후조리원',\n       '접골원', '수의업-기타', '제대혈'], dtype=object)\n\n\n\ndf[\"상권업종소분류명\"].nunique()\n\n34\n\n\n\nlen(df[\"상권업종소분류명\"].unique())\n\n34\n\n\n\n\n\n\n카테고리 형태의 데이터 갯수를 셀 수 있다.\n\n\n# 시도코드 세어보기 -&gt; 결측치...\ndf[\"시도명\"].head()\n\n0      경기도\n1    서울특별시\n2      경기도\n3    부산광역시\n4      경기도\nName: 시도명, dtype: object\n\n\n\n# 시도명 세보면\ncity = df[\"시도명\"].value_counts()\ncity\n\n경기도        21374\n서울특별시      18943\n부산광역시       6473\n경상남도        4973\n인천광역시       4722\n대구광역시       4597\n경상북도        4141\n전라북도        3894\n충청남도        3578\n전라남도        3224\n광주광역시       3214\n대전광역시       3067\n충청북도        2677\n강원도         2634\n울산광역시       1997\n제주특별자치도     1095\n세종특별자치시      353\nName: 시도명, dtype: int64\n\n\n\n# normalize=True 옵션 사용시 비율을 구할 수 있다.\n\ncity_normalize= df[\"시도명\"].value_counts(normalize=True)\ncity_normalize\n\n경기도        0.234993\n서울특별시      0.208266\n부산광역시      0.071166\n경상남도       0.054675\n인천광역시      0.051915\n대구광역시      0.050541\n경상북도       0.045528\n전라북도       0.042812\n충청남도       0.039338\n전라남도       0.035446\n광주광역시      0.035336\n대전광역시      0.033720\n충청북도       0.029432\n강원도        0.028959\n울산광역시      0.021956\n제주특별자치도    0.012039\n세종특별자치시    0.003881\nName: 시도명, dtype: float64\n\n\n\ncity.plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\n # 막대그래프 표현\ncity.plot.barh()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\n# plto.pie() 사용하여 파이그래프 그리기\ncity_normalize.plot.pie(figsize=(7,7))\n\n&lt;AxesSubplot:ylabel='시도명'&gt;\n\n\n\n\n\n\n# seaborn의 countplot 그리기\nsns.countplot(data=df, y=\"시도명\" )\n\n&lt;AxesSubplot:xlabel='count', ylabel='시도명'&gt;\n\n\n\n\n\n\nc=sns.countplot(data=df, y=\"시도명\" ) \n# 변수명에 담아주면 밑에 글씨가 없어진당 \n\n\n\n\n\ndf[\"상권업종대분류명\"].value_counts()\n\n의료    91335\nName: 상권업종대분류명, dtype: int64\n\n\n\nd= df[\"상권업종중분류명\"].value_counts()\nd\n\n병원          60774\n약국/한약방      20923\n수의업          5323\n유사의료업        3774\n의료관련서비스업      541\nName: 상권업종중분류명, dtype: int64\n\n\n\nn =df[\"상권업종중분류명\"].value_counts(normalize=True)\nn\n\n병원          0.665397\n약국/한약방      0.229080\n수의업         0.058280\n유사의료업       0.041320\n의료관련서비스업    0.005923\nName: 상권업종중분류명, dtype: float64\n\n\n\nd.plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nd.plot.bar()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nd.plot.bar(rot=0) # x축 레이블 값 회전\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nn.plot.pie()\n\n&lt;AxesSubplot:ylabel='상권업종중분류명'&gt;\n\n\n\n\n\n\nc = df[\"상권업종소분류명\"].value_counts()\nc\n\n약국         18964\n치과의원       13731\n한의원        13211\n내과/외과      11374\n기타병원        4922\n일반병원        3385\n동물병원        3098\n정형/성형외과     2562\n소아과         2472\n수의업-종합      2216\n치과기공소       1724\n이비인후과의원     1486\n한약방         1442\n피부과         1273\n산부인과        1116\n노인/치매병원     1055\n안과의원        1042\n비뇨기과         809\n종합병원         762\n치과병원         756\n언어치료         664\n유사의료업기타      629\n탕제원          517\n산후조리원        511\n신경외과         421\n한방병원         397\n척추교정치료       338\n침구원          154\n혈액원          130\n응급구조대        125\n조산원           30\n접골원            9\n수의업-기타         9\n제대혈            1\nName: 상권업종소분류명, dtype: int64\n\n\n\nc.plot.barh(figsize=(7, 8), grid= True) #gird: 격자 표시\n\n&lt;AxesSubplot:&gt;"
  },
  {
    "objectID": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#데이터-색인하기",
    "href": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#데이터-색인하기",
    "title": "4: 서울 종합병원 분포 확인하기",
    "section": "",
    "text": "# 상권업종분류명이 약국/한약방인 데이터만 가져와서\n# df_medical이라는 변수에 담고\n# head()통해 미리보기\n\ndf_medical = df[df[\"상권업종중분류명\"] == \"약국/한약방\"].copy()\ndf_medical.head(1)\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n시도명\n시군구코드\n...\n지번주소\n도로명코드\n도로명\n건물본번지\n건물관리번호\n도로명주소\n구우편번호\n신우편번호\n경도\n위도\n\n\n\n\n4\n20364049\n더블유스토어수지점\nS\n의료\nS02\n약국/한약방\nS02A01\n약국\n경기도\n41465.0\n...\n경기도 용인시 수지구 풍덕천동 712-2\n414653205024\n경기도 용인시 수지구 문정로\n32\n4146510100107120002026238\n경기도 용인시 수지구 문정로 32\n448170.0\n16837.0\n127.095522\n37.323528\n\n\n\n\n1 rows × 29 columns\n\n\n\n\n# 상권업종대분류명이 의료만 가져오기\n# df.loc 사용하면 행, 열을 함께 가져온다.\n# 이 기능을 통해 상권업종중뷴려망만 가져온다\n# 가져온 결과를 value_counts를 통해 중분류의 갯수를 세본다.\n\ndf.loc[df[\"상권업종대분류명\"] == \"의료\", \"상권업종중분류명\"].value_counts()\n\n병원          60774\n약국/한약방      20923\n수의업          5323\n유사의료업        3774\n의료관련서비스업      541\nName: 상권업종중분류명, dtype: int64\n\n\n\n# 위와 같은 기능을 수행하는 코드\n# df.loc[df[\"상권업종대분류명\"] == \"의료\"][\"상권업종중분류명\"] 근데 이건 느리다!! \n\n\nm = df[\"상권업종대분류명\"] == \"의료\"\ndf.loc[m, \"상권업종중분류명\"].value_counts()\n\n병원          60774\n약국/한약방      20923\n수의업          5323\n유사의료업        3774\n의료관련서비스업      541\nName: 상권업종중분류명, dtype: int64\n\n\n\n# 유사의료업\ndf[df[\"상권업종중분류명\"] == \"유사의료업\"]\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n시도명\n시군구코드\n...\n지번주소\n도로명코드\n도로명\n건물본번지\n건물관리번호\n도로명주소\n구우편번호\n신우편번호\n경도\n위도\n\n\n\n\n22\n21013731\n세종언어치료센터\nS\n의료\nS03\n유사의료업\nS03B07\n언어치료\n부산광역시\n26410.0\n...\n부산광역시 금정구 구서동 84-1\n264102000010\n부산광역시 금정구 중앙대로\n1817\n2641010700100840001017686\n부산광역시 금정구 중앙대로 1817-11\n609310.0\n46273.0\n129.091662\n35.246528\n\n\n40\n20933900\n고려수지침학회\nS\n의료\nS03\n유사의료업\nS03B03\n침구원\n경상남도\n48123.0\n...\n경상남도 창원시 성산구 상남동 5-2\n481234784088\n경상남도 창원시 성산구 마디미로4번길\n9\n4812312700100050002026799\n경상남도 창원시 성산구 마디미로4번길 9\n642832.0\n51495.0\n128.684678\n35.224113\n\n\n97\n21717820\n청명원\nS\n의료\nS03\n유사의료업\nS03B09\n유사의료업기타\n충청북도\n43760.0\n...\n충청북도 괴산군 청안면 금신리 241\n437604538132\n충청북도 괴산군 청안면 금신로1길\n93\n4376037022102410000007293\n충청북도 괴산군 청안면 금신로1길 93\n367831.0\n28050.0\n127.635740\n36.768935\n\n\n102\n21865854\n응급환자이송센터\nS\n의료\nS03\n유사의료업\nS03B01\n응급구조대\n대전광역시\n30140.0\n...\n대전광역시 중구 대사동 248-237\n301404295026\n대전광역시 중구 계룡로921번길\n40\n3014011000102480237013097\n대전광역시 중구 계룡로921번길 40\n301846.0\n34946.0\n127.417693\n36.321801\n\n\n108\n21914637\n태화아동발달지원센터\nS\n의료\nS03\n유사의료업\nS03B07\n언어치료\n대전광역시\n30140.0\n...\n대전광역시 중구 문화동 27\n301404295402\n대전광역시 중구 보문산로333번길\n29\n3014011600100270000008172\n대전광역시 중구 보문산로333번길 29\n301130.0\n35020.0\n127.412725\n36.312953\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n91300\n16131218\n으뜸치과기공소\nS\n의료\nS03\n유사의료업\nS03B06\n치과기공소\n경상남도\n48170.0\n...\n경상남도 진주시 수정동 39-11\n481704797625\n경상남도 진주시 향교로18번길\n8\n4817011600100390011004490\n경상남도 진주시 향교로18번길 8\n660180.0\n52753.0\n128.084600\n35.197029\n\n\n91310\n16199325\n보령치과기공소\nS\n의료\nS03\n유사의료업\nS03B06\n치과기공소\n서울특별시\n11290.0\n...\n서울특별시 성북구 동소문동4가 103-11\n112903107003\n서울특별시 성북구 동소문로\n47\n1129010700101030014050661\n서울특별시 성북구 동소문로 47-15\n136821.0\n2832.0\n127.010602\n37.591455\n\n\n91311\n16199088\n점프셈교실\nS\n의료\nS03\n유사의료업\nS03B09\n유사의료업기타\n경상북도\n47130.0\n...\n경상북도 경주시 황성동 446\n471304715895\n경상북도 경주시 용담로104번길\n16\n4713012400104460000024894\n경상북도 경주시 용담로104번길 16\n780954.0\n38084.0\n129.211755\n35.865600\n\n\n91319\n16108560\n씨앤디자인치과기공소\nS\n의료\nS03\n유사의료업\nS03B06\n치과기공소\n서울특별시\n11545.0\n...\n서울특별시 금천구 가산동 60-25\n115453116013\n서울특별시 금천구 벚꽃로\n234\n1154510100100600025000001\n서울특별시 금천구 벚꽃로 234\n153798.0\n8513.0\n126.886122\n37.475986\n\n\n91327\n16190388\n오피스알파\nS\n의료\nS03\n유사의료업\nS03B06\n치과기공소\n경기도\n41173.0\n...\n경기도 안양시 동안구 호계동 970-24\n411734349013\n경기도 안양시 동안구 경수대로507번길\n28\n4117310400109700024005182\n경기도 안양시 동안구 경수대로507번길 28\n431849.0\n14120.0\n126.956365\n37.367779\n\n\n\n\n3774 rows × 29 columns\n\n\n\n\n\ndf[df[\"상권업종중분류명\"] == \"유사의료업\"].shape\n\n(3774, 29)\n\n\n\n# 상호명 그룹화해서 갯수\n# value_counts를 사용해 상위 10개 출력\n\ndf[\"상호명\"].value_counts().head(10)\n\n리원       152\n온누리약국    149\n경희한의원    141\n우리약국     119\n중앙약국     111\n전자담배      98\n조은약국      95\n건강약국      87\n제일약국      79\n사랑약국      73\nName: 상호명, dtype: int64\n\n\n\n\ndf[\"상호명\"].value_counts().tail()\n\n메리디언치과          1\n이엘피부과성형외과       1\n금오중국한의원         1\n오케이연합의원         1\n천안김안과천안역본점의원    1\nName: 상호명, dtype: int64\n\n\n\n\ndf_medi = df[df[\"상권업종중분류명\"] == \"유사의료업\"]\ndf_medi\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n시도명\n시군구코드\n...\n지번주소\n도로명코드\n도로명\n건물본번지\n건물관리번호\n도로명주소\n구우편번호\n신우편번호\n경도\n위도\n\n\n\n\n22\n21013731\n세종언어치료센터\nS\n의료\nS03\n유사의료업\nS03B07\n언어치료\n부산광역시\n26410.0\n...\n부산광역시 금정구 구서동 84-1\n264102000010\n부산광역시 금정구 중앙대로\n1817\n2641010700100840001017686\n부산광역시 금정구 중앙대로 1817-11\n609310.0\n46273.0\n129.091662\n35.246528\n\n\n40\n20933900\n고려수지침학회\nS\n의료\nS03\n유사의료업\nS03B03\n침구원\n경상남도\n48123.0\n...\n경상남도 창원시 성산구 상남동 5-2\n481234784088\n경상남도 창원시 성산구 마디미로4번길\n9\n4812312700100050002026799\n경상남도 창원시 성산구 마디미로4번길 9\n642832.0\n51495.0\n128.684678\n35.224113\n\n\n97\n21717820\n청명원\nS\n의료\nS03\n유사의료업\nS03B09\n유사의료업기타\n충청북도\n43760.0\n...\n충청북도 괴산군 청안면 금신리 241\n437604538132\n충청북도 괴산군 청안면 금신로1길\n93\n4376037022102410000007293\n충청북도 괴산군 청안면 금신로1길 93\n367831.0\n28050.0\n127.635740\n36.768935\n\n\n102\n21865854\n응급환자이송센터\nS\n의료\nS03\n유사의료업\nS03B01\n응급구조대\n대전광역시\n30140.0\n...\n대전광역시 중구 대사동 248-237\n301404295026\n대전광역시 중구 계룡로921번길\n40\n3014011000102480237013097\n대전광역시 중구 계룡로921번길 40\n301846.0\n34946.0\n127.417693\n36.321801\n\n\n108\n21914637\n태화아동발달지원센터\nS\n의료\nS03\n유사의료업\nS03B07\n언어치료\n대전광역시\n30140.0\n...\n대전광역시 중구 문화동 27\n301404295402\n대전광역시 중구 보문산로333번길\n29\n3014011600100270000008172\n대전광역시 중구 보문산로333번길 29\n301130.0\n35020.0\n127.412725\n36.312953\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n91300\n16131218\n으뜸치과기공소\nS\n의료\nS03\n유사의료업\nS03B06\n치과기공소\n경상남도\n48170.0\n...\n경상남도 진주시 수정동 39-11\n481704797625\n경상남도 진주시 향교로18번길\n8\n4817011600100390011004490\n경상남도 진주시 향교로18번길 8\n660180.0\n52753.0\n128.084600\n35.197029\n\n\n91310\n16199325\n보령치과기공소\nS\n의료\nS03\n유사의료업\nS03B06\n치과기공소\n서울특별시\n11290.0\n...\n서울특별시 성북구 동소문동4가 103-11\n112903107003\n서울특별시 성북구 동소문로\n47\n1129010700101030014050661\n서울특별시 성북구 동소문로 47-15\n136821.0\n2832.0\n127.010602\n37.591455\n\n\n91311\n16199088\n점프셈교실\nS\n의료\nS03\n유사의료업\nS03B09\n유사의료업기타\n경상북도\n47130.0\n...\n경상북도 경주시 황성동 446\n471304715895\n경상북도 경주시 용담로104번길\n16\n4713012400104460000024894\n경상북도 경주시 용담로104번길 16\n780954.0\n38084.0\n129.211755\n35.865600\n\n\n91319\n16108560\n씨앤디자인치과기공소\nS\n의료\nS03\n유사의료업\nS03B06\n치과기공소\n서울특별시\n11545.0\n...\n서울특별시 금천구 가산동 60-25\n115453116013\n서울특별시 금천구 벚꽃로\n234\n1154510100100600025000001\n서울특별시 금천구 벚꽃로 234\n153798.0\n8513.0\n126.886122\n37.475986\n\n\n91327\n16190388\n오피스알파\nS\n의료\nS03\n유사의료업\nS03B06\n치과기공소\n경기도\n41173.0\n...\n경기도 안양시 동안구 호계동 970-24\n411734349013\n경기도 안양시 동안구 경수대로507번길\n28\n4117310400109700024005182\n경기도 안양시 동안구 경수대로507번길 28\n431849.0\n14120.0\n126.956365\n37.367779\n\n\n\n\n3774 rows × 29 columns\n\n\n\n\ndf_medi[\"상호명\"].value_counts().head(10)\n\n리원          32\n고려수지침       22\n대한적십자사      17\n헌혈의집        12\n고려수지침학회     10\n수치과기공소      10\n제일치과기공소      9\n미소치과기공소      8\n아트치과기공소      8\n이사랑치과기공소     8\nName: 상호명, dtype: int64\n\n\n\n\n\n# 상권업종소분류명이 약국이고 시도명이 서울특별시인 데이터\ndf[\"상권업종소분류명\"] == \"약국\"  and df[\"시도명\"] == \"서울특별시\"\n# 오류! 판다스에서는 & 써야함. \n\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\n\n\n\ndf[\"상권업종소분류명\"] == \"약국\"  & df[\"시도명\"] == \"서울특별시\"\n# 오류! 연산자 우선순위 때문에 오류가 났다.\n\nTypeError: Cannot perform 'rand_' with a dtyped [object] array and scalar of type [bool]\n\n\n\n\ndf_seoul_drug = df[(df[\"상권업종소분류명\"] == \"약국\")  & (df[\"시도명\"] == \"서울특별시\")]\nprint(df_seoul_drug.shape)\ndf_seoul_drug.head(1)\n\n(3579, 29)\n\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n시도명\n시군구코드\n...\n지번주소\n도로명코드\n도로명\n건물본번지\n건물관리번호\n도로명주소\n구우편번호\n신우편번호\n경도\n위도\n\n\n\n\n33\n20816709\n이즈타워약\nS\n의료\nS02\n약국/한약방\nS02A01\n약국\n서울특별시\n11680.0\n...\n서울특별시 강남구 역삼동 821\n116803122010\n서울특별시 강남구 테헤란로\n101\n1168010100108210001000001\n서울특별시 강남구 테헤란로 101\n135080.0\n6134.0\n127.028023\n37.498656\n\n\n\n\n1 rows × 29 columns\n\n\n\n\n\n\n\n# 시군구명으로 그룹화해서 갯수 세어보기\n# 구별로 약국이 몇개가 있는지 확인\nc = df_seoul_drug[\"시군구명\"].value_counts()\nc.head()\n\n강남구     374\n동대문구    261\n광진구     212\n서초구     191\n송파구     188\nName: 시군구명, dtype: int64\n\n\n\nn = df_seoul_drug[\"시군구명\"].value_counts(normalize=True)\nn.head()\n\n강남구     0.104498\n동대문구    0.072925\n광진구     0.059234\n서초구     0.053367\n송파구     0.052529\nName: 시군구명, dtype: float64\n\n\n\nc.plot.bar(rot=60) #rot:글씨를 기울인다\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\n# 상권업종소분류명이 종합병원\n# 시도명이 서울특별시인 데이터\n\ndf_seoul_hospital = df[(df[\"상권업종소분류명\"] == \"종합병원\") & (df[\"시도명\"] == \"서울특별시\")].copy()\n# copy를 해줘야 df_soeul_hospital 이 바뀐다. df까지 바뀌지 않는다. \ndf_seoul_hospital\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n시도명\n시군구코드\n...\n지번주소\n도로명코드\n도로명\n건물본번지\n건물관리번호\n도로명주소\n구우편번호\n신우편번호\n경도\n위도\n\n\n\n\n305\n25155642\n대진의료재단\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11215.0\n...\n서울특별시 광진구 중곡동 58-25\n112153104006\n서울특별시 광진구 긴고랑로\n119\n1121510100100580025000733\n서울특별시 광진구 긴고랑로 119\n143220.0\n4944.0\n127.088279\n37.559048\n\n\n353\n20471487\n홍익병원별관\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11470.0\n...\n서울특별시 양천구 신정동 897-13\n114702005008\n서울특별시 양천구 국회대로\n250\n1147010100108970013001044\n서울특별시 양천구 국회대로 250\n158070.0\n7937.0\n126.862805\n37.529213\n\n\n385\n20737057\nSNUH\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11680.0\n...\n서울특별시 강남구 역삼동 736-55\n116804166727\n서울특별시 강남구 테헤란로26길\n10\n1168010100107360055027688\n서울특별시 강남구 테헤란로26길 10\n135080.0\n6236.0\n127.035825\n37.499630\n\n\n1917\n23210677\n평화드림여의도성모병원의료기매장\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11560.0\n...\n서울특별시 영등포구 여의도동 62\n115603118001\n서울특별시 영등포구 63로\n10\n1156011000100620000031477\n서울특별시 영등포구 63로 10\n150713.0\n7345.0\n126.936693\n37.518296\n\n\n2461\n20024045\n한양\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11200.0\n...\n서울특별시 성동구 행당동 15-1\n112003103002\n서울특별시 성동구 마조로\n22\n1120010700100150001019623\n서울특별시 성동구 마조로 22-2\n133070.0\n4763.0\n127.041325\n37.559469\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n71991\n28505952\n서울성모병원응급의료센터\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11650.0\n...\n서울특별시 서초구 반포동 505\n116502121003\n서울특별시 서초구 반포대로\n222\n1165010700101230000017226\n서울특별시 서초구 반포대로 222\n137701.0\n6591.0\n127.005841\n37.502382\n\n\n76508\n12292992\n라마르의원\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11740.0\n...\n서울특별시 강동구 천호동 453-8\n117404172367\n서울특별시 강동구 천호대로157길\n18\n1174010900104530021010314\n서울특별시 강동구 천호대로157길 18\n134864.0\n5335.0\n127.127466\n37.538485\n\n\n90492\n16031909\n가톨릭대학교여의도성모병원\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11140.0\n...\n서울특별시 중구 명동2가 1-1\n111404103165\n서울특별시 중구 명동길\n74\n1114012700100010001019574\n서울특별시 중구 명동길 74\n100809.0\n4537.0\n126.986758\n37.563662\n\n\n90581\n16332576\n씨엠병원\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11560.0\n...\n서울특별시 영등포구 영등포동4가 90\n115604154717\n서울특별시 영등포구 영등포로36길\n13\n1156010500100900000035097\n서울특별시 영등포구 영등포로36길 13\n150030.0\n7301.0\n126.903857\n37.518807\n\n\n90788\n16162338\n성베드로병원\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11680.0\n...\n서울특별시 강남구 도곡동 910-27\n116802000003\n서울특별시 강남구 남부순환로\n2649\n1168011800109100027000895\n서울특별시 강남구 남부순환로 2649\n135859.0\n6271.0\n127.039567\n37.485604\n\n\n\n\n91 rows × 29 columns\n\n\n\n\ndf_seoul_hospital[\"시군구명\"].value_counts()\n\n강남구     15\n영등포구     8\n광진구      6\n서초구      6\n강동구      5\n중구       5\n송파구      5\n강북구      4\n도봉구      4\n서대문구     4\n양천구      4\n성북구      3\n강서구      2\n중랑구      2\n종로구      2\n동대문구     2\n구로구      2\n노원구      2\n금천구      2\n성동구      2\n관악구      2\n동작구      1\n마포구      1\n용산구      1\n은평구      1\nName: 시군구명, dtype: int64\n\n\n\n\n\n\n# 색인 전 상호명 중에 종합병원이 아닌 데이터 찾기\ndf_seoul_hospital.loc[~df_seoul_hospital[\"상호명\"].str.contains(\"종합병원\"),\"상호명\"].unique()\n\n# str.contains하면 특정 값만 찾을 수 있다. \n# 앞에 물결 표시를 하게 되면 종합병원이 안들어간 것만 찾을 수 있다. \n\narray(['대진의료재단', '홍익병원별관', 'SNUH', '평화드림여의도성모병원의료기매장', '한양', '백산의료재단친구병원',\n       '서울보훈병원', '서울성모병원장례식장꽃배달', '서울대학교병원', '알콜중독및정신질환상담소',\n       '강남성모병원장례식장꽃배달', '제일병원', '이랜드클리닉', '사랑나눔의료재단', '우울증센터', '성심의료재단',\n       '다나의료재단', '서울아산병원신관', '원자력병원장례식장', '국민의원', '고려대학교구로병원', '학교법인일송학원',\n       '삼성의료원장례식장', '희명스포츠의학센터인공신장실', '연세대학교의과대학강남세브란스', '국립정신병원',\n       '코아클리닉', '수서제일의원', '사랑의의원', '한국전력공사부속한일병원', '신촌연세병원', '창동제일의원',\n       '영동세브란스병원', '제일성심의원', '삼성의료재단강북삼성태', '서울시립보라매병원', '서울이의원',\n       '서울대학교병원비상계획외래', '평화드림서울성모병원의료', '홍익병원', '사랑나눔의료재단서', '독일의원',\n       '서울연합의원', '우신향병원', '동부제일병원', '아산재단금강병원', '명곡안연구소', '아산재단서울중앙병원',\n       '메디힐특수여객', '삼성생명공익재단삼성서', '성광의료재단차병원', '한국건강관리협회서울특',\n       '정해복지부설한신메디피아', '성베드로병원', '성애의료재단', '실로암의원', 'Y&T성모마취과', '광진성모의원',\n       '서울현대의원', '이노신경과의원', '송정훼밀리의원', '서울중앙의원', '영남의료재단', '인제대학교서울백병원',\n       '한국필의료재단', '세브란스의원', '가톨릭대학교성바오로병원장례식장', '서울연세의원', '사랑의병원',\n       '성삼의료재단미즈메디병원', '씨엠충무병원', '성신의원', '원진재단부설녹색병원', '송파제일의원',\n       '카톨릭성모의원', '한양성심의원', '관악성모의원', '강남센트럴병원', '우이한솔의원', '우리들병원',\n       '서울성모병원어린이집', '건국대학교병원', '서울적십자병원', '북부성모의원', '한림대학교부속한강성심병원장례식장',\n       '서울성모병원응급의료센터', '라마르의원', '가톨릭대학교여의도성모병원', '씨엠병원'], dtype=object)\n\n\n\ndf_seoul_hospital[df_seoul_hospital[\"상호명\"].str.contains(\"꽃배달\")]\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n시도명\n시군구코드\n...\n지번주소\n도로명코드\n도로명\n건물본번지\n건물관리번호\n도로명주소\n구우편번호\n신우편번호\n경도\n위도\n\n\n\n\n2803\n20895655\n서울성모병원장례식장꽃배달\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11650.0\n...\n서울특별시 서초구 반포동 551\n116504163330\n서울특별시 서초구 사평대로28길\n55\n1165010700105510000017194\n서울특별시 서초구 사평대로28길 55\n137040.0\n6578.0\n127.000682\n37.498257\n\n\n4644\n22020310\n강남성모병원장례식장꽃배달\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11650.0\n...\n서울특별시 서초구 반포동 547-6\n116504163242\n서울특별시 서초구 반포대로39길\n56\n1165010700105470006016762\n서울특별시 서초구 반포대로39길 56-24\n137040.0\n6578.0\n127.001756\n37.499095\n\n\n\n\n2 rows × 29 columns\n\n\n\n\ndf_seoul_hospital[df_seoul_hospital[\"상호명\"].str.contains(\"의료기\")]\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n시도명\n시군구코드\n...\n지번주소\n도로명코드\n도로명\n건물본번지\n건물관리번호\n도로명주소\n구우편번호\n신우편번호\n경도\n위도\n\n\n\n\n1917\n23210677\n평화드림여의도성모병원의료기매장\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11560.0\n...\n서울특별시 영등포구 여의도동 62\n115603118001\n서울특별시 영등포구 63로\n10\n1156011000100620000031477\n서울특별시 영등포구 63로 10\n150713.0\n7345.0\n126.936693\n37.518296\n\n\n\n\n1 rows × 29 columns\n\n\n\n\ndf_seoul_hospital[df_seoul_hospital[\"상호명\"].str.contains(\"꽃배달|의료기|장례식장|상담소|어린이집\")]\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n시도명\n시군구코드\n...\n지번주소\n도로명코드\n도로명\n건물본번지\n건물관리번호\n도로명주소\n구우편번호\n신우편번호\n경도\n위도\n\n\n\n\n1917\n23210677\n평화드림여의도성모병원의료기매장\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11560.0\n...\n서울특별시 영등포구 여의도동 62\n115603118001\n서울특별시 영등포구 63로\n10\n1156011000100620000031477\n서울특별시 영등포구 63로 10\n150713.0\n7345.0\n126.936693\n37.518296\n\n\n2803\n20895655\n서울성모병원장례식장꽃배달\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11650.0\n...\n서울특별시 서초구 반포동 551\n116504163330\n서울특별시 서초구 사평대로28길\n55\n1165010700105510000017194\n서울특별시 서초구 사평대로28길 55\n137040.0\n6578.0\n127.000682\n37.498257\n\n\n4431\n21781516\n알콜중독및정신질환상담소\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11320.0\n...\n서울특별시 도봉구 창동 181-52\n113204127202\n서울특별시 도봉구 마들로13길\n153\n1132010700101810052014414\n서울특별시 도봉구 마들로13길 153\n132040.0\n1411.0\n127.046203\n37.657046\n\n\n4644\n22020310\n강남성모병원장례식장꽃배달\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11650.0\n...\n서울특별시 서초구 반포동 547-6\n116504163242\n서울특별시 서초구 반포대로39길\n56\n1165010700105470006016762\n서울특별시 서초구 반포대로39길 56-24\n137040.0\n6578.0\n127.001756\n37.499095\n\n\n7938\n20625484\n원자력병원장례식장\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11350.0\n...\n서울특별시 노원구 공릉동 215-4\n113503110002\n서울특별시 노원구 노원로\n75\n1135010300102150004014400\n서울특별시 노원구 노원로 75\n139706.0\n1812.0\n127.082670\n37.628808\n\n\n10283\n20024377\n삼성의료원장례식장\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11680.0\n...\n서울특별시 강남구 일원동 50\n116803122009\n서울특별시 강남구 일원로\n81\n1168011400100500000002609\n서울특별시 강남구 일원로 81\n135710.0\n6351.0\n127.089579\n37.490334\n\n\n47008\n21738670\n가톨릭대학교성바오로병원장례식장\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11230.0\n...\n서울특별시 동대문구 전농동 620-56\n112303105008\n서울특별시 동대문구 왕산로\n180\n1123010400106200056027814\n서울특별시 동대문구 왕산로 180\n130709.0\n2559.0\n127.043471\n37.579246\n\n\n60645\n27670796\n서울성모병원어린이집\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11650.0\n...\n서울특별시 서초구 반포동 505\n116502121003\n서울특별시 서초구 반포대로\n222\n1165010700101230000017226\n서울특별시 서초구 반포대로 222\n137701.0\n6591.0\n127.005841\n37.502382\n\n\n70177\n11537223\n한림대학교부속한강성심병원장례식장\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11560.0\n...\n서울특별시 영등포구 영등포동7가 94-200\n115604154428\n서울특별시 영등포구 버드나루로7길\n12\n1156010800100940200033663\n서울특별시 영등포구 버드나루로7길 12\n150030.0\n7247.0\n126.909676\n37.523168\n\n\n\n\n9 rows × 29 columns\n\n\n\n\ndf_seoul_hospital[df_seoul_hospital[\"상호명\"].str.contains(\"꽃배달|의료기|장례식장|상담소|어린이집\")].index\n\nInt64Index([1917, 2803, 4431, 4644, 7938, 10283, 47008, 60645, 70177], dtype='int64')\n\n\n\n# 종합병원과 무관한 데이터를 전처리를 위해 해당 텍스트 한번에 검색\n# 제거할 데이터의 인덱스만 drop_row에 담아주고 list 형태로 변환\n\ndrop_row = df_seoul_hospital[df_seoul_hospital[\"상호명\"].str.contains(\"꽃배달|의료기|장례식장|상담소|어린이집\")].index\ndrop_row = drop_row.tolist() \ndrop_row\n\n[1917, 2803, 4431, 4644, 7938, 10283, 47008, 60645, 70177]\n\n\n\n# 의원으로 끝나는 데이터 인덱스 찾기\n# drop_row2 에 담고 list 변환\n# str.endswith() : ~로 끝나는거\n\ndrop_row2 = df_seoul_hospital[df_seoul_hospital[\"상호명\"].str.endswith(\"의원\")].index\ndrop_row2 = drop_row2.tolist()\ndrop_row2\n\n[8479,\n 12854,\n 13715,\n 14966,\n 16091,\n 18047,\n 20200,\n 20415,\n 30706,\n 32889,\n 34459,\n 34720,\n 35696,\n 37251,\n 45120,\n 49626,\n 51575,\n 55133,\n 56320,\n 56404,\n 56688,\n 57551,\n 62113,\n 76508]\n\n\n\n# 삭제할 행을 drop_row에 합치기\ndrop_row = drop_row + drop_row2\nlen(drop_row)\n\n33\n\n\n\n# 해당 셀을 삭제하고 삭제 전 후의 행의 갯수 비교\nprint(df_seoul_hospital.shape)\ndf_seoul_hospital = df_seoul_hospital.drop(drop_row, axis=0)\nprint(df_seoul_hospital.shape)\n\n(91, 29)\n(58, 29)\n\n\n\n# 시군구명에 따라 종합병원의 숫자\ndf_seoul_hospital[\"시군구명\"].value_counts().plot.bar()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.countplot(data=df_seoul_hospital, x=\"시군구명\", order=df_seoul_hospital[\"시군구명\"].value_counts().index)\n\n&lt;AxesSubplot:xlabel='시군구명', ylabel='count'&gt;\n\n\n\n\n\n\ndf_seoul_hospital[\"상호명\"].unique()\n\narray(['대진의료재단', '홍익병원별관', 'SNUH', '한양', '백산의료재단친구병원', '서울보훈병원',\n       '서울대학교병원', '제일병원', '이랜드클리닉', '사랑나눔의료재단', '우울증센터', '성심의료재단',\n       '다나의료재단', '서울아산병원신관', '고려대학교구로병원', '학교법인일송학원', '희명스포츠의학센터인공신장실',\n       '연세대학교의과대학강남세브란스', '국립정신병원', '코아클리닉', '한국전력공사부속한일병원', '신촌연세병원',\n       '영동세브란스병원', '삼성의료재단강북삼성태', '서울시립보라매병원', '서울대학교병원비상계획외래',\n       '평화드림서울성모병원의료', '홍익병원', '사랑나눔의료재단서', '우신향병원', '동부제일병원', '아산재단금강병원',\n       '명곡안연구소', '아산재단서울중앙병원', '메디힐특수여객', '삼성생명공익재단삼성서', '성광의료재단차병원',\n       '한국건강관리협회서울특', '정해복지부설한신메디피아', '성베드로병원', '성애의료재단', 'Y&T성모마취과',\n       '영남의료재단', '인제대학교서울백병원', '한국필의료재단', '사랑의병원', '성삼의료재단미즈메디병원',\n       '씨엠충무병원', '원진재단부설녹색병원', '강남센트럴병원', '우리들병원', '건국대학교병원', '서울적십자병원',\n       '서울성모병원응급의료센터', '가톨릭대학교여의도성모병원', '씨엠병원'], dtype=object)\n\n\n\n\n\n\n# 서울에 있는 데이터의 위도와 경도 보기\n# 결과를 df_seoul 이라는 df에 저장\n# 새로운 변수에 데이터프레임 저장시 copy()를 사용\n\ndf_seoul= df[df[\"시도명\"] == \"서울특별시\"].copy()\ndf_seoul.shape\n\n(18943, 29)\n\n\n\n# seaborn 의 countplot를 사용해 위에서 만든 데이터프레ㅣㅁ의 시군구명 시각화\ndf_seoul[\"시군구명\"].value_counts().head()\ndf_seoul[\"시군구명\"].value_counts().plot.bar(figsize=(10,4), rot=30)\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.countplot(data=df_seoul, x=\"시군구명\")\n# x축 y축 두개 중 하나만 지정해주면 된다.\n\n&lt;AxesSubplot:xlabel='시군구명', ylabel='count'&gt;\n\n\n\n\n\n\n# pandas의 plot.scatter를 통해 경도와 위도 표시\ndf_seoul[[\"경도\", \"위도\", \"시군구명\"]].plot.scatter()\n# scatter는 x축과 y축이 꼭 들어가야 한다!!\n\nTypeError: scatter() missing 2 required positional arguments: 'x' and 'y'\n\n\n\ndf_seoul[[\"경도\", \"위도\", \"시군구명\"]].plot.scatter(x=\"경도\", y=\"위도\", figsize=(8,7), grid=True)\n\n&lt;AxesSubplot:xlabel='경도', ylabel='위도'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(9,8))\nsns.scatterplot(data=df_seoul,x=\"경도\", y=\"위도\", hue=\"시군구명\") \n# hue: 색상 다르게\n\n&lt;AxesSubplot:xlabel='경도', ylabel='위도'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(9,8))\nsns.scatterplot(data=df_seoul,x=\"경도\", y=\"위도\", hue=\"상권업종중분류명\") \n\n&lt;AxesSubplot:xlabel='경도', ylabel='위도'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(16,12))\nsns.scatterplot(data=df,x=\"경도\", y=\"위도\", hue=\"시도명\") \n\n&lt;AxesSubplot:xlabel='경도', ylabel='위도'&gt;"
  },
  {
    "objectID": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#folium으로-지도-활용",
    "href": "posts/Study/boostcourse/3. 서울 종합병원 분포 확인하기.html#folium으로-지도-활용",
    "title": "4: 서울 종합병원 분포 확인하기",
    "section": "",
    "text": "import folium\n# 아나콘다에서 folium 별도 설치해야함\n# conda install -c conda-forge folium\n# 지도 시각화를 위한 라이브러리\n\nm= folium.Map(location=[45.5236, -122.6750])\n\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nfolium.Map()\n# 세계 지도 출력! \n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nfolium.Map()\n\n\n# 지도의 중심을 지정하기 위해 위도와 경도의 평균을 구한다.\n\ndf_seoul_hospital[\"위도\"].mean()\ndf_seoul_hospital[\"경도\"].mean()\n\n126.9963589356625\n\n\n\nmap = folium.Map(location=[df_seoul_hospital[\"위도\"].mean(),df_seoul_hospital[\"경도\"].mean()], zoom_start=12)\n\n\ndf_seoul_hospital.head(1)\n\n\n\n\n\n\n\n\n상가업소번호\n상호명\n상권업종대분류코드\n상권업종대분류명\n상권업종중분류코드\n상권업종중분류명\n상권업종소분류코드\n상권업종소분류명\n시도명\n시군구코드\n...\n지번주소\n도로명코드\n도로명\n건물본번지\n건물관리번호\n도로명주소\n구우편번호\n신우편번호\n경도\n위도\n\n\n\n\n305\n25155642\n대진의료재단\nS\n의료\nS01\n병원\nS01B01\n종합병원\n서울특별시\n11215.0\n...\n서울특별시 광진구 중곡동 58-25\n112153104006\n서울특별시 광진구 긴고랑로\n119\n1121510100100580025000733\n서울특별시 광진구 긴고랑로 119\n143220.0\n4944.0\n127.088279\n37.559048\n\n\n\n\n1 rows × 29 columns\n\n\n\n\nfor n in df_seoul_hospital.index:\n    name = df_seoul_hospital.loc[n, \"상호명\"]\n    address = df_seoul_hospital.loc[n, \"도로명주소\"]\n    popup = f\"{name}-{address}\"\n    location = [df_seoul_hospital.loc[n, \"위도\"], df_seoul_hospital.loc[n, \"경도\"]]\n    folium.Marker(\n        location = location,\n        popup = popup,\n    ).add_to(map)\nmap\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/Study/boostcourse/2. 데이터 분석 준비하기.html",
    "href": "posts/Study/boostcourse/2. 데이터 분석 준비하기.html",
    "title": "3: 데이터 분석 준비하기",
    "section": "",
    "text": "Zen of Python\n\n파이썬의 철학이 잘 담겨있는 Zen of Python을 출력해 봅니다. (아래의 실습을 통해 확인해 보세요!)\n\n\nimport this # improt를 통해 파이썬의 라이브러리나 패키지를 가져올 수 있음\n\n\n\nboolean\n\n파이썬에서는 명시적인 것이 암시적인 것보다 낫다라는 철학이 있습니다.\nTrue나 False는 0과 1로도 표현할 수 있으나 명시적으로 표현하기 위해 True와 False를 사용합니다.\n\n\nTrue\n\nTrue\n\n\n\nFalse\n\nFalse\n\n\n\nTrue == 1\n\nTrue\n\n\n\nFalse == 0\n\nTrue\n\n\n\nTrue == \"1\" # True와 문자 1과는 다르다! 1따옴포=문자열\n\nFalse\n\n\n\nTrue != \"1\"\n\nTrue\n\n\n\nFalse == \"0\"\n\nFalse\n\n\n\nFalse != \"0\"\n\nTrue\n\n\n\nTrue and True\n\nTrue\n\n\n\nTrue and False\n\nFalse\n\n\n\nTrue or False #or연산자: 하나만 true여도 true\n\nTrue\n\n\n\n\nnumber and String\n\n숫자 1과 문자 “1”은 다르다! 데이터 타입 “type” 사용\n\n\n\"1\"\n\n'1'\n\n\n\ntype(1)\n\nint\n\n\n\ntype(\"1\")\n\nstr\n\n\n\n\nStrings and Lists\n\ntil = \"Today I learned\"\ntil\n\n'Today I learned'\n\n\n\ntil.lower() #다 소문자로 변경\n\n'today i learned'\n\n\n\ntil.upper() #대문자 변경\n\n'TODAY I LEARNED'\n\n\n\n# 비어있는 리스트 만들기. lang라는 변수에 담기\nlang = []\nlang\n\n[]\n\n\n\nlang.append(\"python\")\nlang.append(\"java\")\nlang.append(\"c\")\nlang\n\n['python', 'java', 'c']\n\n\n\nlang[0] #lnag이라는 변수에 담겨있는 언어명을 인덱싱을 통해 가져오기\n\n'python'\n\n\n\nlang[1]\n\n'java'\n\n\n\nlang[-1]\n\n'c'\n\n\n\n\nControl Flow\n\n제어문-조건문, 반복문\n\n\nfor i in lang:\n    print(i)\n\npython\njava\nc\n\n\n\nfor i in lang:\n    if i == \"python\":\n        print(\"python\")\n    else:\n            print(\"기타\") # indent를 맞춰줘야 한다.\n\npython\n기타\n기타\n\n\n\n# 특정 횟수만큼 반복문 실행\ncount = len(lang)\nfor i in range(count):\n    print(lang[i]) \n\npython\njava\nc\n\n\n\n# 짝수일때 python을 홀수일때 java출력\nfor i in range(1,10): # `1에서 9까지\n    if i % 2 == 0: #짝수만 출력\n        print(\"python\")\n    else:\n        print(\"java\")\n\njava\npython\njava\npython\njava\npython\njava\npython\njava\n\n\n\n# enumerate를 사용하면 인덱스 번호와 원소를 같이 가져온다\nfor i, val in enumerate(lang):\n    print(i,val)\n\n0 python\n1 java\n2 c\n\n\n\n문자열\n\naddress = \" 경기도 성남시 분당구 불정로 6 NAVER 그린팩토리 16층 \"\naddress\n\n' 경기도 성남시 분당구 불정로 6 NAVER 그린팩토리 16층 '\n\n\n\n# 앞뒤 공백 제거 \n# 데이터 전처리 시 주로 사용\naddress = address.strip() \naddress\n\n'경기도 성남시 분당구 불정로 6 NAVER 그린팩토리 16층'\n\n\n\n# 문자열 길이\nlen(address)\n\n33\n\n\n\n# 공백으로 문자열 분리\naddress_list = address.split()\naddress_list\n\n['경기도', '성남시', '분당구', '불정로', '6', 'NAVER', '그린팩토리', '16층']\n\n\n\n\n슬라이싱, startswith, in 을 통해 문자열에 경기아 있는지 확인하기\n\naddress[:2]\n\n'경기'\n\n\n\naddress.startswith(\"경기\")\n\nTrue\n\n\n\n\"경기\" in address \n\nTrue\n\n\n\n\n리스트\n\n문자열에서 쓰는 방법과 비슷한 메소드 등을 사용\n\n\naddress_list[2]\n\n'분당구'\n\n\n\nstreet = address_list[3]\nstreet\n\n'불정로'\n\n\n\naddress_list[-1]\n\n'16층'\n\n\n\n# \"\".join(리스트)를 사용하면 리스트를 공백 문자열로 연결 한다\n\"-\".join(address_list)\n\n'경기도-성남시-분당구-불정로-6-NAVER-그린팩토리-16층'\n\n\n\n\"경기\" in address_list\n\nFalse\n\n\n\n\"경기도\" in address_list\n\nTrue\n\n\n\n\"분당구\" in address_list\n\nTrue\n\n\n\n# pandas (패널 데이터 앤 시스템의 약자)\n# 수식과 그래프를 통해 계산 및 시각화하는 도구\n# 엑셀을 사용 했을때보다 대용량 데이터를 쓸수 있고 소스 코드 파일만 데이터 프레임 로드를 해서 소스코드들을 재사용 할 수 있다.\n# 월별 작업과 같은 반복작업.. \n# 아래에 첨부된 10 minutes to pandas를 한 번씩 실행해보시면 판다스의 전반적인 것을 익힐 수 있습니다.\n\n\n# 추가로 같이 첨부된 Pandas Cheat Sheet도 추천드립니다.\n\n\nimport pandas as pd\n\n\npd.DataFrame?\n\n\n# 공식문서 (도움말) 활용하기\npd.DataFrame()\n# shift+tab+tab\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataFrame\n\ndf = pd.DataFrame(\n{\"a\" : [4, 5, 6, 6],\n\"b\" : [7, 8, 9, 9],\n\"c\" : [10, 11, 12, 12]},\nindex = [1, 2, 3, 4])\ndf\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n1\n4\n7\n10\n\n\n2\n5\n8\n11\n\n\n3\n6\n9\n12\n\n\n4\n6\n9\n12\n\n\n\n\n\n\n\n\n\nSeries\n\n# 1차원 자료구조.. (벡터!)\ndf[\"a\"]\n# 출력된 형태: series 데이터\n\n1    4\n2    5\n3    6\nName: a, dtype: int64\n\n\n\n# dataframe으로 변경 (2차원 자료구조) (행렬!)\ndf[[\"a\"]]\n\n\n\n\n\n\n\n\na\n\n\n\n\n1\n4\n\n\n2\n5\n\n\n3\n6\n\n\n\n\n\n\n\n\n\nSubset\n\ndf[df[\"a\"] &gt; 4]\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n2\n5\n8\n11\n\n\n3\n6\n9\n12\n\n\n\n\n\n\n\n\ndf[\"a\"]\n\n1    4\n2    5\n3    6\nName: a, dtype: int64\n\n\n\ndf[[\"a\",\"b\"]]\n# df[\"a\",\"b\"] 이렇게 쓰면 오류가 난다. 대괄호를 또 해주기 \n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n1\n4\n7\n\n\n2\n5\n8\n\n\n3\n6\n9\n\n\n\n\n\n\n\n\n\nSummarize Data\n\ndf[\"a\"].value_counts()\n# 빈도수 계산\n\n6    2\n4    1\n5    1\nName: a, dtype: int64\n\n\n\nlen(df)\n\n4\n\n\n\n\nReahaping\n\nsort_values, drop\n\n\ndf.sort_values(\"a\",ascending=False)\n#ascending= 역수 \n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n3\n6\n9\n12\n\n\n4\n6\n9\n12\n\n\n2\n5\n8\n11\n\n\n1\n4\n7\n10\n\n\n\n\n\n\n\n\ndf = df.drop([\"c\"], axis=1)\n# 기본 설정은 axis=0 으로 되어있으므로 axis를 바꿔줘야 함 \ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n1\n4\n7\n\n\n2\n5\n8\n\n\n3\n6\n9\n\n\n4\n6\n9\n\n\n\n\n\n\n\n\n\nGroup Data\n\nGroupby, pivot_table\n\n\n# \"a\" 컬럼값을 Groupby하여 \"b\"의 컬럼값 구하기\ndf.groupby([\"a\"])[\"b\"].mean()\n\na\n4    7.0\n5    8.0\n6    9.0\nName: b, dtype: float64\n\n\n\ndf.groupby([\"a\"])[\"b\"].agg([\"mean\", \"sum\", \"count\"])\n\n\n\n\n\n\n\n\nmean\nsum\ncount\n\n\na\n\n\n\n\n\n\n\n4\n7.0\n7\n1\n\n\n5\n8.0\n8\n1\n\n\n6\n9.0\n18\n2\n\n\n\n\n\n\n\n\ndf.groupby([\"a\"])[\"b\"].describe() #요약하는거\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\na\n\n\n\n\n\n\n\n\n\n\n\n\n4\n1.0\n7.0\nNaN\n7.0\n7.0\n7.0\n7.0\n7.0\n\n\n5\n1.0\n8.0\nNaN\n8.0\n8.0\n8.0\n8.0\n8.0\n\n\n6\n2.0\n9.0\n0.0\n9.0\n9.0\n9.0\n9.0\n9.0\n\n\n\n\n\n\n\n\npd.pivot_table(df, index=\"a\", values=\"b\", aggfunc=\"sum\")\n\n\n\n\n\n\n\n\nb\n\n\na\n\n\n\n\n\n4\n7\n\n\n5\n8\n\n\n6\n18\n\n\n\n\n\n\n\n\n\nPlotting\n\ndf.plot.bar()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\n!conda env list\n\n# conda environments:\n#\nbase                     /home/koinup4/anaconda3\npy37                  *  /home/koinup4/anaconda3/envs/py37\npy39                     /home/koinup4/anaconda3/envs/py39"
  },
  {
    "objectID": "posts/Study/imbalaced data/imbalaced data.html",
    "href": "posts/Study/imbalaced data/imbalaced data.html",
    "title": "imbalanced data",
    "section": "",
    "text": "https://imbalanced-learn.org/stable/references/index.html#api\nhttps://towardsdatascience.com/imbalanced-classification-in-python-smote-tomek-links-method-6e48dfe69bbc\nhttps://www.kaggle.com/code/rafjaa/resampling-strategies-for-imbalanced-datasets/notebook"
  },
  {
    "objectID": "posts/Study/imbalaced data/imbalaced data.html#generate-the-dataset",
    "href": "posts/Study/imbalaced data/imbalaced data.html#generate-the-dataset",
    "title": "imbalanced data",
    "section": "Generate the dataset",
    "text": "Generate the dataset\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=200, shuffle=True, noise=0.5, random_state=10)\nX = pd.DataFrame(X, columns=[\"feature 1\", \"feature 2\"])\nax = X.plot.scatter(\n    x=\"feature 1\",\n    y=\"feature 2\",\n    c=y,\n    colormap=\"viridis\",\n    colorbar=False,\n)\nsns.despine(ax=ax, offset=10)\nplt.tight_layout()"
  },
  {
    "objectID": "posts/Study/imbalaced data/imbalaced data.html#make-a-dataset-imbalanced",
    "href": "posts/Study/imbalaced data/imbalaced data.html#make-a-dataset-imbalanced",
    "title": "imbalanced data",
    "section": "Make a dataset imbalanced",
    "text": "Make a dataset imbalanced\n\n# pip install imblearn\n\n\nfrom collections import Counter\n\n\ndef ratio_func(y, multiplier, minority_class):\n    target_stats = Counter(y)\n    return {minority_class: int(multiplier * target_stats[minority_class])}\n\n\nfrom imblearn.datasets import make_imbalance\n\nfig, axs = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n\nX.plot.scatter(\n    x=\"feature 1\",\n    y=\"feature 2\",\n    c=y,\n    ax=axs[0, 0],\n    colormap=\"viridis\",\n    colorbar=False,\n)\naxs[0, 0].set_title(\"Original set\")\nsns.despine(ax=axs[0, 0], offset=10)\n\nmultipliers = [0.9, 0.75, 0.5, 0.25, 0.1]\nfor ax, multiplier in zip(axs.ravel()[1:], multipliers):\n    X_resampled, y_resampled = make_imbalance(\n        X,\n        y,\n        sampling_strategy=ratio_func,\n        **{\"multiplier\": multiplier, \"minority_class\": 1},\n    )\n    X_resampled.plot.scatter(\n        x=\"feature 1\",\n        y=\"feature 2\",\n        c=y_resampled,\n        ax=ax,\n        colormap=\"viridis\",\n        colorbar=False,\n    )\n    ax.set_title(f\"Sampling ratio = {multiplier}\")\n    sns.despine(ax=ax, offset=10)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/Study/imbalaced data/imbalaced data.html#effect-of-the-shrinkage-factor-in-random-over-sampling",
    "href": "posts/Study/imbalaced data/imbalaced data.html#effect-of-the-shrinkage-factor-in-random-over-sampling",
    "title": "imbalanced data",
    "section": "Effect of the shrinkage factor in random over-sampling",
    "text": "Effect of the shrinkage factor in random over-sampling\n\nfrom collections import Counter\n\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_samples=100,\n    n_features=2,\n    n_redundant=0,\n    weights=[0.1, 0.9],\n    random_state=0,\n)\nCounter(y)\n\nCounter({1: 90, 0: 10})\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(7, 7))\nscatter = plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.4)\nclass_legend = ax.legend(*scatter.legend_elements(), loc=\"lower left\", title=\"Classes\")\nax.add_artist(class_legend)\nax.set_xlabel(\"Feature #1\")\n_ = ax.set_ylabel(\"Feature #2\")\nplt.tight_layout()\n\n\n\n\n\nfrom imblearn.over_sampling import RandomOverSampler\n\nsampler = RandomOverSampler(random_state=0)\nX_res, y_res = sampler.fit_resample(X, y)\nCounter(y_res)\n\nCounter({1: 90, 0: 90})\n\n\n\n부트스트랩 사용\n\n\nfig, ax = plt.subplots(figsize=(7, 7))\nscatter = plt.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.4)\nclass_legend = ax.legend(*scatter.legend_elements(), loc=\"lower left\", title=\"Classes\")\nax.add_artist(class_legend)\nax.set_xlabel(\"Feature #1\")\n_ = ax.set_ylabel(\"Feature #2\")\nplt.tight_layout()\n\n\n\n\n\nsampler = RandomOverSampler(shrinkage=1, random_state=0)\nX_res, y_res = sampler.fit_resample(X, y)\nCounter(y_res)\n\nCounter({1: 90, 0: 90})\n\n\n\nfig, ax = plt.subplots(figsize=(7, 7))\nscatter = plt.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.4)\nclass_legend = ax.legend(*scatter.legend_elements(), loc=\"lower left\", title=\"Classes\")\nax.add_artist(class_legend)\nax.set_xlabel(\"Feature #1\")\n_ = ax.set_ylabel(\"Feature #2\")\nplt.tight_layout()\n\n\n\n\n\nsampler = RandomOverSampler(shrinkage=3, random_state=0)\nX_res, y_res = sampler.fit_resample(X, y)\nCounter(y_res)\n\nCounter({1: 90, 0: 90})\n\n\n\nfig, ax = plt.subplots(figsize=(7, 7))\nscatter = plt.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.4)\nclass_legend = ax.legend(*scatter.legend_elements(), loc=\"lower left\", title=\"Classes\")\nax.add_artist(class_legend)\nax.set_xlabel(\"Feature #1\")\n_ = ax.set_ylabel(\"Feature #2\")\nplt.tight_layout()\n\n\n\n\n\nsampler = RandomOverSampler(shrinkage=0, random_state=0)\nX_res, y_res = sampler.fit_resample(X, y)\nCounter(y_res)\n\nCounter({1: 90, 0: 90})\n\n\n\nfig, ax = plt.subplots(figsize=(7, 7))\nscatter = plt.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.4)\nclass_legend = ax.legend(*scatter.legend_elements(), loc=\"lower left\", title=\"Classes\")\nax.add_artist(class_legend)\nax.set_xlabel(\"Feature #1\")\n_ = ax.set_ylabel(\"Feature #2\")\nplt.tight_layout()"
  },
  {
    "objectID": "posts/Study/imbalaced data/imbalaced data.html#sample-generator-used-in-smote-like-samplers",
    "href": "posts/Study/imbalaced data/imbalaced data.html#sample-generator-used-in-smote-like-samplers",
    "title": "imbalanced data",
    "section": "Sample generator used in SMOTE-like samplers",
    "text": "Sample generator used in SMOTE-like samplers\n\nprint(__doc__)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nsns.set_context(\"poster\")\n\nrng = np.random.RandomState(18)\n\nf, ax = plt.subplots(figsize=(8, 8))\n\n# generate some data points\ny = np.array([3.65284, 3.52623, 3.51468, 3.22199, 3.21])\nz = np.array([0.43, 0.45, 0.6, 0.4, 0.211])\ny_2 = np.array([3.3, 3.6])\nz_2 = np.array([0.58, 0.34])\n\n# plot the majority and minority samples\nax.scatter(z, y, label=\"Minority class\", s=100)\nax.scatter(z_2, y_2, label=\"Majority class\", s=100)\n\nidx = rng.randint(len(y), size=2)\nannotation = [r\"$x_i$\", r\"$x_{zi}$\"]\n\nfor a, i in zip(annotation, idx):\n    ax.annotate(a, (z[i], y[i]), xytext=tuple([z[i] + 0.01, y[i] + 0.005]), fontsize=15)\n\n# draw the circle in which the new sample will generated\nradius = np.sqrt((z[idx[0]] - z[idx[1]]) ** 2 + (y[idx[0]] - y[idx[1]]) ** 2)\ncircle = plt.Circle((z[idx[0]], y[idx[0]]), radius=radius, alpha=0.2)\nax.add_artist(circle)\n\n# plot the line on which the sample will be generated\nax.plot(z[idx], y[idx], \"--\", alpha=0.5)\n\n# create and plot the new sample\nstep = rng.uniform()\ny_gen = y[idx[0]] + step * (y[idx[1]] - y[idx[0]])\nz_gen = z[idx[0]] + step * (z[idx[1]] - z[idx[0]])\n\nax.scatter(z_gen, y_gen, s=100)\nax.annotate(\n    r\"$x_{new}$\",\n    (z_gen, y_gen),\n    xytext=tuple([z_gen + 0.01, y_gen + 0.005]),\n    fontsize=15,\n)\n\n# make the plot nicer with legend and label\nsns.despine(ax=ax, offset=10)\nax.set_xlim([0.2, 0.7])\nax.set_ylim([3.2, 3.7])\nplt.xlabel(r\"$X_1$\")\nplt.ylabel(r\"$X_2$\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nAutomatically created module for IPython interactive environment"
  },
  {
    "objectID": "posts/Study/imbalaced data/imbalaced data.html#illustration-of-the-definition-of-a-tomek-link",
    "href": "posts/Study/imbalaced data/imbalaced data.html#illustration-of-the-definition-of-a-tomek-link",
    "title": "imbalanced data",
    "section": "Illustration of the definition of a Tomek link",
    "text": "Illustration of the definition of a Tomek link\n\nprint(__doc__)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_context(\"poster\")\n\nAutomatically created module for IPython interactive environment\n\n\n\ndef make_plot_despine(ax):\n    sns.despine(ax=ax, offset=10)\n    ax.set_xlim([0, 3])\n    ax.set_ylim([0, 3])\n    ax.set_xlabel(r\"$X_1$\")\n    ax.set_ylabel(r\"$X_2$\")\n    ax.legend(loc=\"lower right\")\n\n\nimport numpy as np\n\nrng = np.random.RandomState(18)\n\nX_minority = np.transpose(\n    [[1.1, 1.3, 1.15, 0.8, 0.55, 2.1], [1.0, 1.5, 1.7, 2.5, 0.55, 1.9]]\n)\nX_majority = np.transpose(\n    [\n        [2.1, 2.12, 2.13, 2.14, 2.2, 2.3, 2.5, 2.45],\n        [1.5, 2.1, 2.7, 0.9, 1.0, 1.4, 2.4, 2.9],\n    ]\n)\n\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.scatter(\n    X_minority[:, 0],\n    X_minority[:, 1],\n    label=\"Minority class\",\n    s=200,\n    marker=\"_\",\n)\nax.scatter(\n    X_majority[:, 0],\n    X_majority[:, 1],\n    label=\"Majority class\",\n    s=200,\n    marker=\"+\",\n)\n\n# highlight the samples of interest\nax.scatter(\n    [X_minority[-1, 0], X_majority[1, 0]],\n    [X_minority[-1, 1], X_majority[1, 1]],\n    label=\"Tomek link\",\n    s=200,\n    alpha=0.3,\n)\nmake_plot_despine(ax)\nfig.suptitle(\"Illustration of a Tomek link\")\nfig.tight_layout()\n\n\n\n\n\nfrom imblearn.under_sampling import TomekLinks\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n\nsamplers = {\n    \"Removing only majority samples\": TomekLinks(sampling_strategy=\"auto\"),\n    \"Removing all samples\": TomekLinks(sampling_strategy=\"all\"),\n}\n\nfor ax, (title, sampler) in zip(axs, samplers.items()):\n    X_res, y_res = sampler.fit_resample(\n        np.vstack((X_minority, X_majority)),\n        np.array([0] * X_minority.shape[0] + [1] * X_majority.shape[0]),\n    )\n    ax.scatter(\n        X_res[y_res == 0][:, 0],\n        X_res[y_res == 0][:, 1],\n        label=\"Minority class\",\n        s=200,\n        marker=\"_\",\n    )\n    ax.scatter(\n        X_res[y_res == 1][:, 0],\n        X_res[y_res == 1][:, 1],\n        label=\"Majority class\",\n        s=200,\n        marker=\"+\",\n    )\n\n    # highlight the samples of interest\n    ax.scatter(\n        [X_minority[-1, 0], X_majority[1, 0]],\n        [X_minority[-1, 1], X_majority[1, 1]],\n        label=\"Tomek link\",\n        s=200,\n        alpha=0.3,\n    )\n\n    ax.set_title(title)\n    make_plot_despine(ax)\nfig.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "posts/Study/imbalaced data/imbalaced data.html#sample-selection-in-nearmiss",
    "href": "posts/Study/imbalaced data/imbalaced data.html#sample-selection-in-nearmiss",
    "title": "imbalanced data",
    "section": "Sample selection in NearMiss",
    "text": "Sample selection in NearMiss\n\nprint(__doc__)\n\nimport seaborn as sns\n\nsns.set_context(\"poster\")\n\nAutomatically created module for IPython interactive environment\n\n\n\ndef make_plot_despine(ax):\n    sns.despine(ax=ax, offset=10)\n    ax.set_xlim([0, 3.5])\n    ax.set_ylim([0, 3.5])\n    ax.set_xticks(np.arange(0, 3.6, 0.5))\n    ax.set_yticks(np.arange(0, 3.6, 0.5))\n    ax.set_xlabel(r\"$X_1$\")\n    ax.set_ylabel(r\"$X_2$\")\n    ax.legend(loc=\"upper left\", fontsize=16)\n\n\nimport numpy as np\n\nrng = np.random.RandomState(18)\n\nX_minority = np.transpose(\n    [[1.1, 1.3, 1.15, 0.8, 0.8, 0.6, 0.55], [1.0, 1.5, 1.7, 2.5, 2.0, 1.2, 0.55]]\n)\nX_majority = np.transpose(\n    [\n        [2.1, 2.12, 2.13, 2.14, 2.2, 2.3, 2.5, 2.45],\n        [1.5, 2.1, 2.7, 0.9, 1.0, 1.4, 2.4, 2.9],\n    ]\n)\n\n\nNearMiss-1\n\n가장 가까운 이웃의 평균 거리가 가장 작은 다수 클래스에서 표본 선택\n3-NN사용하여 특정 샘플 2개에 대한 평균 거리 계산\n평균거리가 더 작아 녹색 점선으로 연결된 점 선택\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import NearestNeighbors\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.scatter(\n    X_minority[:, 0],\n    X_minority[:, 1],\n    label=\"Minority class\",\n    s=200,\n    marker=\"_\",\n)\nax.scatter(\n    X_majority[:, 0],\n    X_majority[:, 1],\n    label=\"Majority class\",\n    s=200,\n    marker=\"+\",\n)\n\nnearest_neighbors = NearestNeighbors(n_neighbors=3)\nnearest_neighbors.fit(X_minority)\ndist, ind = nearest_neighbors.kneighbors(X_majority[:2, :])\ndist_avg = dist.sum(axis=1) / 3\n\nfor positive_idx, (neighbors, distance, color) in enumerate(\n    zip(ind, dist_avg, [\"g\", \"r\"])\n):\n    for make_plot, sample_idx in enumerate(neighbors):\n        ax.plot(\n            [X_majority[positive_idx, 0], X_minority[sample_idx, 0]],\n            [X_majority[positive_idx, 1], X_minority[sample_idx, 1]],\n            \"--\" + color,\n            alpha=0.3,\n            label=f\"Avg. dist.={distance:.2f}\" if make_plot == 0 else \"\",\n        )\nax.set_title(\"NearMiss-1\")\nmake_plot_despine(ax)\nplt.tight_layout()\n\n\n\n\n\n\nNearMiss-2\n\n가장 먼 이웃의 평균 거리가 가장 작은 샘플 선택\n멀리 있는 세 이웃의 거리가 가장 작은 녹색 점 선택\n\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.scatter(\n    X_minority[:, 0],\n    X_minority[:, 1],\n    label=\"Minority class\",\n    s=200,\n    marker=\"_\",\n)\nax.scatter(\n    X_majority[:, 0],\n    X_majority[:, 1],\n    label=\"Majority class\",\n    s=200,\n    marker=\"+\",\n)\n\nnearest_neighbors = NearestNeighbors(n_neighbors=X_minority.shape[0])\nnearest_neighbors.fit(X_minority)\ndist, ind = nearest_neighbors.kneighbors(X_majority[:2, :])\ndist = dist[:, -3::]\nind = ind[:, -3::]\ndist_avg = dist.sum(axis=1) / 3\n\nfor positive_idx, (neighbors, distance, color) in enumerate(\n    zip(ind, dist_avg, [\"g\", \"r\"])\n):\n    for make_plot, sample_idx in enumerate(neighbors):\n        ax.plot(\n            [X_majority[positive_idx, 0], X_minority[sample_idx, 0]],\n            [X_majority[positive_idx, 1], X_minority[sample_idx, 1]],\n            \"--\" + color,\n            alpha=0.3,\n            label=f\"Avg. dist.={distance:.2f}\" if make_plot == 0 else \"\",\n        )\nax.set_title(\"NearMiss-2\")\nmake_plot_despine(ax)\nplt.tight_layout()\n\n\n\n\n\n\nNearMiss-3\n\n가장 가까운 이웃은 다수 클래스의 샘플을 short-list 하는데 사용\n가장 가까운 이웃의 평균 거리가 가장 큰 표본 선택\n\n\nfig, ax = plt.subplots(figsize=(8.5, 8.5))\nax.scatter(\n    X_minority[:, 0],\n    X_minority[:, 1],\n    label=\"Minority class\",\n    s=200,\n    marker=\"_\",\n)\nax.scatter(\n    X_majority[:, 0],\n    X_majority[:, 1],\n    label=\"Majority class\",\n    s=200,\n    marker=\"+\",\n)\n\nnearest_neighbors = NearestNeighbors(n_neighbors=3)\nnearest_neighbors.fit(X_majority)\n\n# select only the majority point of interest\nselected_idx = nearest_neighbors.kneighbors(X_minority, return_distance=False)\nX_majority = X_majority[np.unique(selected_idx), :]\nax.scatter(\n    X_majority[:, 0],\n    X_majority[:, 1],\n    label=\"Short-listed samples\",\n    s=200,\n    alpha=0.3,\n    color=\"g\",\n)\nnearest_neighbors = NearestNeighbors(n_neighbors=3)\nnearest_neighbors.fit(X_minority)\ndist, ind = nearest_neighbors.kneighbors(X_majority[:2, :])\ndist_avg = dist.sum(axis=1) / 3\n\nfor positive_idx, (neighbors, distance, color) in enumerate(\n    zip(ind, dist_avg, [\"r\", \"g\"])\n):\n    for make_plot, sample_idx in enumerate(neighbors):\n        ax.plot(\n            [X_majority[positive_idx, 0], X_minority[sample_idx, 0]],\n            [X_majority[positive_idx, 1], X_minority[sample_idx, 1]],\n            \"--\" + color,\n            alpha=0.3,\n            label=f\"Avg. dist.={distance:.2f}\" if make_plot == 0 else \"\",\n        )\nax.set_title(\"NearMiss-3\")\nmake_plot_despine(ax)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/Study/tutorial_hand_on.html",
    "href": "posts/Study/tutorial_hand_on.html",
    "title": "TDA tutorial",
    "section": "",
    "text": "https://github.com/SHlee-TDA/PNU_TDA_TUTORIAL\n\n\n# !pip install -r requirements.txt\n\n\n\n#!pip install ipykernel\n#!pip install jupyter\n#!pip install numpy\n#!pip install gudhi\n#!pip install matplotlib\n#!pip install pandas\n#!pip install plotly\n#!pip install nbformat\n#!pip install scikit-learn"
  },
  {
    "objectID": "posts/Study/tutorial_hand_on.html#introduction",
    "href": "posts/Study/tutorial_hand_on.html#introduction",
    "title": "TDA tutorial",
    "section": "1. Introduction",
    "text": "1. Introduction\n위상수학적 데이터 분석(Topological Data Analysis; TDA)는 대수위상수학적 도구를 사용하여, 데이터가 가진 기하학적 특징을 분석하고 활용하는 새로운 데이터 분석 기법입니다.\nTDA는 기존에 통계학과 같이 빈도주의에 따른 기법들에서 쉽사리 분석하기 힘들었던 데이터의 기하학적 구조를 분석하는 도구들을 제공합니다.\n현재 TDA는 의료 이미지 분석, 네트워크 분석, 재료공학 구조 분석, 시계열 분석, 중력파 분석, 머신러닝, 딥러닝 등에 활발히 응용되고 있습니다.\n이번 튜토리얼에서는 TDA를 활용해 데이터에서 기하학적 특징을 추출하고, 그것을 머신러닝에 응용하는 방법에 대하여 배워보도록 하겠습니다."
  },
  {
    "objectID": "posts/Study/tutorial_hand_on.html#persistent-homology",
    "href": "posts/Study/tutorial_hand_on.html#persistent-homology",
    "title": "TDA tutorial",
    "section": "2. Persistent Homology",
    "text": "2. Persistent Homology\n\n2.1) Motivation\n\n\n\nGeorges Seurat, A Sunday on La Grande Jatte, 1884-1886.\n\n\n위 그림은 조르주 쇠라의 &lt;그랑드자트 섬의 일요일 오후&gt;라는 그림입니다.\n이 그림은 멀리서 보기엔 연속적인 곡선과 면으로 이루어진 그림 같지만, 자세히 보면 이 그림은 무수히 많은 점들로 이루어진 그림입니다.\n사람의 눈과 뇌는 이 무수히 많은 점을 보면서 이산적으로 느끼기보다는 연속적인 곡선과 색감으로 인식합니다.\n만일, 우리가 인간을 닮은 인공지능을 개발한다고 하면 이러한 인간의 능력을 컴퓨터에게 부여하는 일은 굉장히 중요할 수 있습니다.\n컴퓨터는 모든 데이터를 이산적으로 받아들이기 때문입니다.\n어떻게 하면 컴퓨터가 이산적인 데이터로부터 연속적인 성질을 인식하도록 할 수 있을까요?\n\nBridge between Discreteness and Continuousness\n위상수학적 데이터 분석은 Persistent homology(PH)라는 도구를 사용해 이산적인 데이터로부터 데이터의 기하학적 성질을 추론합니다.\nPH의 엄밀한 정의와 성질을 설명하기에 앞서서, 데이터의 기하학적 특징을 추출한다는 것을 살펴보고자 합니다.\n이를 위해서는 데이터가 가진 성질과 기하학이 가진 성질 사이의 차이를 이해할 필요가 있습니다.\n\n\n\n컴퓨터가 보는 고양이 사진. (이미지 출처 : InData Labs )\n\n\n이미지 데이터를 생각해봅시다.\n이미지 속의 강아지나 고양이는 본래 실세계에선 연속적인 형태를 띄는 대상입니다.\n그렇기 때문에 우리는 강아지와 고양이의 모양이 가지는 기하학적 성질을 수학적으로 잘 정의하고 계산할 수 있습니다.\n그러나, 이미지 데이터는 실세계에 있던 연속적인 대상을 이산적으로 샘플링 한 것이라고 볼 수 있습니다.\n이미지 데이터는 픽셀에 실세계 대상이 가지고 있던 시각적 정보를 요약하고 압축하여 저장합니다.\n그러므로 데이터화된 이미지에서 본래 사진 안에 있던 강아지와 고양이가 가진 기하학적 성질을 수학적으로 정의하고 계산하기란, 그리고 그것이 대량의 데이터에 일관되게 적용되는 방법을 생각하는 일은 쉬운 일이 아닙니다.\n요컨대 데이터는 이산적인 표본의 성질을 띄며, 기하학은 연속적인 성질을 띄기 때문에 이들 각각을 다루는 수학적인 방법에 차이가 있습니다.\n그럼에도 불구하고 사람에게는 이산적인 정보로부터 연속적인 대상을 생각하고 그 성질을 인식하는 사고능력이 있습니다.\n이러한 사고과정을 이산적인 데이터가 사실은 매끄러운 기하학적 대상(다양체; Manifold)으로부터 표본추출된 것이라고 생각하고, 매끄러운 기하학적 대상이 가지고 있던 성질을 이산적인 데이터로부터 추론하는 것이라고 이해할 수 있습니다.\n이러한 방법론을 기하학적 추론(Geometric inference)라고 합니다.\n위상수학적 데이터 분석은 Persistent homology를 이용해 기하학적 추론을 수행합니다.\n\n\nGeometric Inference\n\n\n\n기하학적 추론과 통계적 추론의 비교\n\n\n기하학적 추론은 통계학에서 표본을 통해 모집단의 성질을 추론하는 통계적 추론(Statistical inference)와 비교한다면, 그 철학을 이해하기가 쉬울 것입니다.\n통계학에서는 표본평균과 표본표준편차와 같은 통계량으로부터 모집단의 모평균과 모표준편차와 같은 모수를 추론했던 것과 유사하게, 위상수학적 데이터 분석에서는 데이터의 PH로부터 모집단 다양체가 가지는 호몰로지를 추론합니다.\n호몰로지가 매끄러운 다양체의 성질을 이해하고 분류하는 데에 중요한 역할을 한다는 것은 수학자들에게 익히 알려진 사실입니다.\n그러므로, 이산적인 데이터로부터 호몰로지를 추정하는데 사용되는 Persistent homology는 호몰로지와 마찬가지로 데이터의 기하학적 성질을 이해하고 데이터를 기하학적인 구조로 분류하는 데에 중요한 역할을 할 수 있다고 생각할 수 있습니다.\n\n\n\n2.2) Definition\n\nScale selection issue\n위상수학은 연결성(connectedness)에 대해 연구하는 수학이라고 할 수 있습니다.\n연결성은 위상공간의 연결성분(connected component), 구멍(loop), 빈 공간(void) 그리고 이것들의 일반화된 구멍의 개수로 나타내어 집니다.\n이러한 정보를 잘 담고 있는 수학적 대상이 위상공간의 호몰로지(homology)입니다.\n그러나 데이터에서 호몰로지를 이용해 이러한 정보를 추출하고자 하면, 데이터들은 이산적으로 모두 떨어져(disconnected)있기 때문에 유용한 정보를 얻기 어렵습니다.\n그래서 기존의 사람들은 데이터들을 적당한 거리 파라미터 \\(\\epsilon\\)을 기준으로, \\(\\epsilon\\)이내에 있는 점들끼리 연결한 후 호몰로지를 계산해왔습니다.\n\n\nFiltration\n이 방법에는 한 가지 문제점이 있는데, 파라미터 \\(\\epsilon\\)의 값이 얼마일 때 데이터의 기하학적인 정보를 가장 잘 나타내는지에 대한 기준이 없었습니다.\n순전히 연구자의 주관과 경험에 따를 수 밖에 없었으며, 이는 빅데이터를 분석하는데에 굉장한 리스크가 되었습니다.\n\n\n\nPersistent homology는 필트레이션을 이용해 데이터로부터 심플리셜 컴플렉스의 진화과정을 만들고, 그 과정 중에 연결성분(파란 막대)이나 구멍(점선)의 지속성을 측정한다. [이미지 출처 : Figure 1 in Otter, Nina, et al. “A roadmap for the computation of persistent homology.” EPJ Data Science 6 (2017)]\n\n\nPersistent homology(PH)는 필트레이션(filtration)과 함자성(functoriality)를 이용해 이러한 약점을 극복합니다.\n먼저 파라미터 \\(\\epsilon\\)을 점점 증가시켜가면서 데이터를 연결해나가면, \\(\\epsilon\\)에 대한 심플리셜 컴플렉스의 축소나열(nested sequence) $X_{0} X{1} X{_n} $ 를 얻습니다.\n각 심플리셜 컴플렉스 \\(X_\\epsilon\\)이 가진 연결성에 대한 정보는 그것의 호몰로지 클래스(homology class)로 포함됩니다.\n필트레이션 안에서 호몰로지 클래스는 함자성(functoriality)에 의해 효과적으로 그 진화과정을 추적할 수 있습니다.\n함자성은 심플리셜 컴플렉스들의 축소나열 $X_{0} X{1} X{n} $를 연결하는 포함사상(inclusion map)을 호몰로지의 축소나열 $H_k(X{0}) H_k(X{1}) H_k(X{_n}) $로 자연스럽게 변환시킵니다.\n이렇게 얻은 호몰로지의 축소나열에서 \\(\\epsilon'&lt;\\epsilon\\)에 대하여, 사상 \\(f_{\\epsilon'}^{\\epsilon} : H_k(X_{\\epsilon'}) \\rightarrow H_k(X_{\\epsilon})\\)의 상 \\(\\text{im}{f_{\\epsilon'}^{\\epsilon}}\\)를 Persistent homology라고 정의합니다.\nPersistent homology \\(\\text{im}{f_{\\epsilon'}^{\\epsilon}}\\)는 \\(X_{\\epsilon'}\\)가 가진 호몰로지 클래스가 \\(X_\\epsilon\\)에서 지속되는지(persist) 또는 소멸하는지(death)를 추적할 수 있게 해줍니다.\n또한, 낮은 차원의 심플렉스들이 연결되면서 높은 차원의 새로운 연결성이 탄생(birth)한 것을 발견하게 해주기도 합니다.\n\n\nPersistence\n이렇게 하면 데이터가 가진 연결성이 어느 파라미터값에서 탄생하고 소멸했는지를 구간 \\([b,d]\\)로 나타낼 수 있습니다.\n이 구간들의 모임을 Persistence Barcode(PB)라고 부릅니다.\nPB에서 긴 수명(lifespan)를 가지는 호몰로지 클래스는 대부분의 파라미터 값에 대해서 데이터가 지니고 있는 내재적 기하학적 특성이므로, 이를 데이터가 추출된 다양체가 가졌던 호몰로지의 추정치로 사용할 수 있을 것입니다.\n반면에, 짧은 수명을 가진 호몰로지 클래스는 노이즈로 간주됩니다.\n이처럼 PH를 이용하면 손쉽게 파라미터의 의존성을 벗어난 데이터의 요약을 얻을 수 있습니다.\nPH의 장점은 이뿐만이 아닙니다.\nPH를 계산하는 과정은 선형대수학을 이용하여 수행할 수 있으므로, 자동화된 컴퓨팅을 이용한 데이터 분석에 적합합니다.\n뿐만 아니라, PH는 노이즈에 대해 견고한(robust)한 특징을 지니고 있습니다. [Cohen steiner, Stability of Persistence Diagrams, 2007]\n즉, 데이터에 노이즈가 다소 첨가되더라도 긴 수명을 가진 호몰로지 클래스에 대한 정보는 크게 변하지 않습니다.\n이러한 특성은 노이즈가 많은 현실의 데이터를 분석하는 데에 굉장히 유용한 역할을 합니다."
  },
  {
    "objectID": "posts/Study/tutorial_hand_on.html#programing-practice",
    "href": "posts/Study/tutorial_hand_on.html#programing-practice",
    "title": "TDA tutorial",
    "section": "3. Programing Practice",
    "text": "3. Programing Practice\n앞서 PH는 매끄러운 다양체가 가진 호몰로지의 추정량처럼 생각할 수 있다고 했습니다.\n다음 예제들은 호몰로지가 잘 알려진 대표적인 매끄러운 다양체들(구면, 토러스, 뫼비우스의 띠, 클레인 병)에서 샘플된 점들로부터 PH를 얻어볼 것입니다.\n이를통해 데이터로부터 얻은 PH가 어떤 방식으로 데이터의 기하학적 성질을 표현하는지 살펴볼 수 있을 것입니다.\n이 작업을 위해 Python의 TDA를 위한 라이브러리 gudhi를 사용할 것입니다.\n\n3.1) Point cloud\nPoint cloud 데이터는 3차원 공간 상에 샘플된 점들로 구성된 데이터입니다.\nLidar 센서와 같이 3차원 공간을 인식하는 기기로부터 얻은 데이터들이 주로 point cloud 형태로 저장됩니다.\n구면, 토러스, 뫼비우스의 띠, 클레인 병 모양의 실제 사물로부터 센서를 이용해 point cloud 데이터를 얻은 상황을 가정합시다.\n각 데이터는 1000~2000개 사이의 점을 샘플링 하여 얻어진 것이며, 기계이기 때문에 조그만 노이즈도 포함되어 있습니다.\n이런 데이터로부터 원본 사물의 모양을 특징지을 수 있는 정보를 만들어낼 수 있을까요?\n\n# Library import\nimport numpy as np\nimport gudhi\nimport matplotlib.pyplot as plt\nfrom utils.generate_data import PointCloud\nfrom utils.plotting import plot_point_cloud\n\ndata_generator = PointCloud(n_samples = 1, n_points = 'random', noise=0.05)\n\n# Generate point cloud data.\nS_data = data_generator.sphere()          # data from Sphere\nT_data = data_generator.torus()           # data from Torus\nM_data = data_generator.mobius()          # data from Mobius band\nK_data = data_generator.klein_bottle()    # data from Klein bottle\n\nModuleNotFoundError: No module named 'utils'\n\n\n\n# Visualization : Sphere\nplot_point_cloud(S_data[0])\n\nNameError: name 'plot_point_cloud' is not defined\n\n\n\n# Visualization : Torus\n\n\n# Visualization : Mobius band\n\n\n# Visualization : Klein bottle\n\n위 데이터 시각화에서 살펴볼 수 있듯, 데이터는 각 다양체들 위에서 샘플링 된 이산적인 점들의 집합입니다.\n우리의 눈으로는 이 점들이 어떠한 다양체의 기하학적 경향성을 띄는 것이 보일 것입니다.\nPH를 이용하면 이러한 경향성을 잘 요약하여 컴퓨터에 입력할 수 있는 자료형으로 나타낼 수 있습니다."
  },
  {
    "objectID": "posts/Study/tutorial_hand_on.html#persistence-barcode-and-diagrams",
    "href": "posts/Study/tutorial_hand_on.html#persistence-barcode-and-diagrams",
    "title": "TDA tutorial",
    "section": "3.2) Persistence Barcode and Diagrams",
    "text": "3.2) Persistence Barcode and Diagrams\ngudhi 라이브러리를 이용하면 다음의 단계를 거쳐서 point cloud 데이터로부터 PH를 계산하여 줍니다.\n\n립스 컴플렉스 생성 : gudhi.RipsComplex 함수 이용해 데이터 포인트들의 거리 파라미터를 점점 증가시켜나가며 연결시켜 나갑니다.\n심플렉스 트리 생성 : 저장된 립스 컴플렉스에 .create_simplex_tree 메서드를 사용하면 호몰로지가 생성되고 죽는 과정을 분석합니다.\n지속성 자료 생성 : 저장된 심플렉스 트리에 .persistence 메서드를 사용하면 심플렉스 트리로부터 Persistent homology를 계산하여 (호몰로지 차원, 생성시점, 소멸시점) 형태로 구성된 데이터를 만들어냅니다.\n자료 시각화 : Persistent homology가 담고 있는 정보를 Persistence Barcode 또는 Persistence diagram 형태로 알기 쉽게 시각화합니다. 이는 각각 gudhi.plot_persistence_barcode 함수와 gudhi.plot_persistence_diagram 함수로 구현됩니다.\n\n\nHomology of Sphere\n3차원 공간상의 구면 \\(S^2\\)의 호몰로지에 대한 랭크 (또는 Betti-number)는 아래와 같습니다.\n\\[\\text{rank}{H_0(S^2)} = 1, \\text{rank}{H_1(S^2)} = 0, \\text{rank}{H_2(S^2)} = 1\\]\n\n# Persistence barcode & diagram : Sphere\n\n# Sphere 데이터로부터 rips complex 자료형을 계산합니다.\nsphere_rips_complex = gudhi.RipsComplex(points=S_data[0], max_edge_length=2.0, sparse=0.5)\n\n# Rips complex를 분석합니다.\nsphere_simplex_tree = sphere_rips_complex.create_simplex_tree(max_dimension=3)\n\n# Persistent homology를 계산합니다.\nsphere_diag = sphere_simplex_tree.persistence(homology_coeff_field=2,\n                                              min_persistence=0.0)\n\n# 결과 시각화\nfig, axs = plt.subplots(nrows=2, ncols=1, figsize=(15,30))\ngudhi.plot_persistence_barcode(sphere_diag, axes=axs[0], legend=True)\ngudhi.plot_persistence_diagram(sphere_diag, axes=axs[1], legend=True)\nplt.show()\n\nusetex mode requires TeX.\n\n\n\n\n\nPersistence Barcode(PB)는 거리 파라미터가 증가할때마다 각 호몰로지 클래스들이 언제 탄생(birth)하고\n언제 소멸(death)하는지를 구간 \\([b,d]\\) 형태로 보여줍니다.\n각 구간의 길이는 개별 호몰로지 클래스의 수명(lifespan) 또는 지속성(persistence)을 나타냅니다.\n지속성이 긴 클래스는 다양체의 성질에서 얻어진 것이라 할 수 있고, 짧은 클래스는 노이즈에 의해 발생한 정보라고 할 수 있습니다.\nPersistence Diagram(PD)은 PB에서 각 구간들의 탄생시점(\\(b\\))과 소멸시점(\\(d\\))을 좌표평면에 좌표 \\((b,d)\\)로 나타낸 것으로, 두 자료형은 정확히 같은 정보를 나타내고 있습니다.\n코드를 통해 얻은 PD에서 살펴볼 수 있듯, 위 데이터는 0th-homology class 하나와 2nd-homology class 하나가 무한한 지속성을 가지고 있는 것을 관찰할 수 있습니다.\n여기서 원래 다양체 \\(S^2\\)가 가지는 호몰로지의 정보와 동일하다는 것을 관찰할 수 있습니다.\n아래의 다른 예제들(토러스, 뫼비우스의 띠, 클레인 병)에서도 동일한 결과를 관찰할 수 있습니다.\n\n\nHomology of Torus\n\\[\\text{rank}{H_0(T^2)} = 1, \\text{rank}{H_1(T^2)} = 2, \\text{rank}{H_2(T^2)} = 1\\]\n\n# Persistence barcode & diagram : Torus\n\n\n\nHomology of Mobius band\n\\[\\text{rank}{H_0(M)} = 1, \\text{rank}{H_1(M)} = 1, \\text{rank}{H_2(M)} = 0\\]\n\n# Persistence barcode & diagram : Mobius band\n\n\n\nHomology of Klein bottle\n\\[\\text{rank}{H_0(K)} = 1, \\text{rank}{H_1(K)} = 1, \\text{rank}{H_2(K)} = 0\\]\n\n# Persistence barcode & diagram : Klein bottle"
  },
  {
    "objectID": "posts/Study/tutorial_hand_on.html#machine-learning",
    "href": "posts/Study/tutorial_hand_on.html#machine-learning",
    "title": "TDA tutorial",
    "section": "4. Machine Learning",
    "text": "4. Machine Learning\n\n \n\n먼저 간단하게 머신러닝이란 무엇인지, 그리고 머신러닝은 전반적으로 어떤 과정을 거쳐 문제를 해결하는지 살펴봅시다.\n머신러닝이란, 주어진 문제를 해결하기 위해 기계가 데이터에 잠재된 패턴을 스스로 학습하고 문제 해결에 최적화된 솔루션을 스스로 만들어내는 모델이라고 할 수 있습니다.\n이때, 데이터가 가진 특징을 사용하는 방법을 사람이 지정해주는 것이 아니라 기계 스스로 학습하여 만들어내기 때문에 인공지능의 한 방법론이라 할 수 있습니다.\n머신러닝은 일반적으로 다음과 같은 파이프라인을 거쳐 문제를 해결합니다.\n\nMachine Learning Pipeline\n\n문제정의 : 개와 고양이 사진을 구분하는 분류, 미래의 주가를 예측하는 회귀, 유사한 데이터를 모으는 군집 등으로 문제를 적절하게 정의합니다.\n데이터 정제 및 가공 : 풀고자 하는 문제에 대한 데이터를 수집하고 머신러닝에 사용할 수 있도록 가공합니다.\n모델 선택 및 학습 : 수집된 데이터의 포멧에 따라 혹은 정의된 문제에 따라 가장 적합한 머신러닝 모델을 선택하고 훈련 데이터를 학습시킵니다.\n모델 평가 : 아직 관측되지 않은 데이터에 대해서도 훈련 상황과 유사한 성능이 나오는지를 평가합니다.\n론칭 : 적절한 수준의 평가점수를 얻은 머신러닝 모델을 론칭합니다.\n\n\n\n# Import necessary libraries\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gudhi\n\n\nfrom sklearn import svm\nfrom sklearn import metrics\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n\n\nfrom gudhi.sklearn.cubical_persistence import CubicalPersistence\nfrom gudhi.representations import DiagramSelector, Landscape\n\n\n# Fix all randomnesses for reproducibility\n\ndef random_seed_fix(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n\nseed = 42\nrandom_seed_fix(seed)\n\n\n4.1) 데이터 소개 및 문제 정의\n\nMNIST 손글씨 숫자 이미지 데이터\n\n\n\nMNIST handwritten digit database, Yann LeCun et al. \n\nMNIST 손글씨 숫자 데이터는 머신러닝을 입문하기 위한 가장 대표적인 데이터로, 0~9까지의 손글씨 숫자가 7만장 준비되어 있습니다.\n각 이미지는 0~255까지의 그래이 스케일 값을 가지는 28x28(784개)의 픽셀로 구성되어 있습니다.\n또한 각 이미지가 어떤 숫자를 나타내는지 정답지도 구비되어 있습니다.\n\n\n이미지 분류 (Classification)\n분류 문제는 가장 대표적인 머신러닝 문제입니다.\n분류 문제의 목표는 사진 데이터를 입력받으면, 이 사진이 어떤 숫자인지 말해주는 머신 혹은 함수를 만드는 것입니다.\n머신러닝이 하는 역할은 이 문제를 위해 손수 모델링을 하는 것이 아니라, 모델이 훈련 데이터를 학습하여 출력 함수를 스스로 설계하도록 만드는 것입니다.\n\n\n호몰로지 클래스 분류 (Homology classification)\n일반적인 MNIST 분류는 단순히 이미지가 어떤 숫자인지를 알아 맞히는 문제입니다.\n그러나, 호몰로지 클래스 분류는 이미지가 가진 구멍의 개수를 알아 맞히는 문제입니다.\n예를들어, 숫자 이미지 1은 구멍이 없으므로 0으로 예측하고, 숫자 이미지 0은 구멍이 하나 있으므로 1로 예측하고, 숫자 이미지 8은 구멍이 두 개 있으므로 2로 예측하길 원하는 것입니다.\n\n\n\n4.2) 데이터 정제 및 가공\n우리는 다음과 같이 데이터를 정제하고 가공할 것입니다.\n\n훈련 데이터 샘플 수 : 7,000 개 (실험을 위한 성능의 제약과 메모리 사용량 완화를 위해)\n데이터 포맷 : 이미지를 길이가 784인 벡터 형태로 입력받아 사용할 것입니다.\n정규화 : 일부 머신러닝 모델들은 데이터의 스케일에 영향을 받으므로 0~255까지인 픽셀 값을 0~1까지의 값으로 정규화해줍니다.\n레이블 수정 : MNIST 데이터의 기본 레이블(정답)은 이미지가 나타내는 0~9 까지의 숫자로 되어 있습니다. 그러나, 우리가 풀 문제는 호몰로지 클래스를 분류하는 문제이므로, 숫자의 구멍의 개수에 따라 레이블을 수정해줍니다.\n\n\n# data load\nmnist = fetch_openml('mnist_784', as_frame=False)\n\n# training/test data split \nX_train, X_test, y_train, y_test = train_test_split(mnist.data, mnist.target, train_size=0.1, random_state=seed)\n\n# data normalization\nX_train = X_train / 255.0\nX_test = X_test / 255.0\n\n/Users/shlee/miniforge3/envs/test_env/lib/python3.8/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn(\n\n\n\n# label transform\ndef homology_class(label):\n    if label == '8':\n        return 2\n    elif label == '0' or label == '6' or label == '9':\n        return 1\n    else:\n        return 0\n\ny_train_hom = np.array(list(map(homology_class, y_train)))\ny_test_hom = np.array(list(map(homology_class, y_test)))\n\n\n# Display data samples\n\nnum_img = 20\nimages = X_train[0:num_img]\nimages = images.reshape(num_img,28,28)\nlabels = y_train_hom[0:num_img]\nnum_row = 4\nnum_col = 5\n\n# plot images\nfig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n\nfor i in range(num_img):\n    ax = axes[i//num_col, i%num_col]\n    ax.imshow(images[i], cmap='gray_r')\n    ax.set_title('Label: {}'.format(labels[i]))\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n4.3) 모델 선택 및 학습\n우리는 가장 단순한 머신러닝 분류기인 ’퍼셉트론’을 사용할 것입니다. 퍼셉트론은 뉴럴 네트워크의 기본 단위가 되는 선형 모델입니다.\n\n\n\n퍼셉트론의 작동방식 도식화\n\n\n퍼셉트론에 대한 자세한 설명은 다음 자료를 참고해주세요. [머신러닝 정리] 퍼셉트론 (Perceptron)\n사이킷런(scikit-learn) 라이브러리를 이용하면 퍼셉트론과 같은 머신러닝 모델을 쉽게 불러오고 학습시킬 수 있습니다.\n\n# Model training\nmodel = Perceptron(random_state=seed)\n\n\n\nmodel.fit(X_train, y_train_hom)\n\nPerceptron(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PerceptronPerceptron(random_state=42)\n\n\n\n\n4.4) 모델 평가\n모델 평가는 모델 학습에 사용되지 않은 테스트 데이터를 사용합니다.\n이렇게 해서 모델이 얼마나 일반적인 상황에서 이 문제를 잘 해결할 수 있는지 평가합니다.\n분류 모델 평가에는 ’정확도(accuracy score)’를 사용합니다.\n정확도는 테스트 데이터에 대한 모델의 예측 레이블과 정답 레이블을 각각 비교하여, 정답을 맞춘 비율을 계산합니다.\n\n# Compute accuracy\ny_pred = model.predict(X_test)\naccuracy_score(y_pred, y_test_hom)\n\n0.8229206349206349\n\n\n우리 모델의 정확도가 약 82퍼센트가 나왔습니다.\n모델의 성능을 조금 더 자세히 살펴보기 위해 오답분석표를 관찰할 수 있습니다.\n혼동행렬(Confusion matrix)는 모델 예측의 정오표를 한눈에 쉽게 관찰할 수 있게 해줍니다.\n혼동행렬의 각 행은 정답 레이블을, 각 열은 예측 레이블을 의미합니다.\n혼동행렬의 대각성분은 예측을 정확히 한 데이터의 개수를 나타냅니다.\n또한 대각성분이 아닌 성분을 관찰하면 모델이 주로 어떤 레이블에서 오답을 범했는지 직관을 얻을 수 있습니다.\n\n# Plot confusion matrix\ncfs_mat = confusion_matrix(y_test_hom, y_pred)\nConfusionMatrixDisplay(cfs_mat).plot()\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x17776a5b0&gt;\n\n\n\n\n\n이 모델은 약 82퍼센트의 정확도를 가지고 있습니다.\n오답을 분석하면 구멍이 없는 (True label = 0) 데이터를 상당히 많이 틀렸다는 것을 알 수 있습니다."
  },
  {
    "objectID": "posts/Study/tutorial_hand_on.html#machine-learning-with-tda",
    "href": "posts/Study/tutorial_hand_on.html#machine-learning-with-tda",
    "title": "TDA tutorial",
    "section": "5. Machine Learning with TDA",
    "text": "5. Machine Learning with TDA\n우리가 훈련한 모델은 이미지의 픽셀값만을 입력받기 때문에 아마 픽셀 자리가 많이 겹치는 3이나 5를 각각 8이나 6 따위로 오해했을 가능성이 있습니다.\n그러므로, 이미지의 픽셀값뿐만 아니라, 이미지가 가진 구멍의 개수에 대한 정보를 추가적으로 입력하여 학습한다면 모델의 성능이 더욱 향상될 것이라고 기대할 수 있습니다.\n호몰로지는 다양체가 가진 구멍의 개수에 대한 정보를 가져다 줍니다. 그러므로 이미지가 가진 PH를 계산하면 이미지가 가진 구멍의 개수에 대한 정보를 추정해낼 수 있습니다.\n이 정보를 추가한다면, 호몰로지 클래스 분류 문제를 푸는 데에 도움이 될 것이라 생각할 수 있습니다.\n\nTopological Feature Addition\n우리의 전략은 기존의 이미지 데이터에 이미지가 가진 구멍의 개수에 대한 정보를 Persistent homology를 이용해 추출하고 데이터에 추가해주는 것입니다.\n이처럼 머신러닝에서 문제 해결에 유용할 것이라 생각되는 정보를 추가해 성능을 향상시키는 전략을 Feature addition 이라고 부릅니다.\n\n\nCubical Complex\n이미지 데이터는 앞서 소개한 point cloud와는 달리 거리가 아니라 픽셀의 농도를 나타내는 gray scale을 파라미터로 사용하고 있습니다.\n이러한 이미지 데이터로부터 위상적인 정보를 표현할 때는 Cubical Complex가 사용됩니다.\n\n\n\nCubical complex\n\n\nCubical complex는 정규화된 그리드 위에 점들이 있다고 생각하고, 점들마다 부여된 gray scale 값을 파라미터로 하여 각 점들을 연결해 나갑니다.\nSimplicial complex와의 차이라면, simplicial complex는 삼각형과 정사면체를 기본 단위로 생각하지만, cubical complex는 사각형과 정육면체를 기본단위로 한다는 것입니다.\n아래의 그림은 이미지 데이터로부터 PB와 PD를 그려나가는 과정을 보여줍니다.\n\n  \n\n\nCubical filtration 과정 (2) 위 과정을 통해 얻어진 Persistence barcode (3) 동일한 정보를 나타내는 Persistence diagram\n\n\n\n\n\n5.1) Topological Data Analysis\n먼저, 이미지 데이터의 위상적 특징을 분석하기 위해 이미지 데이터로부터 PH를 계산해 관찰합니다.\n\n# Persistent homology for Image data\n\nsample = 0\n\ncc = gudhi.CubicalComplex(dimensions=[28,28], top_dimensional_cells=1-X_train[sample])\ndiag = cc.persistence(homology_coeff_field=2)\n\nfig, axs = plt.subplots(nrows=1, ncols=3, figsize=(30,10))\n\naxs[0].imshow(X_train[sample].reshape(28,28), cmap='gray_r')\naxs[0].set_title(f'Label : {y_train_hom[sample]}')\ngudhi.plot_persistence_barcode(diag, axes=axs[1], legend=True)\ngudhi.plot_persistence_diagram(diag, axes=axs[2], legend=True)\nplt.show()\n\nusetex mode requires TeX.\n\n\n\n\n\n위 코드를 통해 데이터를 분석해보면 1st Persistent homology가 보여주는 긴 바코드의 개수가 이미지의 구멍의 개수로 근사하는 것을 관찰할 수 있습니다.\n이로부터 PH가 이 문제를 해결하는 데에 도움이 되는 정보라는 것을 유추할 수 있습니다.\n이제 이 정보를 어떻게 데이터에 추가할 수 있을까요?\n\n\n\n\n\n5.2) Vectorization\n일반적인 머신러닝 모델들은 샘플마다 고정된 길이의 벡터를 입력으로 받습니다.\n그러나, 샘플의 위상적인 정보를 담고 있는 Persistence barcode 또는 Persistence diagram 자료형은 샘플마다 그 길이(바코드의 개수)가 다릅니다.\n따라서, PB의 정보는 최대한 유지하면서 고정된 길이의 벡터 형태로 요약하는 벡터화(Vectorization) 과정이 필요합니다.\n여기서는 Persistence Landscape [Peter Bubenik, 2015]를 사용할 것입니다.\n간략히 말하자면, Persistence Landscape는 Persistence barcode의 각 바코드를 piecewise linear function들로 변환한 뒤, piecewise linear function들을 고정된 길이의 벡터로 저장하는 것입니다.\nPersistence Landscape에 대한 자세한 설명은 다음 영상을 참고해주세요. [https://youtu.be/CQJQg3Ai0ZQ]\n우리는 이미지 벡터 뒤에 Persistence Landscape 벡터를 붙여(concatenate) 새로운 입력 데이터로 사용할 것입니다.\n\n\n\n데이터 벡터화 과정의 도식화\n\n\n\n# Topological features\n\nPH0 = CubicalPersistence(homology_dimensions=0, homology_coeff_field=2, newshape=[-1,28,28], n_jobs=-1)\nPH1 = CubicalPersistence(homology_dimensions=1, homology_coeff_field=2, newshape=[-1,28,28], n_jobs=-1)\nPL = Landscape(num_landscapes=10, resolution=50)\n\ndef topological_vector(X_data):\n    X_data_ = np.ones_like(X_data) - X_data\n\n    # Generate persistence diagram for input dataset.\n    pd0 = PH0.fit_transform(X_data)\n    pd1 = PH1.fit_transform(X_data_)\n\n    # We only use finite length points.\n    pd0 = DiagramSelector(use=True, point_type='finite').fit_transform(pd0)\n    pd1 = DiagramSelector(use=True, point_type='finite').fit_transform(pd1)\n\n    # Generate persistence landscape for each dimensional persistence diagram.\n    pl0 = PL.fit_transform(pd0)\n    pl1 = PL.fit_transform(pd1)\n\n    # Our output vector is of the shape [image vector, 0th-PL, 1st-PL].\n    return np.concatenate([X_data, pl0, pl1], axis=1)   \n\n\n# feature addition \nX_train_top = topological_vector(X_train)\nX_test_top = topological_vector(X_test)\nX_train_top.shape, X_test_top.shape\n\n((7000, 1784), (63000, 1784))\n\n\n\n\n5.3) 모델 학습 및 평가\n이제 새롭게 얻은 데이터로 학습해 지난 모델에 비해 얼마나 성능이 향상되는지 비교해봅시다.\n실험의 형평성을 위해 우리의 모델은 여전히 똑같은 퍼셉트론 모델입니다.\n변한 것은 학습 데이터에 위상적인 정보가 추가된 것 뿐입니다.\n\n# model setting\nmodel_tda = Perceptron(random_state=seed)\n\n\n# model fitting\nmodel_tda.fit(X_train_top, y_train_hom)\n\nPerceptron(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PerceptronPerceptron(random_state=42)\n\n\n\n# model evaluation\ny_pred = model_tda.predict(X_test_top)\naccuracy_score(y_pred, y_test_hom)\n\n0.9128095238095238\n\n\n모델 평가 결과, 정확도가 기존의 약 82%에서 약 91%로, 9% 가량 성능이 향상된 것을 확인할 수 있습니다.\n\n# Experiment\n\nimage = X_test[1]\nplt.imshow(image.reshape(28,28), cmap = 'gray_r')\n\n&lt;matplotlib.image.AxesImage at 0x1774ec2b0&gt;\n\n\n\n\n\n\npred1 = model.predict(image[None,:])\npred2 = model_tda.predict(topological_vector(image[None, :]))\n\nprint(f'일반 퍼셉트론 모델 : 이 숫자 이미지에는 구멍이 {pred1}개 있습니다.')\nprint(f'TDA 퍼셉트론 모델 : 이 숫자 이미지에는 구멍이 {pred2}개 있습니다.')\n\n일반 퍼셉트론 모델 : 이 숫자 이미지에는 구멍이 [1]개 있습니다.\nTDA 퍼셉트론 모델 : 이 숫자 이미지에는 구멍이 [0]개 있습니다.\n\n\n혼동행렬을 관찰하면, 기존의 모델이 구멍이 없는데도 구멍이 있다고 했던 샘플에 대한 오답을 새로운 모델에서 상당히 개선해냈음을 알 수 있습니다.\n이로부터 Persistent homology에 대한 정보를 추가하는 것이 머신러닝의 성능을 향상시킬 수 있음을 관찰할 수 있습니다.\n\ncfs_mat = confusion_matrix(y_test_hom, y_pred)\nConfusionMatrixDisplay(cfs_mat).plot()\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x177c78ac0&gt;"
  },
  {
    "objectID": "posts/Study/tutorial_hand_on.html#마무리",
    "href": "posts/Study/tutorial_hand_on.html#마무리",
    "title": "TDA tutorial",
    "section": "마무리",
    "text": "마무리\n지금까지 TDA를 이용해 데이터로부터 기하학적 정보를 추출하는 방법과 이를 머신러닝에 응용하는 방법에 대해 살펴보았습니다.\nTDA는 다양한 데이터에 다양한 방식으로 응용될 수 있습니다.\n특히, 기존의 여러 머신러닝 방법들은 데이터의 국소적 특징(local feature)을 분석하고 추출하는 방법을 취하곤 합니다.\n데이터의 대역적인 특징(global feature)을 추출하는 TDA를 머신러닝과 함께 사용하고자 하는 것은 수학자에겐 자연스러운 아이디어일 것입니다.\n이로부터 기존의 머신러닝과 데이터 분석에서 더욱 향상된 성능을 기대할 수 있습니다.\n모든 것은 여러분의 아이디어에 달려있습니다.\n이 튜토리얼을 통해 여러분의 학습과 연구에 새로운 인사이트가 생기길 바랍니다.\n여기까지 학습하시느라 수고가 많으셨습니다.\n감사합니다.\n-포항공과대학교 인공지능대학원 박사과정 이성헌-\n\nReferences)\n\nWeinberger, Shmuel. “What is… persistent homology.” Notices of the AMS 58.1 (2011): 36-39.\nCarlsson, Gunnar. “Topology and data.” Bulletin of the American Mathematical Society 46.2 (2009): 255-308.\nGarin, Adélie, and Guillaume Tauzin. “A topological” reading” lesson: Classification of MNIST using TDA.” 2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA). IEEE, 2019.\nBubenik, Peter. “Statistical topological data analysis using persistence landscapes.” J. Mach. Learn. Res. 16.1 (2015): 77-102.\nMaria, Clément, et al. “The gudhi library: Simplicial complexes and persistent homology.” International congress on mathematical software. Springer, Berlin, Heidelberg, 2014.\nLeCun, Yann. “The MNIST database of handwritten digits.” http://yann. lecun. com/exdb/mnist/ (1998).\nOtter, Nina, et al. “A roadmap for the computation of persistent homology.” EPJ Data Science 6 (2017): 1-38."
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 7.html",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 7.html",
    "title": "모형 평가",
    "section": "",
    "text": "선형대수와 통계학으로 배우는 머신러닝 with 파이썬\ngithub"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 7.html#분류classification",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 7.html#분류classification",
    "title": "모형 평가",
    "section": "분류(classification)",
    "text": "분류(classification)\n\n정확도(accuracy)\n\n\\[\\dfrac{1}{n}\\sum_{i=1}^n I(\\hat y_i = y_i)\\]\n\n# 정확도\n#import numpy as np\nfrom sklearn.metrics import accuracy_score\ny_pred = [0, 2, 1, 3]\ny_true = [0, 1, 2, 3]\nprint(accuracy_score(y_true, y_pred))\nprint(accuracy_score(y_true, y_pred, normalize=False))\n\n0.5\n2\n\n\n\n## confusionm matrix\nfrom sklearn.metrics import confusion_matrix\ny_true = [2, 0, 2, 2, 0, 1]\ny_pred = [0, 0, 2, 2, 0, 2]\nconfusion_matrix(y_true, y_pred)\n\narray([[2, 0, 0],\n       [0, 0, 1],\n       [1, 0, 2]])\n\n\n\n## classification report \nfrom sklearn.metrics import classification_report\ny_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 1, 0]\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_true, y_pred, target_names=target_names))\n\n              precision    recall  f1-score   support\n\n     class 0       0.67      1.00      0.80         2\n     class 1       0.00      0.00      0.00         1\n     class 2       1.00      0.50      0.67         2\n\n    accuracy                           0.60         5\n   macro avg       0.56      0.50      0.49         5\nweighted avg       0.67      0.60      0.59         5"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 7.html#회귀regression",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 7.html#회귀regression",
    "title": "모형 평가",
    "section": "회귀(regression)",
    "text": "회귀(regression)\n\nMean Absolute Error\n\n\\[MAE=\\dfrac{1}{n}\\sum_{i=1}^n|y_i-\\hat y_i|\\]\n\n# mean absolute error\nfrom sklearn.metrics import mean_absolute_error\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\n\nprint(mean_absolute_error(y_true, y_pred))\n\n0.5\n\n\n\nMean Squared Error\n\n\\[MSE=\\dfrac{1}{n}\\sum_{i=1}^n(y_i-\\hat y_i)^2\\]\n\n# mean squared error\nfrom sklearn.metrics import mean_squared_error\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nprint(mean_squared_error(y_true, y_pred))\n\n0.375\n\n\n\n# R2\nfrom sklearn.metrics import r2_score\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nprint(r2_score(y_true, y_pred))\n\n0.9486081370449679"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 7.html#군집clustering",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 7.html#군집clustering",
    "title": "모형 평가",
    "section": "군집(clustering)",
    "text": "군집(clustering)\n- 실루엣 스코어(silhouette score)\n\n서로 다른 군집이 얼마나 잘 분리되는지 나타내는 지표\n같은 군집의 데이터는 가까운 거리에 뭉쳐 있고, 다른 군집의 데이터끼리는 멀리 떨어져 있을 수록 높은 점수\na: 집단 내 데이터 거리 평균\nb: 다른 집단 데이터 거리 평균의 최솟값\n실루엣 스코어는 -1~1의 값으 ㄹ가지며 스코어가 높을 수록 좋은 성능\n\n\\[s=\\dfrac{b-a}{max(a,b)}\\]\n\n# silloutte score\nfrom sklearn.metrics import silhouette_score\nX = [[1, 2], [4, 5], [2, 1], [6, 7], [2, 3]]\nlabels = [0, 1, 0, 1, 0] \nsil_score = silhouette_score(X, labels)\nprint(sil_score)\n\n0.5789497702625118\n\n\n\n# adjusted rand index\nfrom sklearn.metrics import adjusted_rand_score\nlabels_true = [0, 0, 0, 1, 1, 1]\nlabels_pred = [0, 0, 1, 1, 2, 2]\n\nprint(adjusted_rand_score(labels_true, labels_pred))\n\n0.24242424242424243"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html",
    "href": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html",
    "title": "인공신경망과 퍼셉트론",
    "section": "",
    "text": "파이썬 데이터 분석"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#패키지-설정",
    "href": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#패키지-설정",
    "title": "인공신경망과 퍼셉트론",
    "section": "1. 패키지 설정",
    "text": "1. 패키지 설정\n\nfrom sklearn.linear_model import Perceptron\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#데이터-준비",
    "href": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#데이터-준비",
    "title": "인공신경망과 퍼셉트론",
    "section": "2. 데이터 준비",
    "text": "2. 데이터 준비\n\n# 데이터 작성(OR연산)\n\nX=np.array([[0,0],[0,1],[1,0],[1,1]])\ny=np.array([0,1,1,1])"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#탐색적-데이터-분석",
    "href": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#탐색적-데이터-분석",
    "title": "인공신경망과 퍼셉트론",
    "section": "3. 탐색적 데이터 분석",
    "text": "3. 탐색적 데이터 분석\n\nplt.scatter(X[:,0], X[:,1], c=y)\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.show()"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#모형화-및-학습",
    "href": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#모형화-및-학습",
    "title": "인공신경망과 퍼셉트론",
    "section": "4. 모형화 및 학습",
    "text": "4. 모형화 및 학습\n\nmodel=Perceptron(verbose=1)\nmodel.fit(X,y)\n\n-- Epoch 1\nNorm: 1.41, NNZs: 2, Bias: 0.000000, T: 4, Avg. loss: 0.250000\nTotal training time: 0.00 seconds.\n-- Epoch 2\nNorm: 2.24, NNZs: 2, Bias: 0.000000, T: 8, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\n-- Epoch 3\nNorm: 2.24, NNZs: 2, Bias: -1.000000, T: 12, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\n-- Epoch 4\nNorm: 2.83, NNZs: 2, Bias: 0.000000, T: 16, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\n-- Epoch 5\nNorm: 2.83, NNZs: 2, Bias: -1.000000, T: 20, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\n-- Epoch 6\nNorm: 2.83, NNZs: 2, Bias: -1.000000, T: 24, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\n-- Epoch 7\nNorm: 2.83, NNZs: 2, Bias: -1.000000, T: 28, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\nConvergence after 7 epochs took 0.00 seconds\n\n\nPerceptron(verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PerceptronPerceptron(verbose=1)\n\n\n\n학습계수 eta0=1\n학습 종료 기준 허용오차 0.001\n이전 단계 epoch의 평균 비용과 현재 epoch의 평균 비용과의 차이가 허용오차보다 작으면 학습 종료ㅡ\n\n\n# 가중치\nprint(model.coef_)\n# 편향\nprint(model.intercept_)\n# 학습 수(epoch)\nprint(model.n_iter_)\n\n[[2. 2.]]\n[-1.]\n7\n\n\n\\[x_2=\\dfrac{w_1}{w_2}x_1 - \\dfrac{b}{w_2}\\]\n\\[x_2=-\\dfrac{2}{2}x_1 - \\dfrac{-1}{2}= -x_1+\\dfrac{1}{2}\\]\n\n#산포도\nplt.scatter(X[:,0], X[:,1],c=y)\nplt.xlabel('x1')\nplt.ylabel('x2')\n\n#파라미터\nw1=model.coef_[0,0]\nw2=model.coef_[0,1]\nb=model.intercept_\n\n#x절편\nx_intercept=-b/w1\n#y절편\ny_intercept=-b/w2\n#선형 분류자 추가\nplt.plot([0,y_intercept],[x_intercept,0], c='red')\nplt.show()\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/core/shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  ary = asanyarray(ary)"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#예측",
    "href": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#예측",
    "title": "인공신경망과 퍼셉트론",
    "section": "5. 예측",
    "text": "5. 예측\n\nX_test=X\npred=model.predict(X_test)\nprint(pred)\n\n[0 1 1 1]"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#패키지-설정-1",
    "href": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#패키지-설정-1",
    "title": "인공신경망과 퍼셉트론",
    "section": "1. 패키지 설정",
    "text": "1. 패키지 설정\n\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#데이터-준비-1",
    "href": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#데이터-준비-1",
    "title": "인공신경망과 퍼셉트론",
    "section": "2. 데이터 준비",
    "text": "2. 데이터 준비\n\niris = datasets.load_iris()\nprint(iris.feature_names)\nprint(iris.target_names)\n\n['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n['setosa' 'versicolor' 'virginica']\n\n\n\nX=iris.data\ny=iris.target\nprint(X[:10])\nprint(y)\n\n[[5.1 3.5 1.4 0.2]\n [4.9 3.  1.4 0.2]\n [4.7 3.2 1.3 0.2]\n [4.6 3.1 1.5 0.2]\n [5.  3.6 1.4 0.2]\n [5.4 3.9 1.7 0.4]\n [4.6 3.4 1.4 0.3]\n [5.  3.4 1.5 0.2]\n [4.4 2.9 1.4 0.2]\n [4.9 3.1 1.5 0.1]]\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#탐색적-데이터-분석-1",
    "href": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#탐색적-데이터-분석-1",
    "title": "인공신경망과 퍼셉트론",
    "section": "3. 탐색적 데이터 분석",
    "text": "3. 탐색적 데이터 분석\n\nplt.figure(figsize=(8,4))\nplt.boxplot([X[:,0],X[:,1],X[:,2],X[:,3]],\n            labels=iris.feature_names)\nplt.title('Iris')\nplt.xlabel('Features')\nplt.ylabel('Length or Width(cm)')\nplt.show()\n\n\n\n\n\nX_df=pd.DataFrame(X)  #boxplot()함수 사용을 위해 데이터 프레임 변환\n\nX_df.columns=iris.feature_names # 특성 이름 열 이름으로 대체\n\nX_df['species']=y\n\nprint(X_df)\n\n     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n0                  5.1               3.5                1.4               0.2   \n1                  4.9               3.0                1.4               0.2   \n2                  4.7               3.2                1.3               0.2   \n3                  4.6               3.1                1.5               0.2   \n4                  5.0               3.6                1.4               0.2   \n..                 ...               ...                ...               ...   \n145                6.7               3.0                5.2               2.3   \n146                6.3               2.5                5.0               1.9   \n147                6.5               3.0                5.2               2.0   \n148                6.2               3.4                5.4               2.3   \n149                5.9               3.0                5.1               1.8   \n\n     species  \n0          0  \n1          0  \n2          0  \n3          0  \n4          0  \n..       ...  \n145        2  \n146        2  \n147        2  \n148        2  \n149        2  \n\n[150 rows x 5 columns]\n\n\n\nsns.pairplot(X_df, hue='species', height=2)"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#데이터-분리",
    "href": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#데이터-분리",
    "title": "인공신경망과 퍼셉트론",
    "section": "4. 데이터 분리",
    "text": "4. 데이터 분리\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#피처-스케일링",
    "href": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#피처-스케일링",
    "title": "인공신경망과 퍼셉트론",
    "section": "5. 피처 스케일링",
    "text": "5. 피처 스케일링\n\nscalerX=StandardScaler()\nscalerX.fit(X_train)\nX_train_std=scalerX.transform(X_train)\nX_test_std=scalerX.transform(X_test)"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#모형화-및-학습-1",
    "href": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#모형화-및-학습-1",
    "title": "인공신경망과 퍼셉트론",
    "section": "6. 모형화 및 학습",
    "text": "6. 모형화 및 학습\n\nmodel=Perceptron(verbose=1)\nmodel.fit(X_train_std,y_train)\n\n-- Epoch 1\nNorm: 3.02, NNZs: 4, Bias: 0.000000, T: 105, Avg. loss: 0.004369\nTotal training time: 0.00 seconds.\n-- Epoch 2\nNorm: 3.02, NNZs: 4, Bias: 0.000000, T: 210, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\n-- Epoch 3\nNorm: 3.02, NNZs: 4, Bias: 0.000000, T: 315, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\n-- Epoch 4\nNorm: 3.02, NNZs: 4, Bias: 0.000000, T: 420, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\n-- Epoch 5\nNorm: 3.02, NNZs: 4, Bias: 0.000000, T: 525, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\n-- Epoch 6\nNorm: 3.02, NNZs: 4, Bias: 0.000000, T: 630, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\n-- Epoch 7\nNorm: 3.02, NNZs: 4, Bias: 0.000000, T: 735, Avg. loss: 0.000000\nTotal training time: 0.00 seconds.\nConvergence after 7 epochs took 0.00 seconds\n-- Epoch 1\nNorm: 1.85, NNZs: 4, Bias: -2.000000, T: 105, Avg. loss: 0.701329\nTotal training time: 0.00 seconds.\n-- Epoch 2\nNorm: 3.98, NNZs: 4, Bias: 1.000000, T: 210, Avg. loss: 0.545394\nTotal training time: 0.00 seconds.\n-- Epoch 3\nNorm: 4.14, NNZs: 4, Bias: -1.000000, T: 315, Avg. loss: 0.808579\nTotal training time: 0.00 seconds.\n-- Epoch 4\nNorm: 5.02, NNZs: 4, Bias: -1.000000, T: 420, Avg. loss: 0.676848\nTotal training time: 0.00 seconds.\n-- Epoch 5\nNorm: 6.59, NNZs: 4, Bias: -1.000000, T: 525, Avg. loss: 0.787455\nTotal training time: 0.00 seconds.\n-- Epoch 6\nNorm: 5.52, NNZs: 4, Bias: -1.000000, T: 630, Avg. loss: 0.698311\nTotal training time: 0.00 seconds.\n-- Epoch 7\nNorm: 5.54, NNZs: 4, Bias: -2.000000, T: 735, Avg. loss: 0.644139\nTotal training time: 0.00 seconds.\nConvergence after 7 epochs took 0.00 seconds\n-- Epoch 1\nNorm: 3.94, NNZs: 4, Bias: -3.000000, T: 105, Avg. loss: 0.119225\nTotal training time: 0.00 seconds.\n-- Epoch 2\nNorm: 4.58, NNZs: 4, Bias: -4.000000, T: 210, Avg. loss: 0.117215\nTotal training time: 0.00 seconds.\n-- Epoch 3\nNorm: 5.79, NNZs: 4, Bias: -4.000000, T: 315, Avg. loss: 0.104179\nTotal training time: 0.00 seconds.\n-- Epoch 4\nNorm: 5.86, NNZs: 4, Bias: -5.000000, T: 420, Avg. loss: 0.095289\nTotal training time: 0.00 seconds.\n-- Epoch 5\nNorm: 7.06, NNZs: 4, Bias: -5.000000, T: 525, Avg. loss: 0.056061\nTotal training time: 0.00 seconds.\n-- Epoch 6\nNorm: 7.47, NNZs: 4, Bias: -6.000000, T: 630, Avg. loss: 0.095611\nTotal training time: 0.00 seconds.\n-- Epoch 7\nNorm: 7.88, NNZs: 4, Bias: -7.000000, T: 735, Avg. loss: 0.093122\nTotal training time: 0.00 seconds.\n-- Epoch 8\nNorm: 7.17, NNZs: 4, Bias: -8.000000, T: 840, Avg. loss: 0.019793\nTotal training time: 0.00 seconds.\n-- Epoch 9\nNorm: 9.08, NNZs: 4, Bias: -7.000000, T: 945, Avg. loss: 0.082413\nTotal training time: 0.00 seconds.\n-- Epoch 10\nNorm: 8.29, NNZs: 4, Bias: -8.000000, T: 1050, Avg. loss: 0.045435\nTotal training time: 0.00 seconds.\n-- Epoch 11\nNorm: 8.41, NNZs: 4, Bias: -8.000000, T: 1155, Avg. loss: 0.067990\nTotal training time: 0.00 seconds.\n-- Epoch 12\nNorm: 8.56, NNZs: 4, Bias: -8.000000, T: 1260, Avg. loss: 0.062187\nTotal training time: 0.00 seconds.\n-- Epoch 13\nNorm: 7.87, NNZs: 4, Bias: -9.000000, T: 1365, Avg. loss: 0.082040\nTotal training time: 0.00 seconds.\nConvergence after 13 epochs took 0.00 seconds\n\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n\n\nPerceptron(verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PerceptronPerceptron(verbose=1)\n\n\n\n# 가중치\nprint(model.coef_)\n# 편향\nprint(model.intercept_)\n\n[[-1.35951428  1.97275265 -1.46733157 -1.0997214 ]\n [-0.91929061 -1.69928112  4.06213278 -3.23501377]\n [-1.60316749 -0.19414391  5.45511955  5.44362091]]\n[ 0. -2. -9.]"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#예측-1",
    "href": "posts/Study/Python Data Analysis/인공신경망과 퍼셉트론.html#예측-1",
    "title": "인공신경망과 퍼셉트론",
    "section": "7. 예측",
    "text": "7. 예측\n\ny_pred=model.predict(X_test_std)\n\n\nprint(y_pred) #예측값\nprint(y_test) #실제값\n\n[1 0 2 2 0 0 2 2 0 1 0 2 0 1 2 1 1 2 0 0 0 2 0 0 1 0 2 2 0 0 2 0 0 0 0 0 2\n 2 2 0 2 0 0 2 2]\n[1 1 2 2 1 0 2 2 0 2 1 2 0 1 2 1 1 2 0 0 0 2 0 0 1 1 2 2 0 0 2 0 0 0 1 0 2\n 2 2 0 2 0 1 2 2]\n\n\n\nprint('Accuracy:%.2f' % accuracy_score(y_test,y_pred))\n\nAccuracy:0.84"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/딥러닝 회귀분석.html",
    "href": "posts/Study/Python Data Analysis/딥러닝 회귀분석.html",
    "title": "딥러닝: 회귀분석",
    "section": "",
    "text": "파이썬 데이터 분석"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/딥러닝 회귀분석.html#패키지-설정",
    "href": "posts/Study/Python Data Analysis/딥러닝 회귀분석.html#패키지-설정",
    "title": "딥러닝: 회귀분석",
    "section": "1. 패키지 설정",
    "text": "1. 패키지 설정\nsklearn에서 2.1 이후로 해당 데이터는 사용할 수 없다.\nfrom sklearn.datasets import load_boston\nIn this special case, you can fetch the dataset from the original source::\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split  #학습용과 테스트용 분리\nfrom sklearn.metrics import mean_absolute_error       #정규화\n#딥러닝 모형화를 위해 keras를 사용\nfrom keras.models import Sequential                   #Sequential 한층씩 추가하여 네트워크를 만든다.    \nfrom keras.layers import Dense                        #Dense 층 간 노드들은 모두 연결되는 모형구조를 만든다.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nimport pandas as pd\nimport numpy as np\n\ndata_url = \"http://lib.stat.cmu.edu/datasets/boston\"\nraw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22)\ndata = raw_df.values[:, :-1]\ntarget = raw_df.values[:, -1]\n\n\nimport pandas as pd\nimport numpy as np\n\ndata_url = \"http://lib.stat.cmu.edu/datasets/boston\"\nraw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=21, header=None)\n\n# 데이터셋에서 14번째 변수(MEDV)는 target 변수로 사용\n# 데이터와 target을 분리\ndata2 = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :3]])\n\nfeautre_names=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT']\n# 데이터셋의 크기를 확인\nprint(data2.shape)  # (506, 14)\n\n(506, 14)\n\n\n\ndata=data2[:,:-1]\ntarget=data2[:,-1]\n\nprint(data.shape)  # (506, 13)\nprint(target.shape)  # (506,)\n\n(506, 13)\n(506,)"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/딥러닝 회귀분석.html#데이터준비",
    "href": "posts/Study/Python Data Analysis/딥러닝 회귀분석.html#데이터준비",
    "title": "딥러닝: 회귀분석",
    "section": "2. 데이터준비",
    "text": "2. 데이터준비\n\nX=pd.DataFrame(data, columns=feautre_names)\nprint(X)\n\n        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n\n     PTRATIO       B  LSTAT  \n0       15.3  396.90   4.98  \n1       17.8  396.90   9.14  \n2       17.8  392.83   4.03  \n3       18.7  394.63   2.94  \n4       18.7  396.90   5.33  \n..       ...     ...    ...  \n501     21.0  391.99   9.67  \n502     21.0  396.90   9.08  \n503     21.0  396.90   5.64  \n504     21.0  393.45   6.48  \n505     21.0  396.90   7.88  \n\n[506 rows x 13 columns]\n\n\n- 더미 변수 CHAS제외\n\nX=X.drop(['CHAS'],axis=1)\nprint(X.head())\n\n      CRIM    ZN  INDUS    NOX     RM   AGE     DIS  RAD    TAX  PTRATIO  \\\n0  0.00632  18.0   2.31  0.538  6.575  65.2  4.0900  1.0  296.0     15.3   \n1  0.02731   0.0   7.07  0.469  6.421  78.9  4.9671  2.0  242.0     17.8   \n2  0.02729   0.0   7.07  0.469  7.185  61.1  4.9671  2.0  242.0     17.8   \n3  0.03237   0.0   2.18  0.458  6.998  45.8  6.0622  3.0  222.0     18.7   \n4  0.06905   0.0   2.18  0.458  7.147  54.2  6.0622  3.0  222.0     18.7   \n\n        B  LSTAT  \n0  396.90   4.98  \n1  396.90   9.14  \n2  392.83   4.03  \n3  394.63   2.94  \n4  396.90   5.33  \n\n\n\nX.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 506 entries, 0 to 505\nData columns (total 12 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   CRIM     506 non-null    float64\n 1   ZN       506 non-null    float64\n 2   INDUS    506 non-null    float64\n 3   NOX      506 non-null    float64\n 4   RM       506 non-null    float64\n 5   AGE      506 non-null    float64\n 6   DIS      506 non-null    float64\n 7   RAD      506 non-null    float64\n 8   TAX      506 non-null    float64\n 9   PTRATIO  506 non-null    float64\n 10  B        506 non-null    float64\n 11  LSTAT    506 non-null    float64\ndtypes: float64(12)\nmemory usage: 47.6 KB\n\n\n\ny=pd.DataFrame(target)\nprint(y)\n\n        0\n0    24.0\n1    21.6\n2    34.7\n3    33.4\n4    36.2\n..    ...\n501  22.4\n502  20.6\n503  23.9\n504  22.0\n505  11.9\n\n[506 rows x 1 columns]"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/딥러닝 회귀분석.html#탐색적-데이터-분석",
    "href": "posts/Study/Python Data Analysis/딥러닝 회귀분석.html#탐색적-데이터-분석",
    "title": "딥러닝: 회귀분석",
    "section": "3. 탐색적 데이터 분석",
    "text": "3. 탐색적 데이터 분석\n\nboston_df=pd.DataFrame(data=X)\nboston_df[\"MEDIV\"]=target\nprint(boston_df)\n\n        CRIM    ZN  INDUS    NOX     RM   AGE     DIS  RAD    TAX  PTRATIO  \\\n0    0.00632  18.0   2.31  0.538  6.575  65.2  4.0900  1.0  296.0     15.3   \n1    0.02731   0.0   7.07  0.469  6.421  78.9  4.9671  2.0  242.0     17.8   \n2    0.02729   0.0   7.07  0.469  7.185  61.1  4.9671  2.0  242.0     17.8   \n3    0.03237   0.0   2.18  0.458  6.998  45.8  6.0622  3.0  222.0     18.7   \n4    0.06905   0.0   2.18  0.458  7.147  54.2  6.0622  3.0  222.0     18.7   \n..       ...   ...    ...    ...    ...   ...     ...  ...    ...      ...   \n501  0.06263   0.0  11.93  0.573  6.593  69.1  2.4786  1.0  273.0     21.0   \n502  0.04527   0.0  11.93  0.573  6.120  76.7  2.2875  1.0  273.0     21.0   \n503  0.06076   0.0  11.93  0.573  6.976  91.0  2.1675  1.0  273.0     21.0   \n504  0.10959   0.0  11.93  0.573  6.794  89.3  2.3889  1.0  273.0     21.0   \n505  0.04741   0.0  11.93  0.573  6.030  80.8  2.5050  1.0  273.0     21.0   \n\n          B  LSTAT  MEDIV  \n0    396.90   4.98   24.0  \n1    396.90   9.14   21.6  \n2    392.83   4.03   34.7  \n3    394.63   2.94   33.4  \n4    396.90   5.33   36.2  \n..      ...    ...    ...  \n501  391.99   9.67   22.4  \n502  396.90   9.08   20.6  \n503  396.90   5.64   23.9  \n504  393.45   6.48   22.0  \n505  396.90   7.88   11.9  \n\n[506 rows x 13 columns]\n\n\n- 히스토그램\n\n#목표변수 값 히스토그램\nsns.set(rc={'figure.figsize':(10,5)})\nsns.distplot(boston_df['MEDIV'],bins=10)\nplt.show()\n\n\n\n\n- 상관계수\n\n# 각 변수별 상관계수\ncorrelation_matrix=boston_df.corr().round(2)\ncorrelation_matrix\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nB\nLSTAT\nMEDIV\n\n\n\n\nCRIM\n1.00\n-0.20\n0.41\n0.42\n-0.22\n0.35\n-0.38\n0.63\n0.58\n0.29\n-0.39\n0.46\n-0.39\n\n\nZN\n-0.20\n1.00\n-0.53\n-0.52\n0.31\n-0.57\n0.66\n-0.31\n-0.31\n-0.39\n0.18\n-0.41\n0.36\n\n\nINDUS\n0.41\n-0.53\n1.00\n0.76\n-0.39\n0.64\n-0.71\n0.60\n0.72\n0.38\n-0.36\n0.60\n-0.48\n\n\nNOX\n0.42\n-0.52\n0.76\n1.00\n-0.30\n0.73\n-0.77\n0.61\n0.67\n0.19\n-0.38\n0.59\n-0.43\n\n\nRM\n-0.22\n0.31\n-0.39\n-0.30\n1.00\n-0.24\n0.21\n-0.21\n-0.29\n-0.36\n0.13\n-0.61\n0.70\n\n\nAGE\n0.35\n-0.57\n0.64\n0.73\n-0.24\n1.00\n-0.75\n0.46\n0.51\n0.26\n-0.27\n0.60\n-0.38\n\n\nDIS\n-0.38\n0.66\n-0.71\n-0.77\n0.21\n-0.75\n1.00\n-0.49\n-0.53\n-0.23\n0.29\n-0.50\n0.25\n\n\nRAD\n0.63\n-0.31\n0.60\n0.61\n-0.21\n0.46\n-0.49\n1.00\n0.91\n0.46\n-0.44\n0.49\n-0.38\n\n\nTAX\n0.58\n-0.31\n0.72\n0.67\n-0.29\n0.51\n-0.53\n0.91\n1.00\n0.46\n-0.44\n0.54\n-0.47\n\n\nPTRATIO\n0.29\n-0.39\n0.38\n0.19\n-0.36\n0.26\n-0.23\n0.46\n0.46\n1.00\n-0.18\n0.37\n-0.51\n\n\nB\n-0.39\n0.18\n-0.36\n-0.38\n0.13\n-0.27\n0.29\n-0.44\n-0.44\n-0.18\n1.00\n-0.37\n0.33\n\n\nLSTAT\n0.46\n-0.41\n0.60\n0.59\n-0.61\n0.60\n-0.50\n0.49\n0.54\n0.37\n-0.37\n1.00\n-0.74\n\n\nMEDIV\n-0.39\n0.36\n-0.48\n-0.43\n0.70\n-0.38\n0.25\n-0.38\n-0.47\n-0.51\n0.33\n-0.74\n1.00\n\n\n\n\n\n\n\n\nsns.set(rc={'figure.figsize':(10,8)})\nsns.heatmap(data=correlation_matrix,annot=True)\nplt.show()"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/딥러닝 회귀분석.html#데이터분리",
    "href": "posts/Study/Python Data Analysis/딥러닝 회귀분석.html#데이터분리",
    "title": "딥러닝: 회귀분석",
    "section": "4. 데이터분리",
    "text": "4. 데이터분리\n\n학습용과 테스트용 데이터를 7:3으로 분리하자.\n\n\nX.shape\n\n(506, 13)\n\n\n\nX=X.drop(['MEDIV'],axis=1)\nX\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nB\nLSTAT\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0.538\n6.575\n65.2\n4.0900\n1.0\n296.0\n15.3\n396.90\n4.98\n\n\n1\n0.02731\n0.0\n7.07\n0.469\n6.421\n78.9\n4.9671\n2.0\n242.0\n17.8\n396.90\n9.14\n\n\n2\n0.02729\n0.0\n7.07\n0.469\n7.185\n61.1\n4.9671\n2.0\n242.0\n17.8\n392.83\n4.03\n\n\n3\n0.03237\n0.0\n2.18\n0.458\n6.998\n45.8\n6.0622\n3.0\n222.0\n18.7\n394.63\n2.94\n\n\n4\n0.06905\n0.0\n2.18\n0.458\n7.147\n54.2\n6.0622\n3.0\n222.0\n18.7\n396.90\n5.33\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n501\n0.06263\n0.0\n11.93\n0.573\n6.593\n69.1\n2.4786\n1.0\n273.0\n21.0\n391.99\n9.67\n\n\n502\n0.04527\n0.0\n11.93\n0.573\n6.120\n76.7\n2.2875\n1.0\n273.0\n21.0\n396.90\n9.08\n\n\n503\n0.06076\n0.0\n11.93\n0.573\n6.976\n91.0\n2.1675\n1.0\n273.0\n21.0\n396.90\n5.64\n\n\n504\n0.10959\n0.0\n11.93\n0.573\n6.794\n89.3\n2.3889\n1.0\n273.0\n21.0\n393.45\n6.48\n\n\n505\n0.04741\n0.0\n11.93\n0.573\n6.030\n80.8\n2.5050\n1.0\n273.0\n21.0\n396.90\n7.88\n\n\n\n\n506 rows × 12 columns\n\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/딥러닝 회귀분석.html#피처-스케일링",
    "href": "posts/Study/Python Data Analysis/딥러닝 회귀분석.html#피처-스케일링",
    "title": "딥러닝: 회귀분석",
    "section": "5. 피처 스케일링",
    "text": "5. 피처 스케일링\n\n학습용 입력 데이터에 대한 정규화 스케일러 만들고 입력데이터를 0~1로 정규화하기\n\n\n# 정규화 스케일러 생성\nscalerX=MinMaxScaler()\n\n# 정규화 스케일러를 학습용 데이터에 맞춤\nscalerX.fit(X_train)\n\n# 정규화 스케일러로 학습용 데이터 변환\nX_train_norm=scalerX.transform(X_train)\n\n# 정규화 스케일러로 테스트용 데이터 변환\nX_test_norm = scalerX.transform(X_test)\n\n\nprint(X_train_norm)\n\n[[0.00680253 0.2        0.11962963 ... 0.04255319 0.98184477 0.08724646]\n [0.07235853 0.         0.64296296 ... 0.80851064 0.24617984 0.27924423]\n [0.1610363  0.         0.64296296 ... 0.80851064 0.93953301 0.7957766 ]\n ...\n [0.11494542 0.         0.64296296 ... 0.80851064 0.95662918 0.44595721]\n [0.1717624  0.         0.64296296 ... 0.80851064 0.91456957 0.59071964]\n [0.00187693 0.         0.33148148 ... 0.70212766 1.         0.27868852]]\n\n\n\nX_train_norm.shape\n\n(354, 12)\n\n\n\nprint(X_test_norm)\n\n[[ 2.05400974e-02  0.00000000e+00  6.97777778e-01 ...  2.23404255e-01\n   9.81617832e-01 -1.66712976e-03]\n [ 1.09008802e-02  0.00000000e+00  7.83333333e-01 ...  9.14893617e-01\n   6.61758031e-01  4.25951653e-01]\n [ 4.66707160e-01  0.00000000e+00  6.42962963e-01 ...  8.08510638e-01\n   8.29946039e-01  7.05751598e-01]\n ...\n [ 1.61036297e-01  0.00000000e+00  6.42962963e-01 ...  8.08510638e-01\n   9.65757224e-01  3.09252570e-01]\n [ 9.47848868e-04  2.00000000e-01  2.30370370e-01 ...  6.38297872e-01\n   9.85980130e-01  3.24256738e-01]\n [ 4.01371790e-04  2.80000000e-01  5.29629630e-01 ...  5.95744681e-01\n   9.95234253e-01  1.71714365e-01]]\n\n\n\nX_test_norm.shape\n\n(152, 12)\n\n\n\n# 정규화 스케일러 생성\nscalerY=MinMaxScaler()\n\n# 정규화 스케일러를 학습용 데이터에 맞춤\nscalerY.fit(y_train)\n\n# 정규화 스케일러로 학습용 데이터 변환\ny_train_norm=scalerY.transform(y_train)\n\n# 정규화 스케일러로 테스트용 데이터 변환\ny_test_norm = scalerY.transform(y_test)\n\n\nprint(y_train_norm[0:10])\n\n[[1.        ]\n [0.24666667]\n [0.11555556]\n [0.3       ]\n [0.17111111]\n [0.30888889]\n [0.28444444]\n [0.32666667]\n [0.33111111]\n [0.52444444]]\n\n\n\ny_train_norm.shape\n\n(354, 1)\n\n\n\nprint(y_test_norm[0:10])\n\n[[1.        ]\n [0.23555556]\n [0.07777778]\n [0.12222222]\n [0.53555556]\n [0.92666667]\n [0.40222222]\n [0.58      ]\n [0.57333333]\n [0.34888889]]\n\n\n\ny_test_norm.shape\n\n(152, 1)"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/딥러닝 회귀분석.html#모형화-및-학습",
    "href": "posts/Study/Python Data Analysis/딥러닝 회귀분석.html#모형화-및-학습",
    "title": "딥러닝: 회귀분석",
    "section": "6. 모형화 및 학습",
    "text": "6. 모형화 및 학습\n\nmodel=Sequential() # 순차모형\nmodel.add(Dense(60,activation='relu', input_shape=(12,))) # 제 1은닉충과 입력층\nmodel.add(Dense(60,activation='relu')) # 제 2은닉충\nmodel.add(Dense(30,activation='relu')) # 제 3은닉충\nmodel.add(Dense(1)) # 출력층 (선형 활섬화함수)\n\n\nmodel.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_12 (Dense)            (None, 60)                780       \n                                                                 \n dense_13 (Dense)            (None, 60)                3660      \n                                                                 \n dense_14 (Dense)            (None, 30)                1830      \n                                                                 \n dense_15 (Dense)            (None, 1)                 31        \n                                                                 \n=================================================================\nTotal params: 6,301\nTrainable params: 6,301\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nParam : 가중치 12x60=720, 편향:60\n\n\nmodel.compile(optimizer='adam',\n              loss='mse',\n              metrics=['mae'])\n\n- 학습\n\nresults=model.fit(X_train_norm, y_train_norm,\n                  validation_data=(X_test_norm, y_test_norm),\n                  epochs=200, batch_size=32)\n\nEpoch 1/200\n12/12 [==============================] - 0s 7ms/step - loss: 0.1031 - mae: 0.2534 - val_loss: 0.0570 - val_mae: 0.1836\nEpoch 2/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0472 - mae: 0.1690 - val_loss: 0.0401 - val_mae: 0.1482\nEpoch 3/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1291 - val_loss: 0.0295 - val_mae: 0.1203\nEpoch 4/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1095 - val_loss: 0.0234 - val_mae: 0.1124\nEpoch 5/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0224 - mae: 0.1113 - val_loss: 0.0205 - val_mae: 0.0991\nEpoch 6/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0176 - mae: 0.0876 - val_loss: 0.0162 - val_mae: 0.0880\nEpoch 7/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0144 - mae: 0.0785 - val_loss: 0.0135 - val_mae: 0.0801\nEpoch 8/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0123 - mae: 0.0711 - val_loss: 0.0118 - val_mae: 0.0791\nEpoch 9/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0110 - mae: 0.0675 - val_loss: 0.0108 - val_mae: 0.0707\nEpoch 10/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0100 - mae: 0.0645 - val_loss: 0.0105 - val_mae: 0.0756\nEpoch 11/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0097 - mae: 0.0650 - val_loss: 0.0095 - val_mae: 0.0690\nEpoch 12/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0089 - mae: 0.0612 - val_loss: 0.0092 - val_mae: 0.0682\nEpoch 13/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0082 - mae: 0.0588 - val_loss: 0.0090 - val_mae: 0.0672\nEpoch 14/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0080 - mae: 0.0565 - val_loss: 0.0102 - val_mae: 0.0783\nEpoch 15/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0089 - mae: 0.0635 - val_loss: 0.0090 - val_mae: 0.0702\nEpoch 16/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0121 - mae: 0.0826 - val_loss: 0.0117 - val_mae: 0.0826\nEpoch 17/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0088 - mae: 0.0666 - val_loss: 0.0088 - val_mae: 0.0703\nEpoch 18/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0544 - val_loss: 0.0077 - val_mae: 0.0625\nEpoch 19/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0067 - mae: 0.0540 - val_loss: 0.0073 - val_mae: 0.0614\nEpoch 20/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0062 - mae: 0.0518 - val_loss: 0.0072 - val_mae: 0.0605\nEpoch 21/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0063 - mae: 0.0513 - val_loss: 0.0070 - val_mae: 0.0599\nEpoch 22/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0059 - mae: 0.0507 - val_loss: 0.0069 - val_mae: 0.0594\nEpoch 23/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0064 - mae: 0.0518 - val_loss: 0.0075 - val_mae: 0.0646\nEpoch 24/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0061 - mae: 0.0507 - val_loss: 0.0065 - val_mae: 0.0573\nEpoch 25/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0057 - mae: 0.0495 - val_loss: 0.0064 - val_mae: 0.0568\nEpoch 26/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0055 - mae: 0.0504 - val_loss: 0.0067 - val_mae: 0.0580\nEpoch 27/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0058 - mae: 0.0512 - val_loss: 0.0066 - val_mae: 0.0570\nEpoch 28/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0056 - mae: 0.0487 - val_loss: 0.0064 - val_mae: 0.0564\nEpoch 29/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0052 - mae: 0.0484 - val_loss: 0.0063 - val_mae: 0.0554\nEpoch 30/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0049 - mae: 0.0469 - val_loss: 0.0063 - val_mae: 0.0560\nEpoch 31/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0049 - mae: 0.0472 - val_loss: 0.0061 - val_mae: 0.0545\nEpoch 32/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0045 - mae: 0.0448 - val_loss: 0.0063 - val_mae: 0.0546\nEpoch 33/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0045 - mae: 0.0452 - val_loss: 0.0061 - val_mae: 0.0537\nEpoch 34/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0044 - mae: 0.0456 - val_loss: 0.0061 - val_mae: 0.0540\nEpoch 35/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0045 - mae: 0.0454 - val_loss: 0.0061 - val_mae: 0.0533\nEpoch 36/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0041 - mae: 0.0436 - val_loss: 0.0060 - val_mae: 0.0520\nEpoch 37/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0043 - mae: 0.0449 - val_loss: 0.0062 - val_mae: 0.0529\nEpoch 38/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0041 - mae: 0.0444 - val_loss: 0.0060 - val_mae: 0.0529\nEpoch 39/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0040 - mae: 0.0439 - val_loss: 0.0060 - val_mae: 0.0518\nEpoch 40/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0046 - mae: 0.0481 - val_loss: 0.0087 - val_mae: 0.0648\nEpoch 41/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0046 - mae: 0.0467 - val_loss: 0.0060 - val_mae: 0.0524\nEpoch 42/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0041 - mae: 0.0453 - val_loss: 0.0060 - val_mae: 0.0533\nEpoch 43/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0043 - mae: 0.0454 - val_loss: 0.0067 - val_mae: 0.0548\nEpoch 44/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0045 - mae: 0.0501 - val_loss: 0.0060 - val_mae: 0.0503\nEpoch 45/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0039 - mae: 0.0451 - val_loss: 0.0063 - val_mae: 0.0548\nEpoch 46/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0038 - mae: 0.0446 - val_loss: 0.0056 - val_mae: 0.0494\nEpoch 47/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0035 - mae: 0.0423 - val_loss: 0.0057 - val_mae: 0.0496\nEpoch 48/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0037 - mae: 0.0426 - val_loss: 0.0060 - val_mae: 0.0509\nEpoch 49/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0036 - mae: 0.0425 - val_loss: 0.0071 - val_mae: 0.0555\nEpoch 50/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0092 - mae: 0.0708 - val_loss: 0.0089 - val_mae: 0.0701\nEpoch 51/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0050 - mae: 0.0526 - val_loss: 0.0069 - val_mae: 0.0579\nEpoch 52/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0041 - mae: 0.0456 - val_loss: 0.0060 - val_mae: 0.0510\nEpoch 53/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0041 - mae: 0.0462 - val_loss: 0.0062 - val_mae: 0.0528\nEpoch 54/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0033 - mae: 0.0409 - val_loss: 0.0063 - val_mae: 0.0516\nEpoch 55/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0414 - val_loss: 0.0064 - val_mae: 0.0529\nEpoch 56/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0414 - val_loss: 0.0064 - val_mae: 0.0545\nEpoch 57/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0035 - mae: 0.0401 - val_loss: 0.0062 - val_mae: 0.0504\nEpoch 58/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0387 - val_loss: 0.0065 - val_mae: 0.0525\nEpoch 59/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0030 - mae: 0.0385 - val_loss: 0.0068 - val_mae: 0.0541\nEpoch 60/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0404 - val_loss: 0.0061 - val_mae: 0.0506\nEpoch 61/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0425 - val_loss: 0.0071 - val_mae: 0.0564\nEpoch 62/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0033 - mae: 0.0405 - val_loss: 0.0070 - val_mae: 0.0545\nEpoch 63/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0395 - val_loss: 0.0068 - val_mae: 0.0534\nEpoch 64/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0399 - val_loss: 0.0064 - val_mae: 0.0527\nEpoch 65/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0382 - val_loss: 0.0062 - val_mae: 0.0517\nEpoch 66/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0379 - val_loss: 0.0063 - val_mae: 0.0512\nEpoch 67/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0379 - val_loss: 0.0062 - val_mae: 0.0505\nEpoch 68/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0365 - val_loss: 0.0063 - val_mae: 0.0513\nEpoch 69/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0383 - val_loss: 0.0066 - val_mae: 0.0514\nEpoch 70/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0370 - val_loss: 0.0066 - val_mae: 0.0505\nEpoch 71/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0386 - val_loss: 0.0066 - val_mae: 0.0520\nEpoch 72/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0373 - val_loss: 0.0067 - val_mae: 0.0515\nEpoch 73/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0367 - val_loss: 0.0063 - val_mae: 0.0509\nEpoch 74/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0364 - val_loss: 0.0064 - val_mae: 0.0519\nEpoch 75/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0422 - val_loss: 0.0060 - val_mae: 0.0496\nEpoch 76/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0353 - val_loss: 0.0068 - val_mae: 0.0534\nEpoch 77/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0381 - val_loss: 0.0064 - val_mae: 0.0499\nEpoch 78/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0357 - val_loss: 0.0060 - val_mae: 0.0495\nEpoch 79/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0353 - val_loss: 0.0059 - val_mae: 0.0490\nEpoch 80/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0366 - val_loss: 0.0065 - val_mae: 0.0517\nEpoch 81/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0363 - val_loss: 0.0064 - val_mae: 0.0507\nEpoch 82/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0350 - val_loss: 0.0062 - val_mae: 0.0499\nEpoch 83/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0342 - val_loss: 0.0064 - val_mae: 0.0513\nEpoch 84/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0351 - val_loss: 0.0062 - val_mae: 0.0499\nEpoch 85/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0335 - val_loss: 0.0061 - val_mae: 0.0500\nEpoch 86/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0346 - val_loss: 0.0062 - val_mae: 0.0517\nEpoch 87/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0345 - val_loss: 0.0061 - val_mae: 0.0507\nEpoch 88/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0331 - val_loss: 0.0061 - val_mae: 0.0500\nEpoch 89/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0331 - val_loss: 0.0062 - val_mae: 0.0512\nEpoch 90/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0330 - val_loss: 0.0062 - val_mae: 0.0498\nEpoch 91/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0327 - val_loss: 0.0062 - val_mae: 0.0496\nEpoch 92/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0328 - val_loss: 0.0061 - val_mae: 0.0496\nEpoch 93/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0319 - val_loss: 0.0063 - val_mae: 0.0506\nEpoch 94/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0329 - val_loss: 0.0062 - val_mae: 0.0506\nEpoch 95/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0318 - val_loss: 0.0061 - val_mae: 0.0503\nEpoch 96/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0315 - val_loss: 0.0062 - val_mae: 0.0507\nEpoch 97/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0317 - val_loss: 0.0061 - val_mae: 0.0501\nEpoch 98/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0315 - val_loss: 0.0063 - val_mae: 0.0505\nEpoch 99/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0330 - val_loss: 0.0062 - val_mae: 0.0498\nEpoch 100/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0313 - val_loss: 0.0067 - val_mae: 0.0527\nEpoch 101/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0310 - val_loss: 0.0063 - val_mae: 0.0507\nEpoch 102/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0318 - val_loss: 0.0066 - val_mae: 0.0519\nEpoch 103/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0343 - val_loss: 0.0063 - val_mae: 0.0517\nEpoch 104/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0317 - val_loss: 0.0063 - val_mae: 0.0508\nEpoch 105/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0325 - val_loss: 0.0065 - val_mae: 0.0513\nEpoch 106/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0339 - val_loss: 0.0060 - val_mae: 0.0501\nEpoch 107/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0311 - val_loss: 0.0068 - val_mae: 0.0528\nEpoch 108/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0386 - val_loss: 0.0067 - val_mae: 0.0512\nEpoch 109/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0335 - val_loss: 0.0070 - val_mae: 0.0530\nEpoch 110/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0404 - val_loss: 0.0065 - val_mae: 0.0517\nEpoch 111/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0407 - val_loss: 0.0058 - val_mae: 0.0501\nEpoch 112/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0328 - val_loss: 0.0060 - val_mae: 0.0507\nEpoch 113/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0331 - val_loss: 0.0059 - val_mae: 0.0495\nEpoch 114/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0341 - val_loss: 0.0064 - val_mae: 0.0505\nEpoch 115/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0324 - val_loss: 0.0065 - val_mae: 0.0518\nEpoch 116/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0303 - val_loss: 0.0061 - val_mae: 0.0500\nEpoch 117/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0303 - val_loss: 0.0060 - val_mae: 0.0502\nEpoch 118/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0320 - val_loss: 0.0062 - val_mae: 0.0509\nEpoch 119/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0317 - val_loss: 0.0060 - val_mae: 0.0499\nEpoch 120/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0309 - val_loss: 0.0064 - val_mae: 0.0512\nEpoch 121/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0355 - val_loss: 0.0063 - val_mae: 0.0514\nEpoch 122/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0379 - val_loss: 0.0075 - val_mae: 0.0617\nEpoch 123/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0388 - val_loss: 0.0055 - val_mae: 0.0484\nEpoch 124/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0330 - val_loss: 0.0065 - val_mae: 0.0531\nEpoch 125/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0325 - val_loss: 0.0063 - val_mae: 0.0504\nEpoch 126/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0306 - val_loss: 0.0065 - val_mae: 0.0512\nEpoch 127/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0317 - val_loss: 0.0061 - val_mae: 0.0503\nEpoch 128/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0292 - val_loss: 0.0060 - val_mae: 0.0502\nEpoch 129/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0297 - val_loss: 0.0065 - val_mae: 0.0531\nEpoch 130/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0302 - val_loss: 0.0063 - val_mae: 0.0503\nEpoch 131/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0293 - val_loss: 0.0066 - val_mae: 0.0525\nEpoch 132/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0305 - val_loss: 0.0060 - val_mae: 0.0499\nEpoch 133/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0315 - val_loss: 0.0061 - val_mae: 0.0505\nEpoch 134/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0299 - val_loss: 0.0066 - val_mae: 0.0515\nEpoch 135/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0301 - val_loss: 0.0065 - val_mae: 0.0520\nEpoch 136/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0321 - val_loss: 0.0064 - val_mae: 0.0509\nEpoch 137/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0343 - val_loss: 0.0059 - val_mae: 0.0505\nEpoch 138/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0303 - val_loss: 0.0058 - val_mae: 0.0489\nEpoch 139/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0308 - val_loss: 0.0061 - val_mae: 0.0500\nEpoch 140/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0287 - val_loss: 0.0062 - val_mae: 0.0494\nEpoch 141/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0285 - val_loss: 0.0062 - val_mae: 0.0496\nEpoch 142/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0280 - val_loss: 0.0061 - val_mae: 0.0497\nEpoch 143/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0279 - val_loss: 0.0061 - val_mae: 0.0499\nEpoch 144/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0300 - val_loss: 0.0061 - val_mae: 0.0492\nEpoch 145/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0293 - val_loss: 0.0064 - val_mae: 0.0512\nEpoch 146/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0304 - val_loss: 0.0065 - val_mae: 0.0515\nEpoch 147/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0374 - val_loss: 0.0062 - val_mae: 0.0497\nEpoch 148/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0316 - val_loss: 0.0061 - val_mae: 0.0515\nEpoch 149/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0305 - val_loss: 0.0062 - val_mae: 0.0523\nEpoch 150/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0281 - val_loss: 0.0059 - val_mae: 0.0492\nEpoch 151/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0283 - val_loss: 0.0062 - val_mae: 0.0512\nEpoch 152/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0289 - val_loss: 0.0062 - val_mae: 0.0501\nEpoch 153/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0283 - val_loss: 0.0061 - val_mae: 0.0488\nEpoch 154/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0277 - val_loss: 0.0061 - val_mae: 0.0492\nEpoch 155/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0272 - val_loss: 0.0063 - val_mae: 0.0489\nEpoch 156/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0268 - val_loss: 0.0063 - val_mae: 0.0494\nEpoch 157/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0269 - val_loss: 0.0062 - val_mae: 0.0494\nEpoch 158/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0259 - val_loss: 0.0063 - val_mae: 0.0489\nEpoch 159/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0260 - val_loss: 0.0062 - val_mae: 0.0500\nEpoch 160/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0263 - val_loss: 0.0073 - val_mae: 0.0537\nEpoch 161/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0360 - val_loss: 0.0071 - val_mae: 0.0572\nEpoch 162/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0360 - val_loss: 0.0069 - val_mae: 0.0546\nEpoch 163/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0342 - val_loss: 0.0066 - val_mae: 0.0527\nEpoch 164/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0356 - val_loss: 0.0061 - val_mae: 0.0509\nEpoch 165/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0331 - val_loss: 0.0065 - val_mae: 0.0516\nEpoch 166/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0319 - val_loss: 0.0063 - val_mae: 0.0499\nEpoch 167/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0325 - val_loss: 0.0077 - val_mae: 0.0570\nEpoch 168/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0374 - val_loss: 0.0066 - val_mae: 0.0519\nEpoch 169/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0365 - val_loss: 0.0066 - val_mae: 0.0520\nEpoch 170/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0377 - val_loss: 0.0058 - val_mae: 0.0501\nEpoch 171/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0311 - val_loss: 0.0061 - val_mae: 0.0517\nEpoch 172/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0300 - val_loss: 0.0060 - val_mae: 0.0501\nEpoch 173/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0322 - val_loss: 0.0060 - val_mae: 0.0498\nEpoch 174/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0274 - val_loss: 0.0061 - val_mae: 0.0494\nEpoch 175/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0269 - val_loss: 0.0063 - val_mae: 0.0495\nEpoch 176/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0263 - val_loss: 0.0061 - val_mae: 0.0495\nEpoch 177/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0258 - val_loss: 0.0060 - val_mae: 0.0501\nEpoch 178/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0401 - val_loss: 0.0057 - val_mae: 0.0493\nEpoch 179/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0354 - val_loss: 0.0061 - val_mae: 0.0501\nEpoch 180/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0301 - val_loss: 0.0063 - val_mae: 0.0511\nEpoch 181/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0275 - val_loss: 0.0070 - val_mae: 0.0533\nEpoch 182/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0277 - val_loss: 0.0059 - val_mae: 0.0485\nEpoch 183/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0289 - val_loss: 0.0061 - val_mae: 0.0484\nEpoch 184/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0271 - val_loss: 0.0071 - val_mae: 0.0546\nEpoch 185/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0305 - val_loss: 0.0066 - val_mae: 0.0513\nEpoch 186/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0270 - val_loss: 0.0061 - val_mae: 0.0483\nEpoch 187/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0267 - val_loss: 0.0062 - val_mae: 0.0481\nEpoch 188/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0283 - val_loss: 0.0066 - val_mae: 0.0496\nEpoch 189/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0261 - val_loss: 0.0060 - val_mae: 0.0476\nEpoch 190/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0266 - val_loss: 0.0062 - val_mae: 0.0483\nEpoch 191/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0300 - val_loss: 0.0070 - val_mae: 0.0524\nEpoch 192/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0287 - val_loss: 0.0065 - val_mae: 0.0515\nEpoch 193/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0283 - val_loss: 0.0062 - val_mae: 0.0481\nEpoch 194/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0269 - val_loss: 0.0059 - val_mae: 0.0482\nEpoch 195/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0264 - val_loss: 0.0060 - val_mae: 0.0490\nEpoch 196/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0251 - val_loss: 0.0059 - val_mae: 0.0480\nEpoch 197/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0255 - val_loss: 0.0063 - val_mae: 0.0510\nEpoch 198/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0277 - val_loss: 0.0059 - val_mae: 0.0479\nEpoch 199/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0257 - val_loss: 0.0065 - val_mae: 0.0513\nEpoch 200/200\n12/12 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0272 - val_loss: 0.0066 - val_mae: 0.0507\n\n\n\nprint(results.history.keys())\n\ndict_keys(['loss', 'mae', 'val_loss', 'val_mae'])\n\n\n\nloss : 학습 데이터 비용\nmae : 학습 데이터 오차\nval_loss : 테스트 데이터 비용\nval_mae : 테스트 데이터 오차\n\n\n# 학습 수에 따른 loss 변화\nplt.figure(figsize=(10,5))\nplt.plot(results.history['loss']) \nplt.plot(results.history['val_loss'])\nplt.title('loss')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['train','test'],loc='upper right')\nplt.show()\n\n\n\n\n\n# 학습 수에 따른 정확도(mae) 변화\nplt.figure(figsize=(10,5))\nplt.plot(results.history['mae']) \nplt.plot(results.history['val_loss'])\nplt.title('accuracy(MAE)')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend(['train','test'],loc='upper right')\nplt.show()"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/딥러닝 회귀분석.html#예측",
    "href": "posts/Study/Python Data Analysis/딥러닝 회귀분석.html#예측",
    "title": "딥러닝: 회귀분석",
    "section": "7. 예측",
    "text": "7. 예측\n\n# 테스트 데이터 예측\ny_pred=model.predict(X_test_norm).flatten()\n\n# 예측 값의 역변환\ny_pred_inverse=scalerY.inverse_transform(y_pred.reshape(-1,1))\nprint(y_pred_inverse[0:10])\n\n5/5 [==============================] - 0s 540us/step\n[[50.411358 ]\n [13.942274 ]\n [ 6.1426053]\n [10.471625 ]\n [30.684546 ]\n [44.55355  ]\n [26.434183 ]\n [31.944752 ]\n [32.296173 ]\n [23.545477 ]]\n\n\n\n# 오차측정(MAE)\nprint('MAE:%.2f' %mean_absolute_error(y_test, y_pred_inverse))\n\nMAE:2.28\n\n\n\n# 실제 값 대비 예측 값의 산포도\nplt.figure(figsize=(7,7))\nplt.scatter(y_test_norm,y_pred,c='r')\nplt.xlabel('TrueValues')\nplt.ylabel('Predictions')\nplt.axis('equal')\nplt.xlim(0,1)\nplt.ylim(0,1)\nplt.plot([0,1],[0,1])\nplt.show()"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/딥러닝 회귀분석.html#드랍아웃-모형-추가",
    "href": "posts/Study/Python Data Analysis/딥러닝 회귀분석.html#드랍아웃-모형-추가",
    "title": "딥러닝: 회귀분석",
    "section": "8. 드랍아웃 모형 추가",
    "text": "8. 드랍아웃 모형 추가\n\nfrom keras.layers import Dropout\n\n\nmodel=Sequential() # 순차모형\nmodel.add(Dense(60,activation='relu', input_shape=(12,))) # 제 1은닉충과 입력층\nmodel.add(Dropout(0.5)) # 제 1은닉충과 2은닉충 사이의 드롭 아웃 50%\nmodel.add(Dense(60,activation='relu')) # 제 2은닉충\nmodel.add(Dense(30,activation='relu')) # 제 3은닉충\nmodel.add(Dropout(0.2)) # 제 3은닉충과 출력층 사이의 드롭 아웃 20%\nmodel.add(Dense(1)) # 출력층 (선형 활섬화함수)\n\n\nmodel.summary()\n\nModel: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_20 (Dense)            (None, 60)                780       \n                                                                 \n dropout_2 (Dropout)         (None, 60)                0         \n                                                                 \n dense_21 (Dense)            (None, 60)                3660      \n                                                                 \n dense_22 (Dense)            (None, 30)                1830      \n                                                                 \n dropout_3 (Dropout)         (None, 30)                0         \n                                                                 \n dense_23 (Dense)            (None, 1)                 31        \n                                                                 \n=================================================================\nTotal params: 6,301\nTrainable params: 6,301\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nmodel.compile(optimizer='adam',\n              loss='mse',\n              metrics=['mae'])\n\n\n# results=model.fit(X_train_norm, y_train_norm,\n#                   validation_data=(X_test_norm, y_test_norm),\n#                   epochs=1000, batch_size=20)\n\n\nprint(results.history.keys())\n\ndict_keys(['loss', 'mae', 'val_loss', 'val_mae'])\n\n\n\n# 학습 수에 따른 loss 변화\nplt.figure(figsize=(10,5))\nplt.plot(results.history['loss']) \nplt.plot(results.history['val_loss'])\nplt.title('loss')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['train','test'],loc='upper right')\nplt.show()\n\n\n\n\n\n# 학습 수에 따른 정확도(mae) 변화\nplt.figure(figsize=(10,5))\nplt.plot(results.history['mae']) \nplt.plot(results.history['val_loss'])\nplt.title('accuracy(MAE)')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend(['train','test'],loc='upper right')\nplt.show()\n\n\n\n\n\n# 테스트 데이터 예측\ny_pred=model.predict(X_test_norm).flatten()\n\n# 예측 값의 역변환\ny_pred_inverse=scalerY.inverse_transform(y_pred.reshape(-1,1))\nprint(y_pred_inverse[0:10])\n\n5/5 [==============================] - 0s 517us/step\n[[36.150234]\n [15.184836]\n [ 8.98333 ]\n [12.427422]\n [26.98419 ]\n [34.612114]\n [20.946304]\n [27.669504]\n [28.082466]\n [23.544407]]\n\n\n\n# 오차측정(MAE)\nprint('MAE:%.2f' %mean_absolute_error(y_test, y_pred_inverse))\n\nMAE:3.00\n\n\n\n# 실제 값 대비 예측 값의 산포도\nplt.figure(figsize=(7,7))\nplt.scatter(y_test_norm,y_pred,c='r')\nplt.xlabel('TrueValues')\nplt.ylabel('Predictions')\nplt.axis('equal')\nplt.xlim(0,1)\nplt.ylim(0,1)\nplt.plot([0,1],[0,1])\nplt.show()"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/딥러닝 회귀분석.html#입력-노드의-드랍-아웃",
    "href": "posts/Study/Python Data Analysis/딥러닝 회귀분석.html#입력-노드의-드랍-아웃",
    "title": "딥러닝: 회귀분석",
    "section": "9. 입력 노드의 드랍 아웃",
    "text": "9. 입력 노드의 드랍 아웃\nmodel=Sequential() # 순차모형\nmodel.add(Dropout(0.2, input_shape=(12,))) # 입력 노드에 대한 드랍아웃 비율 0.2\nmodel.add(Dense(60,activation='relu', input_shape=(12,))) # 제 1은닉충과 입력층\nmodel.add(Dense(60,activation='relu')) # 제 2은닉충\nmodel.add(Dense(30,activation='relu')) # 제 3은닉충\nmodel.add(Dense(1)) # 출력층 (선형 활섬화함수)"
  },
  {
    "objectID": "posts/Study/주성분 분석.html",
    "href": "posts/Study/주성분 분석.html",
    "title": "주성분 분석(PCA)",
    "section": "",
    "text": "REF\nhttp://rfriend.tistory.com/61\n차원축소(dimension reduction) : PCA(Principal Component Analysis)\n\nsecu_com_finance_2007 &lt;- read.csv(\"secu_com_finance_2007.csv\",\n                                  header = TRUE, \n                                  stringsAsFactors = FALSE)\n\n\nV1 : 총자본순이익율\nV2 : 자기자본순이익율\nV3 : 자기자본비율\nV4 : 부채비율\nV5 : 자기자본회전율\n\n\n\n# 표준화 변환 (standardization)\nsecu_com_finance_2007 &lt;- transform(secu_com_finance_2007, \n                                   V1_s = scale(V1), \n                                   V2_s = scale(V2), \n                                   V3_s = scale(V3), \n                                   V4_s = scale(V4),\n                                   V5_s = scale(V5))\n\nV1,V2,V3,V5는 숫자가 클 수록 좋고 V4는 숫자가 클수록 안좋다.\n즉, V4의 방향을 변환(표준화된 이후의 max 값에서 표준화된 이후의 관찰값을 뺌)\n\n# 부채비율(V4_s)을 방향(max(V4_s)-V4_s) 변환\nsecu_com_finance_2007 &lt;- transform(secu_com_finance_2007, \n                                    V4_s2 = max(V4_s) - V4_s)\n\n\n# variable selection\nsecu_com_finance_2007_2 &lt;- secu_com_finance_2007[,c(\"company\", \"V1_s\", \"V2_s\", \"V3_s\", \"V4_s2\", \"V5_s\")]\n\n\n# Correlation analysis\nround(cor(secu_com_finance_2007_2[,-1]), digits=3)\n\n\n\n\n\n\nV1_s\nV2_s\nV3_s\nV4_s2\nV5_s\n\n\n\n\nV1_s\n1.000\n0.617\n0.324\n0.355\n0.014\n\n\nV2_s\n0.617\n1.000\n-0.512\n-0.466\n0.423\n\n\nV3_s\n0.324\n-0.512\n1.000\n0.937\n-0.563\n\n\nV4_s2\n0.355\n-0.466\n0.937\n1.000\n-0.540\n\n\nV5_s\n0.014\n0.423\n-0.563\n-0.540\n1.000\n\n\n\nA matrix: 5 × 5 of type dbl\n\n\n# Scatter plot matrix\nplot(secu_com_finance_2007_2[,-1])\n\n\n\n\n\n# 주성분분석 PCA(Principal Component Analysis)\nsecu_prcomp &lt;- prcomp(secu_com_finance_2007_2[,c(2:6)]) # 첫번째 변수 회사명은 빼고 분석\n \nsummary(secu_prcomp)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5\nStandard deviation     1.6618 1.2671 0.7420 0.25311 0.13512\nProportion of Variance 0.5523 0.3211 0.1101 0.01281 0.00365\nCumulative Proportion  0.5523 0.8734 0.9835 0.99635 1.00000\n\n\n\nprint(secu_prcomp)\n\nStandard deviations (1, .., p=5):\n[1] 1.6617648 1.2671437 0.7419994 0.2531070 0.1351235\n\nRotation (n x k) = (5 x 5):\n              PC1        PC2           PC3          PC4         PC5\nV1_s   0.07608427 0.77966993  0.0008915975  0.140755404  0.60540325\nV2_s  -0.39463007 0.56541218 -0.2953216494 -0.117644166 -0.65078503\nV3_s   0.56970191 0.16228156  0.2412221065  0.637721889 -0.42921686\nV4_s2  0.55982770 0.19654293  0.2565972887 -0.748094314 -0.14992183\nV5_s  -0.44778451 0.08636803  0.8881182665  0.003668418 -0.05711464\n\n\n\n\n# Scree Plot\nplot(prcomp(secu_com_finance_2007_2[,c(2:6)]), type=\"l\",\n      sub = \"Scree Plot\")\n\n\n\n\n\n3개 주성분이 적합\n\n\n# Biplot\nbiplot(prcomp(secu_com_finance_2007_2[,c(2:6)]), cex = c(0.7, 0.8))\n \n # 관측치별 주성분1, 주성분2 점수 계산(PC1 score, PC2 score)\nsecu_pc1 &lt;- predict(secu_prcomp)[,1]\nsecu_pc2 &lt;- predict(secu_prcomp)[,2]\n \n \n# 관측치별 이름 매핑(rownames mapping)\n text(secu_pc1, secu_pc2, labels = secu_com_finance_2007_2$company, cex = 0.7, pos = 3, col = \"blue\")\n\nERROR: Error in text.default(secu_pc1, secu_pc2, labels = secu_com_finance_2007_2$company, : invalid string in PangoCairo_Text"
  },
  {
    "objectID": "posts/GNN/Graph basic.html",
    "href": "posts/GNN/Graph basic.html",
    "title": "[GNN] Graph Basic",
    "section": "",
    "text": "Boostcourse\nhttps://kijungs.github.io/"
  },
  {
    "objectID": "posts/GNN/Graph basic.html#구조",
    "href": "posts/GNN/Graph basic.html#구조",
    "title": "[GNN] Graph Basic",
    "section": "구조",
    "text": "구조\n이웃 정점들의 정보를 집계하는 과정을 반복하여 임베딩을 얻는다.\n층(Layer)마다 임베딩을 얻는다.\n각 층에서는 이웃들의 이전 층 임베딩을 집계하여 새로운 임베딩을 얻는다.\n- 집계 함수\n\n이웃들 정보의 평균 계산\n신경망에 적용\n\n\\[h_v^0 = x_v\\]\n\\[h_v^k = \\sigma \\left( W_k \\sum_{u \\in N(v)} \\frac{h_u^{k-1}}{|N(v)|} + B_kh_v^{k-1} \\right), \\forall k &gt;0\\]\n\\(h_v^0 = x_v\\): 0번 층에서 정점 v의 임베딩으로 정점 v의 속성 벡터로 초기화\n\\(h_v^k\\): 현재 층, 즉 k번 층에서 정점 v의 임베딩\n\\(\\sigma\\): 비선형 함수(ReLU, tanh 등)\n\\(\\sum_{u \\in N(v)} \\frac{h_u^{k-1}}{|N(v)|}\\): 이전 층에서 이웃들의 임베딩에 대한 평균 계산\n\\(h_v^{k-1}\\): 이전 층에서 정점 v의 임베딩\n\n마지막 층에서 정점 별 임베딩이 해당 정점의 출력 임베딩\n\n- 한계: 이웃들의 정보를 동일한 가중치로 평균을 낸다."
  },
  {
    "objectID": "posts/GNN/Graph basic.html#학습",
    "href": "posts/GNN/Graph basic.html#학습",
    "title": "[GNN] Graph Basic",
    "section": "학습",
    "text": "학습\n- 학습 변수(Trainable Parameter)는 층 별 신경망의 가중치\n\n\\(W_k, B_k\\): 학습 변수\n\n- 손실함수 결정: 정점간 거리 보존하는 것 목표\n인접성 기반 유사도 정의 \\(\\to\\) 손실 함수\n\\[{\\cal L} = \\sum_{(u,v) \\in V \\times V} ||z_u^Tz_v - A_{u,v}||^2\\]\n\\(\\cal L\\): 비용함수\n\\(\\sum_{(u,v) \\in V \\times V}\\):모든 정점 쌍에 대하여 합산\n\\(z_u^Tz_v\\): 임베딩 공간에서의 유사도\n\\(A\\): 그래프에서의 유사도\n\n위의 내용은 분류를 하기 위한 전 단계이며, 분류(Classfier)를 하기 위한 손실함수는 따로 있음.\n\n\\(\\to\\) 후속 과제(Downstream Task)의 손실함수를 이용한 종단종(End-to-End) 학습도 가능\n분류기의 손실함수, 예를 들어 교차 엔트로피(Cross-Entropy)를, 전체 프로세스의 손실함수로 사용하여 종단종 학습을 할 수 있다.\n\\[{\\cal L} = \\sum_{v \\in V} y_v log(\\sigma(z_v^T \\theta)) + (1-y_v)log(1-\\sigma(z_v^T \\theta))\\]\n\\(y_v\\): 정점의 실제 유형(0 혹은 1)\n\\(z_v^T\\): 정점의 임베딩\n\\(\\theta\\): 분류기의 학습 변수\n\n\n변환적 정점 임베딩 이후에 별도의 분류기를 학습하는 것보다 정확도가 대체로 높다.\n학습에 사용할 대상 정점을 결정하여 학습 데이터를 구성한다.\n오차역전파(Backpropagation)을 통해 손실함수를 최소화한다. 신경망의 학습 변수를 학습한다.\n학습된 신경망을 적용해, 학습에 사용되지 않은 정점의 임베딩을 얻는다."
  },
  {
    "objectID": "posts/GNN/Graph basic.html#활용",
    "href": "posts/GNN/Graph basic.html#활용",
    "title": "[GNN] Graph Basic",
    "section": "활용",
    "text": "활용\n\n학습 이후에 추가된 정점의 임베딩을 얻는다.\n\nEX) 온라인 소셜네트워크 등 실제 그래프들은 시간에 따라서 변화한다.\n\n학습된 그래프 신경망을, 새로운 그래프에 적용한다."
  },
  {
    "objectID": "posts/GNN/Graph basic.html#그래프-합성곱-신경망",
    "href": "posts/GNN/Graph basic.html#그래프-합성곱-신경망",
    "title": "[GNN] Graph Basic",
    "section": "그래프 합성곱 신경망",
    "text": "그래프 합성곱 신경망\n- 집계함수\n\\[h_v^0 = x_v\\]\n\\[h_b^k = \\sigma \\left( W_k \\sum_{u \\in N(v) \\cup v} \\frac{h_u^{k-1}}{\\sqrt{|N(u)||N(v)|}} \\right), \\forall k \\in \\{1, \\dots, K \\}\\]\n\\[z_v = h_v^K\\]\n\n기존 집계 함수와 비교 했을 때 정규화 방법이 변화되었고, 동일 신경망 사용으로 학습 변수를 공유한다.\n\n- 한계: 단순히 연결성을 고려한 가중치로 평균을 낸다."
  },
  {
    "objectID": "posts/GNN/Graph basic.html#graphsage",
    "href": "posts/GNN/Graph basic.html#graphsage",
    "title": "[GNN] Graph Basic",
    "section": "GraphSAGE",
    "text": "GraphSAGE\n- 집계함수\n\n이웃들의 임베딩 AGG 함수를 이용해 합치고 자신의 임베딩과 연결(Concatenation)\n\n\\[h_v^k = \\sigma([W_k \\cdot \\text{AGG} (\\{h_u^{k-1}, \\forall u \\in N(v)\\}), B_kh_v^{k-1}])\\]\n- AGG 함수\n\nMEAN: AGG = \\(\\sum_{u \\in N(v)} \\frac{h_u^{k-1}}{|N(v)|}\\)\nPool: AGG = \\(\\gamma(\\{\\text{Q}h_u^{k-1}, \\forall u \\in N(v)\\})\\)\n\n\\(\\gamma\\): 원소별 최대\n\nLSTM: AGG = \\(\\text{LSTM}([h_u^{k-1}, \\forall u \\in \\pi(N(v))])\\)"
  },
  {
    "objectID": "posts/GNN/GNN논문.html",
    "href": "posts/GNN/GNN논문.html",
    "title": "[GNN] 논문 정리 (~ing)",
    "section": "",
    "text": "Node Classification\n\n\nNode embedding을 통해 점 분류 (인용 네트워크, Reddit게시물, Youtube동영상)\n\n\nLink Prediction\n\n\n그래프 점들 사이에 관계 파악 및 두 점 사이 연관성 예측\n\n\nGraph Classification\n\n\n그래프 전체 여러가지 카테고리로 분류"
  },
  {
    "objectID": "posts/GNN/GNN논문.html#ref",
    "href": "posts/GNN/GNN논문.html#ref",
    "title": "[GNN] 논문 정리 (~ing)",
    "section": "REF",
    "text": "REF\nhttps://wonbae.github.io/2021-02-26-Graph9-GNN/\nhttps://thejb.ai/comprehensive-gnns-3/ \\(\\star\\)\nhttps://github.com/heartcored98/Standalone-DeepLearning/tree/master\nhttps://ahjeong.tistory.com/15"
  },
  {
    "objectID": "posts/GNN/GNN논문.html#논문-종류",
    "href": "posts/GNN/GNN논문.html#논문-종류",
    "title": "[GNN] 논문 정리 (~ing)",
    "section": "논문 종류",
    "text": "논문 종류\n\nRecurrent Graph Neural Network\nSpatial Convolutional Network\nSpectral Convolutional Network"
  },
  {
    "objectID": "posts/GNN/GNN논문.html#section-1",
    "href": "posts/GNN/GNN논문.html#section-1",
    "title": "[GNN] 논문 정리 (~ing)",
    "section": "1-1",
    "text": "1-1\nhttps://arxiv.org/ftp/arxiv/papers/1812/1812.08434.pdf\nPropagation Modeule - Recurrent Operator - Convergence - GNN\nGraph neural networks: A review of methods and applications\n\npopular platforms for graph computing |Platform|Link|Reference| |—|—|—| |PyTorch Geometric |https://github.com/rusty1s/pytorch_geometric| Fey and Lenssen (2019)| |Deep Graph Library |https://github.com/dmlc/dgl |Wang et al. (2019b)| |AliGraph |https://github.com/alibaba/aligraph |Zhu et al. (2019a)| |GraphVite |https://github.com/DeepGraphLearning/graphvite |Zhu et al. (2019b)| |Paddle Graph Learning |https://github.com/PaddlePaddle/PGL|| |Euler |https://github.com/alibaba/euler|| |Plato |https://github.com/tencent/plato|| |CogDL |https://github.com/THUDM/cogdl/|| |OpenNE h|ttps://github.com/thunlp/OpenNE/tree/pytorch||"
  },
  {
    "objectID": "posts/GNN/GNN논문.html#section-2",
    "href": "posts/GNN/GNN논문.html#section-2",
    "title": "[GNN] 논문 정리 (~ing)",
    "section": "1-2",
    "text": "1-2\nhttps://ieeexplore.ieee.org/document/4700287\n\n@ARTICLE{4700287,\n  author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},\n  journal={IEEE Transactions on Neural Networks}, \n  title={The Graph Neural Network Model}, \n  year={2009},\n  volume={20},\n  number={1},\n  pages={61-80},\n  doi={10.1109/TNN.2008.2005605}}"
  },
  {
    "objectID": "posts/GNN/GNN논문.html#section-4",
    "href": "posts/GNN/GNN논문.html#section-4",
    "title": "[GNN] 논문 정리 (~ing)",
    "section": "2-1",
    "text": "2-1\nhttps://arxiv.org/abs/1609.02907\n‘Semi-Supervised Classification with Graph Convolutional Networks’"
  },
  {
    "objectID": "posts/GNN/GNN논문.html#section-5",
    "href": "posts/GNN/GNN논문.html#section-5",
    "title": "[GNN] 논문 정리 (~ing)",
    "section": "2-2",
    "text": "2-2\n참고자료\n\nF. Scarselli, M. Gori, “The graph neural network model,” IEEE Transactions on Neural Networks, 2009\n\n\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700287\n\n\nZ. Wu, S. Pan, F. Chen, G. Long, C. Zhang, Philip S. Yu, “A Comprehensive Survey on Graph Neural Networks”, arXiv:1901.00596\n\n\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9046288\n\n\nT. N. Kipf and M. Welling, “Semi-supervised classification with graph convolutional networks,” in Proc. of ICLR, 2017\n\n\nhttps://arxiv.org/pdf/1609.02907.pdf%EF%BC%89\n\n\nJ. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl, “Neural Message Passing for Quantum Chemistry”, in Proc. of ICML, 2017\n\n\nhttp://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf\n이건 화학적 분자구조를 뉴럴로 한거같은데,, 약간 참고자료용??\n\n\nD. Xu, Y. Zhu, C. B. Choy, and L. Fei-Fei, “Scene graph generation by iterative message passing,” in Proc. of CVPR, 2017\n\n\nhttps://openaccess.thecvf.com/content_cvpr_2017/papers/Xu_Scene_Graph_Generation_CVPR_2017_paper.pdf\n시각적 기반이 되는 이미지 그래픽 구조\n입력 이미지로부터 구조화된 장면 표현하는 모델\n\n\nJ. Johnson, A. Gupta, and L. Fei-Fei, “Image generation from scene graphs,” in Proc. of CVPR, 2018\n\n\n그래프 모델을 사용한 이미지 생성\n\n\nD. Teney, L. Liu and A. van den Hengel, “Graph-Structured Representations for Visual Question Answering”, in Proc. of CVPR, 2017\nB. Sanchez-Lengeling, J. N. Wei, B. K. Lee, R. C. Gerkin, A. Aspuru-Guzik, and A. B. Wiltschko, “Machine Learning for Scent: Learning Generalizable Perceptual Representations of Small Molecules”, arXiv: 1910.10685\nR. van den Berg, T. N. Kipf, and M. Welling, “Graph Convolutional Matrix Completion”, arXiv:1706.02263\n\nAlet, Ferran, et al. “Graph element networks: adaptive, structured computation and memory.” International Conference on Machine Learning. PMLR, 2019.\n\nspatial processes가 없는 공간\n\nAllamanis, Miltiadis, Marc Brockschmidt, and Mahmoud Khademi. “Learning to represent programs with graphs.” arXiv preprint arXiv:1711.00740 (2017)."
  },
  {
    "objectID": "posts/GNN/GNN논문.html#그래프-합성곱-네트워크",
    "href": "posts/GNN/GNN논문.html#그래프-합성곱-네트워크",
    "title": "[GNN] 논문 정리 (~ing)",
    "section": "1. 그래프 합성곱 네트워크",
    "text": "1. 그래프 합성곱 네트워크\n\n(1) 스펙트럼 방법(여러 가지 파동들의 집합)\n\n출처: https://tootouch.github.io/research/gnn_summary/\n\n- Spectral Convolutional Neural Network (Spectral CNN) 스펙트럼 네트워크\n(Denton, Emily L., et al. “Exploiting linear structure within convolutional networks for efficient evaluation.” Advances in neural information processing systems 27 (2014).)\n\n푸리에 영역에서 그래프 라플라시안의 고유 분해 계산\n\n\\[x \\in {\\mathbb R}, \\theta \\in {\\mathbb R}^N, g_\\theta=diah(\\theta)\\]\n\\[g_\\theta \\times x = U_{g_\\theta}(\\wedge)U^Tx\\]\n\\[L=I_N-D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}=U \\wedge U^T\\]\nU: L의 고유벡터로 이루어진 행렬, \\(\\wedge\\): L의 고윳값으로 이뤄진 대각행렬\n\n\n- Chebyshev Spectral Convolutional Neural Network(ChebNet)\n\n논문링크\n거리가 K이하인 범위를 다루는 합성곱을 사용해 합성곱 신경망을 정의함-&gt;라플라시안 고유벡터 전부 계산 안해도 된다.\n\n\\[g_\\theta  = \\sum_{i=0}^K \\theta_i T_i(\\tilde \\wedge)\\]\n\n체비쇼프 다항식\n\n\\(T_k(x) = 2xT_{k-1}(x) - T_{k-2}(x)\\)\n\\(T_0(x)=1, T_1(x) = x\\)\n\n\n- Graph Convolutional Network (GCN)\n\n(https://arxiv.org/pdf/1706.02263.pdf)\nhttps://arxiv.org/abs/1609.02907\nChebNet의 식에서 \\(K=1, \\lambda_{max}=2\\)로 근사\n\n\\[g_\\theta' \\times x \\approx \\theta_0' x + \\theta_1'(L-I_N)x = \\theta_0' x - \\theta_1' D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}x\\]\n\n위 식에서 \\(\\theta=\\theta_0' = -\\theta_1'\\) 이라고 가정(오버피팅 방지)\n\n\\[g_\\theta \\times x \\approx \\theta(I_N + D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}})x\\]\n\n\n- AGCN\n\nLi, Ruoyu, et al. “Adaptive graph convolutional neural networks.” Proceedings of the AAAI conference on artificial intelligence. Vol. 32. No. 1. 2018\n\n\n\n\n(2) 공간 방법\n\n공간적으로 가까운 이웃 노드에 직접 적용하는 합성곱 정의\n\n\n- 뉴럴 FPS\n논문:Convolutional Networks on Graphs for Learning Molecular Fingerprints\n다른 차수의 노드에 다른 가중치 행렬 사용\n\\[x=h_v^{t-1} + \\sum_{i=1}^{|N_v|}h_i^{t-1}\\]\n\\[h_v^t = \\sigma (x W_t^{|N_v|})\\]\n\\(h_v^t\\): t번째 층에서 노드 v의 임베딩\n\\(N_v\\): 노드 v의 이웃 집합\n\\(W_t^{|N_v|}\\): t번째 층에서 차수가 \\(|N_V|\\)인 노드의 가중치 행렬\n차수별로 다른 행렬을 사용하므로ㅗ 노드 차수가 많은 큰 규모의 그래프에서 사용할 수 없다.\n\n\n- PATCHY-SAN\n논문:Learning Convolutional Neural Networks for Graphs\n1. 노드 선택\n\n모든 노드에서 단계 진행하지 않고 그래프 레이블링 통해 노드의 순서를 정하고 그 순서를 기반으로 노드를 W개 선택\n\n2. 이웃 모으기\n\n각 단계에서 선택한 노드 각각을 기준으로 수용 영역을 만든다.\n너비 우선 선택(breadth-first search)을 통해 k개를 뽑는다.\n거리가 1인 노드를 고르고 부족하면 거리를 늘려서 가까운 노드 k개를 고른다.\n\n3. 그래프 정규화\n\n수용 영역에 있는 노드의 순서를 매겨서 순서가 있는 그래프 공간을 벡터 공간으로 바꾼다.\n\\(\\star\\) 구조적으로 비슷한 역할을 하는 노드는 다른 그래프에 있어도 비슷한 위치로 여긴다.\n\n4. 합성곱 구조\n-정규화된 이웃을 수용 영역으로 노드와 에지 속성을 채널로 한다.\n\n\n- Diffusion-Convolutional Neural Network(DCNN:확산 합성곱 신경망)\n논문: Diffusion-Convolutional Neural Networks\n추이행렬(transition matrix) 사용: 노드의 이웃 정의\n\\(P\\): 그래프 인접행렬 A로부터 얻은 차수 정규화 추이행렬\n\\(P*\\): 행렬 P의 거듭제곱근수 \\(\\{P, P^2, \\dots, P^K\\}\\)로 이뤄진 \\(N \\times K \\times N\\)(N은 노드수) 텐서\n\\[H=\\sigma(W^c \\odot P^*X)\\]\n\\(X\\): \\(N \\times F\\)입력 특징 텐서\n\\(X\\)에 \\(P^*\\)를 곱해서 각 원소는 K홉 그래프 확산을 뜻하는 \\(K \\times F\\) 행렬인 확산 합성곱 표현으로 바뀜\n\n\n- DGCN\n논문: Dual Graph Convolutional Networks for Graph-Based Semi-Supervised Classification\n\n그래프의 부분 일관성과 전체 일관성 모두 고려\n합성곱 그래프 2개와 비지도 손실 함수 2개 사용\n\n- 첫번째 합성곱 네트워트\n\\[Z={\\tilde D}^{-\\frac{1}{2}}{\\tilde A}{\\tilde D}^{-\\frac{1}{2}}X \\Theta\\]\n\\(\\tilde A= A+I_N\\)\nC: 입력 채널 수\n\\(X \\in {\\mathbb R}^{N \\times C}\\): 신호\n\\(\\Theta \\in {\\mathbb R}^{C \\times F}\\): 필터 파라미터 행렬\n\\(Z \\in {\\mathbb R}^{N \\times C}\\): 합성곱 적용한 신호 행렬\n\n가까운 노드는 비슷한 레이블을 가질 가능성이 높다는 것을 의미하는 부분 일관성: \\(\\text{Conv}_A\\)\n\n- 두번째 합성곱 네트워크\n\n인접행렬 대신 양의 점별 상호 정보(PPMI:Positive Pointwise Mutual Information) 행렬\n\n\\[H'=\\sigma(D^{-\\frac{1}{2}}_P X_P D_P^{-\\frac{1}{2}} H \\Theta)\\]\n\\(X_P\\): PPMI행렬\n\\(D_P\\): \\(X_P\\)의 대각행렬\n\n비슷한 내용을 가진 노드는 비슷한 레이블을 가질 가능성이 높다는 것을 의미하는 전체 일관성: \\(\\text{Conv}_P\\)\n\n- 손실 함수1\n\\[L=L_0(\\text{Conv}_A)+\\lambda(t)L_{reg}(\\text{Conv}_A,\\text{Conv}_P)\\]\n\\(\\lambda(t)\\): 두 손실 함수의 중요성 조절하는 가중치\n\\(L_0(\\text{Conv}_A)\\): 주어진 노드 레이블에 대한 지도 손실 함수\n\n크로스 엔트로피 에러를 사용해 \\(L_0(\\text{Conv}_A)\\) 계산\n\n\\[L_0(\\text{Conv}_A)= -\\frac{1}{|y_L|} \\sum_{l \\in y_L} \\sum_{i=1}^c Y_{l,i} ln(\\hat Z_{l,i}^A)\\]\n\\(y_L\\): 학습 데이터의 인덱스 집합\n\\(Y\\): 정답값\n- 손실 함수2\n\\[L_{reg}(\\text{Conv}_A, \\text{Conv}_P) = \\frac{1}{n} \\sum_{i=1}^n ||\\hat Z_{i,:}^P - \\hat Z_{i,:}^A||^2\\]\n- 모델 구조\n\n\n\n- LGCN\n논문:Large-Scale Learnable Graph Convolutional Networks\nLearnable Graph Convolutional Network: 학습 가능한 그래프 합성곱 네트워크\nhttps://github.com/HongyangGao/LGCN\n:학습 가능한 그래프 합성곱 층(LGCL:Learnable Graph Convolutional Layer)+부분 그래프 합성 전략\n- LGCL\n\n관련 높은 k개의 특징 요소를 얻기 위해서 노드의 이웃 행렬에 최대 풀링을 적용 후 1차원 CNN을 적용해 은닉 표현 계산\n전파단계\n\n\\[\\hat H_t = g(H_t, A, k)\\]\n\\[H_{t+1}=c (\\hat H_t)\\]\n\\(A\\): 인접행렬\n\\(g()\\): 가장 큰 노드 k개 뽑는 연산\n\\(c()\\): 일반적인 1차원 CNN\n- 예시\n\n??? 예시를 봐도 잘 모르겠다..\n\n\n- MoNET\n\n논문:Geometric deep learning on graphs and manifolds using mixture model CNNs\n\n과거 결과들을 일반화하는 모델\n\n\n- 예시\n\nGCNN(geodesic CNN)\nACNN(anisotropic CNN)\nGCN\nDCNN\n\n- 가중치 함수\n\\[D_j(x)f = \\sum_{y \\in N_x} w_j(u(x,y))f(y)\\]\n노드와 이웃 간의 가짜 좌표 \\(u(x,y)\\)를 계사나고 이를 이용해 가중치 함수를 정의한다.\n노드의 이웃에 각각 가중치를 두는 방법\n\\(u\\)와 \\(w(u)\\)를 어떻게 정의하는지에 따라 모델을 정의할 수 있다.\n\n\n- GraphSAGE\n논문:Representation learning on graphs: Methods and applications\n논문:Inductive representation learning on large graphs\n근처에 있는 노드의 특징을 샘플링하고 모아서 임베딩 계산\n- 전파 단계\n\\[h_{N_v}^t = \\text{AGGREGATE}_t(\\{h_u^{t-1}, \\forall u \\in N_v \\})\\]\n\\[h_v^t = \\sigma(W^t \\cdot [h_v^{t-1}||h_{N_v}^t])\\]\n이웃 노드를 모두 사용하지 않고 정해진 개수만 골고루 샘플링"
  },
  {
    "objectID": "posts/GNN/GNN논문.html#gated-graph-neural-networkggnn게이트-그래프-신경망",
    "href": "posts/GNN/GNN논문.html#gated-graph-neural-networkggnn게이트-그래프-신경망",
    "title": "[GNN] 논문 정리 (~ing)",
    "section": "Gated Graph Neural Network(GGNN:게이트 그래프 신경망)",
    "text": "Gated Graph Neural Network(GGNN:게이트 그래프 신경망)\n논문:Gated graph sequence neural networks\nhttps://www.cs.toronto.edu/~yujiali/files/talks/iclr16_ggnn_talk.pdf\nhttps://github.com/chingyaoc/ggnn.pytorch\nhttps://katefvision.github.io/LanguageGrounding/Slides/27.pdf\n정해진 횟수에 대한 순환 신경망을 풀고 시간을 통해 역전파해서 그레이디언트 계산\nGated Graph Sequence Neural Network(GGS-NN)"
  },
  {
    "objectID": "posts/GNN/GNN논문.html#tree-lstm",
    "href": "posts/GNN/GNN논문.html#tree-lstm",
    "title": "[GNN] 논문 정리 (~ing)",
    "section": "Tree-LSTM",
    "text": "Tree-LSTM\n논문:Improved semantic representations from tree-structured long short-term memory networks)\n\n1. Child-Sum Tree-LSTM\n\\(\\tilde h_v^{t-1} = \\sum_{k \\in N_v} h_k^{t-1}\\)\n입력 게이트 \\(i_v^t = \\sigma(W^ix_v^t + U^i {\\tilde h_v^{t-1}} + b^i)\\)\n망각 게이트 \\(f^t_{vk} = \\sigma(W^f x_v^t + U^fh_k^{t-1} + b^f)\\)\n출력 게이트 \\(o_v^t = \\sigma(W^ox_v^t + U^o {\\tilde h_v^{t-1}} + b^o)\\)\n\\(u_v^t = tanh(W^ux_v^t + U^u {\\tilde h_v^{t-1}} + b^u)\\)\n메모리 셀 \\(c_v^t = i_v^t \\bigodot u_v^t + \\sum_{k \\in N_v} f_{vk}^t \\bigodot c_k^{t-1}\\)\n은닉 상태 \\(h_v^t = o_v^t \\bigodot tahn(c_v^t)\\)\n\n\n2. N-ary Tree-LSTM\n\n트리의 각 노드 자식 수가 K보다 작고 자식들에게 1부터 K까지 순서를 매길 때 사용\n\n\\(i_v^t = \\sigma(W^i x_v^t + \\sum_{l=1}^K U_l^i h_{vl}^{t-1} + b^i)\\)\n\\(f_{vk}^t = \\sigma(W^f x_v^t + \\sum_{l=1}^K U_{kl}^f h_{bl}^{t-1} + b^f)\\)\n\\(o_v^t = \\sigma(W^o x_v^t + \\sum_{l=1}^K U_t^o h_{vt}^{t-1} + b^o)\\)\n\\(u_v^t = tanh(W^ux_v^t + \\sum_{l=1}^K U_l^u h_{vl}^{t-1} + b^u)\\)\n\\(c_v^t = i_v^t \\bigodot u_v^t + \\sum_{l=1}^K f_{bl}^t \\bigodot c_{vl}^{t-1}\\)"
  },
  {
    "objectID": "posts/GNN/GNN논문.html#그래프-lstm",
    "href": "posts/GNN/GNN논문.html#그래프-lstm",
    "title": "[GNN] 논문 정리 (~ing)",
    "section": "그래프 LSTM",
    "text": "그래프 LSTM"
  },
  {
    "objectID": "posts/GNN/GNN논문.html#sentence-lstms-lstm",
    "href": "posts/GNN/GNN논문.html#sentence-lstms-lstm",
    "title": "[GNN] 논문 정리 (~ing)",
    "section": "Sentence-LSTM(S-LSTM)",
    "text": "Sentence-LSTM(S-LSTM)\n텍스트를 그래프로 변환하고 그래프 LSTM을 이용해 학습\n논문:Sentence-state LSTM for text representation\n음.. 이건 약간 자연어 처리 쪽.."
  },
  {
    "objectID": "posts/GNN/GNN논문.html#gat",
    "href": "posts/GNN/GNN논문.html#gat",
    "title": "[GNN] 논문 정리 (~ing)",
    "section": "GAT",
    "text": "GAT\nGraph Attention Networkx"
  },
  {
    "objectID": "posts/GNN/GNN논문.html#gaan",
    "href": "posts/GNN/GNN논문.html#gaan",
    "title": "[GNN] 논문 정리 (~ing)",
    "section": "GaAN",
    "text": "GaAN"
  },
  {
    "objectID": "posts/Synthetic/2023-07-02-CTGAN-TOY.html",
    "href": "posts/Synthetic/2023-07-02-CTGAN-TOY.html",
    "title": "[CTGAN] CTGAN ToyEX",
    "section": "",
    "text": "imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom ctgan import CTGAN\nfrom ctgan import load_demo\n\n\n\ndata\n\nx1 = np.random.randn(1000).tolist()\ny1 = ['A']*1000\nx2 = (np.random.randn(100)+5).tolist()\ny2 = ['B']*100\n\n\ndf = pd.DataFrame({'x':x1+x2, 'y':y1+y2})\ndf\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n-0.557100\nA\n\n\n1\n1.586981\nA\n\n\n2\n1.867204\nA\n\n\n3\n-1.377673\nA\n\n\n4\n-0.013144\nA\n\n\n...\n...\n...\n\n\n1095\n5.758539\nB\n\n\n1096\n7.837548\nB\n\n\n1097\n5.462159\nB\n\n\n1098\n4.565613\nB\n\n\n1099\n2.790882\nB\n\n\n\n\n1100 rows × 2 columns\n\n\n\n\nplt.hist(df.x,bins=50)\n\n(array([ 1.,  0.,  1.,  3.,  2., 12., 19., 32., 44., 30., 63., 66., 85.,\n        73., 96., 87., 77., 73., 66., 56., 36., 28., 19., 17.,  7.,  3.,\n         4.,  2.,  2.,  2.,  4.,  5.,  5.,  5., 10.,  9.,  8., 13.,  6.,\n         6.,  8.,  3.,  4.,  2.,  1.,  0.,  2.,  0.,  0.,  3.]),\n array([-3.30563479, -3.08115892, -2.85668304, -2.63220717, -2.4077313 ,\n        -2.18325542, -1.95877955, -1.73430368, -1.5098278 , -1.28535193,\n        -1.06087606, -0.83640018, -0.61192431, -0.38744844, -0.16297256,\n         0.06150331,  0.28597918,  0.51045506,  0.73493093,  0.9594068 ,\n         1.18388268,  1.40835855,  1.63283442,  1.8573103 ,  2.08178617,\n         2.30626204,  2.53073792,  2.75521379,  2.97968966,  3.20416554,\n         3.42864141,  3.65311728,  3.87759316,  4.10206903,  4.3265449 ,\n         4.55102078,  4.77549665,  4.99997252,  5.22444839,  5.44892427,\n         5.67340014,  5.89787601,  6.12235189,  6.34682776,  6.57130363,\n         6.79577951,  7.02025538,  7.24473125,  7.46920713,  7.693683  ,\n         7.91815887]),\n &lt;BarContainer object of 50 artists&gt;)\n\n\n\n\n\n\n\nCTGAN\n\n# Names of the columns that are discrete\ndiscrete_columns = ['y']\nctgan = CTGAN(epochs=500) # 겁나많이해야하네?\nctgan.fit(df, discrete_columns)\n\n# Create synthetic data\ndf2 = ctgan.sample(1000)\n\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f37484ab430&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f37484ab430&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n\n\n\ndf2.groupby('y').count()\n\n\n\n\n\n\n\n\nx\n\n\ny\n\n\n\n\n\nA\n530\n\n\nB\n470\n\n\n\n\n\n\n\n\nplt.hist(df.x,bins=50,alpha=0.5,label='real')\nplt.hist(df2.x,bins=50,alpha=0.5,label='syn')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f3730581f70&gt;\n\n\n\n\n\n\ndf2[df2.y=='A'].hist()\n\narray([[&lt;Axes: title={'center': 'x'}&gt;]], dtype=object)\n\n\n\n\n\n\ndf2[df2.y=='B'].hist()\n\narray([[&lt;Axes: title={'center': 'x'}&gt;]], dtype=object)"
  },
  {
    "objectID": "posts/Synthetic/Modeling Tabular Data using Conditional GAN.html",
    "href": "posts/Synthetic/Modeling Tabular Data using Conditional GAN.html",
    "title": "[논문] Modeling Tabular Data using Conditional GAN",
    "section": "",
    "text": "Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, Kalyan Veeramachaneni, “Modeling Tabular data using Conditional GAN” , NIPS’19, 2019\nCTGAN model open source\nbenchmark\nShine’s dev log Tstory"
  },
  {
    "objectID": "posts/Synthetic/Modeling Tabular Data using Conditional GAN.html#related-work",
    "href": "posts/Synthetic/Modeling Tabular Data using Conditional GAN.html#related-work",
    "title": "[논문] Modeling Tabular Data using Conditional GAN",
    "section": "Related Work",
    "text": "Related Work\n과거에 synthetic data는 랜덤변수, 결합 다변량 분포를 모형화하여 표본을 추출했다."
  },
  {
    "objectID": "posts/Synthetic/Modeling Tabular Data using Conditional GAN.html#notations",
    "href": "posts/Synthetic/Modeling Tabular Data using Conditional GAN.html#notations",
    "title": "[논문] Modeling Tabular Data using Conditional GAN",
    "section": "Notations",
    "text": "Notations\n– \\(x_1 \\oplus x_2 \\oplus \\dots\\): concatenate vectors \\(x_1, x_2, \\dots\\)\n– \\(gumbel_{\\gamma} (x)\\): apply Gumbel softmax with parameter \\(\\gamma\\) on a vector \\(x\\)\n– \\(leaky_{\\gamma}(x)\\): apply a leaky ReLU activation on \\(x\\) with leaky ratio \\(\\gamma\\)\n– \\(FC_{u \\to v}(x)\\): apply a linear transformation on a \\(u\\)-dim input to get a \\(v\\)-dim output.\n이 외에도 tanh, ReLU, softmax, BN, drop 등을 사용한다."
  },
  {
    "objectID": "posts/Synthetic/Modeling Tabular Data using Conditional GAN.html#mode-specific-normalization",
    "href": "posts/Synthetic/Modeling Tabular Data using Conditional GAN.html#mode-specific-normalization",
    "title": "[논문] Modeling Tabular Data using Conditional GAN",
    "section": "Mode-specific Normalization",
    "text": "Mode-specific Normalization\n\n\n연속형 변수 \\(C_i\\)는 variational Gaussian mixture model(VGM) 을 사용한다.\n\n위 예시에서 VGM은 \\(\\eta_1,\\eta_2,\\eta_3\\)인 \\(m_i=3\\)인 mode를 찾는다.\nGaussian mixture = \\(\\mathbb{P}_{C_i}(c_{i,j}) = \\sum_{k=1}^3 \\mu_k N (c_{i,j}: \\eta_k, \\phi_k)\\)\n\\(\\mu_k, \\phi_k\\) 의 가중치와 분산은 각각 구해놓는다.\n\n\\(c_{i,j}\\)의 확률밀도함수는 각각 \\(\\rho_1, \\rho_2, \\rho_3\\)이다.\n\ns.t \\(\\rho_k = \\mu_k N(c_{i,j}: \\eta_k, \\phi_k)\\)\n\n3번째 mode를 선택한다. 원핫인코딩을 통해서 \\(c_{i,j} \\to \\beta_{i,j}=[0,0,1]\\)로 바꾼다.\n\n그리고 \\(\\alpha_{i,j}= \\dfrac{c_{i,j}0 - \\eta_3}{4\\phi_3}\\) 가중치를 곱한다.\n연속형과 이산형 열들을 바꿔준다.\n\\(r_j = \\alpha_{1,j}\\oplus \\beta_{1,j}\\oplus \\dots \\oplus \\alpha_{N_{c,j}} \\oplus \\beta_{N_{c,j}} \\oplus d_{1,j} \\oplus \\dots \\oplus d_{N_{d,j}}\\)"
  },
  {
    "objectID": "posts/Synthetic/Modeling Tabular Data using Conditional GAN.html#conditional-generator-and-training-by-sampling",
    "href": "posts/Synthetic/Modeling Tabular Data using Conditional GAN.html#conditional-generator-and-training-by-sampling",
    "title": "[논문] Modeling Tabular Data using Conditional GAN",
    "section": "Conditional Generator and Training-by-Sampling",
    "text": "Conditional Generator and Training-by-Sampling\nclass imbalance\n만약 훈련 데이터가 랜덤샘플에서 훈련된다면 열은 적은카테고리를 가지고 있는 분류를 충분히 대표할수 없다.\n이 문제를 the conditional vector, the generator loss, the training-by-sampling method를 이용해 해결하자.\n\n\nConditional vector\n이산형 분포인 \\(D_{N_d}\\)를 원핫인코딩을 통해 \\(d_{N_d}\\)로 바꾼다.\nFor instance, for two discrete columns,D1 = f1, 2, 3g and D2 = f1, 2g,the condition (D2 = 1) is expressed by the mask vectors m1 = [0, 0, 0] and m2 = [1, 0]; so cond = [0, 0, 0, 1, 0].\n\n\nGenerator loss\n\n\nTraining-by-sampling\n- Figure2\n\\(m_i^{(k)} = I(if i=i^* \\  and \\ k=k^*)\\)\n\n\\(D_2\\) 열을 선택한다. 즉 \\(i^*=2\\)이다.\n\\(D_2\\)에서 첫번째를 선택한다. 즉 \\(k^*=1\\)이다.\n\\(m_1=[0,0,0], m_2=[1,0], cond=[0,0,0,1,0]\\)"
  },
  {
    "objectID": "posts/Synthetic/Modeling Tabular Data using Conditional GAN.html#network-structure",
    "href": "posts/Synthetic/Modeling Tabular Data using Conditional GAN.html#network-structure",
    "title": "[논문] Modeling Tabular Data using Conditional GAN",
    "section": "Network Structure",
    "text": "Network Structure\n\ngenerator \\(\\mathbb{g}(z,cond)\\)\n\n\\(h_0 = z \\oplus cond\\)\n\\(h_1 = h_0 \\oplus ReLU(BN(FC_{|cond|+|z| \\to 256}(h_0)))\\)\n\\(h_2 = h_1 \\oplus ReLU(BN(FC_{|cond|+|z|+256 \\to 256}(h_1)))\\)\n\\(\\widehat \\alpha_i = tanh(FC_{|cond|+|z|+512 \\to 1}(h_2)), \\ 1 \\leq i \\leq N_c\\)\n\\(\\widehat \\beta_i = gumbel_{0.2}(FC_{|cond|+|z|+512 \\to m_i} (h_2)), \\ 1 \\leq i \\leq N_c\\)\n\\(\\widehat d_i = gumbel_{0.2}(FC_{|cond|+|z|+512 \\to |D_i|}(h_2)), \\ 1 \\leq i \\leq N_c\\)\n\n\n\ncritic \\(C(r_1,\\dots,r_{10},cond_1,\\dots,cond_{10})\\)\n- use PacGAN with 10 samples in each pac to prevent mode collapse\n\n\\(h_0 = r_1 \\oplus \\dots \\oplus r_{10} \\oplus cond_1 \\oplus \\dots \\oplus cond_{10}\\)\n\\(h_1 = drop(leaky_{0.2}(FC_{10|r|+10|cond| \\to 256}(h_0)))\\)\n\\(h_2 = drop(leaky_{0.2}(FC_{256 \\to 256}(h_1)))\\)\n\\(C(\\cdot) = FC_{256 \\to 1}(h_2)\\)\n\nAdam optimaizer 사용한 학습률 \\(2·10^{-4}\\)"
  },
  {
    "objectID": "posts/Synthetic/Modeling Tabular Data using Conditional GAN.html#tvae-model",
    "href": "posts/Synthetic/Modeling Tabular Data using Conditional GAN.html#tvae-model",
    "title": "[논문] Modeling Tabular Data using Conditional GAN",
    "section": "TVAE Model",
    "text": "TVAE Model\n- TVAE generator\n\n\\(h_1 = ReLU(FC_{128 \\to 128}(z_j))\\)\n\\(h_2 = ReLU(FC_{128 \\to 128}(h_1))\\)\n\\(\\bar \\alpha _{i,j} = tanh(FC_{128 \\to 1}(h_2)) \\ , \\ 1 \\leq i \\leq N_c\\)\n\\(\\widehat \\alpha_{i,j} \\sim N(\\bar \\alpha_{i,j} , \\delta_i) \\ , \\ 1 \\leq i \\leq N_c\\)\n\\(\\widehat \\beta_{i,j} \\sim softmax(FC_{128 \\to m_i} (h_2)) \\ , \\ 1 \\leq i \\leq N_c\\)\n\\(\\widehat d_{i,j} \\sim softmax(FC_{128 \\to |D_i|}(h_2)) \\ , \\ 1 \\leq i \\leq N_d\\)\n\\(p_\\theta(r_j|z_j) = \\Pi_{i=1}^{N_c} \\mathbb{P}(\\widehat \\alpha_{i,j} = \\alpha_{i,j}) \\Pi_{i=1}^{N_c} \\mathbb{P}(\\widehat \\beta_{i,j} = \\beta_{i,j}) \\Pi_{i=1}^{N_d} \\mathbb{P}(\\widehat \\alpha_{i,j} = \\alpha_{i,j})\\)\n\n\\(\\widehat \\alpha_{i,j}, \\widehat \\beta_{i,j}, \\widehat d_{i,j}\\) : random variable\n\\(p_\\theta(r_j|z_j)\\) : joint distribuion\nTVAE를 Adam으로 학습한 학습률: \\(1e^{-3}\\)"
  },
  {
    "objectID": "posts/Synthetic/Modeling Tabular Data using Conditional GAN.html#baselines-and-datasets",
    "href": "posts/Synthetic/Modeling Tabular Data using Conditional GAN.html#baselines-and-datasets",
    "title": "[논문] Modeling Tabular Data using Conditional GAN",
    "section": "Baselines and Datasets",
    "text": "Baselines and Datasets\n\nsimulated data\n\n오라클 S로부터 \\(T_{train}, T_{test}\\)을 만든다.\n이 오라클은 가우시안 혼합 모델 또는 베이지안 네트워크이다.\nGridR:각 모드에 랜덤 오프셋을 추가\n베이지안 네트워크: alarm, child, asia, insurance를 사용\n\n\n\nreal datasets\n\nUCI머신러닝에서 사용되는 6개 사용\nadult, census, covertype, intrusion,news\nMNIST 사용"
  },
  {
    "objectID": "posts/Synthetic/Modeling Tabular Data using Conditional GAN.html#evaluation-metrics-and-framework",
    "href": "posts/Synthetic/Modeling Tabular Data using Conditional GAN.html#evaluation-metrics-and-framework",
    "title": "[논문] Modeling Tabular Data using Conditional GAN",
    "section": "Evaluation Metrics and Framework",
    "text": "Evaluation Metrics and Framework\n\n\nLikelihood fitness metric\n\nsimulated data 사용\n$T_{syn} L_{syn},T_{test} L_{test}, $\n\\(L_{syn}\\)이 과적합되는 문제를 해결하기 위하 \\(L_{test}\\)를 사용\n\n\n\nMachine learning efficacy\n\nreal dataset 사용\naccruracy와 F1, \\(\\mathbb{R^2}\\) 측정"
  },
  {
    "objectID": "posts/Synthetic/Modeling Tabular Data using Conditional GAN.html#benchmarking-results",
    "href": "posts/Synthetic/Modeling Tabular Data using Conditional GAN.html#benchmarking-results",
    "title": "[논문] Modeling Tabular Data using Conditional GAN",
    "section": "Benchmarking Results",
    "text": "Benchmarking Results\n\n\nGM Sim : Gaussian mixture\nBN Sim : Bayesian networks\nTVAE가 CTGAN보다 우수한 편이지만 privacy해결은 못하므로 privacy생각하면 CTGAN사용.."
  },
  {
    "objectID": "posts/Synthetic/2023-07-01-CTGAN.html",
    "href": "posts/Synthetic/2023-07-01-CTGAN.html",
    "title": "[CTGAN] CTGAN",
    "section": "",
    "text": "!conda env list\n\n# conda environments:\n#\nbase                     /home/coco/anaconda3\npy38                  *  /home/coco/anaconda3/envs/py38\n\n\n\n- Ref: https://github.com/sdv-dev/CTGAN\n\nfrom ctgan import CTGAN\nfrom ctgan import load_demo\n\n\n\"\"\"Demo module.\"\"\"\n\nimport pandas as pd\n\nDEMO_URL = 'http://ctgan-demo.s3.amazonaws.com/census.csv.gz'\n\n\ndef load_demo():\n    \"\"\"Load the demo.\"\"\"\n    return pd.read_csv(DEMO_URL, compression='gzip')\n\n\nload_demo()\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\nincome\n\n\n\n\n0\n39\nState-gov\n77516\nBachelors\n13\nNever-married\nAdm-clerical\nNot-in-family\nWhite\nMale\n2174\n0\n40\nUnited-States\n&lt;=50K\n\n\n1\n50\nSelf-emp-not-inc\n83311\nBachelors\n13\nMarried-civ-spouse\nExec-managerial\nHusband\nWhite\nMale\n0\n0\n13\nUnited-States\n&lt;=50K\n\n\n2\n38\nPrivate\n215646\nHS-grad\n9\nDivorced\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n3\n53\nPrivate\n234721\n11th\n7\nMarried-civ-spouse\nHandlers-cleaners\nHusband\nBlack\nMale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n4\n28\nPrivate\n338409\nBachelors\n13\nMarried-civ-spouse\nProf-specialty\nWife\nBlack\nFemale\n0\n0\n40\nCuba\n&lt;=50K\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n32556\n27\nPrivate\n257302\nAssoc-acdm\n12\nMarried-civ-spouse\nTech-support\nWife\nWhite\nFemale\n0\n0\n38\nUnited-States\n&lt;=50K\n\n\n32557\n40\nPrivate\n154374\nHS-grad\n9\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n&gt;50K\n\n\n32558\n58\nPrivate\n151910\nHS-grad\n9\nWidowed\nAdm-clerical\nUnmarried\nWhite\nFemale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n32559\n22\nPrivate\n201490\nHS-grad\n9\nNever-married\nAdm-clerical\nOwn-child\nWhite\nMale\n0\n0\n20\nUnited-States\n&lt;=50K\n\n\n32560\n52\nSelf-emp-inc\n287927\nHS-grad\n9\nMarried-civ-spouse\nExec-managerial\nWife\nWhite\nFemale\n15024\n0\n40\nUnited-States\n&gt;50K\n\n\n\n\n32561 rows × 15 columns\n\n\n\n\nreal_data = load_demo()\n\n# Names of the columns that are discrete\ndiscrete_columns = [\n    'workclass',\n    'education',\n    'marital-status',\n    'occupation',\n    'relationship',\n    'race',\n    'sex',\n    'native-country',\n    'income'\n]\n\nctgan = CTGAN(epochs=10)\nctgan.fit(real_data, discrete_columns)\n\n# Create synthetic data\nsynthetic_data = ctgan.sample(1000)\n\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7fd20e2df430&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7fd2b8692550&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7fd20df09f70&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7fd20e2df430&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7fd20df09f70&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7fd20df09c10&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n\n\n- 실제자료\n\nreal_data\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\nincome\n\n\n\n\n0\n39\nState-gov\n77516\nBachelors\n13\nNever-married\nAdm-clerical\nNot-in-family\nWhite\nMale\n2174\n0\n40\nUnited-States\n&lt;=50K\n\n\n1\n50\nSelf-emp-not-inc\n83311\nBachelors\n13\nMarried-civ-spouse\nExec-managerial\nHusband\nWhite\nMale\n0\n0\n13\nUnited-States\n&lt;=50K\n\n\n2\n38\nPrivate\n215646\nHS-grad\n9\nDivorced\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n3\n53\nPrivate\n234721\n11th\n7\nMarried-civ-spouse\nHandlers-cleaners\nHusband\nBlack\nMale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n4\n28\nPrivate\n338409\nBachelors\n13\nMarried-civ-spouse\nProf-specialty\nWife\nBlack\nFemale\n0\n0\n40\nCuba\n&lt;=50K\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n32556\n27\nPrivate\n257302\nAssoc-acdm\n12\nMarried-civ-spouse\nTech-support\nWife\nWhite\nFemale\n0\n0\n38\nUnited-States\n&lt;=50K\n\n\n32557\n40\nPrivate\n154374\nHS-grad\n9\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n&gt;50K\n\n\n32558\n58\nPrivate\n151910\nHS-grad\n9\nWidowed\nAdm-clerical\nUnmarried\nWhite\nFemale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n32559\n22\nPrivate\n201490\nHS-grad\n9\nNever-married\nAdm-clerical\nOwn-child\nWhite\nMale\n0\n0\n20\nUnited-States\n&lt;=50K\n\n\n32560\n52\nSelf-emp-inc\n287927\nHS-grad\n9\nMarried-civ-spouse\nExec-managerial\nWife\nWhite\nFemale\n15024\n0\n40\nUnited-States\n&gt;50K\n\n\n\n\n32561 rows × 15 columns\n\n\n\n- 합성된자료\n\nsynthetic_data\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\nincome\n\n\n\n\n0\n19\nSelf-emp-inc\n167797\nHS-grad\n9\nNever-married\nArmed-Forces\nHusband\nWhite\nMale\n111\n-1\n40\nUnited-States\n&lt;=50K\n\n\n1\n31\nPrivate\n66993\nSome-college\n9\nDivorced\nSales\nNot-in-family\nAsian-Pac-Islander\nMale\n88\n-4\n40\nUnited-States\n&lt;=50K\n\n\n2\n40\nFederal-gov\n129625\nBachelors\n9\nMarried-civ-spouse\nAdm-clerical\nOther-relative\nWhite\nFemale\n-7\n-1\n67\nUnited-States\n&lt;=50K\n\n\n3\n20\nPrivate\n174876\nBachelors\n14\nMarried-civ-spouse\nProf-specialty\nHusband\nWhite\nMale\n27\n-7\n19\nUnited-States\n&lt;=50K\n\n\n4\n53\nPrivate\n36484\nMasters\n16\nWidowed\nProf-specialty\nNot-in-family\nWhite\nFemale\n13\n0\n30\nUnited-States\n&gt;50K\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n42\nPrivate\n255160\nHS-grad\n14\nNever-married\nAdm-clerical\nNot-in-family\nBlack\nFemale\n29\n3\n47\nUnited-States\n&lt;=50K\n\n\n996\n38\nSelf-emp-inc\n149788\nHS-grad\n9\nNever-married\nAdm-clerical\nHusband\nWhite\nMale\n106\n-6\n40\nUnited-States\n&lt;=50K\n\n\n997\n44\nState-gov\n163575\nHS-grad\n9\nMarried-civ-spouse\n?\nUnmarried\nAmer-Indian-Eskimo\nFemale\n-4\n4\n52\nFrance\n&lt;=50K\n\n\n998\n20\nSelf-emp-not-inc\n135323\nSome-college\n9\nMarried-civ-spouse\nTech-support\nHusband\nWhite\nMale\n-48\n-4\n40\nPhilippines\n&lt;=50K\n\n\n999\n40\nPrivate\n218873\n10th\n11\nNever-married\nExec-managerial\nHusband\nAsian-Pac-Islander\nFemale\n67\n-3\n40\nUnited-States\n&lt;=50K\n\n\n\n\n1000 rows × 15 columns"
  },
  {
    "objectID": "posts/Synthetic/Practical Synthetic Data Generation.html",
    "href": "posts/Synthetic/Practical Synthetic Data Generation.html",
    "title": "Practical Synthetic Data Generation",
    "section": "",
    "text": "부제: 머신러닝을 위한 실전 데이터셋\n출판사: 한빛미디어\n저자: 칼리드 엘 에맘, 루시 모스케라, 리처드 홉트로프\n옮긴이: 심상진 옮김\n소개글: 머신러닝 모델을 구축하고, 테스트를 진행하려면 크고 다양한 종류의 데이터가 필요하다. 그러나 대부분의 데이터셋은 개인 정보 문제로 사용이 제한적이라 광범위하게 사용할 수 없다. 이 책에서는 실제 데이터로 새로운 데이터를 만드는 실용적인 합성 데이터 기술을 소개한다. 합성 데이터는 이차 분석에 용이하여 데이터 연구, 고객 행동의 이해, 신제품 개발 등 다양한 목적으로 활용될 수 있다. 이 책은 실제 데이터를 합성해 다양한 산업에서 사용할 수 있는 방법을 제공하며, 개인 정보 문제를 해결하는 방법을 다룬다. 또한 실제 데이터셋에서 합성 데이터를 생성하기 위한 원칙과 단계를 배운다. 더 나아가 합성 데이터가 제품이나 솔루션 개발에 드는 시간을 어떻게 단축할 수 있는지를 학습한다.\n\n\n\n- 합성 데이터 정의\n\n실제 데이터가 아니라 실제 데이터에서 생성되어 통계속성이 동일한 데이터\n\n- 합성 데이터 유형\n\n실제 데이터로 합성하기\n실제 데이터 없이 합성하기\n두 가지 유형을 합친 하이브리드\n\n- 합성 데이터 이점\n\n식별 가능한 개인 데이터가 아님 \\(\\to\\) 개인 정보 보호 규정 적용x\n데이터 이차 목적 사용 가능\n수집이 어렵거나 비실용적, 비윤리적인 경우도 사용 가능\n초기 모델을 훈련 \\(\\to\\) 데이터 모델의 정합화 촉진\n\n- 합성 데이터 활용 사례\n\n제조/유통\n헬스케어\n금융 서비스\n교통수단\n\n- 공공 데이터는 통제되지 않는다.\n- 영국 공중보건국 합성 암 등록 데이터\n\n\n\n- 데이터 합성이 데이터에 접근하는 최선의 방식인가?\n\\(\\to\\) 구현 프로세스 고려\n- 결정기준\n\n프라이버시, 운영 비용, 데이터 효용성, 소비자 신뢰도\n\n\n\n\n- 데이터 합성 방법론과 기술\n\n분포 모델링(정규 분포/지수 분포 등 고전적인 분포)에 개별 변수를 적합시키거나 데이터 구조 모델링 사용\n\n- 과적합 해결법\n\n분포를 중립 지점에서 시작해 데이터에 더 가깝게 더 가까운 쪽으로 이동하여 각 단계별 분포의 단순성과 적합도 사이에서 균형을 이루게 하는 접근법\n최고의 절충점에 언제 도착했는지 측정해서 방지\n분포 적합성 접근법?\n\n\n\n\n- 일변량\n\n실제 데이터와 합성 데이터에서 각 변수 간의 분포 차이를 측정하기 위한 헬링거 거리 계산\n0: 분포간에 차이 없음 ~ 1: 분포 차이 많음\n\n- 이변량\n\n실제 데이터와 합성 데이터의 모든 변수 쌍 간의 상관관계의 절대적 차이 = 데이터 효용성의 척도\n\n- 다변량\n\n10겹 교차 검증\n데이터 셋을 10개의 동일한 크기의 서브셋으로 나눔\n서브셋1을 테스트셋으로 하고 나머지 9개 데이터셋 모델을 만든다.\n서브셋1에서 모델 테스트, AUROC 계산\n훈련 데이터로 서브셋2 테스트 사용, AUROC 계산\n… 10회 반복 \\(\\to\\) AUROC 10개 값 \\(\\to\\) 평균계산\n\n- 합성데이터와 실제 데이터에 대한 대응 모델에 대한 계산\n- 두 AUROC값의 절대적 차이 계산\n- 모든 절대값 차이에 대한 상자 그림 생성\n- 경향점수.. (어렵… 103p)\n\n\n\n- 방법\n\n정규분포로부터 샘플링\n샘플링 프로세스 중 상관관계 유도\n코퓰러 사용\n\n- 코퓰러?\n\n확률변수들 간의 상관관계 또는 종속성을 나타내는 함수\n흠.. 나중에 찾아보자..\n\n- 머신러닝 방법 - 의사결정 트리(CART) 사용\n- 딥러닝 방법 - 변이형 오토인코더(VAE) - 생성적 적대 신경망(GAN)\n- GAN - 생성기: 입력 무작위 데이터, 정규 분포 또는 균일 분포로부터 샘플링하며 합성 데이터 생성 - 판별기: 합성 데이터와 실제 데이터 비교하여 유사한 경향 점수 생성 차이 결과를 생성 기 훈련을 위해 다시 제공\n\n\n\n\n노출 유형\n신원 노출(정보 이득이 0이라면 신원 노출에 아무런 의미가 없음)\n속성 노출"
  },
  {
    "objectID": "posts/Synthetic/Practical Synthetic Data Generation.html#chapter-1-합성-데이터-생성-소개",
    "href": "posts/Synthetic/Practical Synthetic Data Generation.html#chapter-1-합성-데이터-생성-소개",
    "title": "Practical Synthetic Data Generation",
    "section": "",
    "text": "- 합성 데이터 정의\n\n실제 데이터가 아니라 실제 데이터에서 생성되어 통계속성이 동일한 데이터\n\n- 합성 데이터 유형\n\n실제 데이터로 합성하기\n실제 데이터 없이 합성하기\n두 가지 유형을 합친 하이브리드\n\n- 합성 데이터 이점\n\n식별 가능한 개인 데이터가 아님 \\(\\to\\) 개인 정보 보호 규정 적용x\n데이터 이차 목적 사용 가능\n수집이 어렵거나 비실용적, 비윤리적인 경우도 사용 가능\n초기 모델을 훈련 \\(\\to\\) 데이터 모델의 정합화 촉진\n\n- 합성 데이터 활용 사례\n\n제조/유통\n헬스케어\n금융 서비스\n교통수단\n\n- 공공 데이터는 통제되지 않는다.\n- 영국 공중보건국 합성 암 등록 데이터"
  },
  {
    "objectID": "posts/Synthetic/Practical Synthetic Data Generation.html#chapter-2-데이터-합성",
    "href": "posts/Synthetic/Practical Synthetic Data Generation.html#chapter-2-데이터-합성",
    "title": "Practical Synthetic Data Generation",
    "section": "",
    "text": "- 데이터 합성이 데이터에 접근하는 최선의 방식인가?\n\\(\\to\\) 구현 프로세스 고려\n- 결정기준\n\n프라이버시, 운영 비용, 데이터 효용성, 소비자 신뢰도"
  },
  {
    "objectID": "posts/Synthetic/Practical Synthetic Data Generation.html#chapter-3-시작-분포-적합",
    "href": "posts/Synthetic/Practical Synthetic Data Generation.html#chapter-3-시작-분포-적합",
    "title": "Practical Synthetic Data Generation",
    "section": "",
    "text": "- 데이터 합성 방법론과 기술\n\n분포 모델링(정규 분포/지수 분포 등 고전적인 분포)에 개별 변수를 적합시키거나 데이터 구조 모델링 사용\n\n- 과적합 해결법\n\n분포를 중립 지점에서 시작해 데이터에 더 가깝게 더 가까운 쪽으로 이동하여 각 단계별 분포의 단순성과 적합도 사이에서 균형을 이루게 하는 접근법\n최고의 절충점에 언제 도착했는지 측정해서 방지\n분포 적합성 접근법?"
  },
  {
    "objectID": "posts/Synthetic/Practical Synthetic Data Generation.html#chapter-4-합성-데이터의-효용성-평가",
    "href": "posts/Synthetic/Practical Synthetic Data Generation.html#chapter-4-합성-데이터의-효용성-평가",
    "title": "Practical Synthetic Data Generation",
    "section": "",
    "text": "- 일변량\n\n실제 데이터와 합성 데이터에서 각 변수 간의 분포 차이를 측정하기 위한 헬링거 거리 계산\n0: 분포간에 차이 없음 ~ 1: 분포 차이 많음\n\n- 이변량\n\n실제 데이터와 합성 데이터의 모든 변수 쌍 간의 상관관계의 절대적 차이 = 데이터 효용성의 척도\n\n- 다변량\n\n10겹 교차 검증\n데이터 셋을 10개의 동일한 크기의 서브셋으로 나눔\n서브셋1을 테스트셋으로 하고 나머지 9개 데이터셋 모델을 만든다.\n서브셋1에서 모델 테스트, AUROC 계산\n훈련 데이터로 서브셋2 테스트 사용, AUROC 계산\n… 10회 반복 \\(\\to\\) AUROC 10개 값 \\(\\to\\) 평균계산\n\n- 합성데이터와 실제 데이터에 대한 대응 모델에 대한 계산\n- 두 AUROC값의 절대적 차이 계산\n- 모든 절대값 차이에 대한 상자 그림 생성\n- 경향점수.. (어렵… 103p)"
  },
  {
    "objectID": "posts/Synthetic/Practical Synthetic Data Generation.html#chapter-5-데이터-합성-방법",
    "href": "posts/Synthetic/Practical Synthetic Data Generation.html#chapter-5-데이터-합성-방법",
    "title": "Practical Synthetic Data Generation",
    "section": "",
    "text": "- 방법\n\n정규분포로부터 샘플링\n샘플링 프로세스 중 상관관계 유도\n코퓰러 사용\n\n- 코퓰러?\n\n확률변수들 간의 상관관계 또는 종속성을 나타내는 함수\n흠.. 나중에 찾아보자..\n\n- 머신러닝 방법 - 의사결정 트리(CART) 사용\n- 딥러닝 방법 - 변이형 오토인코더(VAE) - 생성적 적대 신경망(GAN)\n- GAN - 생성기: 입력 무작위 데이터, 정규 분포 또는 균일 분포로부터 샘플링하며 합성 데이터 생성 - 판별기: 합성 데이터와 실제 데이터 비교하여 유사한 경향 점수 생성 차이 결과를 생성 기 훈련을 위해 다시 제공"
  },
  {
    "objectID": "posts/Synthetic/Practical Synthetic Data Generation.html#chapter-6-합성-데이터의-신원-식별",
    "href": "posts/Synthetic/Practical Synthetic Data Generation.html#chapter-6-합성-데이터의-신원-식별",
    "title": "Practical Synthetic Data Generation",
    "section": "",
    "text": "노출 유형\n신원 노출(정보 이득이 0이라면 신원 노출에 아무런 의미가 없음)\n속성 노출"
  },
  {
    "objectID": "posts/Synthetic/Practical Synthetic Data Generation.html#data",
    "href": "posts/Synthetic/Practical Synthetic Data Generation.html#data",
    "title": "Practical Synthetic Data Generation",
    "section": "data",
    "text": "data\n\nfrom sdmetrics import load_demo \n\nreal_data, synthetic_data, metadata = load_demo(modality='single_table')\n\n\nmetadata\n\n{'fields': {'start_date': {'type': 'datetime', 'format': '%Y-%m-%d'},\n  'end_date': {'type': 'datetime', 'format': '%Y-%m-%d'},\n  'salary': {'type': 'numerical', 'subtype': 'integer'},\n  'duration': {'type': 'numerical', 'subtype': 'integer'},\n  'student_id': {'type': 'id', 'subtype': 'integer'},\n  'high_perc': {'type': 'numerical', 'subtype': 'float'},\n  'high_spec': {'type': 'categorical'},\n  'mba_spec': {'type': 'categorical'},\n  'second_perc': {'type': 'numerical', 'subtype': 'float'},\n  'gender': {'type': 'categorical'},\n  'degree_perc': {'type': 'numerical', 'subtype': 'float'},\n  'placed': {'type': 'boolean'},\n  'experience_years': {'type': 'numerical', 'subtype': 'float'},\n  'employability_perc': {'type': 'numerical', 'subtype': 'float'},\n  'mba_perc': {'type': 'numerical', 'subtype': 'float'},\n  'work_experience': {'type': 'boolean'},\n  'degree_type': {'type': 'categorical'}},\n 'constraints': [],\n 'model_kwargs': {},\n 'name': None,\n 'primary_key': 'student_id',\n 'sequence_index': None,\n 'entity_columns': [],\n 'context_columns': []}\n\n\n\nreal_data.head()\n\n\n\n\n\n\n\n\nstudent_id\ngender\nsecond_perc\nhigh_perc\nhigh_spec\ndegree_perc\ndegree_type\nwork_experience\nexperience_years\nemployability_perc\nmba_spec\nmba_perc\nsalary\nplaced\nstart_date\nend_date\nduration\n\n\n\n\n0\n17264\nM\n67.00\n91.00\nCommerce\n58.00\nSci&Tech\nFalse\n0\n55.0\nMkt&HR\n58.80\n27000.0\nTrue\n2020-07-23\n2020-10-12\n3.0\n\n\n1\n17265\nM\n79.33\n78.33\nScience\n77.48\nSci&Tech\nTrue\n1\n86.5\nMkt&Fin\n66.28\n20000.0\nTrue\n2020-01-11\n2020-04-09\n3.0\n\n\n2\n17266\nM\n65.00\n68.00\nArts\n64.00\nComm&Mgmt\nFalse\n0\n75.0\nMkt&Fin\n57.80\n25000.0\nTrue\n2020-01-26\n2020-07-13\n6.0\n\n\n3\n17267\nM\n56.00\n52.00\nScience\n52.00\nSci&Tech\nFalse\n0\n66.0\nMkt&HR\n59.43\nNaN\nFalse\nNaT\nNaT\nNaN\n\n\n4\n17268\nM\n85.80\n73.60\nCommerce\n73.30\nComm&Mgmt\nFalse\n0\n96.8\nMkt&Fin\n55.50\n42500.0\nTrue\n2020-07-04\n2020-09-27\n3.0\n\n\n\n\n\n\n\n\nsynthetic_data.head()\n\n\n\n\n\n\n\n\nstudent_id\ngender\nsecond_perc\nhigh_perc\nhigh_spec\ndegree_perc\ndegree_type\nwork_experience\nexperience_years\nemployability_perc\nmba_spec\nmba_perc\nsalary\nplaced\nstart_date\nend_date\nduration\n\n\n\n\n0\n0\nF\n41.361060\n85.425072\nCommerce\n74.972674\nComm&Mgmt\nFalse\n0\n49.986653\nMkt&Fin\n57.291083\nNaN\nTrue\n2020-02-11\n2020-08-02\n3.0\n\n\n1\n1\nM\n63.720169\n99.059033\nCommerce\n62.769650\nOthers\nFalse\n0\n78.962948\nMkt&HR\n79.068319\nNaN\nFalse\nNaT\nNaT\nNaN\n\n\n2\n2\nM\n58.473884\n89.241528\nScience\n83.066328\nSci&Tech\nTrue\n0\n47.980244\nMkt&Fin\n77.042950\n26727.0\nTrue\n2020-02-13\n2020-05-27\n3.0\n\n\n3\n3\nF\n77.232204\n100.523788\nCommerce\n61.010445\nComm&Mgmt\nTrue\n0\n61.016218\nMkt&HR\n68.132991\n22058.0\nTrue\n2020-09-24\n2020-11-07\n3.0\n\n\n4\n4\nF\n54.067830\n109.611537\nCommerce\n72.846753\nOthers\nTrue\n0\n66.949987\nMkt&Fin\n66.363138\nNaN\nFalse\nNaT\nNaT\nNaN\n\n\n\n\n\n\n\n\nfrom sdmetrics.reports.single_table import QualityReport\n\nreport = QualityReport()\nreport.generate(real_data, synthetic_data, metadata)\n\nCreating report: 100%|██████████| 4/4 [00:00&lt;00:00, 12.97it/s]\n\n\n\nOverall Quality Score: 81.44%\n\nProperties:\nColumn Shapes: 81.56%\nColumn Pair Trends: 81.33%\n\n\n\nreport.get_details(property_name='Column Shapes')\n\n\n\n\n\n\n\n\nColumn\nMetric\nQuality Score\n\n\n\n\n0\nsecond_perc\nKSComplement\n0.627907\n\n\n1\nhigh_perc\nKSComplement\n0.553488\n\n\n2\ndegree_perc\nKSComplement\n0.627907\n\n\n3\nexperience_years\nKSComplement\n0.800000\n\n\n4\nemployability_perc\nKSComplement\n0.781395\n\n\n5\nmba_perc\nKSComplement\n0.841860\n\n\n6\nsalary\nKSComplement\n0.869155\n\n\n7\nstart_date\nKSComplement\n0.701107\n\n\n8\nend_date\nKSComplement\n0.768919\n\n\n9\nduration\nKSComplement\n0.826051\n\n\n10\ngender\nTVComplement\n0.939535\n\n\n11\nhigh_spec\nTVComplement\n0.902326\n\n\n12\ndegree_type\nTVComplement\n0.925581\n\n\n13\nwork_experience\nTVComplement\n0.972093\n\n\n14\nmba_spec\nTVComplement\n0.995349\n\n\n15\nplaced\nTVComplement\n0.916279\n\n\n\n\n\n\n\n\n시각화\n\nreport.get_visualization(property_name='Column Shapes')\n\n                                                \n\n\n- high Quality\n\nget_column_plot?\n\n\nSignature: get_column_plot(real_data, synthetic_data, column_name, metadata)\nDocstring:\nReturn a plot of the real and synthetic data for a given column.\nArgs:\n    real_data (pandas.DataFrame):\n        The real table data.\n    synthetic_data (pandas.DataFrame):\n        The synthetic table data.\n    column_name (str):\n        The name of the column.\n    metadata (dict):\n        The table metadata.\nReturns:\n    plotly.graph_objects._figure.Figure\nFile:      ~/anaconda3/envs/py37/lib/python3.7/site-packages/sdmetrics/reports/utils.py\nType:      function\n\n\n\n\n\nfrom sdmetrics.reports.utils import get_column_plot\n\nfig = get_column_plot(\n    real_data=real_data,\n    synthetic_data=synthetic_data,\n    metadata=metadata,\n    column_name='mba_spec'\n)\n\nfig.show()\n\n                                                \n\n\n- low Quality\n\nfrom sdmetrics.reports.utils import get_column_plot\n\nfig = get_column_plot(\n    real_data=real_data,\n    synthetic_data=synthetic_data,\n    metadata=metadata,\n    column_name='second_perc'\n)\n\nfig.show()\n\n                                                \n\n\n\nreport.get_visualization(property_name='Column Pair Trends')\n\n                                                \n\n\n\nfrom sdmetrics.reports.utils import get_column_pair_plot\n\n\nget_column_pair_plot?\n\n\nSignature: get_column_pair_plot(real_data, synthetic_data, column_names, metadata)\nDocstring:\nReturn a plot of the real and synthetic data for a given column pair.\nArgs:\n    real_data (pandas.DataFrame):\n        The real table data.\n    synthetic_column (pandas.Dataframe):\n        The synthetic table data.\n    column_names (list[string]):\n        The names of the two columns to plot.\n    metadata (dict):\n        The table metadata.\nReturns:\n    plotly.graph_objects._figure.Figure\nFile:      ~/anaconda3/envs/py37/lib/python3.7/site-packages/sdmetrics/reports/utils.py\nType:      function\n\n\n\n\n\nfig = get_column_pair_plot(\n    real_data=real_data,\n    synthetic_data=synthetic_data,\n    metadata=metadata,\n    column_names=['start_date', 'second_perc']\n)\n\nfig.show()\n\n                                                \n\n\n\n\n데이터 보고서 형태로 저장\nreport.save(filepath='sdmetrics_quality_demo.pkl')\n\n# load the report at a later time\nreport = QualityReport.load(filepath='sdmetrics_quality_demo.pkl')"
  },
  {
    "objectID": "posts/Note/콰트로 블로그 만드는 법.html",
    "href": "posts/Note/콰트로 블로그 만드는 법.html",
    "title": "콰트로 블로그",
    "section": "",
    "text": "git에서 Repositories 생성\n\n\n생성시 Read.Me 체크\n\n\n만들어진 레퍼토리의 HTTPS주소 복사\n\n3.명령 프롬프트에서 Dropbox로 들어가기\ncd Dropbox\n\ngit clone “2번복사한주소”\n콰트로 블로그 설정파일 자동 생성\n\nquarto create-project --type website:blog\n\n유저등록 (최초 한번)\n\ngit config --global user.email \"내 깃허브 이메일\"\n\ngit config --global user.name \"내 깃허브 닉네임\"\n\ngit config credential.helper store  #(아이디와 비번 저장)\n\n\n\ngit add .\ngit commit -m .\ngit push   (실행시 id/password작성)\n\npublish\n\nquarto publish gh-pages --no-prompt --no-bropwer"
  },
  {
    "objectID": "posts/Note/콰트로 블로그 만드는 법.html#콰트로-블로그-만드는-법",
    "href": "posts/Note/콰트로 블로그 만드는 법.html#콰트로-블로그-만드는-법",
    "title": "콰트로 블로그",
    "section": "",
    "text": "git에서 Repositories 생성\n\n\n생성시 Read.Me 체크\n\n\n만들어진 레퍼토리의 HTTPS주소 복사\n\n3.명령 프롬프트에서 Dropbox로 들어가기\ncd Dropbox\n\ngit clone “2번복사한주소”\n콰트로 블로그 설정파일 자동 생성\n\nquarto create-project --type website:blog\n\n유저등록 (최초 한번)\n\ngit config --global user.email \"내 깃허브 이메일\"\n\ngit config --global user.name \"내 깃허브 닉네임\"\n\ngit config credential.helper store  #(아이디와 비번 저장)\n\n\n\ngit add .\ngit commit -m .\ngit push   (실행시 id/password작성)\n\npublish\n\nquarto publish gh-pages --no-prompt --no-bropwer"
  },
  {
    "objectID": "Synthetic.html",
    "href": "Synthetic.html",
    "title": "Synthetic",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 19, 2023\n\n\n[CTGAN] CTGAN\n\n\n신록예찬 \n\n\n\n\nJul 19, 2023\n\n\n[CTGAN] CTGAN ToyEX\n\n\n신록예찬 \n\n\n\n\nMar 31, 2023\n\n\n[논문] Modeling Tabular Data using Conditional GAN\n\n\n김보람 \n\n\n\n\nMar 29, 2023\n\n\n[R] synthpop\n\n\n김보람 \n\n\n\n\nMar 24, 2023\n\n\n[논문] Synthetic Data\n\n\n김보람 \n\n\n\n\nMar 22, 2023\n\n\nAdvanced Deep Learning with TensorFlow 2 and Keras(-ing)\n\n\n김보람 \n\n\n\n\nMar 13, 2023\n\n\nSynthetic Data\n\n\n김보람 \n\n\n\n\nMar 10, 2023\n\n\nPractical Synthetic Data Generation\n\n\n김보람 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Note/Ref.html",
    "href": "posts/Note/Ref.html",
    "title": "Ref",
    "section": "",
    "text": "- ref: https://3months.tistory.com/392\n\n주피터 랩 단축키\n\n- ref: Markdown 문법\n\n- Quarto 블로그\n\nref: Quarto, Quarto\n블로그 만들기\n\n\n- 라텍스\n\nMathcha(수식편집)\nTables generator(표편집)\nMyscript(손글씨인식)"
  },
  {
    "objectID": "posts/Note/Ref.html#essi-alizadeh",
    "href": "posts/Note/Ref.html#essi-alizadeh",
    "title": "Ref",
    "section": "Essi Alizadeh",
    "text": "Essi Alizadeh"
  },
  {
    "objectID": "posts/Synthetic/synthetic data.html",
    "href": "posts/Synthetic/synthetic data.html",
    "title": "Synthetic Data",
    "section": "",
    "text": "합성데이터 연구 동향\n\n딥러닝 기술 발전 \\(\\to\\) 합성 데이터 생성 기술, 생성적 적대 신경망(GAN) 등 이미지, 음성, 자연어 등의 합성 데이터 생성\n유효성 검증 기술 \\(\\to\\) 실제 데이터와 합성 데이터의 차이 최소화, 합성 데이터가 실제 데이터와 비슷하게 생성되었는지 검증\n개인정보 보호 문제에 대한 대응책 \\(\\to\\) 개인정보 마스킹 기술이나 개인정보 사용하지 않고도 유사한 합성데이터 생성 기술 연구\n활용 분야 \\(\\to\\) 컴퓨터 비전, 자연어 처리, 의료 분야, 로봇 분야 등\n효과 입증 \\(\\to\\) 모델 학습시 성능이 개선되는 경우가 있음\n\n\n\n합성데이터 논문\n\n“딥러닝을 이용한 합성 데이터 생성 기술 연구 동향” (한국정보기술학회 논문지, 2019)\n\n\n딥러닝 기술을 이용한 합성 데이터 생성 기술의 연구 동향을 조사하고 분석한 내용\n\n\n“합성 데이터를 이용한 자율주행 차량 인식 모델의 성능 분석” (한국컴퓨터정보학회 논문지, 2019)\n\n\n합성 데이터를 이용하여 자율주행 차량 인식 모델을 학습시키고 성능을 분석한 내용\n\n\n“의료 영상 데이터를 위한 합성 데이터 생성 기술” (한국정보과학회 논문지, 2020)\n\n\n의료 영상 데이터를 위한 합성 데이터 생성 기술을 소개하고, 생성된 합성 데이터를 이용하여 의료 영상 인식 모델을 학습시키는 실험을 수행한 내용\n\n\n“합성 데이터 생성 기술을 이용한 동작 인식 성능 개선에 관한 연구” (한국지능시스템학회 논문지, 2020)\n\n\n합성 데이터 생성 기술을 이용하여 동작 인식 모델을 학습시키고 성능을 개선하는 실험을 수행한 내용\n\n\n“합성 데이터를 이용한 자연어 처리 분야에서의 문제 해결 방안 연구” (한국인터넷정보학회 논문지, 2021)\n\n\n합성 데이터를 이용하여 자연어 처리 분야에서의 문제를 해결하는 방안을 제시하고, 생성된 합성 데이터를 이용하여 자연어 처리 모델을 학습시키는 실험을 수행한 내용\n\n\n위 관련 내용은 chat GPT가 소개해준 내용\n\n\n합성 데이터 관련 논문이나 코드가 너무 없담… 어디서 찾지..\n\n\n\n예시1: Faker 데이터 사용\n\n!pip install Faker\n\nCollecting Faker\n  Downloading Faker-17.6.0-py3-none-any.whl (1.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 10.5 MB/s eta 0:00:0031m38.6 MB/s eta 0:00:01\nRequirement already satisfied: python-dateutil&gt;=2.4 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from Faker) (2.8.2)\nRequirement already satisfied: six&gt;=1.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from python-dateutil&gt;=2.4-&gt;Faker) (1.16.0)\nInstalling collected packages: Faker\nSuccessfully installed Faker-17.6.0\n\n\n\nFaker: 가짜 데이터 생성하는 라이브러리\n\n\nimport numpy as np\nimport pandas as pd\n\n\nfrom faker import Faker\nfake = Faker()\n\n# 가짜 이름, 주소, 전화번호, 이메일 생성하기\nname = fake.name()\naddress = fake.address()\nphone_number = fake.phone_number()\nemail = fake.email()\n\n\ndata = np.random.rand(100)\n\n\nnames = [fake.name() for i in range(100)]\naddresses = [fake.address() for i in range(100)]\nages = [fake.random_int(min=18, max=80, step=1) for i in range(100)]\n\n\ndf = pd.DataFrame({'Name': names, 'Address': addresses, 'Age': ages})\ndf # 100개의 가짜 데이터 생성\n\n\n\n\n\n\n\n\nName\nAddress\nAge\n\n\n\n\n0\nJason Williams\n88899 Miller Fall Apt. 222\\nNew Eric, VA 87882\n36\n\n\n1\nBrett Ramos\n81526 Jacqueline Corners Suite 818\\nJessicaton...\n73\n\n\n2\nMario Mitchell\n54833 Cox Lake Suite 142\\nChristianville, PW 0...\n22\n\n\n3\nDavid Ryan\n80999 Melissa Club\\nNorth Curtis, MI 28118\n32\n\n\n4\nMarcus Adkins\n6770 Jessica Radial\\nFloresberg, AR 35810\n58\n\n\n...\n...\n...\n...\n\n\n95\nFrancisco Porter\n84916 Brown Mission\\nWest Williamborough, MI 3...\n43\n\n\n96\nAlexander Martinez\n3583 David View\\nPatrickfurt, IA 27730\n50\n\n\n97\nJoshua Torres\n4304 Macdonald Lake Suite 363\\nLake Melissashi...\n24\n\n\n98\nLauren Morris\n584 Walker Squares Suite 817\\nSharonshire, MO ...\n48\n\n\n99\nJay Benson\n64335 Smith Rest Suite 370\\nNorth Robertland, ...\n54\n\n\n\n\n100 rows × 3 columns\n\n\n\n\n\n예시2: GAN 사용\n\n!pip install tensorflow\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nCollecting tensorflow\n  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 588.3/588.3 MB 8.2 MB/s eta 0:00:00m eta 0:00:01[36m0:00:01\nCollecting grpcio&lt;2.0,&gt;=1.24.3\n  Downloading grpcio-1.51.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 86.3 MB/s eta 0:00:0031m106.4 MB/s eta 0:00:01\nRequirement already satisfied: six&gt;=1.12.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from tensorflow) (1.16.0)\nCollecting opt-einsum&gt;=2.3.2\n  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 16.0 MB/s eta 0:00:00\nCollecting astunparse&gt;=1.6.0\n  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nCollecting google-pasta&gt;=0.1.1\n  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 13.7 MB/s eta 0:00:00\nCollecting tensorboard&lt;2.12,&gt;=2.11\n  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 93.8 MB/s eta 0:00:0031m124.4 MB/s eta 0:00:01\nRequirement already satisfied: numpy&gt;=1.20 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from tensorflow) (1.24.2)\nCollecting protobuf&lt;3.20,&gt;=3.9.2\n  Downloading protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 95.7 MB/s eta 0:00:00\nCollecting termcolor&gt;=1.1.0\n  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\nCollecting tensorflow-io-gcs-filesystem&gt;=0.23.1\n  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 97.2 MB/s eta 0:00:00\nCollecting keras&lt;2.12,&gt;=2.11.0\n  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 100.2 MB/s eta 0:00:00\nCollecting gast&lt;=0.4.0,&gt;=0.2.1\n  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\nCollecting h5py&gt;=2.9.0\n  Downloading h5py-3.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 98.9 MB/s eta 0:00:002 MB/s eta 0:00:01\nCollecting flatbuffers&gt;=2.0\n  Downloading flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\nRequirement already satisfied: setuptools in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from tensorflow) (65.6.3)\nCollecting wrapt&gt;=1.11.0\n  Downloading wrapt-1.15.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.5/81.5 kB 24.3 MB/s eta 0:00:00\nCollecting tensorflow-estimator&lt;2.12,&gt;=2.11.0\n  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 439.2/439.2 kB 76.0 MB/s eta 0:00:00\nCollecting libclang&gt;=13.0.0\n  Downloading libclang-15.0.6.1-py2.py3-none-manylinux2010_x86_64.whl (21.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.5/21.5 MB 73.7 MB/s eta 0:00:00m eta 0:00:010:01:01\nCollecting absl-py&gt;=1.0.0\n  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 30.3 MB/s eta 0:00:00\nRequirement already satisfied: packaging in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from tensorflow) (23.0)\nRequirement already satisfied: typing-extensions&gt;=3.6.6 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from tensorflow) (4.4.0)\nRequirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from astunparse&gt;=1.6.0-&gt;tensorflow) (0.38.4)\nCollecting tensorboard-data-server&lt;0.7.0,&gt;=0.6.0\n  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 100.6 MB/s eta 0:00:001m123.6 MB/s eta 0:00:01\nCollecting google-auth-oauthlib&lt;0.5,&gt;=0.4.1\n  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\nCollecting google-auth&lt;3,&gt;=1.6.3\n  Downloading google_auth-2.16.2-py2.py3-none-any.whl (177 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.2/177.2 kB 41.2 MB/s eta 0:00:00\nCollecting markdown&gt;=2.6.8\n  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.3/93.3 kB 22.9 MB/s eta 0:00:00\nCollecting tensorboard-plugin-wit&gt;=1.6.0\n  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 83.9 MB/s eta 0:00:00\nRequirement already satisfied: requests&lt;3,&gt;=2.21.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (2.28.2)\nRequirement already satisfied: werkzeug&gt;=1.0.1 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (2.2.3)\nCollecting cachetools&lt;6.0,&gt;=2.0.0\n  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\nCollecting pyasn1-modules&gt;=0.2.1\n  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 38.6 MB/s eta 0:00:00\nCollecting rsa&lt;5,&gt;=3.1.4\n  Downloading rsa-4.9-py3-none-any.whl (34 kB)\nCollecting requests-oauthlib&gt;=0.7.0\n  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\nRequirement already satisfied: importlib-metadata&gt;=4.4 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from markdown&gt;=2.6.8-&gt;tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (6.0.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (2022.12.7)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (2.1.1)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (3.4)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (1.26.14)\nRequirement already satisfied: MarkupSafe&gt;=2.1.1 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (2.1.1)\nRequirement already satisfied: zipp&gt;=0.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from importlib-metadata&gt;=4.4-&gt;markdown&gt;=2.6.8-&gt;tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (3.15.0)\nCollecting pyasn1&lt;0.5.0,&gt;=0.4.6\n  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 18.9 MB/s eta 0:00:00\nCollecting oauthlib&gt;=3.0.0\n  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 37.3 MB/s eta 0:00:00\nInstalling collected packages: tensorboard-plugin-wit, pyasn1, libclang, flatbuffers, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, pyasn1-modules, protobuf, opt-einsum, oauthlib, keras, h5py, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, markdown, google-auth, google-auth-oauthlib, tensorboard, tensorflow\nSuccessfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 flatbuffers-23.3.3 gast-0.4.0 google-auth-2.16.2 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.51.3 h5py-3.8.0 keras-2.11.0 libclang-15.0.6.1 markdown-3.4.1 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.2.0 wrapt-1.15.0\n\n\n\n# Define the generator model\ndef make_generator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(256, input_shape=(100,), use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Dense(512, use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Dense(28*28*1, use_bias=False, activation='tanh'))\n    model.add(layers.Reshape((28, 28, 1)))\n\n    return model\n\nmake_generator_model()\n- 생성기 모델 정의\n\n100차원 벡터를 입력으로 사용하고 데이터 세트의 입력 이미지와 동일한 크기의 출력 이미지 생성\nbatch normalization \\(\\to\\) leaky ReLU activation function \\(\\to\\) dense layer with a hyperbolic tangent activation function\n\n\n# Define the discriminator model\ndef make_discriminator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Flatten(input_shape=(28, 28, 1)))\n    model.add(layers.Dense(512, use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dense(256, use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dense(1))\n\n    return model\n\nmake_discriminator_model()\n- 판별기 모델\n\n이미지를 입력으로 받아 실제여부 구별하여 스칼라 값 출력(마지막 레이어가)\nbatch normalization, leaky ReLU activation function,final dense layer with no activation function\n\n\n# Define the GAN model\ndef make_gan_model(generator, discriminator):\n    discriminator.trainable = False\n\n    model = tf.keras.Sequential()\n    model.add(generator)\n    model.add(discriminator)\n\n    return model\n\nmake_gan_model( , )\n- GAN 모델\n\n생성기와 판별기 모델 단일 모델로 결합\ngenerator: discriminator가 실제로 분류하는 이미지 생성하도록 훈련\ndisciminator: generator가 생성하는 실제 이미지와 가짜 이미지 정확하게 분류하도록 훈련\n\n\n# Define the loss functions and optimizers\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\n\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n\n\nGAN 훈련시키려면 loss functions와 optimizers 정의\nbinary cross-entropy loss 사용\nAdam:0.0001\n\n\n# Train the GAN model\ndef train_gan(gan_model, dataset, epochs, batch_size):\n    generator, discriminator = gan_model.layers\n\n    for epoch in range(epochs):\n        for batch in dataset:\n            noise = tf.random.normal([batch_size, 100])\n\n            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n                generated_images = generator(noise, training=True)\n\n                real_output = discriminator(batch, training=True)\n                fake_output = discriminator(generated_images, training=True)\n\n                gen_loss = generator_loss(fake_output)\n                disc_loss = discriminator_loss(real_output, fake_output)\n\n            gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n            gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n            generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n            discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n\n        print(\"Epoch {} complete\".format(epoch))\n\n\n# Generate synthetic images using the trained generator\ndef generate_images(generator, num_images):\n    noise = tf.random.normal([num_images, 100])\n    generated_images = generator(noise, training=False)\n    generated_images = (generated_images + 1) / 2.0  # scale images to [0, 1]\n    return generated_images.numpy()\n\n\n훈련된 generator로 synthetic imges\n\n\n# Load and preprocess real images for training\n(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\nx_train = x_train.reshape(x_train.shape[0], 28,28,1)\n\n\n\n예시3: VAE사용 (개념부터 알아야할듯..)\n\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\n# load and preprocess the real images for training\n(train_images, _), (_, _) = keras.datasets.mnist.load_data()\ntrain_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\ntrain_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]\n\n\n# define the VAE model\nlatent_dim = 2\n\n\nlatent_dim = 2\n\nencoder_inputs = keras.Input(shape=(28, 28, 1))\nx = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\nx = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\nx = layers.Flatten()(x)\nx = layers.Dense(16, activation=\"relu\")(x)\nz_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\nz_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\nencoder = keras.Model(encoder_inputs, [z_mean, z_log_var], name=\"encoder\")\n\nlatent_inputs = keras.Input(shape=(latent_dim,))\nx = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\nx = layers.Reshape((7, 7, 64))(x)\nx = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\nx = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\ndecoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\ndecoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n\nclass Sampling(layers.Layer):\n    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n\n    def call(self, inputs):\n        z_mean, z_log_var = inputs\n        batch = keras.backend.shape(z_mean)[0]\n        dim = keras.backend.int_shape(z_mean)[1]\n        epsilon = keras.backend.random_normal(shape=(batch, dim))\n        return z_mean + keras.backend.exp(0.5 * z_log_var) * epsilon\n\nz = Sampling()([z_mean, z_log_var])\noutputs = decoder(z)\nvae = keras.Model(encoder_inputs, outputs, name=\"vae\")\n\n\n\n# define the loss function\nreconstruction_loss = keras.losses.binary_crossentropy(encoder_inputs, outputs)\nreconstruction_loss *= 28 * 28\nkl_loss = 1 + z_log_var - keras.backend.square(z_mean) - keras.backend.exp(z_log_var)\nkl_loss = keras.backend.sum(kl_loss, axis=-1)\nkl_loss *= -0.5\nvae_loss = keras.backend.mean(reconstruction_loss + kl_loss)\nvae.add_loss(vae_loss)\n\n\n\n# compile the model\nvae.compile(optimizer='adam')\n\n\n\n# train the model\nvae.fit(train_images, epochs=10, batch_size=128)\n\nEpoch 1/10\n\n\nInvalidArgumentError: Graph execution error:\n\nDetected at node 'gradient_tape/vae/tf.__operators__.add_1/BroadcastGradientArgs' defined at (most recent call last):\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in &lt;module&gt;\n      app.launch_new_instance()\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 619, in start\n      self.io_loop.start()\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/ioloop.py\", line 688, in &lt;lambda&gt;\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n      ret = callback()\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/gen.py\", line 814, in inner\n      self.ctx_run(self.run)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/gen.py\", line 775, in run\n      yielded = self.gen.send(value)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 358, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 536, in execute_request\n      self.do_execute(\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"&lt;ipython-input-79-41cbac8b1cbb&gt;\", line 2, in &lt;module&gt;\n      vae.fit(train_images, epochs=10, batch_size=128)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/keras/engine/training.py\", line 1027, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 526, in minimize\n      grads_and_vars = self.compute_gradients(loss, var_list, tape)\n    File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 259, in compute_gradients\n      grads = tape.gradient(loss, var_list)\nNode: 'gradient_tape/vae/tf.__operators__.add_1/BroadcastGradientArgs'\nIncompatible shapes: [128,28,28] vs. [128]\n     [[{{node gradient_tape/vae/tf.__operators__.add_1/BroadcastGradientArgs}}]] [Op:__inference_train_function_3053]\n\n\n\n# generate synthetic data\nn_samples = 10\nrandom_latent_vectors = np.random.normal(size=(n_samples, latent_dim))\ngenerated_images = decoder.predict(random_latent_vectors)\n\n1/1 [==============================] - 0s 55ms/step\n\n\n\n# display the generated images\nfor i in range(n_samples):\n    plt.imshow(generated_images[i].reshape(28, 28))\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n예시4: a simple feedforward neural network\n\n!pip install scikit-learn\n\nCollecting scikit-learn\n  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.8/9.8 MB 80.6 MB/s eta 0:00:00m eta 0:00:010:01\nRequirement already satisfied: scipy&gt;=1.3.2 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\nCollecting joblib&gt;=1.1.1\n  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.0/298.0 kB 58.5 MB/s eta 0:00:00\nRequirement already satisfied: numpy&gt;=1.17.3 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from scikit-learn) (1.24.2)\nCollecting threadpoolctl&gt;=2.0.0\n  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\nInstalling collected packages: threadpoolctl, joblib, scikit-learn\nSuccessfully installed joblib-1.2.0 scikit-learn-1.2.2 threadpoolctl-3.1.0\n\n\n\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\n# generate and preprocess the real data for training\nX, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n\n# define the feedforward neural network\nmodel = keras.Sequential([\n    layers.Dense(16, activation='relu', input_shape=(X.shape[1],)),\n    layers.Dense(8, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n])\n\n\n\n# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n\n\n# train the model\nmodel.fit(X, y, epochs=100, batch_size=32)\n\nEpoch 1/100\n32/32 [==============================] - 0s 553us/step - loss: 0.5901\nEpoch 2/100\n32/32 [==============================] - 0s 472us/step - loss: 0.5224\nEpoch 3/100\n32/32 [==============================] - 0s 477us/step - loss: 0.4699\nEpoch 4/100\n32/32 [==============================] - 0s 472us/step - loss: 0.4287\nEpoch 5/100\n32/32 [==============================] - 0s 459us/step - loss: 0.3960\nEpoch 6/100\n32/32 [==============================] - 0s 461us/step - loss: 0.3715\nEpoch 7/100\n32/32 [==============================] - 0s 466us/step - loss: 0.3530\nEpoch 8/100\n32/32 [==============================] - 0s 466us/step - loss: 0.3412\nEpoch 9/100\n32/32 [==============================] - 0s 464us/step - loss: 0.3338\nEpoch 10/100\n32/32 [==============================] - 0s 470us/step - loss: 0.3267\nEpoch 11/100\n32/32 [==============================] - 0s 470us/step - loss: 0.3219\nEpoch 12/100\n32/32 [==============================] - 0s 471us/step - loss: 0.3182\nEpoch 13/100\n32/32 [==============================] - 0s 471us/step - loss: 0.3158\nEpoch 14/100\n32/32 [==============================] - 0s 461us/step - loss: 0.3131\nEpoch 15/100\n32/32 [==============================] - 0s 459us/step - loss: 0.3110\nEpoch 16/100\n32/32 [==============================] - 0s 456us/step - loss: 0.3095\nEpoch 17/100\n32/32 [==============================] - 0s 458us/step - loss: 0.3075\nEpoch 18/100\n32/32 [==============================] - 0s 454us/step - loss: 0.3059\nEpoch 19/100\n32/32 [==============================] - 0s 448us/step - loss: 0.3042\nEpoch 20/100\n32/32 [==============================] - 0s 459us/step - loss: 0.3028\nEpoch 21/100\n32/32 [==============================] - 0s 458us/step - loss: 0.3017\nEpoch 22/100\n32/32 [==============================] - 0s 457us/step - loss: 0.3018\nEpoch 23/100\n32/32 [==============================] - 0s 462us/step - loss: 0.3001\nEpoch 24/100\n32/32 [==============================] - 0s 456us/step - loss: 0.2992\nEpoch 25/100\n32/32 [==============================] - 0s 457us/step - loss: 0.2979\nEpoch 26/100\n32/32 [==============================] - 0s 467us/step - loss: 0.2966\nEpoch 27/100\n32/32 [==============================] - 0s 465us/step - loss: 0.2960\nEpoch 28/100\n32/32 [==============================] - 0s 467us/step - loss: 0.2950\nEpoch 29/100\n32/32 [==============================] - 0s 467us/step - loss: 0.2938\nEpoch 30/100\n32/32 [==============================] - 0s 467us/step - loss: 0.2929\nEpoch 31/100\n32/32 [==============================] - 0s 469us/step - loss: 0.2925\nEpoch 32/100\n32/32 [==============================] - 0s 473us/step - loss: 0.2915\nEpoch 33/100\n32/32 [==============================] - 0s 469us/step - loss: 0.2906\nEpoch 34/100\n32/32 [==============================] - 0s 472us/step - loss: 0.2896\nEpoch 35/100\n32/32 [==============================] - 0s 473us/step - loss: 0.2891\nEpoch 36/100\n32/32 [==============================] - 0s 461us/step - loss: 0.2874\nEpoch 37/100\n32/32 [==============================] - 0s 456us/step - loss: 0.2871\nEpoch 38/100\n32/32 [==============================] - 0s 455us/step - loss: 0.2865\nEpoch 39/100\n32/32 [==============================] - 0s 457us/step - loss: 0.2855\nEpoch 40/100\n32/32 [==============================] - 0s 457us/step - loss: 0.2848\nEpoch 41/100\n32/32 [==============================] - 0s 459us/step - loss: 0.2843\nEpoch 42/100\n32/32 [==============================] - 0s 454us/step - loss: 0.2835\nEpoch 43/100\n32/32 [==============================] - 0s 457us/step - loss: 0.2827\nEpoch 44/100\n32/32 [==============================] - 0s 444us/step - loss: 0.2818\nEpoch 45/100\n32/32 [==============================] - 0s 447us/step - loss: 0.2813\nEpoch 46/100\n32/32 [==============================] - 0s 457us/step - loss: 0.2801\nEpoch 47/100\n32/32 [==============================] - 0s 460us/step - loss: 0.2795\nEpoch 48/100\n32/32 [==============================] - 0s 458us/step - loss: 0.2789\nEpoch 49/100\n32/32 [==============================] - 0s 462us/step - loss: 0.2781\nEpoch 50/100\n32/32 [==============================] - 0s 459us/step - loss: 0.2775\nEpoch 51/100\n32/32 [==============================] - 0s 459us/step - loss: 0.2766\nEpoch 52/100\n32/32 [==============================] - 0s 461us/step - loss: 0.2762\nEpoch 53/100\n32/32 [==============================] - 0s 460us/step - loss: 0.2757\nEpoch 54/100\n32/32 [==============================] - 0s 462us/step - loss: 0.2750\nEpoch 55/100\n32/32 [==============================] - 0s 457us/step - loss: 0.2729\nEpoch 56/100\n32/32 [==============================] - 0s 462us/step - loss: 0.2722\nEpoch 57/100\n32/32 [==============================] - 0s 465us/step - loss: 0.2711\nEpoch 58/100\n32/32 [==============================] - 0s 447us/step - loss: 0.2708\nEpoch 59/100\n32/32 [==============================] - 0s 451us/step - loss: 0.2704\nEpoch 60/100\n32/32 [==============================] - 0s 449us/step - loss: 0.2687\nEpoch 61/100\n32/32 [==============================] - 0s 454us/step - loss: 0.2683\nEpoch 62/100\n32/32 [==============================] - 0s 464us/step - loss: 0.2673\nEpoch 63/100\n32/32 [==============================] - 0s 469us/step - loss: 0.2663\nEpoch 64/100\n32/32 [==============================] - 0s 473us/step - loss: 0.2658\nEpoch 65/100\n32/32 [==============================] - 0s 466us/step - loss: 0.2651\nEpoch 66/100\n32/32 [==============================] - 0s 467us/step - loss: 0.2641\nEpoch 67/100\n32/32 [==============================] - 0s 467us/step - loss: 0.2633\nEpoch 68/100\n32/32 [==============================] - 0s 467us/step - loss: 0.2622\nEpoch 69/100\n32/32 [==============================] - 0s 463us/step - loss: 0.2618\nEpoch 70/100\n32/32 [==============================] - 0s 463us/step - loss: 0.2611\nEpoch 71/100\n32/32 [==============================] - 0s 464us/step - loss: 0.2598\nEpoch 72/100\n32/32 [==============================] - 0s 466us/step - loss: 0.2596\nEpoch 73/100\n32/32 [==============================] - 0s 460us/step - loss: 0.2582\nEpoch 74/100\n32/32 [==============================] - 0s 448us/step - loss: 0.2583\nEpoch 75/100\n32/32 [==============================] - 0s 452us/step - loss: 0.2573\nEpoch 76/100\n32/32 [==============================] - 0s 466us/step - loss: 0.2558\nEpoch 77/100\n32/32 [==============================] - 0s 462us/step - loss: 0.2556\nEpoch 78/100\n32/32 [==============================] - 0s 462us/step - loss: 0.2543\nEpoch 79/100\n32/32 [==============================] - 0s 451us/step - loss: 0.2547\nEpoch 80/100\n32/32 [==============================] - 0s 448us/step - loss: 0.2527\nEpoch 81/100\n32/32 [==============================] - 0s 453us/step - loss: 0.2516\nEpoch 82/100\n32/32 [==============================] - 0s 450us/step - loss: 0.2505\nEpoch 83/100\n32/32 [==============================] - 0s 451us/step - loss: 0.2502\nEpoch 84/100\n32/32 [==============================] - 0s 453us/step - loss: 0.2485\nEpoch 85/100\n32/32 [==============================] - 0s 452us/step - loss: 0.2475\nEpoch 86/100\n32/32 [==============================] - 0s 448us/step - loss: 0.2468\nEpoch 87/100\n32/32 [==============================] - 0s 453us/step - loss: 0.2461\nEpoch 88/100\n32/32 [==============================] - 0s 451us/step - loss: 0.2448\nEpoch 89/100\n32/32 [==============================] - 0s 450us/step - loss: 0.2438\nEpoch 90/100\n32/32 [==============================] - 0s 447us/step - loss: 0.2431\nEpoch 91/100\n32/32 [==============================] - 0s 456us/step - loss: 0.2421\nEpoch 92/100\n32/32 [==============================] - 0s 452us/step - loss: 0.2413\nEpoch 93/100\n32/32 [==============================] - 0s 452us/step - loss: 0.2403\nEpoch 94/100\n32/32 [==============================] - 0s 454us/step - loss: 0.2395\nEpoch 95/100\n32/32 [==============================] - 0s 460us/step - loss: 0.2388\nEpoch 96/100\n32/32 [==============================] - 0s 457us/step - loss: 0.2381\nEpoch 97/100\n32/32 [==============================] - 0s 460us/step - loss: 0.2368\nEpoch 98/100\n32/32 [==============================] - 0s 459us/step - loss: 0.2362\nEpoch 99/100\n32/32 [==============================] - 0s 460us/step - loss: 0.2349\nEpoch 100/100\n32/32 [==============================] - 0s 459us/step - loss: 0.2347\n\n\n&lt;keras.callbacks.History at 0x7f71e8106490&gt;\n\n\n\n# generate synthetic data\nn_samples = 10\nrandom_vectors = np.random.normal(size=(n_samples, X.shape[1]))\ngenerated_data = model.predict(random_vectors)\n\n1/1 [==============================] - 0s 22ms/step\n\n\n\n# display the generated data\nprint(generated_data)\n\n[[0.00134968]\n [0.11726338]\n [0.6508618 ]\n [0.9898428 ]\n [0.9719319 ]\n [0.9829191 ]\n [0.6400001 ]\n [0.00335612]\n [0.15759172]\n [0.00645017]]\n\n\n\n\n예시5: KDE(Kernel Density Estimation)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import KernelDensity\n\n\nX, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n\n\nkde = KernelDensity(kernel='gaussian', bandwidth=0.5)\nkde.fit(X)\n\nKernelDensity(bandwidth=0.5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KernelDensityKernelDensity(bandwidth=0.5)\n\n\n\nn_samples = 10\nsynthetic_data = kde.sample(n_samples)\nsynthetic_data\n\narray([[-6.40543316e-01, -2.00816029e-01, -1.38112318e+00,\n        -5.04647026e-01, -1.16412835e-01,  2.31176462e-01,\n         2.67275989e+00, -2.43788602e+00, -1.07609433e+00,\n        -7.60376957e-01],\n       [-1.96154361e+00, -5.44035464e-01, -1.29299914e+00,\n        -1.79243149e+00, -7.91254247e-01, -1.49100460e+00,\n         2.59878119e+00, -7.00449777e-01,  2.46705029e+00,\n        -8.13271665e-01],\n       [ 4.92102583e-01,  4.61885958e-01,  9.06165151e-01,\n         7.09125038e-01,  3.62018015e-01,  1.33662227e+00,\n        -1.73166578e+00, -1.54776550e+00, -6.47684988e-01,\n         1.50384591e+00],\n       [-1.37270120e+00,  1.11711271e+00,  1.69950848e-01,\n         1.71370541e+00, -9.95787406e-01,  9.37075645e-01,\n         6.86130469e-01,  2.68568255e+00,  5.23540728e-01,\n         1.28789991e+00],\n       [-6.09739471e-01,  2.74003692e-01, -4.99186042e-01,\n         7.55177619e-01,  1.10635851e+00, -6.33249957e-01,\n         1.45340497e+00,  1.50254494e-01,  1.70527839e+00,\n        -1.68688562e+00],\n       [ 2.82712325e-01,  6.99937229e-01,  2.35281704e-01,\n        -1.38757829e+00, -3.07830479e-01, -1.66666566e+00,\n         3.49788502e-01,  1.81251770e+00,  1.41807360e+00,\n        -1.14791545e-01],\n       [ 6.89280678e-01, -2.94374178e-03,  1.00903417e+00,\n         1.42917675e+00,  9.38636180e-01,  6.54756154e-01,\n        -1.50349162e+00, -1.72798315e-01,  7.09793089e-01,\n         2.34657027e+00],\n       [ 1.40766106e+00,  1.33457643e+00,  6.06365408e-02,\n        -1.61743252e+00,  1.29343257e+00, -4.73977756e-01,\n         2.15440056e-01, -1.41240310e+00, -3.73397331e+00,\n         1.15540476e+00],\n       [-4.13886212e-02,  1.31264769e+00,  2.42505327e-01,\n        -9.10608344e-01,  1.92476492e+00,  5.73378858e-01,\n        -9.79660592e-01,  1.66989518e+00, -3.47583358e-02,\n         2.49848587e-01],\n       [ 1.73735646e-01, -8.64171575e-01, -1.98493560e-01,\n         1.64845898e-01, -8.99243472e-01, -1.51645063e-01,\n        -8.09655616e-01, -7.19465453e-01, -9.90817553e-01,\n        -1.45541083e+00]])\n\n\n\nfig, ax = plt.subplots()\nax.scatter(X[:, 0], X[:, 1], c=y)\nax.scatter(synthetic_data[:, 0], synthetic_data[:, 1], c='r')\nplt.show()\n\n\n\n\n\n\n예시6: random sampling technique\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\n\n\nX, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n\n\nn_samples = 10\nsynthetic_data = np.random.rand(n_samples, X.shape[1])\n\n\nfig, ax = plt.subplots()\nax.scatter(X[:, 0], X[:, 1], c=y)\nax.scatter(synthetic_data[:, 0], synthetic_data[:, 1], c='r')\nplt.show()\n\n\n\n\n\n\n예시7: linear regression model\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\n\n\nX, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n\n\nlr = LinearRegression()\nlr.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nn_samples = 10\nsynthetic_data = lr.predict(np.random.rand(n_samples, 1))\n\n\nfig, ax = plt.subplots()\nax.scatter(X, y)\nax.scatter(np.random.rand(n_samples, 1), synthetic_data, c='r')\nplt.show()"
  },
  {
    "objectID": "posts/Synthetic/A Comparison of Synthetic Data Approaches Using Utility and Disclosure Risk Measures.html",
    "href": "posts/Synthetic/A Comparison of Synthetic Data Approaches Using Utility and Disclosure Risk Measures.html",
    "title": "[논문] Synthetic Data",
    "section": "",
    "text": "Deep generative model\nDiscolsure risk\nNonparametric Bayesian\nSequential regression\nSynthetic data\nUtility\n\nSeongbin An, Trang Doan, Juhee Lee, Jiwwo Kim, Yong Jae Kim, Yunji Kim, Changwon Yoon, Sungkyu Jung, Dongha Kim, Sunghoon Kwon, Hang J Kim, Jeongyoun Ahn, Cheolwoo Park\nThe Korean Journal of Applied Statistics\n\n\n- 재현자료 생성기법\n\n순차적 회귀분석\n비모수 베이지안\n인공지능 기반: CTGAN, TVAE\n\n- 유용성 지표\n. (대역 유용성):자료 전체의 분포적인 특성을 얼마나 비슷하게 유지 - Propensity Score, 거리측도, α-정밀도, β-wogusdbf\n. (특정 유용성): 특정 분석이 데이터의 적용될 것을 가정하고 해당 분석에서 원본자료와 재현자료가 얼마나 유사한 결과를 나타내는지 기반으로 유용성 판단 - 신뢰구간 중첩\n- 노출 위험도 지표\n\n신원 노출 위험도\n속성 노출 위험도\n독창성 점수\n\n\n\n\n\n2019년 전국 사업체 조사 데이터\n\n이항형, 다항형, 연속형 변수\n\n\n\n구분\n변수명\n변수설명\n\n\n\n\n범주형\nSEX\n대표자 성별(남/여)\n\n\n\nSUMMAT_CD\n매출 금액(9단계 범주)\n\n\n연속형\nWORKER_T\n총 근로자수\n\n\n\nEMP_T\n상용근로 종사자수\n\n\n\nBIS_MNTH\n영업개월수\n\n\n\n\\[WORKER_T&gt;=EMP_T\\]\n\n\n\n\n\n변수의 순서에 따라 결합분포의 추정값이 다르다.\n\\(X_j\\) 범주형 \\(\\to\\) 분류 의사결정나무 : 지니계수\n\\(X_j\\) 수치형 \\(\\to\\) 회귀 의사결정나무 : 엔트로피\n\n\n\n\n\n\n\n\n\n\n\n\nPropensity Score: 공변량 X가 주어졌을 때 처리그룹으로 배치될 확률 \\(Pr(Treatment = 1 | X)\\)\n재현자료로 배치되는 경우를 처리 그룹으로 배치되는 경우로 생각\n\\[ pMSE = \\dfrac{1}{n_s+n_o}\\sum_{i=1}^{n_s+n_o}(p̂_i-c)^2\\]\n재현자료의 유용성이 높을수록 \\(pMSE\\)는 0에 가까움\n\n원본자료와 재현자료를 분포적으로 구분할 수 있는지 수치화\n개별적 비교 필요 없이 변수의 관계성을 고려하여 평가 가능\n분류 모델에 따라 \\(pMSE\\) 값이 달라지므로 귀무분포를 고려해야 함\n\n\n\n\n\n원본자료와 재현자료에서 각 변수의 분포를 각각 계산하여 유용성 판단 가능\n변수간의 상관성 고려 못함\n\n- KL괴리도\n\nKullback-Leibler\n\n\\[ D(f||g) = \\int_{-\\infty}^{\\infty}f(x) log \\dfrac{f(x)}{g(x)}dx\\]\n- Wasser-stein 거리\n\\[ W_r(f,g) = (\\int_{0}^{1} |F_f^{-1}(t) - F_g^{-1}(t)|^r)^{1/r} dt \\]\n\n\n\n\n\n\nNotation\n설명\n\n\n\n\nn\n원본(재현)자료 관측치 개수\n\n\n\\(f_i\\)\n원본자료의 i번째 관측치에 대해 준식별자 값이 같은 관측치 개수\n\n\n\\(X_i\\)\n원본자료의 i번째 관측치의 민감 변수 값\n\n\n\\(P_i\\)\n원본자료에서 \\(X_i\\) 와 같은 값을 갖는 관측치의 비율\n\n\n\\(d_i\\)\n1-\\(p_i\\)\n\n\n\\(Y_i\\)\n원본자료의 i번째 관측치와 연결된 재현자료 민감 변수 값\n\n\n\\((d_i)'\\)\n원본자료에서 \\(X_i\\)가 속한 군집에 있는 관측치의 비율\n\n\n\n- 민감변수: 준식별자를 제외한 나머지 변수\n\n민감변수(명목형)\n\n\\[ d_i \\times I(X_i=Y_t) &gt; \\sqrt{p_i(1-p_i)}, i=1,2,\\dots,n\\]\n\n민감변수(연속형): k-means를 이용해 값을 군집화하고 부등식 확인\n\n\\[d'_i \\times |X_i - Y_i| &lt; 1.48 \\times MAD , i=1,2,\\dots,n \\]\nMAD:중위절대편차\n원본자료의 i번째 관측치에서 위 부등식을 만족하는 민감벼눗의 비율이 5% 이상이면 1, 그렇지 않으면 0 \\(\\to\\) 지시함수 \\(R_i\\)\n- 신원 노출 위험도\n\\[ \\dfrac{1}{n} \\sum_{i=1}^{n}(\\dfrac{1}{f_i}\\times I_i \\times R_i)\\]\n작을수록 신원 추출 가능성이 작아짐\n\n구현 시간이 오래 걸림\n준식별자와 민감 변수로 구분시 명확한 기준이 없음\n\n\n\n\n\n공격자가 개인의 신원을 식별할 수는 없지만 특정 민감한 변수의 속성을 추론할 수 있을때 발생\n완전 재현자료여도 속성 노출 위험도 항상 존재\n\n- CAP(correct attribution probability)\n\n공격자가 원본자료의 일부 변수(K:key bariables)를 가지고 있고 하나의 특정 변수의 값에 대하여 알고자 하는(T:target variable) 상황에서 계산\n\\(K\\)와 \\(T\\) 모두 범주형이어야 계산 가능, 연속형 변수는 K-MEANS를 실시하여..\n\n\n\n\n\n원본자료와 재현자료의 토대를 추정\n테이블, 이미지 등 다양한 형태 데이터 져핸에 대한 평가 지표\n잠재공간으로 임베딩시 hyperparameter설정에 따라 결과가 다르게 나옴\n\n- α정밀도\n\n재현자료가 원본자료를 얼마나 충실하게 재현하는가\n재현자료 유용성 측정지표\nα정밀도가 높은 재현자료는 현실성이 높은 관측치를 포함\n\n원본 데이터 \\(D_O\\)의 확률분포의 서포트 안에서 α 만큼의 확률을 가지는 가장 작은 토대(α-support)를 \\(S_0^α\\)\n\\[ α정밀도:P_α\\] \\[ P_α := Pr(x_s \\in S_0^α), for α \\in [0,1]\\]\n\\[재현자료가 원본자료의 분포에서 나타날 가능성\\]\n- β재현율\n\n재현자료가 원본자료의 다양성을 충분히 반영하는가\n재현자료 유용성 측정지표\nβ재현율이 낮은 재현자료는 원본자료의 일부만을 반복적으로 재현\n\n\\[β재현율: R_β\\] \\[ R_β := Pr(x_o \\in S_0^β), for β \\in [0,1]\\]\n\\[재현자료의 분포가 원본자료를 얼마나 포함하지는지\\]\n- 독창섬점수 - 재현자료를 얼마나 원본자료에 존재하지 않는 새로운 관측치들을 만들어 내는가 - 정보노출의 위험성 측정 지표 - 재현자료가 원본자료를 과적합하여 그대로 사용하고 있는지?"
  },
  {
    "objectID": "posts/Synthetic/A Comparison of Synthetic Data Approaches Using Utility and Disclosure Risk Measures.html#서론",
    "href": "posts/Synthetic/A Comparison of Synthetic Data Approaches Using Utility and Disclosure Risk Measures.html#서론",
    "title": "[논문] Synthetic Data",
    "section": "",
    "text": "- 재현자료 생성기법\n\n순차적 회귀분석\n비모수 베이지안\n인공지능 기반: CTGAN, TVAE\n\n- 유용성 지표\n. (대역 유용성):자료 전체의 분포적인 특성을 얼마나 비슷하게 유지 - Propensity Score, 거리측도, α-정밀도, β-wogusdbf\n. (특정 유용성): 특정 분석이 데이터의 적용될 것을 가정하고 해당 분석에서 원본자료와 재현자료가 얼마나 유사한 결과를 나타내는지 기반으로 유용성 판단 - 신뢰구간 중첩\n- 노출 위험도 지표\n\n신원 노출 위험도\n속성 노출 위험도\n독창성 점수"
  },
  {
    "objectID": "posts/Synthetic/A Comparison of Synthetic Data Approaches Using Utility and Disclosure Risk Measures.html#survey-est",
    "href": "posts/Synthetic/A Comparison of Synthetic Data Approaches Using Utility and Disclosure Risk Measures.html#survey-est",
    "title": "[논문] Synthetic Data",
    "section": "",
    "text": "2019년 전국 사업체 조사 데이터\n\n이항형, 다항형, 연속형 변수\n\n\n\n구분\n변수명\n변수설명\n\n\n\n\n범주형\nSEX\n대표자 성별(남/여)\n\n\n\nSUMMAT_CD\n매출 금액(9단계 범주)\n\n\n연속형\nWORKER_T\n총 근로자수\n\n\n\nEMP_T\n상용근로 종사자수\n\n\n\nBIS_MNTH\n영업개월수\n\n\n\n\\[WORKER_T&gt;=EMP_T\\]"
  },
  {
    "objectID": "posts/Synthetic/A Comparison of Synthetic Data Approaches Using Utility and Disclosure Risk Measures.html#재현자료-생성기법",
    "href": "posts/Synthetic/A Comparison of Synthetic Data Approaches Using Utility and Disclosure Risk Measures.html#재현자료-생성기법",
    "title": "[논문] Synthetic Data",
    "section": "",
    "text": "변수의 순서에 따라 결합분포의 추정값이 다르다.\n\\(X_j\\) 범주형 \\(\\to\\) 분류 의사결정나무 : 지니계수\n\\(X_j\\) 수치형 \\(\\to\\) 회귀 의사결정나무 : 엔트로피"
  },
  {
    "objectID": "posts/Synthetic/A Comparison of Synthetic Data Approaches Using Utility and Disclosure Risk Measures.html#재현자료의-평가-지표",
    "href": "posts/Synthetic/A Comparison of Synthetic Data Approaches Using Utility and Disclosure Risk Measures.html#재현자료의-평가-지표",
    "title": "[논문] Synthetic Data",
    "section": "",
    "text": "Propensity Score: 공변량 X가 주어졌을 때 처리그룹으로 배치될 확률 \\(Pr(Treatment = 1 | X)\\)\n재현자료로 배치되는 경우를 처리 그룹으로 배치되는 경우로 생각\n\\[ pMSE = \\dfrac{1}{n_s+n_o}\\sum_{i=1}^{n_s+n_o}(p̂_i-c)^2\\]\n재현자료의 유용성이 높을수록 \\(pMSE\\)는 0에 가까움\n\n원본자료와 재현자료를 분포적으로 구분할 수 있는지 수치화\n개별적 비교 필요 없이 변수의 관계성을 고려하여 평가 가능\n분류 모델에 따라 \\(pMSE\\) 값이 달라지므로 귀무분포를 고려해야 함\n\n\n\n\n\n원본자료와 재현자료에서 각 변수의 분포를 각각 계산하여 유용성 판단 가능\n변수간의 상관성 고려 못함\n\n- KL괴리도\n\nKullback-Leibler\n\n\\[ D(f||g) = \\int_{-\\infty}^{\\infty}f(x) log \\dfrac{f(x)}{g(x)}dx\\]\n- Wasser-stein 거리\n\\[ W_r(f,g) = (\\int_{0}^{1} |F_f^{-1}(t) - F_g^{-1}(t)|^r)^{1/r} dt \\]\n\n\n\n\n\n\nNotation\n설명\n\n\n\n\nn\n원본(재현)자료 관측치 개수\n\n\n\\(f_i\\)\n원본자료의 i번째 관측치에 대해 준식별자 값이 같은 관측치 개수\n\n\n\\(X_i\\)\n원본자료의 i번째 관측치의 민감 변수 값\n\n\n\\(P_i\\)\n원본자료에서 \\(X_i\\) 와 같은 값을 갖는 관측치의 비율\n\n\n\\(d_i\\)\n1-\\(p_i\\)\n\n\n\\(Y_i\\)\n원본자료의 i번째 관측치와 연결된 재현자료 민감 변수 값\n\n\n\\((d_i)'\\)\n원본자료에서 \\(X_i\\)가 속한 군집에 있는 관측치의 비율\n\n\n\n- 민감변수: 준식별자를 제외한 나머지 변수\n\n민감변수(명목형)\n\n\\[ d_i \\times I(X_i=Y_t) &gt; \\sqrt{p_i(1-p_i)}, i=1,2,\\dots,n\\]\n\n민감변수(연속형): k-means를 이용해 값을 군집화하고 부등식 확인\n\n\\[d'_i \\times |X_i - Y_i| &lt; 1.48 \\times MAD , i=1,2,\\dots,n \\]\nMAD:중위절대편차\n원본자료의 i번째 관측치에서 위 부등식을 만족하는 민감벼눗의 비율이 5% 이상이면 1, 그렇지 않으면 0 \\(\\to\\) 지시함수 \\(R_i\\)\n- 신원 노출 위험도\n\\[ \\dfrac{1}{n} \\sum_{i=1}^{n}(\\dfrac{1}{f_i}\\times I_i \\times R_i)\\]\n작을수록 신원 추출 가능성이 작아짐\n\n구현 시간이 오래 걸림\n준식별자와 민감 변수로 구분시 명확한 기준이 없음\n\n\n\n\n\n공격자가 개인의 신원을 식별할 수는 없지만 특정 민감한 변수의 속성을 추론할 수 있을때 발생\n완전 재현자료여도 속성 노출 위험도 항상 존재\n\n- CAP(correct attribution probability)\n\n공격자가 원본자료의 일부 변수(K:key bariables)를 가지고 있고 하나의 특정 변수의 값에 대하여 알고자 하는(T:target variable) 상황에서 계산\n\\(K\\)와 \\(T\\) 모두 범주형이어야 계산 가능, 연속형 변수는 K-MEANS를 실시하여..\n\n\n\n\n\n원본자료와 재현자료의 토대를 추정\n테이블, 이미지 등 다양한 형태 데이터 져핸에 대한 평가 지표\n잠재공간으로 임베딩시 hyperparameter설정에 따라 결과가 다르게 나옴\n\n- α정밀도\n\n재현자료가 원본자료를 얼마나 충실하게 재현하는가\n재현자료 유용성 측정지표\nα정밀도가 높은 재현자료는 현실성이 높은 관측치를 포함\n\n원본 데이터 \\(D_O\\)의 확률분포의 서포트 안에서 α 만큼의 확률을 가지는 가장 작은 토대(α-support)를 \\(S_0^α\\)\n\\[ α정밀도:P_α\\] \\[ P_α := Pr(x_s \\in S_0^α), for α \\in [0,1]\\]\n\\[재현자료가 원본자료의 분포에서 나타날 가능성\\]\n- β재현율\n\n재현자료가 원본자료의 다양성을 충분히 반영하는가\n재현자료 유용성 측정지표\nβ재현율이 낮은 재현자료는 원본자료의 일부만을 반복적으로 재현\n\n\\[β재현율: R_β\\] \\[ R_β := Pr(x_o \\in S_0^β), for β \\in [0,1]\\]\n\\[재현자료의 분포가 원본자료를 얼마나 포함하지는지\\]\n- 독창섬점수 - 재현자료를 얼마나 원본자료에 존재하지 않는 새로운 관측치들을 만들어 내는가 - 정보노출의 위험성 측정 지표 - 재현자료가 원본자료를 과적합하여 그대로 사용하고 있는지?"
  },
  {
    "objectID": "posts/Synthetic/[R] synthpop.html",
    "href": "posts/Synthetic/[R] synthpop.html",
    "title": "[R] synthpop",
    "section": "",
    "text": "https://cran.r-project.org/web/packages/synthpop/synthpop.pdf\n\nR에서의 synthpop사용\n\n\nlibrary(synthpop)\n\nFind out more at https://www.synthpop.org.uk/\n\n\n\n\nrm(list = ls())                # to clean out workspace\n\n\ndata: SD2011\n\nhelp(SD2011)                   # this will give you information about it\n\n\n\n\n\n\nSD2011 {synthpop}\nR Documentation\n\n\n\n\n\n\nSocial Diagnosis 2011 - Objective and Subjective Quality of Life in Poland\n\n\nDescription\n\nSample of 5,000 individuals from the Social Diagnosis 2011 survey;\nselected variables only.\n\n\n\nUsage\n\nSD2011\n\n\nFormat\n\nA data frame with 5,000 observations on the following 35 variables:\n\n\n\nsexSex\n\nageAge of person, 2011\n\nagegrAge group, 2011\n\nplacesizeCategory of the place of residence\n\nregionRegion (voivodeship)\n\neduHighest educational qualification, 2011\n\neduspecDiscipline of completed qualification\n\nsocprofSocio-economic status, 2011\n\nunempdurTotal duration of unemployment in the last 2 years (in months)\n\nincomePersonal monthly net income\n\nmaritalMarital status\n\nmmarrMonth of marriage\n\nymarrYear of marriage\n\nmsepdivMonth of separation/divorce\n\nysepdivYear of separation/divorce\n\nlsPerception of life as a whole\n\ndepressDepression symptoms indicator\n\ntrustView on interpersonal trust\n\ntrustfamTrust in own family members\n\ntrustneighTrust in neighbours\n\nsportActive engagement in some form of sport or exercise\n\nnofriendNumber of friends\n\nsmokeSmoking cigarettes\n\nnocigaNumber of cigarettes smoked per day\n\nalcabuseDrinking too much alcohol\n\nalcsolStarting to use alcohol to cope with troubles\n\nworkabWorking abroad in 2007-2011\n\nwkabdurTotal time spent on working abroad\n\nwkabintPlans to go abroad to work in the next two years\n\nwkabintdurIntended duration of working abroad\n\nemccIntended destination country\n\nenglangKnowledge of English language\n\nheightHeight of person\n\nweightWeight of person\n\nbmiBody mass index\n\n\n\n\nNote\n\nPlease note that the original variable names have been changed to make them \nmore self-explanatory. Some variable labels have been adjusted as well.\n\n\nSource\n\nCouncil for Social Monitoring. Social Diagnosis 2000-2011: integrated database.\nhttp://www.diagnoza.com/index-en.html [downloaded on 13/12/2013]\n\n\n\nReferences\n\nCzapinski J. and Panek T. (Eds.) (2011). Social Diagnosis 2011. Objective and \nSubjective Quality of Life in Poland - full report. Contemporary Economics, \nVolume 5, Issue 3 (special issue) http://ce.vizja.pl/en/issues/volume/5/issue/3#art254 \n\n\n\nExamples\n\n  spineplot(englang ~ agegr, data = SD2011, xlab = \"Age group\", ylab = \"Knowledge of English\")\n  boxplot(income ~ sex, data = SD2011[SD2011$income != -8,])\n\n\n[Package synthpop version 1.8-0 ]"
  },
  {
    "objectID": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html",
    "href": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "",
    "text": "import tensorflow as tf\nprint(tf.__version__)\n\n2.11.0\nfrom tensorflow.keras import backend as K\nprint(K.epsilon())\n\n1e-07"
  },
  {
    "objectID": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#mlp",
    "href": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#mlp",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "MLP",
    "text": "MLP\n\n멀티레이어 퍼셉트론(Multilayer Perceptrom)\n\n: 완전 연결 네트워크, 심층 피드-포워드망, 피드-포워드 신경망"
  },
  {
    "objectID": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#cnn",
    "href": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#cnn",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "CNN",
    "text": "CNN"
  },
  {
    "objectID": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#rnn",
    "href": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#rnn",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "RNN",
    "text": "RNN"
  },
  {
    "objectID": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#오토인코더-구축",
    "href": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#오토인코더-구축",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "오토인코더 구축",
    "text": "오토인코더 구축\n\n'''Example of autoencoder model on MNIST dataset\n\nThis autoencoder has modular design. The encoder, decoder and autoencoder\nare 3 models that share weights. For example, after training the\nautoencoder, the encoder can be used to  generate latent vectors\nof input data for low-dim visualization like PCA or TSNE.\n'''\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.layers import Conv2D, Flatten\nfrom tensorflow.keras.layers import Reshape, Conv2DTranspose\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras import backend as K\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# load MNIST dataset\n(x_train, _), (x_test, _) = mnist.load_data()\n\n# reshape to (28, 28, 1) and normalize input images\nimage_size = x_train.shape[1]\nx_train = np.reshape(x_train, [-1, image_size, image_size, 1])\nx_test = np.reshape(x_test, [-1, image_size, image_size, 1])\nx_train = x_train.astype('float32') / 255\nx_test = x_test.astype('float32') / 255\n\n# network parameters\ninput_shape = (image_size, image_size, 1)\nbatch_size = 32\nkernel_size = 3\nlatent_dim = 16\n# encoder/decoder number of CNN layers and filters per layer\nlayer_filters = [32, 64]\n\n# build the autoencoder model\n# first build the encoder model\ninputs = Input(shape=input_shape, name='encoder_input')\nx = inputs\n# stack of Conv2D(32)-Conv2D(64)\nfor filters in layer_filters:\n    x = Conv2D(filters=filters,\n               kernel_size=kernel_size,\n               activation='relu',\n               strides=2,\n               padding='same')(x)\n\n# shape info needed to build decoder model\n# so we don't do hand computation\n# the input to the decoder's first\n# Conv2DTranspose will have this shape\n# shape is (7, 7, 64) which is processed by\n# the decoder back to (28, 28, 1)\nshape = K.int_shape(x)\n\n# generate latent vector\nx = Flatten()(x)\nlatent = Dense(latent_dim, name='latent_vector')(x)\n\n# instantiate encoder model\nencoder = Model(inputs,\n                latent,\n                name='encoder')\nencoder.summary()\nplot_model(encoder,\n           to_file='encoder.png',\n           show_shapes=True)\n\n# build the decoder model\nlatent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n# use the shape (7, 7, 64) that was earlier saved\nx = Dense(shape[1] * shape[2] * shape[3])(latent_inputs)\n# from vector to suitable shape for transposed conv\nx = Reshape((shape[1], shape[2], shape[3]))(x)\n\n# stack of Conv2DTranspose(64)-Conv2DTranspose(32)\nfor filters in layer_filters[::-1]:\n    x = Conv2DTranspose(filters=filters,\n                        kernel_size=kernel_size,\n                        activation='relu',\n                        strides=2,\n                        padding='same')(x)\n\n# reconstruct the input\noutputs = Conv2DTranspose(filters=1,\n                          kernel_size=kernel_size,\n                          activation='sigmoid',\n                          padding='same',\n                          name='decoder_output')(x)\n\n# instantiate decoder model\ndecoder = Model(latent_inputs, outputs, name='decoder')\ndecoder.summary()\nplot_model(decoder, to_file='decoder.png', show_shapes=True)\n\n# autoencoder = encoder + decoder\n# instantiate autoencoder model\nautoencoder = Model(inputs,\n                    decoder(encoder(inputs)),\n                    name='autoencoder')\nautoencoder.summary()\nplot_model(autoencoder,\n           to_file='autoencoder.png',\n           show_shapes=True)\n\n# Mean Square Error (MSE) loss function, Adam optimizer\nautoencoder.compile(loss='mse', optimizer='adam')\n\n# train the autoencoder\nautoencoder.fit(x_train,\n                x_train,\n                validation_data=(x_test, x_test),\n                epochs=1,\n                batch_size=batch_size)\n\n# predict the autoencoder output from test data\nx_decoded = autoencoder.predict(x_test)\n\n# display the 1st 8 test input and decoded images\nimgs = np.concatenate([x_test[:8], x_decoded[:8]])\nimgs = imgs.reshape((4, 4, image_size, image_size))\nimgs = np.vstack([np.hstack(i) for i in imgs])\nplt.figure()\nplt.axis('off')\nplt.title('Input: 1st 2 rows, Decoded: last 2 rows')\nplt.imshow(imgs, interpolation='none', cmap='gray')\nplt.savefig('input_and_decoded.png')\nplt.show()\n\nModel: \"encoder\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n encoder_input (InputLayer)  [(None, 28, 28, 1)]       0         \n                                                                 \n conv2d (Conv2D)             (None, 14, 14, 32)        320       \n                                                                 \n conv2d_1 (Conv2D)           (None, 7, 7, 64)          18496     \n                                                                 \n flatten (Flatten)           (None, 3136)              0         \n                                                                 \n latent_vector (Dense)       (None, 16)                50192     \n                                                                 \n=================================================================\nTotal params: 69,008\nTrainable params: 69,008\nNon-trainable params: 0\n_________________________________________________________________\nYou must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\nModel: \"decoder\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n decoder_input (InputLayer)  [(None, 16)]              0         \n                                                                 \n dense (Dense)               (None, 3136)              53312     \n                                                                 \n reshape (Reshape)           (None, 7, 7, 64)          0         \n                                                                 \n conv2d_transpose (Conv2DTra  (None, 14, 14, 64)       36928     \n nspose)                                                         \n                                                                 \n conv2d_transpose_1 (Conv2DT  (None, 28, 28, 32)       18464     \n ranspose)                                                       \n                                                                 \n decoder_output (Conv2DTrans  (None, 28, 28, 1)        289       \n pose)                                                           \n                                                                 \n=================================================================\nTotal params: 108,993\nTrainable params: 108,993\nNon-trainable params: 0\n_________________________________________________________________\nYou must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\nModel: \"autoencoder\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n encoder_input (InputLayer)  [(None, 28, 28, 1)]       0         \n                                                                 \n encoder (Functional)        (None, 16)                69008     \n                                                                 \n decoder (Functional)        (None, 28, 28, 1)         108993    \n                                                                 \n=================================================================\nTotal params: 178,001\nTrainable params: 178,001\nNon-trainable params: 0\n_________________________________________________________________\nYou must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0212 - val_loss: 0.0104\n313/313 [==============================] - 1s 2ms/step\n\n\n\n\n\n\n인코더 모델은 낮은 차원의 잠재 벡터를 생성하기 위해서 Conv2D(32)-Conv2D(64)-Dense(16)으로 구성\n디코더 모델은 Dense(16)-Conv2DTranspose(64)-Conv2DTranspose(32)-Conv2DTranspose(1)으로 구성\n입력은 원본 입력을 복원하기 위한 디코딩된 잠재 벡터"
  },
  {
    "objectID": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#잠재벡터-시각화",
    "href": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#잠재벡터-시각화",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "잠재벡터 시각화",
    "text": "잠재벡터 시각화\n\n- 잠재 코드 차원\n\n숫자 0: 왼쪽 아래 사분면\n숫자 1: 오른쪽 위 사분면"
  },
  {
    "objectID": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#노이즈-제거-오토인코더dae",
    "href": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#노이즈-제거-오토인코더dae",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "노이즈 제거 오토인코더(DAE)",
    "text": "노이즈 제거 오토인코더(DAE)\n노이즈 제거 Denoising\n\\[ x = x_{orig} + noise \\]\n인코더의 목적: 잠재벡터인 \\(z\\)를 생성하는 방법을 찾는 것 \\(\\to\\) 잠재 벡터를 디코더가 MSE와 같은 손실 함수의 비 유사성을 최소화하여 \\(x_{orig}\\)로 복원\n\\[ L(x_{orig}, \\tilde{x}) = MSE = \\frac{1}{m} \\sum _{i=1} ^{i=m} (x _{origi} - {\\tilde{x _{i}}} ) ^{2}\\]\n\n'''Trains a denoising autoencoder on MNIST dataset.\n\nDenoising is one of the classic applications of autoencoders.\nThe denoising process removes unwanted noise that corrupted the\ntrue data.\n\nNoise + Data ---&gt; Denoising Autoencoder ---&gt; Data\n\nGiven a training dataset of corrupted data as input and\ntrue data as output, a denoising autoencoder can recover the\nhidden structure to generate clean data.\n\nThis example has modular design. The encoder, decoder and autoencoder\nare 3 models that share weights. For example, after training the\nautoencoder, the encoder can be used to  generate latent vectors\nof input data for low-dim visualization like PCA or TSNE.\n'''\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.layers import Conv2D, Flatten\nfrom tensorflow.keras.layers import Reshape, Conv2DTranspose\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.datasets import mnist\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nnp.random.seed(1337)\n\n# load MNIST dataset\n(x_train, _), (x_test, _) = mnist.load_data()\n\n# reshape to (28, 28, 1) and normalize input images\nimage_size = x_train.shape[1]\nx_train = np.reshape(x_train, [-1, image_size, image_size, 1])\nx_test = np.reshape(x_test, [-1, image_size, image_size, 1])\nx_train = x_train.astype('float32') / 255\nx_test = x_test.astype('float32') / 255\n\n# generate corrupted MNIST images by adding noise with normal dist\n# centered at 0.5 and std=0.5\nnoise = np.random.normal(loc=0.5, scale=0.5, size=x_train.shape)\nx_train_noisy = x_train + noise\nnoise = np.random.normal(loc=0.5, scale=0.5, size=x_test.shape)\nx_test_noisy = x_test + noise\n\n# adding noise may exceed normalized pixel values&gt;1.0 or &lt;0.0\n# clip pixel values &gt;1.0 to 1.0 and &lt;0.0 to 0.0\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)\n\n# network parameters\ninput_shape = (image_size, image_size, 1)\nbatch_size = 32\nkernel_size = 3\nlatent_dim = 16\n# encoder/decoder number of CNN layers and filters per layer\nlayer_filters = [32, 64]\n\n# build the autoencoder model\n# first build the encoder model\ninputs = Input(shape=input_shape, name='encoder_input')\nx = inputs\n\n# stack of Conv2D(32)-Conv2D(64)\nfor filters in layer_filters:\n    x = Conv2D(filters=filters,\n               kernel_size=kernel_size,\n               strides=2,\n               activation='relu',\n               padding='same')(x)\n\n# shape info needed to build decoder model so we don't do hand computation\n# the input to the decoder's first Conv2DTranspose will have this shape\n# shape is (7, 7, 64) which can be processed by the decoder back to (28, 28, 1)\nshape = K.int_shape(x)\n\n# generate the latent vector\nx = Flatten()(x)\nlatent = Dense(latent_dim, name='latent_vector')(x)\n\n# instantiate encoder model\nencoder = Model(inputs, latent, name='encoder')\nencoder.summary()\n\n# build the decoder model\nlatent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n# use the shape (7, 7, 64) that was earlier saved\nx = Dense(shape[1] * shape[2] * shape[3])(latent_inputs)\n# from vector to suitable shape for transposed conv\nx = Reshape((shape[1], shape[2], shape[3]))(x)\n\n# stack of Conv2DTranspose(64)-Conv2DTranspose(32)\nfor filters in layer_filters[::-1]:\n    x = Conv2DTranspose(filters=filters,\n                        kernel_size=kernel_size,\n                        strides=2,\n                        activation='relu',\n                        padding='same')(x)\n\n# reconstruct the denoised input\noutputs = Conv2DTranspose(filters=1,\n                          kernel_size=kernel_size,\n                          padding='same',\n                          activation='sigmoid',\n                          name='decoder_output')(x)\n\n# instantiate decoder model\ndecoder = Model(latent_inputs, outputs, name='decoder')\ndecoder.summary()\n\n# autoencoder = encoder + decoder\n# instantiate autoencoder model\nautoencoder = Model(inputs, decoder(encoder(inputs)), name='autoencoder')\nautoencoder.summary()\n\n# Mean Square Error (MSE) loss function, Adam optimizer\nautoencoder.compile(loss='mse', optimizer='adam')\n\n# train the autoencoder\nautoencoder.fit(x_train_noisy,\n                x_train,\n                validation_data=(x_test_noisy, x_test),\n                epochs=10,\n                batch_size=batch_size)\n\n# predict the autoencoder output from corrupted test images\nx_decoded = autoencoder.predict(x_test_noisy)\n\n# 3 sets of images with 9 MNIST digits\n# 1st rows - original images\n# 2nd rows - images corrupted by noise\n# 3rd rows - denoised images\nrows, cols = 3, 9\nnum = rows * cols\nimgs = np.concatenate([x_test[:num], x_test_noisy[:num], x_decoded[:num]])\nimgs = imgs.reshape((rows * 3, cols, image_size, image_size))\nimgs = np.vstack(np.split(imgs, rows, axis=1))\nimgs = imgs.reshape((rows * 3, -1, image_size, image_size))\nimgs = np.vstack([np.hstack(i) for i in imgs])\nimgs = (imgs * 255).astype(np.uint8)\nplt.figure()\nplt.axis('off')\nplt.title('Original images: top rows, '\n          'Corrupted Input: middle rows, '\n          'Denoised Input:  third rows')\nplt.imshow(imgs, interpolation='none', cmap='gray')\nImage.fromarray(imgs).save('corrupted_and_denoised.png')\nplt.show()\n\nModel: \"encoder\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n encoder_input (InputLayer)  [(None, 28, 28, 1)]       0         \n                                                                 \n conv2d_4 (Conv2D)           (None, 14, 14, 32)        320       \n                                                                 \n conv2d_5 (Conv2D)           (None, 7, 7, 64)          18496     \n                                                                 \n flatten_2 (Flatten)         (None, 3136)              0         \n                                                                 \n latent_vector (Dense)       (None, 16)                50192     \n                                                                 \n=================================================================\nTotal params: 69,008\nTrainable params: 69,008\nNon-trainable params: 0\n_________________________________________________________________\nModel: \"decoder\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n decoder_input (InputLayer)  [(None, 16)]              0         \n                                                                 \n dense_2 (Dense)             (None, 3136)              53312     \n                                                                 \n reshape_2 (Reshape)         (None, 7, 7, 64)          0         \n                                                                 \n conv2d_transpose_4 (Conv2DT  (None, 14, 14, 64)       36928     \n ranspose)                                                       \n                                                                 \n conv2d_transpose_5 (Conv2DT  (None, 28, 28, 32)       18464     \n ranspose)                                                       \n                                                                 \n decoder_output (Conv2DTrans  (None, 28, 28, 1)        289       \n pose)                                                           \n                                                                 \n=================================================================\nTotal params: 108,993\nTrainable params: 108,993\nNon-trainable params: 0\n_________________________________________________________________\nModel: \"autoencoder\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n encoder_input (InputLayer)  [(None, 28, 28, 1)]       0         \n                                                                 \n encoder (Functional)        (None, 16)                69008     \n                                                                 \n decoder (Functional)        (None, 28, 28, 1)         108993    \n                                                                 \n=================================================================\nTotal params: 178,001\nTrainable params: 178,001\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/10\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0367 - val_loss: 0.0205\nEpoch 2/10\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0193 - val_loss: 0.0180\nEpoch 3/10\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0176 - val_loss: 0.0172\nEpoch 4/10\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0166\nEpoch 5/10\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0163 - val_loss: 0.0163\nEpoch 6/10\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0160 - val_loss: 0.0161\nEpoch 7/10\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0157 - val_loss: 0.0160\nEpoch 8/10\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0154 - val_loss: 0.0160\nEpoch 9/10\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0153 - val_loss: 0.0157\nEpoch 10/10\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0151 - val_loss: 0.0156\n313/313 [==============================] - 1s 2ms/step"
  },
  {
    "objectID": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#자동-채색-오토인코더",
    "href": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#자동-채색-오토인코더",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "자동 채색 오토인코더",
    "text": "자동 채색 오토인코더\n\n해당 코딩은 너무 길어서 생략. 자세한 것은 여기 링크 참고 \\(\\to\\) 자동 채색 오토인코더\n입력: 회색도 사진, 출력: 해당하는 채색된 사진들로 오토인코더를 훈련"
  },
  {
    "objectID": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#요약",
    "href": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#요약",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "요약",
    "text": "요약\n\n노이즈 제거, 채색 등 구조적인 변환을 효율적으로 하기 위하여 데이터를 낮은 차원의 표현으로 압축하는 신경망"
  },
  {
    "objectID": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#생성기-개발을-위한-클래스",
    "href": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#생성기-개발을-위한-클래스",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "생성기 개발을 위한 클래스",
    "text": "생성기 개발을 위한 클래스\nclass Generator:\n    \n    def __init__(self):\n        self.initVariable = 1\n        \n    def lossFunction(self):\n        \n        return\n    \n    def buldModel(self):\n        \n        return\n    \n    def trainModel(self, inputX, inputY):\n        \n        return"
  },
  {
    "objectID": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#판별기-개발을-위한-클래스",
    "href": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#판별기-개발을-위한-클래스",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "판별기 개발을 위한 클래스",
    "text": "판별기 개발을 위한 클래스\n\nclass Discriminator:\n    \n    def __init__(self):\n        self.initVariable = 1\n        \n    def lossFunction(self):\n        \n        return\n    \n    def buildModel(self):\n        \n        return\n    \n    def trainModel(self,inputX,inputY):\n        \n        return"
  },
  {
    "objectID": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#손실-함수",
    "href": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#손실-함수",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "손실 함수",
    "text": "손실 함수\n\nclass Loss:\n    \n    def __init__(self):\n        self.initVariable = 1\n        \n    def lossBaseFunction1(self):\n        \n        return\n    \n    def lossBaseFunction2(self):\n        \n        return\n    \n    \n    def lossBaseFunction3(self):\n        \n        return\n- 적대적 훈련을 할 때 생성기에서 사용하는 손실함수\n\\[\\nabla \\theta_g \\sum_{i=1}^{m} log(1-D(G(z^{(i)}))) \\]\n- GAN에서 적용되는 표준 교차 엔트로피 구현\n\\[ \\nabla \\theta_d \\dfrac{1}{m} \\sum_{i=1}^{m} [logD(x^{(i)})+log(1-D(G(z^{(i)})))] \\]\n\n굿펠로우 논문에 나오는 함수\n\n\nnote: 교수님이 설명해주신 코드 보는게 더 나을듯 하다. 교수님 코드"
  },
  {
    "objectID": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#dcgan",
    "href": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#dcgan",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "DCGAN",
    "text": "DCGAN\n\n심층 CNN을 이용하여 초기 GAN를 성공적으로 구현"
  },
  {
    "objectID": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#조건부conditional-gan",
    "href": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#조건부conditional-gan",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "조건부(Conditional) GAN",
    "text": "조건부(Conditional) GAN\n\n원-핫 벡터를 제외하면 DCGAN과 유사\n생성자와 판별자의 출력에 조건을 부여하기 위해 원-핫 벡터 사용"
  },
  {
    "objectID": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#wasserstein-gan",
    "href": "posts/Synthetic/Advaced Deep Learning with TensorFlow 2 and Keras.html#wasserstein-gan",
    "title": "Advanced Deep Learning with TensorFlow 2 and Keras(-ing)",
    "section": "Wasserstein GAN",
    "text": "Wasserstein GAN\n\nGAN의 불안정성은 Jensen-Shannon (JS) 거리에 기초한 손실함수 때문이라고 주장\nGAN의 최적화에 더 알맞게 JS거리 함수를 대체하기에 적합한 것을 찾아야함\n\nref: https://lilianweng.github.io/posts/2017-08-20-gan/\n\n책 개념이 너무 어렵당.. 관련 수식에 대해 이해하고 싶은뎀 수식에 대한 내용이 자세하진 않음.. 일단 수식에 대한 내용이해 먼저 하고 추후에 다시 책 읽어보는 걸로~~"
  },
  {
    "objectID": "posts/GNN/An Introduction to Graph Neural Network(GNN) For Analysing Structured Data.html",
    "href": "posts/GNN/An Introduction to Graph Neural Network(GNN) For Analysing Structured Data.html",
    "title": "[GNN] An Introduction to Graph Neural Network(GNN) For Analysing Structured Data",
    "section": "",
    "text": "ref\nhttps://towardsdatascience.com/an-introduction-to-graph-neural-network-gnn-for-analysing-structured-data-afce79f4cfdc\n\n\nGNN\n- GNN개념\n\nRecurrent Graph Neural Network (순환 그래프 신경망)\nSpatial Convolutional Network (공간 컨볼루션 네트워크)\nSpectral Convolutional Network (스펙트럼 컨볼루션 네트워크)\n\n- GNN 분류\n\n노드 분류: 그래프의 모든 노드에 대한 노드 임베딩 예측\n링크 예측: 그래프에서 개체 간의 관계를 이해하고 두 개체 사이에 연결이 있는지 예측\n그래프 분류: 전체 그래프를 다양한 카테고리로 분류\n\n- GNN 사례\n\n자연어 처리(NLP): 단어나 문서의 내부 관계 활용하여 카테고리 예측\n컴퓨터 비전(Computer Vision)\n\n\n\nRecGNN\nLet (X,d) be a complete metric space.\nLet (T:X→X) be a contraction mapping.\nThen T has a unique fixed point (x∗) and for any x∈X the sequence T_n(x) for n→∞ converges to (x∗).\nThis means if I apply the mapping T on x for k times, x^k should be almost equal to x^(k-1)\n\n\n\n\\(l_n\\) 현재 노드의 특징\n\\(l_{co}\\) 노드의 에지\n\\(x_{ne}\\) 인접 노드의 상태\n\\(l_{ne}\\) 인접 노드의 특징(노드 레이블)\n\n\n\n\n이웃 노드의 정보를 기반으로 노드 상태 업데이트\n\n\n- 출력 함수\n\n\n\nSpatial Convolutional Network\n\nCNN과 유사(이미지에서 컨볼루션은 매개변수화된 크기와 학습이 가능한 가중치를 가진 필터로 지정된 중심 픽셀 주변의 인접 픽셀을 합산)\n공간 컨볼루션 네트워크는 인접 노드의 특징을 중심 노드에 합산하여 동일한 아이디어 채택\n\n\n\nSpectral Convolutional Network\n\n\n체비셰프 다항식 근사법\n\n\n\n2계층 신경망 구조\n\\(\\hat A\\): 그래프 인접 행렬 A의 사전 처리된 라플라스 행렬\n\n- 예시\n\n\n\n인접행렬의 대각선은 모든 노드에 대한self-loop를 하기 위해 1로 변경\n특정 집계를 수행할 때 모든 노든 자체의 특징 포함\n\n\n- 그래프 신경망\n\n메시지 전달 함수\n노드 업데이트 함수\n판독 함수가 있는 메시지 전달 신경망\n\n\n\nReferences\n\nF.Scarselli, M.Gori, “The graph neural network model,” IEEE Transactions on Neural Networks, 2009\nT. N. Kipf and M. Welling, “Semi-supervised classification with graph convolutional networks,” in Proc. of ICLR, 2017.\nZ. Wu, S. Pan, F. Chen, G. Long, C. Zhang, Philip S. Yu, “A Comprehensive Survey on Graph Neural Networks”, arXiv:1901.00596\nD. Xu, Y. Zhu, C. B. Choy, and L. Fei-Fei, “Scene graph generation by iterative message passing,” in Proc. of CVPR, vol. 2, 2017\nJ. Johnson, A. Gupta, and L. Fei-Fei, “Image generation from scene graphs,” in Proc. of CVPR, 2018\nX. Wang, Y. Ye, and A. Gupta, “Zero-shot recognition via semantic embeddings and knowledge graphs,” in CVPR 2018"
  },
  {
    "objectID": "posts/GNN/Neural Network.html",
    "href": "posts/GNN/Neural Network.html",
    "title": "[GNN] Neural Network",
    "section": "",
    "text": "https://python-course.eu/machine-learning/neural-networks-structure-weights-and-matrices.php"
  },
  {
    "objectID": "posts/GNN/Neural Network.html#introduction",
    "href": "posts/GNN/Neural Network.html#introduction",
    "title": "[GNN] Neural Network",
    "section": "Introduction",
    "text": "Introduction\n\n\nimport numpy as np\n\ninput_vector = np.array([2, 4, 11])\nprint(input_vector)\n\n[ 2  4 11]\n\n\n\nimport numpy as np\n\ninput_vector = np.array([2, 4, 11])\ninput_vector = np.array(input_vector, ndmin=2).T\nprint(\"The input vector:\\n\", input_vector)\n\nprint(\"The shape of this vector: \", input_vector.shape)\n\nThe input vector:\n [[ 2]\n [ 4]\n [11]]\nThe shape of this vector:  (3, 1)"
  },
  {
    "objectID": "posts/GNN/Neural Network.html#weights-and-matrices",
    "href": "posts/GNN/Neural Network.html#weights-and-matrices",
    "title": "[GNN] Neural Network",
    "section": "Weights and Matrices",
    "text": "Weights and Matrices"
  },
  {
    "objectID": "posts/GNN/Neural Network.html#initializing-the-weight-matrices",
    "href": "posts/GNN/Neural Network.html#initializing-the-weight-matrices",
    "title": "[GNN] Neural Network",
    "section": "Initializing the weight matrices",
    "text": "Initializing the weight matrices\n\nimport numpy as np\n\nnumber_of_samples = 1200\nlow = -1\nhigh = 0\ns = np.random.uniform(low, high, number_of_samples)\n\n# all values of s are within the half open interval [-1, 0) :\nprint(np.all(s &gt;= -1) and np.all(s &lt; 0))\n\nTrue\n\n\n\nimport matplotlib.pyplot as plt\nplt.hist(s)\nplt.show()\n\n\n\n\n\ns = np.random.binomial(100, 0.5, 1200)\nplt.hist(s)\nplt.show()\n\n\n\n\n\nfrom scipy.stats import truncnorm\n\ns = truncnorm(a=-2/3., b=2/3., scale=1, loc=0).rvs(size=1000)\n\nplt.hist(s)\nplt.show()\n\n\n\n\n\ndef truncated_normal(mean=0, sd=1, low=0, upp=10):\n    return truncnorm(\n        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n\nX = truncated_normal(mean=0, sd=0.4, low=-0.5, upp=0.5)\ns = X.rvs(10000)\n\nplt.hist(s)\nplt.show()\n\n\n\n\n\nX1 = truncated_normal(mean=2, sd=1, low=1, upp=10)\nX2 = truncated_normal(mean=5.5, sd=1, low=1, upp=10)\nX3 = truncated_normal(mean=8, sd=1, low=1, upp=10)\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(3, sharex=True)\nax[0].hist(X1.rvs(10000), density=True)\nax[1].hist(X2.rvs(10000), density=True)\nax[2].hist(X3.rvs(10000), density=True)\nplt.show()\n\n\n\n\n\nno_of_input_nodes = 3\nno_of_hidden_nodes = 4\nrad = 1 / np.sqrt(no_of_input_nodes)\n\nX = truncated_normal(mean=2, sd=1, low=-rad, upp=rad)\nwih = X.rvs((no_of_hidden_nodes, no_of_input_nodes))\nwih\n\narray([[ 0.38185974,  0.26766393,  0.3533069 ],\n       [-0.320298  ,  0.18587275, -0.33873619],\n       [ 0.20662638, -0.33617038,  0.37324201],\n       [-0.21246189,  0.21407227, -0.26193507]])\n\n\n\nno_of_hidden_nodes = 4\nno_of_output_nodes = 2\nrad = 1 / np.sqrt(no_of_hidden_nodes)  # this is the input in this layer!\n\nX = truncated_normal(mean=2, sd=1, low=-rad, upp=rad)\nwho = X.rvs((no_of_output_nodes, no_of_hidden_nodes))\nwho\n\narray([[ 0.32899603,  0.11932429,  0.19259498, -0.12701962],\n       [-0.09709214,  0.4111771 ,  0.43259814,  0.43646957]])"
  },
  {
    "objectID": "posts/GNN/Neural Network.html#a-neural-network-class",
    "href": "posts/GNN/Neural Network.html#a-neural-network-class",
    "title": "[GNN] Neural Network",
    "section": "A Neural Network Class",
    "text": "A Neural Network Class\n\nimport numpy as np\nfrom scipy.stats import truncnorm\n\ndef truncated_normal(mean=0, sd=1, low=0, upp=10):\n    return truncnorm(\n        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n\nclass NeuralNetwork:\n   \n    def __init__(self, \n                 no_of_in_nodes, \n                 no_of_out_nodes,  # corresponds to the number of classes\n                 no_of_hidden_nodes,\n                 learning_rate):\n        self.no_of_in_nodes = no_of_in_nodes\n        self.no_of_out_nodes = no_of_out_nodes \n        self.no_of_hidden_nodes = no_of_hidden_nodes\n        self.learning_rate = learning_rate  \n        self.create_weight_matrices()\n        \n    def create_weight_matrices(self):\n        rad = 1 / np.sqrt(self.no_of_in_nodes)\n        X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n        self.weights_in_hidden = X.rvs((self.no_of_hidden_nodes, \n                                       self.no_of_in_nodes))\n        rad = 1 / np.sqrt(self.no_of_hidden_nodes)\n        X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n        self.weights_hidden_out = X.rvs((self.no_of_out_nodes, \n                                        self.no_of_hidden_nodes))\n           \n    \n    def train(self):\n        pass\n    \n    def run(self):\n        pass\n\n\nsimple_network = NeuralNetwork(no_of_in_nodes = 3, \n                               no_of_out_nodes = 2, \n                               no_of_hidden_nodes = 4,\n                               learning_rate = 0.1)\nprint(simple_network.weights_in_hidden)\nprint(simple_network.weights_hidden_out)\n\n[[-0.19553012  0.23292966  0.0867214 ]\n [ 0.35698122 -0.04817757  0.55330426]\n [-0.21701452 -0.2177187   0.24132042]\n [ 0.24152886 -0.19581161 -0.15966314]]\n[[ 0.24162019  0.03084706 -0.06963859 -0.31704571]\n [-0.03889086  0.15380858 -0.19090609  0.46296621]]"
  },
  {
    "objectID": "posts/GNN/Neural Network.html#activation-functions-sigmoid-and-relu",
    "href": "posts/GNN/Neural Network.html#activation-functions-sigmoid-and-relu",
    "title": "[GNN] Neural Network",
    "section": "Activation Functions, Sigmoid and ReLU",
    "text": "Activation Functions, Sigmoid and ReLU\n\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef sigma(x):\n    return 1 / (1 + np.exp(-x))\n\nX = np.linspace(-5, 5, 100)\n\n\nplt.plot(X, sigma(X),'b')\nplt.xlabel('X Axis')\nplt.ylabel('Y Axis')\nplt.title('Sigmoid Function')\n\nplt.grid()\n\nplt.text(2.3, 0.84, r'$\\sigma(x)=\\frac{1}{1+e^{-x}}$', fontsize=16)\n\n\nplt.show()\n\n\n\n\n\nfrom scipy.special import expit\nprint(expit(3.4))\nprint(expit([3, 4, 1]))\nprint(expit(np.array([0.8, 2.3, 8])))\n\n0.9677045353015494\n[0.95257413 0.98201379 0.73105858]\n[0.68997448 0.90887704 0.99966465]\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef sigma(x):\n    return 1 / (1 + np.exp(-x))\n\nX = np.linspace(-5, 5, 100)\n\nplt.plot(X, sigma(X))\nplt.plot(X, sigma(X) * (1 - sigma(X)))\n\nplt.xlabel('X Axis')\nplt.ylabel('Y Axis')\nplt.title('Sigmoid Function')\n\nplt.grid()\n\nplt.text(2.3, 0.84, r'$\\sigma(x)=\\frac{1}{1+e^{-x}}$', fontsize=16)\nplt.text(0.3, 0.1, r'$\\sigma\\'(x) = \\sigma(x)(1 - \\sigma(x))$', fontsize=16)\n\n\nplt.show()\n\n\n\n\n\n@np.vectorize\ndef sigmoid(x):\n    return 1 / (1 + np.e ** -x)\n\n#sigmoid = np.vectorize(sigmoid)\nsigmoid([3, 4, 5])\n\narray([0.95257413, 0.98201379, 0.99330715])\n\n\n\n# alternative activation function\ndef ReLU(x):\n    return np.maximum(0.0, x)\n\n# derivation of relu\ndef ReLU_derivation(x):\n    if x &lt;= 0:\n        return 0\n    else:\n        return 1\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nX = np.linspace(-5, 6, 100)\nplt.plot(X, ReLU(X),'b')\nplt.xlabel('X Axis')\nplt.ylabel('Y Axis')\nplt.title('ReLU Function')\nplt.grid()\nplt.text(0.8, 0.4, r'$ReLU(x)=max(0, x)$', fontsize=14)\nplt.show()"
  },
  {
    "objectID": "posts/GNN/Neural Network.html#adding-a-run-method",
    "href": "posts/GNN/Neural Network.html#adding-a-run-method",
    "title": "[GNN] Neural Network",
    "section": "Adding a run Method",
    "text": "Adding a run Method\n\nfrom scipy.special import expit as activation_function\n\n\nimport numpy as np\nfrom scipy.special import expit as activation_function\nfrom scipy.stats import truncnorm\n\ndef truncated_normal(mean=0, sd=1, low=0, upp=10):\n    return truncnorm(\n        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n\n\nclass NeuralNetwork:\n           \n    def __init__(self, \n                 no_of_in_nodes, \n                 no_of_out_nodes, \n                 no_of_hidden_nodes,\n                 learning_rate):\n        self.no_of_in_nodes = no_of_in_nodes\n        self.no_of_out_nodes = no_of_out_nodes\n        self.no_of_hidden_nodes = no_of_hidden_nodes\n        self.learning_rate = learning_rate \n        self.create_weight_matrices()\n        \n    def create_weight_matrices(self):\n        \"\"\" A method to initialize the weight matrices of the neural network\"\"\"\n        rad = 1 / np.sqrt(self.no_of_in_nodes)\n        X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n        self.weights_in_hidden = X.rvs((self.no_of_hidden_nodes, \n                                       self.no_of_in_nodes))\n        rad = 1 / np.sqrt(self.no_of_hidden_nodes)\n        X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n        self.weights_hidden_out = X.rvs((self.no_of_out_nodes, \n                                        self.no_of_hidden_nodes))\n    \n    \n    def train(self, input_vector, target_vector):\n        pass\n            \n    \n    def run(self, input_vector):\n        \"\"\"\n        running the network with an input vector 'input_vector'. \n        'input_vector' can be tuple, list or ndarray\n        \"\"\"\n        # turning the input vector into a column vector\n        # turn one-dimensional into 2-dimensional column vector:\n        input_vector = np.array(input_vector, ndmin=2).T\n        input_hidden = activation_function(self.weights_in_hidden @ input_vector)\n        output_vector = activation_function(self.weights_hidden_out @ input_hidden)\n        return output_vector\n\n\nsimple_network = NeuralNetwork(no_of_in_nodes=2, \n                               no_of_out_nodes=2, \n                               no_of_hidden_nodes=4,\n                               learning_rate=0.6)\n\n\nsimple_network.run([(3, 4)])\n\narray([[0.42388305],\n       [0.65497086]])"
  },
  {
    "objectID": "posts/GNN/Laplacian.html",
    "href": "posts/GNN/Laplacian.html",
    "title": "[GNN] Laplacian matrix",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Laplacian_matrix\nhttps://process-mining.tistory.com/157\nhttps://ok-lab.tistory.com/169"
  },
  {
    "objectID": "posts/GNN/Laplacian.html#unnormalized-laplacian",
    "href": "posts/GNN/Laplacian.html#unnormalized-laplacian",
    "title": "[GNN] Laplacian matrix",
    "section": "unnormalized Laplacian",
    "text": "unnormalized Laplacian\n\\[L=D-A\\]\n\\[L^{sym}_{ij}=\\begin{cases} d(v_i), \\ i=j \\\\ -1, \\ {v_i,v_j} \\in E and i \\neq j \\\\ 0, \\ o.w \\end{cases}\\]\nD: degree matrix, A: ajacency matrix\n\n L: 미분,차분, 변동의 느낌..\n\n- ex\n\n- 특징\n\nsymmetric matrix (\\(L=L^T\\))\n모든 vector x(dimesion 노드의 크기)에 대해 다음과 같은 특성을 지닌다. \\(x^TLx=\\frac{1}{2}\\sum_{u \\in {\\cal V}}\\sum_{v \\in {\\cal V}} A[u,v](X[u]-X[v])^2=\\sum_{(u,v)\\in {\\cal \\epsilon}} (X[u] - X[v])^2\\)"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/경사하강법.html",
    "href": "posts/Study/Python Data Analysis/경사하강법.html",
    "title": "경사하강법",
    "section": "",
    "text": "파이썬 데이터 분석"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/경사하강법.html#패키지-설정",
    "href": "posts/Study/Python Data Analysis/경사하강법.html#패키지-설정",
    "title": "경사하강법",
    "section": "1. 패키지 설정",
    "text": "1. 패키지 설정\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/경사하강법.html#데이터-준비",
    "href": "posts/Study/Python Data Analysis/경사하강법.html#데이터-준비",
    "title": "경사하강법",
    "section": "2. 데이터 준비",
    "text": "2. 데이터 준비\n\nX_train = np.array([10,22,30,38,50])\ny_train = np.array([41,45,62,75,85])"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/경사하강법.html#탐색적-데이터-분석",
    "href": "posts/Study/Python Data Analysis/경사하강법.html#탐색적-데이터-분석",
    "title": "경사하강법",
    "section": "3. 탐색적 데이터 분석",
    "text": "3. 탐색적 데이터 분석\n\nplt.scatter(X_train, y_train, color='b')\nplt.show()\n\n\n\n\n\nnp.corrcoef(X_train, y_train)\n\narray([[1.        , 0.97319891],\n       [0.97319891, 1.        ]])"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/경사하강법.html#피처-스케일링",
    "href": "posts/Study/Python Data Analysis/경사하강법.html#피처-스케일링",
    "title": "경사하강법",
    "section": "4. 피처 스케일링",
    "text": "4. 피처 스케일링\n\nX_train = X_train.reshape(-1,1)   # NX1 매트릭스\nprint(X_train)\ny_train= y_train.reshape(-1,1)\nprint(y_train)\n\n[[10]\n [22]\n [30]\n [38]\n [50]]\n[[41]\n [45]\n [62]\n [75]\n [85]]\n\n\n- 정규화\n\nscalerX=StandardScaler()\nscalerX.fit(X_train)\nX_train=scalerX.transform(X_train)\nprint(X_train)\n\n[[-1.46805055e+00]\n [-5.87220220e-01]\n [-8.88178420e-17]\n [ 5.87220220e-01]\n [ 1.46805055e+00]]\n\n\n\nscalerY=StandardScaler()\nscalerY.fit(y_train)\ny_train=scalerY.transform(y_train)\nprint(y_train)\n\n[[-1.21929784]\n [-0.98254098]\n [ 0.02367569]\n [ 0.79313549]\n [ 1.38502764]]"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/경사하강법.html#모형화-및-학습",
    "href": "posts/Study/Python Data Analysis/경사하강법.html#모형화-및-학습",
    "title": "경사하강법",
    "section": "5. 모형화 및 학습",
    "text": "5. 모형화 및 학습\n\nmodel=SGDRegressor(verbose=1)\n\n\nmodel.fit(X_train, y_train)\n\n-- Epoch 1\nNorm: 0.04, NNZs: 1, Bias: -0.001467, T: 5, Avg. loss: 0.489615\nTotal training time: 0.00 seconds.\n-- Epoch 2\nNorm: 0.07, NNZs: 1, Bias: -0.002592, T: 10, Avg. loss: 0.454060\nTotal training time: 0.00 seconds.\n-- Epoch 3\nNorm: 0.09, NNZs: 1, Bias: -0.002601, T: 15, Avg. loss: 0.429691\nTotal training time: 0.00 seconds.\n-- Epoch 4\nNorm: 0.11, NNZs: 1, Bias: -0.002910, T: 20, Avg. loss: 0.408928\nTotal training time: 0.00 seconds.\n-- Epoch 5\nNorm: 0.13, NNZs: 1, Bias: -0.002887, T: 25, Avg. loss: 0.390899\nTotal training time: 0.00 seconds.\n-- Epoch 6\nNorm: 0.15, NNZs: 1, Bias: -0.003017, T: 30, Avg. loss: 0.374571\nTotal training time: 0.00 seconds.\n-- Epoch 7\nNorm: 0.17, NNZs: 1, Bias: -0.002987, T: 35, Avg. loss: 0.359796\nTotal training time: 0.00 seconds.\n-- Epoch 8\nNorm: 0.19, NNZs: 1, Bias: -0.003047, T: 40, Avg. loss: 0.346121\nTotal training time: 0.00 seconds.\n-- Epoch 9\nNorm: 0.20, NNZs: 1, Bias: -0.003015, T: 45, Avg. loss: 0.333511\nTotal training time: 0.00 seconds.\n-- Epoch 10\nNorm: 0.22, NNZs: 1, Bias: -0.003040, T: 50, Avg. loss: 0.321704\nTotal training time: 0.00 seconds.\n-- Epoch 11\nNorm: 0.23, NNZs: 1, Bias: -0.003007, T: 55, Avg. loss: 0.310696\nTotal training time: 0.00 seconds.\n-- Epoch 12\nNorm: 0.24, NNZs: 1, Bias: -0.003012, T: 60, Avg. loss: 0.300312\nTotal training time: 0.00 seconds.\n-- Epoch 13\nNorm: 0.26, NNZs: 1, Bias: -0.002979, T: 65, Avg. loss: 0.290560\nTotal training time: 0.00 seconds.\n-- Epoch 14\nNorm: 0.27, NNZs: 1, Bias: -0.002972, T: 70, Avg. loss: 0.281311\nTotal training time: 0.00 seconds.\n-- Epoch 15\nNorm: 0.28, NNZs: 1, Bias: -0.002940, T: 75, Avg. loss: 0.272580\nTotal training time: 0.00 seconds.\n-- Epoch 16\nNorm: 0.29, NNZs: 1, Bias: -0.002925, T: 80, Avg. loss: 0.264268\nTotal training time: 0.00 seconds.\n-- Epoch 17\nNorm: 0.30, NNZs: 1, Bias: -0.002893, T: 85, Avg. loss: 0.256388\nTotal training time: 0.00 seconds.\n-- Epoch 18\nNorm: 0.31, NNZs: 1, Bias: -0.002873, T: 90, Avg. loss: 0.248864\nTotal training time: 0.00 seconds.\n-- Epoch 19\nNorm: 0.33, NNZs: 1, Bias: -0.002843, T: 95, Avg. loss: 0.241709\nTotal training time: 0.00 seconds.\n-- Epoch 20\nNorm: 0.34, NNZs: 1, Bias: -0.002819, T: 100, Avg. loss: 0.234860\nTotal training time: 0.00 seconds.\n-- Epoch 21\nNorm: 0.35, NNZs: 1, Bias: -0.002790, T: 105, Avg. loss: 0.228330\nTotal training time: 0.00 seconds.\n-- Epoch 22\nNorm: 0.36, NNZs: 1, Bias: -0.002764, T: 110, Avg. loss: 0.222066\nTotal training time: 0.00 seconds.\n-- Epoch 23\nNorm: 0.36, NNZs: 1, Bias: -0.002736, T: 115, Avg. loss: 0.216080\nTotal training time: 0.00 seconds.\n-- Epoch 24\nNorm: 0.37, NNZs: 1, Bias: -0.002709, T: 120, Avg. loss: 0.210329\nTotal training time: 0.00 seconds.\n-- Epoch 25\nNorm: 0.38, NNZs: 1, Bias: -0.002681, T: 125, Avg. loss: 0.204824\nTotal training time: 0.00 seconds.\n-- Epoch 26\nNorm: 0.39, NNZs: 1, Bias: -0.002653, T: 130, Avg. loss: 0.199526\nTotal training time: 0.00 seconds.\n-- Epoch 27\nNorm: 0.40, NNZs: 1, Bias: -0.002626, T: 135, Avg. loss: 0.194447\nTotal training time: 0.00 seconds.\n-- Epoch 28\nNorm: 0.41, NNZs: 1, Bias: -0.002598, T: 140, Avg. loss: 0.189553\nTotal training time: 0.00 seconds.\n-- Epoch 29\nNorm: 0.42, NNZs: 1, Bias: -0.002572, T: 145, Avg. loss: 0.184854\nTotal training time: 0.00 seconds.\n-- Epoch 30\nNorm: 0.42, NNZs: 1, Bias: -0.002543, T: 150, Avg. loss: 0.180321\nTotal training time: 0.00 seconds.\n-- Epoch 31\nNorm: 0.43, NNZs: 1, Bias: -0.002518, T: 155, Avg. loss: 0.175964\nTotal training time: 0.00 seconds.\n-- Epoch 32\nNorm: 0.44, NNZs: 1, Bias: -0.002489, T: 160, Avg. loss: 0.171756\nTotal training time: 0.00 seconds.\n-- Epoch 33\nNorm: 0.45, NNZs: 1, Bias: -0.002465, T: 165, Avg. loss: 0.167707\nTotal training time: 0.00 seconds.\n-- Epoch 34\nNorm: 0.45, NNZs: 1, Bias: -0.002436, T: 170, Avg. loss: 0.163793\nTotal training time: 0.00 seconds.\n-- Epoch 35\nNorm: 0.46, NNZs: 1, Bias: -0.002413, T: 175, Avg. loss: 0.160023\nTotal training time: 0.00 seconds.\n-- Epoch 36\nNorm: 0.47, NNZs: 1, Bias: -0.002385, T: 180, Avg. loss: 0.156377\nTotal training time: 0.00 seconds.\n-- Epoch 37\nNorm: 0.48, NNZs: 1, Bias: -0.002361, T: 185, Avg. loss: 0.152861\nTotal training time: 0.00 seconds.\n-- Epoch 38\nNorm: 0.48, NNZs: 1, Bias: -0.002334, T: 190, Avg. loss: 0.149458\nTotal training time: 0.00 seconds.\n-- Epoch 39\nNorm: 0.49, NNZs: 1, Bias: -0.002311, T: 195, Avg. loss: 0.146173\nTotal training time: 0.00 seconds.\n-- Epoch 40\nNorm: 0.50, NNZs: 1, Bias: -0.002284, T: 200, Avg. loss: 0.142992\nTotal training time: 0.00 seconds.\n-- Epoch 41\nNorm: 0.50, NNZs: 1, Bias: -0.002262, T: 205, Avg. loss: 0.139920\nTotal training time: 0.00 seconds.\n-- Epoch 42\nNorm: 0.51, NNZs: 1, Bias: -0.002236, T: 210, Avg. loss: 0.136943\nTotal training time: 0.00 seconds.\n-- Epoch 43\nNorm: 0.51, NNZs: 1, Bias: -0.002214, T: 215, Avg. loss: 0.134066\nTotal training time: 0.00 seconds.\n-- Epoch 44\nNorm: 0.52, NNZs: 1, Bias: -0.002188, T: 220, Avg. loss: 0.131275\nTotal training time: 0.00 seconds.\n-- Epoch 45\nNorm: 0.53, NNZs: 1, Bias: -0.002168, T: 225, Avg. loss: 0.128577\nTotal training time: 0.00 seconds.\n-- Epoch 46\nNorm: 0.53, NNZs: 1, Bias: -0.002142, T: 230, Avg. loss: 0.125959\nTotal training time: 0.00 seconds.\n-- Epoch 47\nNorm: 0.54, NNZs: 1, Bias: -0.002122, T: 235, Avg. loss: 0.123425\nTotal training time: 0.00 seconds.\n-- Epoch 48\nNorm: 0.54, NNZs: 1, Bias: -0.002097, T: 240, Avg. loss: 0.120966\nTotal training time: 0.00 seconds.\n-- Epoch 49\nNorm: 0.55, NNZs: 1, Bias: -0.002077, T: 245, Avg. loss: 0.118586\nTotal training time: 0.00 seconds.\n-- Epoch 50\nNorm: 0.55, NNZs: 1, Bias: -0.002053, T: 250, Avg. loss: 0.116273\nTotal training time: 0.00 seconds.\n-- Epoch 51\nNorm: 0.56, NNZs: 1, Bias: -0.002034, T: 255, Avg. loss: 0.114034\nTotal training time: 0.00 seconds.\n-- Epoch 52\nNorm: 0.56, NNZs: 1, Bias: -0.002010, T: 260, Avg. loss: 0.111858\nTotal training time: 0.00 seconds.\n-- Epoch 53\nNorm: 0.57, NNZs: 1, Bias: -0.001992, T: 265, Avg. loss: 0.109749\nTotal training time: 0.00 seconds.\n-- Epoch 54\nNorm: 0.57, NNZs: 1, Bias: -0.001968, T: 270, Avg. loss: 0.107699\nTotal training time: 0.00 seconds.\n-- Epoch 55\nNorm: 0.58, NNZs: 1, Bias: -0.001950, T: 275, Avg. loss: 0.105712\nTotal training time: 0.00 seconds.\n-- Epoch 56\nNorm: 0.58, NNZs: 1, Bias: -0.001927, T: 280, Avg. loss: 0.103779\nTotal training time: 0.00 seconds.\n-- Epoch 57\nNorm: 0.59, NNZs: 1, Bias: -0.001910, T: 285, Avg. loss: 0.101905\nTotal training time: 0.00 seconds.\n-- Epoch 58\nNorm: 0.59, NNZs: 1, Bias: -0.001887, T: 290, Avg. loss: 0.100081\nTotal training time: 0.00 seconds.\n-- Epoch 59\nNorm: 0.60, NNZs: 1, Bias: -0.001870, T: 295, Avg. loss: 0.098312\nTotal training time: 0.00 seconds.\n-- Epoch 60\nNorm: 0.60, NNZs: 1, Bias: -0.001849, T: 300, Avg. loss: 0.096591\nTotal training time: 0.00 seconds.\n-- Epoch 61\nNorm: 0.61, NNZs: 1, Bias: -0.001832, T: 305, Avg. loss: 0.094920\nTotal training time: 0.00 seconds.\n-- Epoch 62\nNorm: 0.61, NNZs: 1, Bias: -0.001811, T: 310, Avg. loss: 0.093293\nTotal training time: 0.00 seconds.\n-- Epoch 63\nNorm: 0.62, NNZs: 1, Bias: -0.001795, T: 315, Avg. loss: 0.091713\nTotal training time: 0.00 seconds.\n-- Epoch 64\nNorm: 0.62, NNZs: 1, Bias: -0.001774, T: 320, Avg. loss: 0.090175\nTotal training time: 0.00 seconds.\n-- Epoch 65\nNorm: 0.62, NNZs: 1, Bias: -0.001758, T: 325, Avg. loss: 0.088681\nTotal training time: 0.00 seconds.\n-- Epoch 66\nNorm: 0.63, NNZs: 1, Bias: -0.001738, T: 330, Avg. loss: 0.087226\nTotal training time: 0.00 seconds.\n-- Epoch 67\nNorm: 0.63, NNZs: 1, Bias: -0.001723, T: 335, Avg. loss: 0.085812\nTotal training time: 0.00 seconds.\n-- Epoch 68\nNorm: 0.64, NNZs: 1, Bias: -0.001703, T: 340, Avg. loss: 0.084435\nTotal training time: 0.00 seconds.\n-- Epoch 69\nNorm: 0.64, NNZs: 1, Bias: -0.001688, T: 345, Avg. loss: 0.083096\nTotal training time: 0.00 seconds.\n-- Epoch 70\nNorm: 0.64, NNZs: 1, Bias: -0.001669, T: 350, Avg. loss: 0.081791\nTotal training time: 0.00 seconds.\n-- Epoch 71\nNorm: 0.65, NNZs: 1, Bias: -0.001654, T: 355, Avg. loss: 0.080522\nTotal training time: 0.00 seconds.\n-- Epoch 72\nNorm: 0.65, NNZs: 1, Bias: -0.001635, T: 360, Avg. loss: 0.079286\nTotal training time: 0.00 seconds.\n-- Epoch 73\nNorm: 0.65, NNZs: 1, Bias: -0.001621, T: 365, Avg. loss: 0.078083\nTotal training time: 0.00 seconds.\n-- Epoch 74\nNorm: 0.66, NNZs: 1, Bias: -0.001603, T: 370, Avg. loss: 0.076910\nTotal training time: 0.00 seconds.\n-- Epoch 75\nNorm: 0.66, NNZs: 1, Bias: -0.001589, T: 375, Avg. loss: 0.075769\nTotal training time: 0.00 seconds.\n-- Epoch 76\nNorm: 0.67, NNZs: 1, Bias: -0.001571, T: 380, Avg. loss: 0.074657\nTotal training time: 0.00 seconds.\n-- Epoch 77\nNorm: 0.67, NNZs: 1, Bias: -0.001558, T: 385, Avg. loss: 0.073574\nTotal training time: 0.00 seconds.\n-- Epoch 78\nNorm: 0.67, NNZs: 1, Bias: -0.001540, T: 390, Avg. loss: 0.072518\nTotal training time: 0.00 seconds.\n-- Epoch 79\nNorm: 0.68, NNZs: 1, Bias: -0.001527, T: 395, Avg. loss: 0.071490\nTotal training time: 0.00 seconds.\n-- Epoch 80\nNorm: 0.68, NNZs: 1, Bias: -0.001510, T: 400, Avg. loss: 0.070487\nTotal training time: 0.00 seconds.\n-- Epoch 81\nNorm: 0.68, NNZs: 1, Bias: -0.001497, T: 405, Avg. loss: 0.069511\nTotal training time: 0.00 seconds.\n-- Epoch 82\nNorm: 0.69, NNZs: 1, Bias: -0.001480, T: 410, Avg. loss: 0.068558\nTotal training time: 0.00 seconds.\n-- Epoch 83\nNorm: 0.69, NNZs: 1, Bias: -0.001468, T: 415, Avg. loss: 0.067630\nTotal training time: 0.00 seconds.\n-- Epoch 84\nNorm: 0.69, NNZs: 1, Bias: -0.001452, T: 420, Avg. loss: 0.066725\nTotal training time: 0.00 seconds.\n-- Epoch 85\nNorm: 0.69, NNZs: 1, Bias: -0.001440, T: 425, Avg. loss: 0.065843\nTotal training time: 0.00 seconds.\nConvergence after 85 epochs took 0.00 seconds\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nSGDRegressor(verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SGDRegressorSGDRegressor(verbose=1)"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/경사하강법.html#예측",
    "href": "posts/Study/Python Data Analysis/경사하강법.html#예측",
    "title": "경사하강법",
    "section": "6.예측",
    "text": "6.예측\n\nX_test = np.array([45]).reshape(-1,1)\nX_test=scalerX.transform(X_test)\nX_test\n\narray([[45.]])\n\n\n\ny_pred=model.predict(X_test)\nprint(y_pred)\n\n[31.27270054]\n\n\ny_pred_inverse=scalerY.inverse_transform(y_pred)\nprint(y_pred_inverse)"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/경사하강법.html#패키지-설정-1",
    "href": "posts/Study/Python Data Analysis/경사하강법.html#패키지-설정-1",
    "title": "경사하강법",
    "section": "1. 패키지 설정",
    "text": "1. 패키지 설정\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/경사하강법.html#데이터-준비-1",
    "href": "posts/Study/Python Data Analysis/경사하강법.html#데이터-준비-1",
    "title": "경사하강법",
    "section": "2.데이터 준비",
    "text": "2.데이터 준비\n\ndiabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n\n\nprint(np.shape(diabetes_X))\nprint(np.shape(diabetes_y))\n\n(442, 10)\n(442,)\n\n\n\nprint(diabetes_X)\n\n[[ 0.03807591  0.05068012  0.06169621 ... -0.00259226  0.01990749\n  -0.01764613]\n [-0.00188202 -0.04464164 -0.05147406 ... -0.03949338 -0.06833155\n  -0.09220405]\n [ 0.08529891  0.05068012  0.04445121 ... -0.00259226  0.00286131\n  -0.02593034]\n ...\n [ 0.04170844  0.05068012 -0.01590626 ... -0.01107952 -0.04688253\n   0.01549073]\n [-0.04547248 -0.04464164  0.03906215 ...  0.02655962  0.04452873\n  -0.02593034]\n [-0.04547248 -0.04464164 -0.0730303  ... -0.03949338 -0.00422151\n   0.00306441]]\n\n\n\nprint(diabetes_y)\n\n[151.  75. 141. 206. 135.  97. 138.  63. 110. 310. 101.  69. 179. 185.\n 118. 171. 166. 144.  97. 168.  68.  49.  68. 245. 184. 202. 137.  85.\n 131. 283. 129.  59. 341.  87.  65. 102. 265. 276. 252.  90. 100.  55.\n  61.  92. 259.  53. 190. 142.  75. 142. 155. 225.  59. 104. 182. 128.\n  52.  37. 170. 170.  61. 144.  52. 128.  71. 163. 150.  97. 160. 178.\n  48. 270. 202. 111.  85.  42. 170. 200. 252. 113. 143.  51.  52. 210.\n  65. 141.  55. 134.  42. 111.  98. 164.  48.  96.  90. 162. 150. 279.\n  92.  83. 128. 102. 302. 198.  95.  53. 134. 144. 232.  81. 104.  59.\n 246. 297. 258. 229. 275. 281. 179. 200. 200. 173. 180.  84. 121. 161.\n  99. 109. 115. 268. 274. 158. 107.  83. 103. 272.  85. 280. 336. 281.\n 118. 317. 235.  60. 174. 259. 178. 128.  96. 126. 288.  88. 292.  71.\n 197. 186.  25.  84.  96. 195.  53. 217. 172. 131. 214.  59.  70. 220.\n 268. 152.  47.  74. 295. 101. 151. 127. 237. 225.  81. 151. 107.  64.\n 138. 185. 265. 101. 137. 143. 141.  79. 292. 178.  91. 116.  86. 122.\n  72. 129. 142.  90. 158.  39. 196. 222. 277.  99. 196. 202. 155.  77.\n 191.  70.  73.  49.  65. 263. 248. 296. 214. 185.  78.  93. 252. 150.\n  77. 208.  77. 108. 160.  53. 220. 154. 259.  90. 246. 124.  67.  72.\n 257. 262. 275. 177.  71.  47. 187. 125.  78.  51. 258. 215. 303. 243.\n  91. 150. 310. 153. 346.  63.  89.  50.  39. 103. 308. 116. 145.  74.\n  45. 115. 264.  87. 202. 127. 182. 241.  66.  94. 283.  64. 102. 200.\n 265.  94. 230. 181. 156. 233.  60. 219.  80.  68. 332. 248.  84. 200.\n  55.  85.  89.  31. 129.  83. 275.  65. 198. 236. 253. 124.  44. 172.\n 114. 142. 109. 180. 144. 163. 147.  97. 220. 190. 109. 191. 122. 230.\n 242. 248. 249. 192. 131. 237.  78. 135. 244. 199. 270. 164.  72.  96.\n 306.  91. 214.  95. 216. 263. 178. 113. 200. 139. 139.  88. 148.  88.\n 243.  71.  77. 109. 272.  60.  54. 221.  90. 311. 281. 182. 321.  58.\n 262. 206. 233. 242. 123. 167.  63. 197.  71. 168. 140. 217. 121. 235.\n 245.  40.  52. 104. 132.  88.  69. 219.  72. 201. 110.  51. 277.  63.\n 118.  69. 273. 258.  43. 198. 242. 232. 175.  93. 168. 275. 293. 281.\n  72. 140. 189. 181. 209. 136. 261. 113. 131. 174. 257.  55.  84.  42.\n 146. 212. 233.  91. 111. 152. 120.  67. 310.  94. 183.  66. 173.  72.\n  49.  64.  48. 178. 104. 132. 220.  57.]\n\n\n- 10열(s6-혈당수치) 할당\n\nX_data= diabetes_X[:,9]\nprint(X_data)\n\n[-0.01764613 -0.09220405 -0.02593034 -0.00936191 -0.04664087 -0.09634616\n -0.03835666  0.00306441  0.01134862 -0.01350402 -0.03421455 -0.05906719\n -0.04249877 -0.01350402 -0.07563562 -0.04249877  0.02791705 -0.0010777\n -0.01764613 -0.05492509  0.01549073 -0.01764613 -0.01350402  0.13561183\n -0.05492509 -0.03421455 -0.0052198   0.04034337 -0.05492509  0.05276969\n -0.00936191 -0.04249877  0.02791705  0.00306441 -0.06735141  0.01963284\n -0.02593034 -0.0052198   0.02377494  0.00306441  0.09419076 -0.00936191\n -0.03421455  0.07348023 -0.01764613 -0.05078298 -0.08806194 -0.08391984\n -0.02178823 -0.01350402  0.00720652 -0.02178823 -0.01350402  0.04862759\n  0.00720652  0.00720652 -0.01764613 -0.06735141 -0.05078298  0.01963284\n -0.03421455  0.01963284 -0.02593034 -0.07149352  0.01134862  0.01549073\n  0.01963284  0.00306441  0.00306441  0.01963284  0.04034337  0.06105391\n -0.01764613 -0.00936191  0.07348023  0.02791705  0.01549073 -0.0010777\n -0.08391984 -0.02593034 -0.0052198  -0.0010777  -0.07977773 -0.04249877\n -0.12948301 -0.07149352 -0.03421455 -0.03835666  0.06933812  0.01963284\n  0.01134862  0.01963284  0.01134862 -0.05492509 -0.09220405 -0.0052198\n  0.03205916  0.07762233 -0.06735141 -0.05492509 -0.0010777   0.03620126\n -0.0010777  -0.03007245 -0.0632093   0.01963284 -0.08391984 -0.0052198\n  0.04034337  0.10661708  0.01549073 -0.04664087  0.00306441  0.09004865\n  0.02377494  0.00306441  0.02791705  0.13561183  0.0569118   0.02791705\n -0.02178823  0.01134862  0.04862759  0.07348023 -0.01350402  0.00720652\n -0.05492509 -0.09634616 -0.07149352  0.1190434   0.07348023 -0.07977773\n  0.02377494 -0.03007245 -0.04664087  0.0569118  -0.02178823 -0.01350402\n  0.07348023 -0.0052198   0.01549073  0.13146972 -0.0010777  -0.04249877\n  0.00720652 -0.05078298  0.00720652  0.04034337 -0.0010777  -0.03835666\n -0.00936191 -0.01350402  0.07348023  0.01134862  0.00306441  0.04862759\n -0.03421455  0.02791705 -0.05078298 -0.01764613 -0.04664087  0.08590655\n -0.0010777   0.06105391 -0.01350402 -0.01350402 -0.10463037  0.10661708\n  0.12732762  0.01963284 -0.01764613 -0.04664087  0.08590655 -0.05492509\n  0.00720652  0.00306441  0.01963284  0.03205916 -0.03007245 -0.00936191\n -0.0010777  -0.0632093  -0.00936191  0.0569118   0.03205916 -0.01764613\n -0.03835666 -0.10463037 -0.00936191 -0.02593034 -0.0052198  -0.00936191\n -0.05906719  0.01963284  0.02377494  0.02377494  0.01134862  0.03620126\n -0.05906719  0.00720652  0.04448548  0.02791705 -0.03007245  0.02791705\n  0.07762233 -0.02593034  0.00306441  0.03620126  0.1190434  -0.01350402\n -0.05906719 -0.02178823  0.01963284  0.00720652 -0.05492509  0.10661708\n  0.04862759  0.04448548 -0.01350402 -0.03007245 -0.06735141 -0.03835666\n  0.01963284 -0.05078298 -0.00936191  0.00306441 -0.05492509  0.00720652\n -0.05078298  0.01134862  0.06933812  0.01134862  0.03205916 -0.03835666\n  0.0569118   0.04862759  0.01134862 -0.0010777  -0.02178823  0.04862759\n  0.01963284  0.0569118  -0.07149352 -0.04249877 -0.0052198  -0.12948301\n -0.05906719 -0.09220405  0.04034337  0.00720652  0.03620126  0.04034337\n -0.03007245  0.06933812  0.09004865  0.00720652  0.01134862 -0.05078298\n  0.0569118   0.00720652 -0.01764613  0.01549073  0.00306441 -0.01764613\n -0.02178823  0.04034337  0.00306441  0.02791705  0.07348023 -0.03835666\n  0.01549073  0.00720652 -0.09220405  0.03620126  0.01549073  0.06933812\n -0.02178823  0.01549073  0.03205916 -0.02593034 -0.01764613  0.00306441\n  0.02791705 -0.03007245 -0.05492509 -0.02178823 -0.0052198  -0.0052198\n -0.01764613 -0.03421455  0.04448548 -0.05906719 -0.03835666 -0.0010777\n -0.03421455  0.00306441 -0.05906719 -0.05492509 -0.01350402 -0.0010777\n  0.04448548  0.03620126  0.02791705 -0.02178823 -0.01350402 -0.06735141\n -0.01350402  0.08176444 -0.07977773  0.08176444  0.03205916  0.02377494\n -0.03007245 -0.0010777  -0.03835666 -0.0010777   0.04034337  0.06933812\n  0.03205916  0.06105391  0.00306441  0.06105391  0.08176444  0.0569118\n  0.01963284  0.04862759  0.02791705 -0.0052198   0.01963284  0.04448548\n  0.01549073 -0.0052198   0.03620126  0.03205916 -0.05492509 -0.05906719\n  0.05276969  0.00306441 -0.05906719  0.01549073 -0.03007245 -0.01764613\n  0.02377494  0.00720652 -0.03007245 -0.0010777   0.01549073 -0.03007245\n -0.0010777  -0.07149352  0.13561183 -0.04249877 -0.03835666  0.00306441\n  0.06105391  0.01134862 -0.07563562 -0.05906719 -0.04664087  0.06105391\n  0.04034337  0.02377494  0.04034337  0.01134862  0.09833287  0.09833287\n  0.04862759  0.03205916  0.08176444  0.02791705 -0.07563562 -0.01764613\n -0.02178823 -0.0010777  -0.07977773  0.01963284 -0.0010777  -0.0632093\n -0.03835666  0.02377494 -0.02593034 -0.05492509  0.03620126  0.00720652\n -0.03835666 -0.01350402  0.00306441 -0.07563562 -0.03835666 -0.08806194\n  0.07348023 -0.05078298  0.06519601 -0.02178823  0.00720652 -0.05492509\n -0.03421455  0.02377494 -0.06735141  0.01963284 -0.00936191  0.04034337\n  0.04448548  0.04034337 -0.00936191 -0.03007245 -0.13776723  0.01963284\n  0.04034337 -0.0052198  -0.04249877 -0.02593034  0.08590655  0.01963284\n  0.00720652  0.00306441  0.03620126  0.00306441  0.00720652 -0.0052198\n  0.06105391  0.01549073 -0.00936191 -0.04664087  0.02377494 -0.05078298\n  0.08590655  0.03205916  0.13146972 -0.03835666 -0.01764613 -0.0010777\n  0.10661708 -0.00936191  0.03205916 -0.03835666 -0.04664087  0.00720652\n  0.04448548  0.01549073 -0.02593034  0.00306441]"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/경사하강법.html#탐색적-데이터-분석-1",
    "href": "posts/Study/Python Data Analysis/경사하강법.html#탐색적-데이터-분석-1",
    "title": "경사하강법",
    "section": "3.탐색적 데이터 분석",
    "text": "3.탐색적 데이터 분석\n\n#입력데이터의 박스플롯\ng1=plt.subplot(1,2,1)\ng1.boxplot(X_data, labels=['s6'])\nplt.title('Diabetes')\nplt.xlabel('Features')\n\ng2=plt.subplot(1,2,2)\ng2.boxplot(diabetes_y, labels=['disease progression'])\nplt.title('Diabetes')\nplt.xlabel('Target')\n\nplt.show()\n\n\n\n\n\nplt.scatter(X_data, diabetes_y, color='b')\nplt.title('Diabetes')\nplt.xlabel('s6')\nplt.ylabel('Disease progression')\nplt.show()\n\n\n\n\n\nnp.corrcoef(X_data,diabetes_y)\n\narray([[1.        , 0.38248348],\n       [0.38248348, 1.        ]])"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/경사하강법.html#데이터-분리",
    "href": "posts/Study/Python Data Analysis/경사하강법.html#데이터-분리",
    "title": "경사하강법",
    "section": "4.데이터 분리",
    "text": "4.데이터 분리\n\nX_train, X_test, y_train, y_test = train_test_split(X_data,diabetes_y,\n                                                    test_size=0.5, random_state=1234)"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/경사하강법.html#피처-스케일링-1",
    "href": "posts/Study/Python Data Analysis/경사하강법.html#피처-스케일링-1",
    "title": "경사하강법",
    "section": "5.피처 스케일링",
    "text": "5.피처 스케일링\n\n# 학습 데이터 정규화\n# 2차원 행렬 변환\nX_train = X_train.reshape(-1,1)\ny_train = y_train.reshape(-1,1)\n\n# 입력 데이터 정규화\nscalerX=MinMaxScaler()\nscalerX.fit(X_train)\nX_train_scaled = scalerX.transform(X_train)\n\n# 목표 데이터 정규화\nscalerY=MinMaxScaler()\nscalerY.fit(y_train)\ny_train_scaled = scalerY.transform(y_train)\n\n\n# 테스트 데이터 정규화\nX_test = X_test.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\n\n# 학습용 데이터의 정규화 스케일에 맞추어 입력과 목표 데이터를 정규화\nX_test_scaled=scalerX.transform(X_test)\ny_test_scaled=scalerY.transform(y_test)"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/경사하강법.html#모형화-및-학습-1",
    "href": "posts/Study/Python Data Analysis/경사하강법.html#모형화-및-학습-1",
    "title": "경사하강법",
    "section": "6.모형화 및 학습",
    "text": "6.모형화 및 학습\n\nmodel = SGDRegressor(verbose=1)\n\n\nmodel.fit(X_train_scaled, y_train_scaled)\n\n-- Epoch 1\nNorm: 0.11, NNZs: 1, Bias: 0.188126, T: 221, Avg. loss: 0.058981\nTotal training time: 0.00 seconds.\n-- Epoch 2\nNorm: 0.14, NNZs: 1, Bias: 0.242482, T: 442, Avg. loss: 0.035508\nTotal training time: 0.00 seconds.\n-- Epoch 3\nNorm: 0.16, NNZs: 1, Bias: 0.268429, T: 663, Avg. loss: 0.030988\nTotal training time: 0.00 seconds.\n-- Epoch 4\nNorm: 0.17, NNZs: 1, Bias: 0.280449, T: 884, Avg. loss: 0.029699\nTotal training time: 0.00 seconds.\n-- Epoch 5\nNorm: 0.18, NNZs: 1, Bias: 0.285992, T: 1105, Avg. loss: 0.029280\nTotal training time: 0.00 seconds.\n-- Epoch 6\nNorm: 0.18, NNZs: 1, Bias: 0.290268, T: 1326, Avg. loss: 0.029095\nTotal training time: 0.00 seconds.\n-- Epoch 7\nNorm: 0.19, NNZs: 1, Bias: 0.291204, T: 1547, Avg. loss: 0.028998\nTotal training time: 0.00 seconds.\n-- Epoch 8\nNorm: 0.19, NNZs: 1, Bias: 0.291239, T: 1768, Avg. loss: 0.028947\nTotal training time: 0.00 seconds.\n-- Epoch 9\nNorm: 0.19, NNZs: 1, Bias: 0.290878, T: 1989, Avg. loss: 0.028905\nTotal training time: 0.00 seconds.\nConvergence after 9 epochs took 0.00 seconds\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nSGDRegressor(verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SGDRegressorSGDRegressor(verbose=1)\n\n\n\nprint('y절편:', model.intercept_)\nprint('기울기:', model.coef_)\n\ny절편: [0.29087809]\n기울기: [0.19422995]\n\n\n\nplt.scatter(X_train_scaled, y_train_scaled, color='b')\nx=np.array([min(X_train_scaled),max(X_train_scaled)])\ny=model.coef_*x+model.intercept_\nplt.plot(x,y,c='orange', label='regression line')\nplt.show()"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/경사하강법.html#예측-1",
    "href": "posts/Study/Python Data Analysis/경사하강법.html#예측-1",
    "title": "경사하강법",
    "section": "7.예측",
    "text": "7.예측\n\n# 테스트 데이터 예측\ny_pred=model.predict(X_test_scaled)\n\n# 테스트 데이터 실제 값\nplt.scatter(X_test_scaled, y_test_scaled, color='g')\n\n# 테스트 데이터 예측 값\nplt.scatter(X_test_scaled, y_pred, color='b')\nplt.show()\n\n\n\n\n\n# 예측 값의 역변환(실제 스케일)\ny_pred = y_pred.reshape(-1,1)\ny_pred_inverse = scalerY.inverse_transform(y_pred)"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html",
    "href": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "",
    "text": "파이썬 데이터 분석"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#패키지-설정",
    "href": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#패키지-설정",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "1. 패키지 설정",
    "text": "1. 패키지 설정\n\nfrom sklearn.neural_network import MLPClassifier\nimport numpy as np"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#데이터-작성",
    "href": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#데이터-작성",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "2. 데이터 작성",
    "text": "2. 데이터 작성\n\nX=np.array([[0,0],[0,1],[1,0],[1,1]])\ny=np.array([0,1,1,0])\n\n\n입력 값, 목표 값 작성"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#모형화",
    "href": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#모형화",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "3. 모형화",
    "text": "3. 모형화\n\nmodel=MLPClassifier(hidden_layer_sizes=(2),\n                    activation='logistic',\n                    solver='lbfgs',\n                    max_iter=100)"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#학습",
    "href": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#학습",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "4. 학습",
    "text": "4. 학습\n\nmodel.fit(X,y)\n\nMLPClassifier(activation='logistic', hidden_layer_sizes=2, max_iter=100,\n              solver='lbfgs')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MLPClassifierMLPClassifier(activation='logistic', hidden_layer_sizes=2, max_iter=100,\n              solver='lbfgs')"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#예측",
    "href": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#예측",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "5. 예측",
    "text": "5. 예측\n\nprint(model.predict(X))\n\n[0 0 1 0]"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#패키지-설정-1",
    "href": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#패키지-설정-1",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "1. 패키지 설정",
    "text": "1. 패키지 설정\n\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_percentage_error\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#데이터-준비",
    "href": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#데이터-준비",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "2. 데이터 준비",
    "text": "2. 데이터 준비\n\nX=np.array(range(1,101))\nprint(X)\n\n[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n  91  92  93  94  95  96  97  98  99 100]\n\n\n\ny=0.5*(X-50)**3 - 50000/X + 120000\nprint(y)\n\n[ 11175.5         39704.          51421.83333333  58832.\n  64437.5         69074.66666667  73103.64285714  76706.\n  79983.94444444  83000.          85795.04545455  88397.33333333\n  90827.34615385  93100.57142857  95229.16666667  97223.\n  99090.32352941 100838.22222222 102472.92105263 104000.\n 105424.54761905 106751.27272727 107984.58695652 109128.66666667\n 110187.5        111164.92307692 112064.64814815 112890.28571429\n 113645.36206897 114333.33333333 114957.59677419 115521.5\n 116028.34848485 116481.41176471 116883.92857143 117239.11111111\n 117550.14864865 117820.21052632 118052.44871795 118250.\n 118415.98780488 118553.52380952 118665.70930233 118755.63636364\n 118826.38888889 118881.04347826 118922.67021277 118954.33333333\n 118979.09183673 119000.         119020.10784314 119042.46153846\n 119070.10377358 119106.07407407 119153.40909091 119215.14285714\n 119294.30701754 119393.93103448 119517.04237288 119666.66666667\n 119845.82786885 120057.5483871  120304.84920635 120590.75\n 120918.26923077 121290.42424242 121710.23134328 122180.70588235\n 122704.86231884 123285.71428571 123926.27464789 124629.55555556\n 125398.56849315 126236.32432432 127145.83333333 128130.10526316\n 129192.14935065 130334.97435897 131561.58860759 132875.\n 134278.21604938 135774.24390244 137366.09036145 139056.76190476\n 140849.26470588 142746.60465116 144751.78735632 146867.81818182\n 149097.70224719 151444.44444444 153911.04945055 156500.52173913\n 159215.8655914  162060.08510638 165036.18421053 168147.16666667\n 171396.03608247 174785.79591837 178319.44949495 182000.        ]"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#탐색적-데이터-분석",
    "href": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#탐색적-데이터-분석",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "3. 탐색적 데이터 분석",
    "text": "3. 탐색적 데이터 분석\n\nplt.scatter(X, y, color='b')\nplt.title('y=0.5*(X-50)**3 - 50000/X + 120000')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.ylim(0,200000)\nplt.show()"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#데이터-분리",
    "href": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#데이터-분리",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "4. 데이터 분리",
    "text": "4. 데이터 분리\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,\n                                                    random_state=1234)\n\n\nprint(X_train)\nprint(X_test)\n\n[  5  65  11  94  58  73  37   8  55  78  22  19  71  87  23   7  45   9\n  42  17  46  21  26  56  79  32  93   6  85  33  53  14  92  18  29  47\n  61  15  66  13  20   3   4   1  12  68  98  35  38  96  51 100  74  81\n  70  59  91  90  44  31  27  24  50  16  25  77  54  39  84  48]\n[41 36 82 62 99 69 86 28 40 43 34 60 64 95 57 88 97  2 72 83 10 52 30 89\n 76 75 63 67 80 49]\n\n\n\nprint(y_train)\nprint(y_test)\n\n[ 64437.5        120918.26923077  85795.04545455 162060.08510638\n 119393.93103448 125398.56849315 117550.14864865  76706.\n 119153.40909091 130334.97435897 106751.27272727 102472.92105263\n 123926.27464789 144751.78735632 107984.58695652  73103.64285714\n 118826.38888889  79983.94444444 118553.52380952  99090.32352941\n 118881.04347826 105424.54761905 111164.92307692 119215.14285714\n 131561.58860759 115521.5        159215.8655914   69074.66666667\n 140849.26470588 116028.34848485 119070.10377358  93100.57142857\n 156500.52173913 100838.22222222 113645.36206897 118922.67021277\n 119845.82786885  95229.16666667 121290.42424242  90827.34615385\n 104000.          51421.83333333  58832.          11175.5\n  88397.33333333 122180.70588235 174785.79591837 116883.92857143\n 117820.21052632 168147.16666667 119020.10784314 182000.\n 126236.32432432 134278.21604938 123285.71428571 119517.04237288\n 153911.04945055 151444.44444444 118755.63636364 114957.59677419\n 112064.64814815 109128.66666667 119000.          97223.\n 110187.5        129192.14935065 119106.07407407 118052.44871795\n 139056.76190476 118954.33333333]\n[118415.98780488 117239.11111111 135774.24390244 120057.5483871\n 178319.44949495 122704.86231884 142746.60465116 112890.28571429\n 118250.         118665.70930233 116481.41176471 119666.66666667\n 120590.75       165036.18421053 119294.30701754 146867.81818182\n 171396.03608247  39704.         124629.55555556 137366.09036145\n  83000.         119042.46153846 114333.33333333 149097.70224719\n 128130.10526316 127145.83333333 120304.84920635 121710.23134328\n 132875.         118979.09183673]"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#피처-스케일링",
    "href": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#피처-스케일링",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "5. 피처 스케일링",
    "text": "5. 피처 스케일링\n\nX_train = X_train.reshape(-1,1)\nX_test=X_test.reshape(-1,1)\ny_train=y_train.reshape(-1,1)\ny_test=y_test.reshape(-1,1)\nprint(X_train)\n\n[[  5]\n [ 65]\n [ 11]\n [ 94]\n [ 58]\n [ 73]\n [ 37]\n [  8]\n [ 55]\n [ 78]\n [ 22]\n [ 19]\n [ 71]\n [ 87]\n [ 23]\n [  7]\n [ 45]\n [  9]\n [ 42]\n [ 17]\n [ 46]\n [ 21]\n [ 26]\n [ 56]\n [ 79]\n [ 32]\n [ 93]\n [  6]\n [ 85]\n [ 33]\n [ 53]\n [ 14]\n [ 92]\n [ 18]\n [ 29]\n [ 47]\n [ 61]\n [ 15]\n [ 66]\n [ 13]\n [ 20]\n [  3]\n [  4]\n [  1]\n [ 12]\n [ 68]\n [ 98]\n [ 35]\n [ 38]\n [ 96]\n [ 51]\n [100]\n [ 74]\n [ 81]\n [ 70]\n [ 59]\n [ 91]\n [ 90]\n [ 44]\n [ 31]\n [ 27]\n [ 24]\n [ 50]\n [ 16]\n [ 25]\n [ 77]\n [ 54]\n [ 39]\n [ 84]\n [ 48]]\n\n\n\nscalerX=MinMaxScaler()\nscalerX.fit(X_train)\nX_train_norm=scalerX.transform(X_train)\n\n\nscalerY=MinMaxScaler()\nscalerY.fit(y_train)\ny_train_norm=scalerY.transform(y_train)\n\n\nX_test_norm=scalerX.transform(X_test)\ny_test_norm=scalerY.transform(y_test)"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#모형화-및-학습",
    "href": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#모형화-및-학습",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "6. 모형화 및 학습",
    "text": "6. 모형화 및 학습\n\nmodel=MLPRegressor(hidden_layer_sizes=(4),\n                   activation='logistic',\n                   solver='lbfgs',\n                   max_iter=500)\n\n\nmodel.fit(X_train_norm,y_train_norm)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nMLPRegressor(activation='logistic', hidden_layer_sizes=4, max_iter=500,\n             solver='lbfgs')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MLPRegressorMLPRegressor(activation='logistic', hidden_layer_sizes=4, max_iter=500,\n             solver='lbfgs')"
  },
  {
    "objectID": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#예측-1",
    "href": "posts/Study/Python Data Analysis/다층 퍼셉트론과 딥러닝.html#예측-1",
    "title": "다층 퍼셉트론과 딥러닝",
    "section": "7. 예측",
    "text": "7. 예측\n\ny_pred=model.predict(X_test_norm)\nprint(y_pred)\n\n[0.58532633 0.5596793  0.78666172 0.69076416 0.86381245 0.72489815\n 0.80520178 0.51832425 0.58021083 0.59553508 0.5493746  0.68090896\n 0.70057482 0.84605491 0.66604552 0.81438502 0.85496516 0.38241064\n 0.73934651 0.79131812 0.42434114 0.64106953 0.52869543 0.81895453\n 0.75843272 0.7536807  0.69567514 0.71520447 0.77730687 0.62596952]\n\n\n\n# 데이터 구조의 변형\ny_pred=y_pred.reshape(-1,1)\n\n\n# 예측 값의 역변환\ny_pred_inverse=scalerY.inverse_transform(y_pred)\nprint(y_pred_inverse)\n\n[[111163.57767477]\n [106782.43697013]\n [145556.59434008]\n [129174.94277171]\n [158735.82920511]\n [135005.86461054]\n [148723.69087489]\n [ 99717.9814556 ]\n [110289.72571243]\n [112907.48205149]\n [105022.14122011]\n [127491.43180009]\n [130850.84418785]\n [155702.4069167 ]\n [124952.39323735]\n [150292.41387152]\n [157224.4958677 ]\n [ 76500.60587199]\n [137473.99775244]\n [146352.02276425]\n [ 83663.3625764 ]\n [120685.88219775]\n [101489.63200544]\n [151072.99795266]\n [140734.38957096]\n [139922.62822513]\n [130013.85862295]\n [133349.94535149]\n [143958.55773291]\n [118106.43080755]]\n\n\n\nplt.scatter(X,y, color='g')\n# 원데이터\nplt.title('y=0.5*(X-50)**3 - 50000/X + 120000')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.ylim(0,200000)\n\n# 테스트데이터\nplt.scatter(X_test, y_pred_inverse, color='r')\nplt.show()\n\n\n\n\n\nprint('MAPE:%.2f' % \n      mean_absolute_percentage_error(y_test,y_pred_inverse))\n\nMAPE:0.10"
  },
  {
    "objectID": "posts/Study/KSS-DataFrame.html",
    "href": "posts/Study/KSS-DataFrame.html",
    "title": "[한국통계학회] 통계계산연구회 튜토리얼",
    "section": "",
    "text": "해당 자료는 2023 한국통계학회 통계계산연구회 여인권 교수님 자료임\nhttps://github.com/statfunny/Bigdata-statistical-Analysis\n아래 내용 실행하기 전에 깔아야 할 게 엄청 많음……\n\n# pip install pyarrow pyspark==3.4.0"
  },
  {
    "objectID": "posts/Study/KSS-DataFrame.html#ref",
    "href": "posts/Study/KSS-DataFrame.html#ref",
    "title": "[한국통계학회] 통계계산연구회 튜토리얼",
    "section": "",
    "text": "해당 자료는 2023 한국통계학회 통계계산연구회 여인권 교수님 자료임\nhttps://github.com/statfunny/Bigdata-statistical-Analysis\n아래 내용 실행하기 전에 깔아야 할 게 엄청 많음……\n\n# pip install pyarrow pyspark==3.4.0"
  },
  {
    "objectID": "posts/Study/KSS-DataFrame.html#효율적인-메모리-관리와-프로그램-작성",
    "href": "posts/Study/KSS-DataFrame.html#효율적인-메모리-관리와-프로그램-작성",
    "title": "[한국통계학회] 통계계산연구회 튜토리얼",
    "section": "효율적인 메모리 관리와 프로그램 작성",
    "text": "효율적인 메모리 관리와 프로그램 작성\n\n빅데이터분석에서의 메모리 관리\n\n문자열보다는 범주형\n범위가 제한적인 정수형\n최소한의 실수형\n이진(binary)인 경우 Boolean(True/False)\n\n\nimport pandas as pd\nimport numpy as np\n\n\ndef 데이터프레임생성(size):\n    df = pd.DataFrame()\n    df[\"나이\"] = np.random.choice(100,size)\n    df[\"수행평가1\"] = np.random.choice([\"A\",\"B\",\"C\",\"D\",\"F\"], size)\n    df[\"수행평가2\"] = np.random.choice([\"상\",\"중\",\"하\"], size)    \n    df[\"학점\"] = np.random.choice([\"[0,3)\",\"[3,3.5)\",\"[3.5,4)\",\"[4,4.3]\"], size)\n    df[\"합격확률\"] = np.random.uniform(0,1,size)\n    df[\"결과\"] = np.random.choice([\"합격\",\"불합격\"],size)\n    return df\n\n\ndf = 데이터프레임생성(1000000)\n# df = 데이터프레임생성(1_000_000) # 컴마 대신에 언더바를 작성해서 나눠주기\ndf1 = df.copy()\ndf2 = df.copy()\ndf1.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 6 columns):\n #   Column  Non-Null Count    Dtype  \n---  ------  --------------    -----  \n 0   나이      1000000 non-null  int64  \n 1   수행평가1   1000000 non-null  object \n 2   수행평가2   1000000 non-null  object \n 3   학점      1000000 non-null  object \n 4   합격확률    1000000 non-null  float64\n 5   결과      1000000 non-null  object \ndtypes: float64(1), int64(1), object(4)\nmemory usage: 45.8+ MB\n\n\n\n\n수행작업\n\n수행평가1, 학점에 따라 데이터를 나누고 그 안에서 나이의 순위\n수행평가1, 학점에 따라 데이터를 나누고 그 안에서 합격확률의 순위\n수행평가1, 학점, 결과에 따라 데이터를 나누고 그 안에서 합격확률의 순위\n수행시간계산\n\n%timeit : 반복 작업을 하며 해당 프로그램을 수행하는데 걸린 시간의 평균과 표준편차 제고\n\n\n\n%timeit df1[\"순위1\"] = df1.groupby([\"수행평가1\",\"학점\"])[\"나이\"].rank()\n%timeit df1[\"순위2\"] = df1.groupby([\"수행평가1\",\"학점\"])[\"합격확률\"].rank()\n%timeit df1[\"순위3\"] = df1.groupby([\"수행평가1\",\"학점\",\"결과\"])[\"합격확률\"].rank()\n\n157 ms ± 533 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n219 ms ± 2.95 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n256 ms ± 384 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n\nstring \\(\\rightarrow\\) 범주형\n\ndf2[\"수행평가1\"] = df2[\"수행평가1\"].astype('category')\ndf2[\"수행평가2\"] = df2[\"수행평가2\"].astype('category')\ndf2[\"학점\"] = df2[\"학점\"].astype('category')\ndf2.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 6 columns):\n #   Column  Non-Null Count    Dtype   \n---  ------  --------------    -----   \n 0   나이      1000000 non-null  int64   \n 1   수행평가1   1000000 non-null  category\n 2   수행평가2   1000000 non-null  category\n 3   학점      1000000 non-null  category\n 4   합격확률    1000000 non-null  float64 \n 5   결과      1000000 non-null  object  \ndtypes: category(3), float64(1), int64(1), object(1)\nmemory usage: 25.7+ MB\n\n\n\n\nDowncastrng\n\nint8: -128~127\n\nuint8: 0~255\n\nint16: -32,768 ~ 32,767\n\nuint16: 0~65,535\n\nint32: -2,147,483,648~2,147,483,647\n\nuint32: 0~ 4,294,967,295\n\nint64: -9,223,372,036,854,775,808 ~ -9,223,372,036,854,775,807\n\nuint64: 0~18,446,744,073,709,551,615\n\n\n\ndf2[\"나이\"] = df2[\"나이\"].astype('int8')\ndf2.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 6 columns):\n #   Column  Non-Null Count    Dtype   \n---  ------  --------------    -----   \n 0   나이      1000000 non-null  int8    \n 1   수행평가1   1000000 non-null  category\n 2   수행평가2   1000000 non-null  category\n 3   학점      1000000 non-null  category\n 4   합격확률    1000000 non-null  float64 \n 5   결과      1000000 non-null  object  \ndtypes: category(3), float64(1), int8(1), object(1)\nmemory usage: 19.1+ MB\n\n\n\ndf2[\"합격확률\"] = df2[\"합격확률\"].astype('float32')\ndf2.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 6 columns):\n #   Column  Non-Null Count    Dtype   \n---  ------  --------------    -----   \n 0   나이      1000000 non-null  int8    \n 1   수행평가1   1000000 non-null  category\n 2   수행평가2   1000000 non-null  category\n 3   학점      1000000 non-null  category\n 4   합격확률    1000000 non-null  float32 \n 5   결과      1000000 non-null  object  \ndtypes: category(3), float32(1), int8(1), object(1)\nmemory usage: 15.3+ MB\n\n\n\ndf2[\"결과\"] = df2[\"결과\"].map({\"합격\":True,\"불합격\":False})\ndf2.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 6 columns):\n #   Column  Non-Null Count    Dtype   \n---  ------  --------------    -----   \n 0   나이      1000000 non-null  int8    \n 1   수행평가1   1000000 non-null  category\n 2   수행평가2   1000000 non-null  category\n 3   학점      1000000 non-null  category\n 4   합격확률    1000000 non-null  float32 \n 5   결과      1000000 non-null  bool    \ndtypes: bool(1), category(3), float32(1), int8(1)\nmemory usage: 8.6 MB\n\n\n\n%timeit df2[\"순위1\"] = df2.groupby([\"수행평가1\",\"학점\"])[\"나이\"].rank()\n%timeit df2[\"순위2\"] = df2.groupby([\"수행평가1\",\"학점\"])[\"합격확률\"].rank()\n%timeit df2[\"순위3\"] = df2.groupby([\"수행평가1\",\"학점\",\"결과\"])[\"합격확률\"].rank()\n\n104 ms ± 377 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n164 ms ± 2.18 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n169 ms ± 1.55 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n## 파일 저장 및 읽기\n변수 = [\"나이\",\"수행평가1\",\"수행평가2\",\"학점\",\"합격확률\",\"결과\"] \ndf1 = df1[변수]\ndf2 = df2[변수]\ndf1.to_csv(\"BSA03_df1.csv\",index=False)\ndf2.to_csv('BSA03_df2.csv',index=False)\ndf1csv = pd.read_csv('BSA03_df1.csv')\ndf2csv = pd.read_csv('BSA03_df2.csv')\n\ndf1.to_csv : csv파일로 저장해라.\n\ndf1csv.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 6 columns):\n #   Column  Non-Null Count    Dtype  \n---  ------  --------------    -----  \n 0   나이      1000000 non-null  int64  \n 1   수행평가1   1000000 non-null  object \n 2   수행평가2   1000000 non-null  object \n 3   학점      1000000 non-null  object \n 4   합격확률    1000000 non-null  float64\n 5   결과      1000000 non-null  object \ndtypes: float64(1), int64(1), object(4)\nmemory usage: 45.8+ MB\n\n\n\ndf2csv.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 6 columns):\n #   Column  Non-Null Count    Dtype  \n---  ------  --------------    -----  \n 0   나이      1000000 non-null  int64  \n 1   수행평가1   1000000 non-null  object \n 2   수행평가2   1000000 non-null  object \n 3   학점      1000000 non-null  object \n 4   합격확률    1000000 non-null  float64\n 5   결과      1000000 non-null  bool   \ndtypes: bool(1), float64(1), int64(1), object(3)\nmemory usage: 39.1+ MB\n\n\n\n# pip install pyarrow\ndf2.to_parquet('BSA03_df2.parquet')\ndf2pqt = pd.read_parquet('BSA03_df2.parquet')\ndf2pqt.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 6 columns):\n #   Column  Non-Null Count    Dtype   \n---  ------  --------------    -----   \n 0   나이      1000000 non-null  int8    \n 1   수행평가1   1000000 non-null  category\n 2   수행평가2   1000000 non-null  category\n 3   학점      1000000 non-null  category\n 4   합격확률    1000000 non-null  float32 \n 5   결과      1000000 non-null  bool    \ndtypes: bool(1), category(3), float32(1), int8(1)\nmemory usage: 8.6 MB\n\n\n\npandas에서 제공함"
  },
  {
    "objectID": "posts/Study/KSS-DataFrame.html#효율적인-프로그램",
    "href": "posts/Study/KSS-DataFrame.html#효율적인-프로그램",
    "title": "[한국통계학회] 통계계산연구회 튜토리얼",
    "section": "효율적인 프로그램",
    "text": "효율적인 프로그램\n\n반복적인 작업 진행시 for문 말고 아래와 같은 문법 사용\n\n\n수행작업\n“평가”라는 새로운 변수에 - “나이”가 65세 미만이거나 “합격확률”이 0.6 이상이고 “학점”이 [4,4.3]이면 “수행평가1”를 - 위 조건이 아니면 “수행평가2”를 대입\n\ndef 변수추가(행자료):\n    if 행자료[\"나이\"] &lt; 65:\n        return 행자료[\"수행평가1\"]\n    if (행자료[\"합격확률\"] &gt;= 0.6) & (행자료[\"학점\"] == \"[4,4.3]\"):\n        return 행자료[\"수행평가1\"]\n    return(행자료[\"수행평가2\"])\n\n\n\nLoop를 이용한 프로그램\n\ndf =데이터프레임생성(100_000)\ndf1 = df.copy()\ndf2 = df.copy()\ndf3 = df.copy()\n\n\n%%timeit\nfor index, row in df1.iterrows():\n    df1.loc[index,\"평가\"] = 변수추가(row)\n\n17.3 s ± 1.05 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n\nApply를 이용한 프로그램\n\n%%timeit\ndf2[\"평가\"] = df2.apply(변수추가,axis=1)\n\n1.54 s ± 104 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n\nVectorized를 이용한 프로그램\n\n(df3[\"나이\"] &lt; 65) | ((df3[\"합격확률\"] &gt;= 0.6) & (df3[\"학점\"] == \"[4,4.3]\"))\n\n0         True\n1         True\n2        False\n3        False\n4         True\n         ...  \n99995     True\n99996     True\n99997     True\n99998     True\n99999    False\nLength: 100000, dtype: bool\n\n\n\n%%timeit\ndf3[\"평가\"] = df3[\"수행평가2\"]\n조건 = (df3[\"나이\"] &lt; 65) | ((df3[\"합격확률\"] &gt;= 0.6) & (df3[\"학점\"] == \"[4,4.3]\"))\ndf3.loc[조건,\"평가\"] = df[\"수행평가1\"]\n\n23.2 ms ± 1.66 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\nVectorized로 바꾸는게 가장 빠르게 할 수 있음."
  },
  {
    "objectID": "posts/Study/KSS-DataFrame.html#set-up-the-environment-variables-for-pyspark-java-spark-and-python",
    "href": "posts/Study/KSS-DataFrame.html#set-up-the-environment-variables-for-pyspark-java-spark-and-python",
    "title": "[한국통계학회] 통계계산연구회 튜토리얼",
    "section": "Set up the environment variables for Pyspark, Java, Spark, and python",
    "text": "Set up the environment variables for Pyspark, Java, Spark, and python\n\n오류 발생 시\n\nimport os\nimport sys\nos.environ['JAVA_HOME'] = \"C:\\Java\"\nos.environ['SPARK_HOME'] = \"C:\\spark-3.4.0\"\nos.environ['PYLIB'] = \"C:\\spark-3.4.0\\python\\lib\"\nsys.path.insert(0,os.environ['PYLIB']+\"\\py4j-0.10.9.7-src.zip\")\nsys.path.insert(0,os.environ['PYLIB']+\"\\pyspark.zip\")\n\nimport pyspark\nfrom pyspark.sql import SparkSession\n\n\n스파크 = SparkSession.builder.appName('Test').getOrCreate()\n스파크\n\n\n            \n                SparkSession - in-memory\n                \n        \n            SparkContext\n\n            Spark UI\n\n            \n              Version\n                v3.3.2\n              Master\n                local[*]\n              AppName\n                Test\n            \n        \n        \n            \n        \n\n\n\n웹브라우저에서 localhost:4040 연결\n\n\nPyspark에서 hdfs 데이터 불러오기\n\nCMD에서 start-dfs.cmd와 start-yarn.cmd 실행 후\nSpark 경로를 찾지 못하는 경우\n\n!pip install findspark\nimport findspark\nimport os\nfindspark.find()\nfindspark.init(os.environ.get(\"SPARK_HOME\"))\n\nsparkDF1 = 스파크.read.csv(\"hdfs://localhost:9000/Spark/BSA03_df1.csv\")\nsparkDF1.show(10)\n\n+----+---------+---------+-------+-------------------+------+\n| _c0|      _c1|      _c2|    _c3|                _c4|   _c5|\n+----+---------+---------+-------+-------------------+------+\n|나이|수행평가1|수행평가2|   학점|           합격확률|  결과|\n|  19|        C|       하|  [0,3)| 0.1704850998911155|불합격|\n|  78|        F|       중|[3,3.5)| 0.7007295241834984|불합격|\n|  78|        F|       중|[4,4.3]|0.06793823954810418|불합격|\n|  23|        A|       중|[3.5,4)| 0.8262506446089442|  합격|\n|  97|        C|       하|[4,4.3]| 0.5911258463622218|불합격|\n|  45|        B|       하|  [0,3)| 0.3677844602679712|  합격|\n|  66|        A|       하|[3,3.5)| 0.9721303956886912|  합격|\n|  66|        F|       중|[4,4.3]|0.33333421672000396|  합격|\n|  61|        A|       상|[3,3.5)| 0.7048925310189916|불합격|\n+----+---------+---------+-------+-------------------+------+\nonly showing top 10 rows\n\n\n\n\nsparkDF1.printSchema()\n\nroot\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n |-- _c4: string (nullable = true)\n |-- _c5: string (nullable = true)\n\n\n\n\nsparkDF2 = 스파크.read.csv(\"hdfs://localhost:9000/Spark/BSA03_df1.csv\", header=True, encoding=\"utf-8\", inferSchema=\"true\")\n#sparkDF2 = 스파크.read.option('encoding','utf-8').option('header',True).option(inferSchema='True') \\\n#    .csv(\"hdfs://localhost:9000/Spark/BSA03_df1.csv\")\nsparkDF2.printSchema()\n\nroot\n |-- 나이: integer (nullable = true)\n |-- 수행평가1: string (nullable = true)\n |-- 수행평가2: string (nullable = true)\n |-- 학점: string (nullable = true)\n |-- 합격확률: double (nullable = true)\n |-- 결과: string (nullable = true)\n\n\n\n- 하둡에 설치된 데이터를 가지고 올 때\n\ninferSchema=\"true\" 설정을 해주면 형태에 맞게 가지고 옴 (위에서는 string으로 다 가져옴)\n\n\n\n여러개의 CSV 파일 읽기\n\ncan also read multiple csv files, just pass all file names by separating comma as a path\n\nsparkDF = 스파크.read.csv(\"path1,path2,path3\")\n\n\n직접 읽을 데이터의 type 지정\nimport pyspark\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType \nfrom pyspark.sql.types import ArrayType, DoubleType, BooleanType\n\n스키마 = StructType() \\\n    .add(\"나이\",IntegerType(),True) \\\n    .add(\"수행평가1\",StringType(),True) \\\n    .add(\"수행평가2\",StringType(),True) \\\n    .add(\"학점\",StringType(),True) \\\n    .add(\"합격확률\",DoubleType(),True) \\\n    .add(\"결과\",StringType(),True)\n\nsparkDF = 스파크.read.format('csv')\\\n    .option('header',True).schema(스키마)\\\n    load(\"hdfs://localhost:9000/Spark/BSA03_df1.csv\")\n\nsparkDF3 = 스파크.read.parquet(\"hdfs://localhost:9000/Spark/BSA03_df2.parquet\")\nsparkDF3.show(10)\n\n+----+---------+---------+-------+----------+-----+\n|나이|수행평가1|수행평가2|   학점|  합격확률| 결과|\n+----+---------+---------+-------+----------+-----+\n|  19|        C|       하|  [0,3)| 0.1704851|false|\n|  78|        F|       중|[3,3.5)|0.70072955|false|\n|  78|        F|       중|[4,4.3]|0.06793824|false|\n|  23|        A|       중|[3.5,4)| 0.8262507| true|\n|  97|        C|       하|[4,4.3]|0.59112585|false|\n|  45|        B|       하|  [0,3)|0.36778447| true|\n|  66|        A|       하|[3,3.5)| 0.9721304| true|\n|  66|        F|       중|[4,4.3]| 0.3333342| true|\n|  61|        A|       상|[3,3.5)| 0.7048925|false|\n|  46|        C|       상|[3,3.5)| 0.8856867|false|\n+----+---------+---------+-------+----------+-----+\nonly showing top 10 rows\n\n\n\n\n\nDataFrame을 다른 형식으로 변환하고 저장/불러오기\n\npandas DataFrame \\(\\Longleftrightarrow\\) spark DataFrame\ncsv \\(\\Longleftrightarrow\\) parquet\n\n\npandasDF_spark = sparkDF2.toPandas()\npandasDF_spark.head()\n\n\n\n\n\n\n\n\n나이\n수행평가1\n수행평가2\n학점\n합격확률\n결과\n\n\n\n\n0\n19\nC\n하\n[0,3)\n0.170485\n불합격\n\n\n1\n78\nF\n중\n[3,3.5)\n0.700730\n불합격\n\n\n2\n78\nF\n중\n[4,4.3]\n0.067938\n불합격\n\n\n3\n23\nA\n중\n[3.5,4)\n0.826251\n합격\n\n\n4\n97\nC\n하\n[4,4.3]\n0.591126\n불합격\n\n\n\n\n\n\n\n\npandas DataFrame을 spark DataFrame으로\nApache Spark uses Apache Arrow which is an in-memory columnar format to transfer the data between Python and JVM. You need to enable to use Arrow as this is disabled by default and have Apache Arrow (PyArrow) install on all Spark cluster nodes using pip install pyspark[sql] or by directly downloading from Apache Arrow for Python.\n\n### !pip install pyarrow\n스파크.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\")\n## pandas 2.0 : AttributeError: 'DataFrame' object has no attribute 'iteritems'\n## --&gt;  iteritems was removed in pandas 2.0 ==&gt; pandas downgrade \n#sparkDF_pandas = 스파크.createDataFrame(df1csv)\n#sparkDF_pandas.show(10)\n\n\n\nWhen an error occurs,\nSpark automatically fallback to non-Arrow optimization implementation, this can be controlled by spark.sql.execution.arrow.pyspark.fallback.enabled.\nspark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\",\"true\")\n\n\n\nPyspark vs Python\n\npyspark는 scala로 만든 spark의 python 버전\nscala는 JVM object \\(\\rightarrow\\) pyspark 또한 JVM object\npython은 python object\npython(pandas) \\(\\rightarrow\\) py4J \\(\\rightarrow\\) scala(pyspark) : 오류가 자주 발생\n\n\nSpark에서\n\nsparkDF.write.csv(“경로”)\nsparkDF.format(‘csv’).save(“경로”)\n기존 파일이 있는 경우\n\n덮어쓰기: sparkDF.write.mode(‘overwrite’).csv(“경로”)\n추가하기: sparkDF.write.mode(‘append’).csv(“경로”)\n무시하기: sparkDF.write.mode(‘ignore’).csv(“경로”)\n오류발생: sparkDF.write.mode(‘error’).csv(“경로”) \\(\\Leftarrow\\) default\n\n\n\nsparkDF3.write.csv(\"hdfs://localhost:9000/Test/csv\")\nsparkDF3.write.parquet(\"hdfs://localhost:9000/Test/parquet\")\n\n\n\n\nPyspark 기본 예제\n\ndf_hdfs = 스파크.read.csv(\"hdfs://localhost:9000/Spark/Employee.csv\", header=True, encoding='cp949', inferSchema='true')\ndf_hdfs.show(10)\n\n+---+------+----+------+-------+--------+-------+-------+--------+\n| id|gender|educ|jobcat| salary|salbegin|jobtime|prevexp|minority|\n+---+------+----+------+-------+--------+-------+-------+--------+\n|  1|  남성|  15|경영자|57000.0|   27000|     98|    144|      No|\n|  2|  남성|  16|사무직|40200.0|   18750|     98|     36|      No|\n|  3|  여성|  12|사무직|21450.0|   12000|     98|    381|      No|\n|  4|  여성|   8|사무직|21900.0|   13200|     98|    190|      No|\n|  5|  남성|  15|사무직|45000.0|   21000|     98|    138|      No|\n|  6|  남성|  15|사무직|32100.0|   13500|     98|     67|      No|\n|  7|  남성|  15|사무직|36000.0|   18750|     98|    114|      No|\n|  8|  여성|  12|사무직|21900.0|    9750|     98|      0|      No|\n|  9|  여성|  15|사무직|27900.0|   12750|     98|    115|      No|\n| 10|  여성|  12|사무직|24000.0|   13500|     98|    244|      No|\n+---+------+----+------+-------+--------+-------+-------+--------+\nonly showing top 10 rows\n\n\n\n\ndf_hdfs.where('jobcat==\"경영자\"').show(10)\n\n+---+------+----+------+--------+--------+-------+-------+--------+\n| id|gender|educ|jobcat|  salary|salbegin|jobtime|prevexp|minority|\n+---+------+----+------+--------+--------+-------+-------+--------+\n|  1|  남성|  15|경영자| 57000.0|   27000|     98|    144|      No|\n| 18|  남성|  16|경영자|103750.0|   27510|     97|     70|      No|\n| 27|  남성|  19|경영자| 60375.0|   27480|     96|     96|      No|\n| 29|  남성|  19|경영자|135000.0|   79980|     96|    199|      No|\n| 32|  남성|  19|경영자|110625.0|   45000|     96|    120|      No|\n| 34|  남성|  19|경영자| 92000.0|   39990|     96|    175|      No|\n| 35|  남성|  17|경영자| 81250.0|   30000|     96|     18|      No|\n| 50|  남성|  16|경영자| 60000.0|   23730|     94|     59|      No|\n| 53|  남성|  18|경영자| 73750.0|   26250|     94|     56|      No|\n| 62|  남성|  16|경영자| 48000.0|   21750|     93|     22|      No|\n+---+------+----+------+--------+--------+-------+-------+--------+\nonly showing top 10 rows\n\n\n\n\nfrom pyspark.sql.functions import col, log, exp, when\ndf_hdfs.withColumn(\"Lsalary\",log(\"salary\")).withColumn(\"LBsalary\",log(\"salbegin\")).show(5)\n\n+---+------+----+------+-------+--------+-------+-------+--------+------------------+------------------+\n| id|gender|educ|jobcat| salary|salbegin|jobtime|prevexp|minority|           Lsalary|          LBsalary|\n+---+------+----+------+-------+--------+-------+-------+--------+------------------+------------------+\n|  1|  남성|  15|경영자|57000.0|   27000|     98|    144|      No|10.950806546816688|10.203592144986466|\n|  2|  남성|  16|사무직|40200.0|   18750|     98|     36|      No|10.601622274607113| 9.838949031398556|\n|  3|  여성|  12|사무직|21450.0|   12000|     98|    381|      No| 9.973479924356162| 9.392661928770137|\n|  4|  여성|   8|사무직|21900.0|   13200|     98|    190|      No| 9.994241915804592| 9.487972108574462|\n|  5|  남성|  15|사무직|45000.0|   21000|     98|    138|      No|10.714417768752456|  9.95227771670556|\n+---+------+----+------+-------+--------+-------+-------+--------+------------------+------------------+\nonly showing top 5 rows\n\n\n\n\ndf_hdfs.select([\"gender\",\"jobcat\"]).distinct().show()\n\n+------+------+\n|gender|jobcat|\n+------+------+\n|  남성|사무직|\n|  여성|사무직|\n|  여성|경영자|\n|  남성|경영자|\n|  남성|관리직|\n+------+------+\n\n\n\n\ndf_hdfs.withColumn(\"Job\",when(col(\"jobcat\")==\"경영자\",\"임원\").otherwise(\"사원\")).show(5)\n\n+---+------+----+------+-------+--------+-------+-------+--------+----+\n| id|gender|educ|jobcat| salary|salbegin|jobtime|prevexp|minority| Job|\n+---+------+----+------+-------+--------+-------+-------+--------+----+\n|  1|  남성|  15|경영자|57000.0|   27000|     98|    144|      No|임원|\n|  2|  남성|  16|사무직|40200.0|   18750|     98|     36|      No|사원|\n|  3|  여성|  12|사무직|21450.0|   12000|     98|    381|      No|사원|\n|  4|  여성|   8|사무직|21900.0|   13200|     98|    190|      No|사원|\n|  5|  남성|  15|사무직|45000.0|   21000|     98|    138|      No|사원|\n+---+------+----+------+-------+--------+-------+-------+--------+----+\nonly showing top 5 rows\n\n\n\n\n\nHow to use SQL in Pyspark\n\nDF: DSL(domain specific language)\nTables: pure SQL(Structured Query Language)\nDF(DataFrame)으로부터 (temporary, permanant) table를 create할 수 있음\n\n\ndf_hdfs.registerTempTable(\"table1\")\n\nc:\\users\\smu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pyspark\\sql\\dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n\n\n\nDatabase 형태로 만드는 작업\n\n\n스파크.sql(\"select count(*) from table1 group by jobcat\").show()\n\n+--------+\n|count(1)|\n+--------+\n|      27|\n|      84|\n|     363|\n+--------+\n\n\n\n\n스파크.sql(\"show databases\").show()\n\n+---------+\n|namespace|\n+---------+\n|  default|\n+---------+\n\n\n\n\n스파크.sql(\"create database db1\")\n스파크.sql(\"show databases\").show()\n\n+---------+\n|namespace|\n+---------+\n|      db1|\n|  default|\n+---------+\n\n\n\n\ndf_hdfs.registerTempTable(\"table1\") \n스파크.sql(\"show tables in default\").show()\n\n+---------+---------+-----------+\n|namespace|tableName|isTemporary|\n+---------+---------+-----------+\n|         |   table1|       true|\n+---------+---------+-----------+\n\n\n\n\ndf_hdfs.registerTempTable(\"table1\")\ndf_hdfs.write.saveAsTable(\"db1.permtable\")  \n## 현재 jupyter 실행 폴더의 spark-warehouse/db1.db/permtable에 저장\n## 기존 table이 존재하는 경우 오류\n\n\nmode 변경:\n\n관리자권한\nappend, overwrite, error, errorifexists, ignore (default: error) 중 선택\n\ndf_hdfs.write.mode('overwrite').saveAsTable(\"db1.permtable\")\n\n스파크.sql(\"show tables in db1\").show()\n\n+---------+---------+-----------+\n|namespace|tableName|isTemporary|\n+---------+---------+-----------+\n|      db1|permtable|      false|\n|         |   table1|       true|\n+---------+---------+-----------+"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html",
    "title": "앙상블 학습",
    "section": "",
    "text": "선형대수와 통계학으로 배우는 머신러닝 with 파이썬\ngithub"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#import",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#import",
    "title": "앙상블 학습",
    "section": "import",
    "text": "import\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import VotingClassifier\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#data",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#data",
    "title": "앙상블 학습",
    "section": "data",
    "text": "data\n\nraw_iris = datasets.load_iris()\n\n\nX = raw_iris.data\ny = raw_iris.target\n\n\nX_tn, X_te, y_tn, y_te = train_test_split(X,y, random_state=0)\n\n\n# 데이터 표준화\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std  = std_scale.transform(X_te)"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#학습",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#학습",
    "title": "앙상블 학습",
    "section": "학습",
    "text": "학습\n\n# 보팅 학습\nclf1 = LogisticRegression(multi_class='multinomial', \n                          random_state=1)\nclf2 = svm.SVC(kernel='linear', \n               random_state=1) \nclf3 = GaussianNB()\n\n\nclf_voting = VotingClassifier(\n                estimators=[\n                    ('lr', clf1), \n                    ('svm', clf2), \n                    ('gnb', clf3)\n                ],\n                voting='hard',\n                weights=[1,1,1])\nclf_voting.fit(X_tn_std, y_tn)\n\nVotingClassifier(estimators=[('lr',\n                              LogisticRegression(multi_class='multinomial',\n                                                 random_state=1)),\n                             ('svm', SVC(kernel='linear', random_state=1)),\n                             ('gnb', GaussianNB())],\n                 weights=[1, 1, 1])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.VotingClassifierVotingClassifier(estimators=[('lr',\n                              LogisticRegression(multi_class='multinomial',\n                                                 random_state=1)),\n                             ('svm', SVC(kernel='linear', random_state=1)),\n                             ('gnb', GaussianNB())],\n                 weights=[1, 1, 1])lrLogisticRegressionLogisticRegression(multi_class='multinomial', random_state=1)svmSVCSVC(kernel='linear', random_state=1)gnbGaussianNBGaussianNB()\n\n\n\nestimators는 미리 만든 세가지 모형\nvoting 옵션: hard(투표 결과 과반수가 넘는 라벨,defalut)/soft(확률이 가장 높은 라벨)\nweights: 세 가지 모형의 비율"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#예측",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#예측",
    "title": "앙상블 학습",
    "section": "예측",
    "text": "예측\n\npred_voting = clf_voting.predict(X_te_std)\nprint(pred_voting)\n\n[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n 2]"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#평가",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#평가",
    "title": "앙상블 학습",
    "section": "평가",
    "text": "평가\n\naccuracy = accuracy_score(y_te, pred_voting)\nprint(accuracy)\n\n0.9736842105263158\n\n\n\n# confusion matrix 확인 \nconf_matrix = confusion_matrix(y_te, pred_voting)\nprint(conf_matrix)\n\n[[13  0  0]\n [ 0 15  1]\n [ 0  0  9]]\n\n\n\n\nclass_report = classification_report(y_te, pred_voting)\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        13\n           1       1.00      0.94      0.97        16\n           2       0.90      1.00      0.95         9\n\n    accuracy                           0.97        38\n   macro avg       0.97      0.98      0.97        38\nweighted avg       0.98      0.97      0.97        38"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#import-1",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#import-1",
    "title": "앙상블 학습",
    "section": "import",
    "text": "import\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#data-1",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#data-1",
    "title": "앙상블 학습",
    "section": "data",
    "text": "data\n\nraw_wine = datasets.load_wine()\n\n\nX = raw_wine.data\ny = raw_wine.target\n\n\n\nX_tn, X_te, y_tn, y_te=train_test_split(X,y,random_state=0)\n\n\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std= std_scale.transform(X_te)"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#학습-1",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#학습-1",
    "title": "앙상블 학습",
    "section": "학습",
    "text": "학습\n\n\nclf_rf = RandomForestClassifier(max_depth=2, \n                                random_state=0)\nclf_rf.fit(X_tn_std, y_tn)\n\nRandomForestClassifier(max_depth=2, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(max_depth=2, random_state=0)"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#예측-1",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#예측-1",
    "title": "앙상블 학습",
    "section": "예측",
    "text": "예측\n\npred_rf = clf_rf.predict(X_te_std)\nprint(pred_rf)\n\n[0 2 1 0 1 1 0 2 1 1 2 2 0 1 2 1 0 0 2 0 0 0 0 1 1 1 1 1 1 2 0 0 1 0 0 0 2\n 1 1 2 0 0 1 1 1]"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#평가-1",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#평가-1",
    "title": "앙상블 학습",
    "section": "평가",
    "text": "평가\n\naccuracy = accuracy_score(y_te, pred_rf)\nprint(accuracy)\n\nconf_matrix = confusion_matrix(y_te, pred_rf)\nprint(conf_matrix)\n\nclass_report = classification_report(y_te, pred_rf)\nprint(class_report)\n\n0.9555555555555556\n[[16  0  0]\n [ 1 19  1]\n [ 0  0  8]]\n              precision    recall  f1-score   support\n\n           0       0.94      1.00      0.97        16\n           1       1.00      0.90      0.95        21\n           2       0.89      1.00      0.94         8\n\n    accuracy                           0.96        45\n   macro avg       0.94      0.97      0.95        45\nweighted avg       0.96      0.96      0.96        45"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#import-2",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#import-2",
    "title": "앙상블 학습",
    "section": "import",
    "text": "import\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#data-2",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#data-2",
    "title": "앙상블 학습",
    "section": "data",
    "text": "data\n\nraw_breast_cancer = datasets.load_breast_cancer()\n\nX = raw_breast_cancer.data\ny = raw_breast_cancer.target\n\nX_tn, X_te, y_tn, y_te=train_test_split(X,y,random_state=0)\n\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std  = std_scale.transform(X_te)"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#학습-2",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#학습-2",
    "title": "앙상블 학습",
    "section": "학습",
    "text": "학습\n\nclf_ada = AdaBoostClassifier(random_state=0)\nclf_ada.fit(X_tn_std, y_tn)\n\nAdaBoostClassifier(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostClassifierAdaBoostClassifier(random_state=0)"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#예측-2",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#예측-2",
    "title": "앙상블 학습",
    "section": "예측",
    "text": "예측\n\npred_ada = clf_ada.predict(X_te_std)\nprint(pred_ada)\n\n[0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1\n 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0\n 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1\n 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0]"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#평가-2",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#평가-2",
    "title": "앙상블 학습",
    "section": "평가",
    "text": "평가\n\naccuracy = accuracy_score(y_te, pred_ada)\nprint(accuracy)\n\nconf_matrix = confusion_matrix(y_te, pred_ada)\nprint(conf_matrix)\n\nclass_report = classification_report(y_te, pred_ada)\nprint(class_report)\n\n0.9790209790209791\n[[52  1]\n [ 2 88]]\n              precision    recall  f1-score   support\n\n           0       0.96      0.98      0.97        53\n           1       0.99      0.98      0.98        90\n\n    accuracy                           0.98       143\n   macro avg       0.98      0.98      0.98       143\nweighted avg       0.98      0.98      0.98       143"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#import-3",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#import-3",
    "title": "앙상블 학습",
    "section": "import",
    "text": "import\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#data-3",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#data-3",
    "title": "앙상블 학습",
    "section": "data",
    "text": "data\n\n\nraw_breast_cancer = datasets.load_breast_cancer()\n\nX = raw_breast_cancer.data\ny = raw_breast_cancer.target\n\nX_tn, X_te, y_tn, y_te=train_test_split(X,y,random_state=0)\n\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std  = std_scale.transform(X_te)"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#학습-3",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#학습-3",
    "title": "앙상블 학습",
    "section": "학습",
    "text": "학습\n\nclf_gbt = GradientBoostingClassifier(max_depth=2, \n                                     learning_rate=0.01,\n                                     random_state=0)\nclf_gbt.fit(X_tn_std, y_tn)\n\nGradientBoostingClassifier(learning_rate=0.01, max_depth=2, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier(learning_rate=0.01, max_depth=2, random_state=0)"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#예측-3",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#예측-3",
    "title": "앙상블 학습",
    "section": "예측",
    "text": "예측\n\n\npred_gboost = clf_gbt.predict(X_te_std)\nprint(pred_gboost)\n\n[0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1\n 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 1\n 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1\n 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0]"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#평가-3",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#평가-3",
    "title": "앙상블 학습",
    "section": "평가",
    "text": "평가\n\n\naccuracy = accuracy_score(y_te, pred_gboost)\nprint(accuracy)\n\n\nconf_matrix = confusion_matrix(y_te, pred_gboost)\nprint(conf_matrix)\n\nclass_report = classification_report(y_te, pred_gboost)\nprint(class_report)\n\n0.965034965034965\n[[49  4]\n [ 1 89]]\n              precision    recall  f1-score   support\n\n           0       0.98      0.92      0.95        53\n           1       0.96      0.99      0.97        90\n\n    accuracy                           0.97       143\n   macro avg       0.97      0.96      0.96       143\nweighted avg       0.97      0.97      0.96       143"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#import-4",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#import-4",
    "title": "앙상블 학습",
    "section": "import",
    "text": "import\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import StackingClassifier\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#data-4",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#data-4",
    "title": "앙상블 학습",
    "section": "data",
    "text": "data\n\nraw_breast_cancer = datasets.load_breast_cancer()\n\nX = raw_breast_cancer.data\ny = raw_breast_cancer.target\n\nX_tn, X_te, y_tn, y_te=train_test_split(X,y,random_state=0)\n\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std  = std_scale.transform(X_te)"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#학습-4",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#학습-4",
    "title": "앙상블 학습",
    "section": "학습",
    "text": "학습\n\nclf1 = svm.SVC(kernel='linear', random_state=1) \nclf2 = GaussianNB()\n\nclf_stkg = StackingClassifier(\n            estimators=[\n                ('svm', clf1), \n                ('gnb', clf2)\n            ],\n            final_estimator=LogisticRegression())\nclf_stkg.fit(X_tn_std, y_tn)\n\nStackingClassifier(estimators=[('svm', SVC(kernel='linear', random_state=1)),\n                               ('gnb', GaussianNB())],\n                   final_estimator=LogisticRegression())In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StackingClassifierStackingClassifier(estimators=[('svm', SVC(kernel='linear', random_state=1)),\n                               ('gnb', GaussianNB())],\n                   final_estimator=LogisticRegression())svmSVCSVC(kernel='linear', random_state=1)gnbGaussianNBGaussianNB()final_estimatorLogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#예측-4",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#예측-4",
    "title": "앙상블 학습",
    "section": "예측",
    "text": "예측\n\npred_stkg = clf_stkg.predict(X_te_std)\nprint(pred_stkg)\n\n[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1\n 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0\n 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1\n 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0]"
  },
  {
    "objectID": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#평가-4",
    "href": "posts/Study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.html#평가-4",
    "title": "앙상블 학습",
    "section": "평가",
    "text": "평가\n\naccuracy = accuracy_score(y_te, pred_stkg)\nprint(accuracy)\n\nconf_matrix = confusion_matrix(y_te, pred_stkg)\nprint(conf_matrix)\n\nclass_report = classification_report(y_te, pred_stkg)\nprint(class_report)\n\n0.965034965034965\n[[50  3]\n [ 2 88]]\n              precision    recall  f1-score   support\n\n           0       0.96      0.94      0.95        53\n           1       0.97      0.98      0.97        90\n\n    accuracy                           0.97       143\n   macro avg       0.96      0.96      0.96       143\nweighted avg       0.96      0.97      0.96       143"
  },
  {
    "objectID": "posts/Study/imbalaced data/plot_comparison_under_sampling.html",
    "href": "posts/Study/imbalaced data/plot_comparison_under_sampling.html",
    "title": "plot_comparison_under_sampling",
    "section": "",
    "text": "ref: imbalaced-learn\n%matplotlib inline"
  },
  {
    "objectID": "posts/Study/imbalaced data/plot_comparison_under_sampling.html#prototype-generation-under-sampling-by-generating-new-samples",
    "href": "posts/Study/imbalaced data/plot_comparison_under_sampling.html#prototype-generation-under-sampling-by-generating-new-samples",
    "title": "plot_comparison_under_sampling",
    "section": "Prototype generation: under-sampling by generating new samples",
    "text": "Prototype generation: under-sampling by generating new samples\n:class:~imblearn.under_sampling.ClusterCentroids under-samples by replacing the original samples by the centroids of the cluster found.\n\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import MiniBatchKMeans\n\nfrom imblearn import FunctionSampler\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.under_sampling import ClusterCentroids\n\nX, y = create_dataset(n_samples=400, weights=(0.05, 0.15, 0.8), class_sep=0.8)\n\nsamplers = {\n    FunctionSampler(),  # identity resampler\n    ClusterCentroids(\n        estimator=MiniBatchKMeans(n_init=1, random_state=0), random_state=0\n    ),\n}\n\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X, y, model, ax[0], title=f\"Decision function with {sampler.__class__.__name__}\"\n    )\n    plot_resampling(X, y, sampler, ax[1])\n\nfig.tight_layout()"
  },
  {
    "objectID": "posts/Study/imbalaced data/plot_comparison_under_sampling.html#prototype-selection-under-sampling-by-selecting-existing-samples",
    "href": "posts/Study/imbalaced data/plot_comparison_under_sampling.html#prototype-selection-under-sampling-by-selecting-existing-samples",
    "title": "plot_comparison_under_sampling",
    "section": "Prototype selection: under-sampling by selecting existing samples",
    "text": "Prototype selection: under-sampling by selecting existing samples\nThe algorithm performing prototype selection can be subdivided into two groups: (i) the controlled under-sampling methods and (ii) the cleaning under-sampling methods.\nWith the controlled under-sampling methods, the number of samples to be selected can be specified. :class:~imblearn.under_sampling.RandomUnderSampler is the most naive way of performing such selection by randomly selecting a given number of samples by the targetted class.\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\nX, y = create_dataset(n_samples=400, weights=(0.05, 0.15, 0.8), class_sep=0.8)\n\nsamplers = {\n    FunctionSampler(),  # identity resampler\n    RandomUnderSampler(random_state=0),\n}\n\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X, y, model, ax[0], title=f\"Decision function with {sampler.__class__.__name__}\"\n    )\n    plot_resampling(X, y, sampler, ax[1])\n\nfig.tight_layout()\n\n\n\n\n:class:~imblearn.under_sampling.NearMiss algorithms implement some heuristic rules in order to select samples. NearMiss-1 selects samples from the majority class for which the average distance of the \\(k\\)` nearest samples of the minority class is the smallest. NearMiss-2 selects the samples from the majority class for which the average distance to the farthest samples of the negative class is the smallest. NearMiss-3 is a 2-step algorithm: first, for each minority sample, their \\(m\\) nearest-neighbors will be kept; then, the majority samples selected are the on for which the average distance to the \\(k\\) nearest neighbors is the largest.\n\nfrom imblearn.under_sampling import NearMiss\n\nX, y = create_dataset(n_samples=1000, weights=(0.05, 0.15, 0.8), class_sep=1.5)\n\nsamplers = [NearMiss(version=1), NearMiss(version=2), NearMiss(version=3)]\n\nfig, axs = plt.subplots(nrows=3, ncols=2, figsize=(15, 25))\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X,\n        y,\n        model,\n        ax[0],\n        title=f\"Decision function for {sampler.__class__.__name__}-{sampler.version}\",\n    )\n    plot_resampling(\n        X,\n        y,\n        sampler,\n        ax[1],\n        title=f\"Resampling using {sampler.__class__.__name__}-{sampler.version}\",\n    )\nfig.tight_layout()\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:203: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:203: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:203: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:203: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n  warnings.warn(\n\n\n\n\n\n:class:~imblearn.under_sampling.EditedNearestNeighbours removes samples of the majority class for which their class differ from the one of their nearest-neighbors. This sieve can be repeated which is the principle of the :class:~imblearn.under_sampling.RepeatedEditedNearestNeighbours. :class:~imblearn.under_sampling.AllKNN is slightly different from the :class:~imblearn.under_sampling.RepeatedEditedNearestNeighbours by changing the \\(k\\) parameter of the internal nearest neighors algorithm, increasing it at each iteration.\n\nfrom imblearn.under_sampling import (\n    AllKNN,\n    EditedNearestNeighbours,\n    RepeatedEditedNearestNeighbours,\n)\n\nX, y = create_dataset(n_samples=500, weights=(0.2, 0.3, 0.5), class_sep=0.8)\n\nsamplers = [\n    EditedNearestNeighbours(),\n    RepeatedEditedNearestNeighbours(),\n    AllKNN(allow_minority=True),\n]\n\nfig, axs = plt.subplots(3, 2, figsize=(15, 25))\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X, y, clf, ax[0], title=f\"Decision function for \\n{sampler.__class__.__name__}\"\n    )\n    plot_resampling(\n        X, y, sampler, ax[1], title=f\"Resampling using \\n{sampler.__class__.__name__}\"\n    )\n\nfig.tight_layout()\n\n\n\n\n:class:~imblearn.under_sampling.CondensedNearestNeighbour makes use of a 1-NN to iteratively decide if a sample should be kept in a dataset or not. The issue is that :class:~imblearn.under_sampling.CondensedNearestNeighbour is sensitive to noise by preserving the noisy samples. :class:~imblearn.under_sampling.OneSidedSelection also used the 1-NN and use :class:~imblearn.under_sampling.TomekLinks to remove the samples considered noisy. The :class:~imblearn.under_sampling.NeighbourhoodCleaningRule use a :class:~imblearn.under_sampling.EditedNearestNeighbours to remove some sample. Additionally, they use a 3 nearest-neighbors to remove samples which do not agree with this rule.\n\nfrom imblearn.under_sampling import (\n    CondensedNearestNeighbour,\n    NeighbourhoodCleaningRule,\n    OneSidedSelection,\n)\n\nX, y = create_dataset(n_samples=500, weights=(0.2, 0.3, 0.5), class_sep=0.8)\n\nfig, axs = plt.subplots(nrows=3, ncols=2, figsize=(15, 25))\n\nsamplers = [\n    CondensedNearestNeighbour(random_state=0),\n    OneSidedSelection(random_state=0),\n    NeighbourhoodCleaningRule(),\n]\n\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X, y, clf, ax[0], title=f\"Decision function for \\n{sampler.__class__.__name__}\"\n    )\n    plot_resampling(\n        X, y, sampler, ax[1], title=f\"Resampling using \\n{sampler.__class__.__name__}\"\n    )\nfig.tight_layout()\n\n\n\n\n:class:~imblearn.under_sampling.InstanceHardnessThreshold uses the prediction of classifier to exclude samples. All samples which are classified with a low probability will be removed.\n\nfrom imblearn.under_sampling import InstanceHardnessThreshold\n\nsamplers = {\n    FunctionSampler(),  # identity resampler\n    InstanceHardnessThreshold(\n        estimator=LogisticRegression(),\n        random_state=0,\n    ),\n}\n\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X,\n        y,\n        model,\n        ax[0],\n        title=f\"Decision function with \\n{sampler.__class__.__name__}\",\n    )\n    plot_resampling(\n        X, y, sampler, ax[1], title=f\"Resampling using \\n{sampler.__class__.__name__}\"\n    )\n\nfig.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/Study/imbalaced data/plot_comparison_over_sampling.html",
    "href": "posts/Study/imbalaced data/plot_comparison_over_sampling.html",
    "title": "plot_comparison_over_sampling",
    "section": "",
    "text": "ref: imbalaced-learn\n%matplotlib inline"
  },
  {
    "objectID": "posts/Study/imbalaced data/plot_comparison_over_sampling.html#illustration-of-the-influence-of-the-balancing-ratio",
    "href": "posts/Study/imbalaced data/plot_comparison_over_sampling.html#illustration-of-the-influence-of-the-balancing-ratio",
    "title": "plot_comparison_over_sampling",
    "section": "Illustration of the influence of the balancing ratio",
    "text": "Illustration of the influence of the balancing ratio\nWe will first illustrate the influence of the balancing ratio on some toy data using a logistic regression classifier which is a linear model.\n\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\n\nWe will fit and show the decision boundary model to illustrate the impact of dealing with imbalanced classes.\n\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 12))\n\nweights_arr = (\n    (0.01, 0.01, 0.98),\n    (0.01, 0.05, 0.94),\n    (0.2, 0.1, 0.7),\n    (0.33, 0.33, 0.33),\n)\nfor ax, weights in zip(axs.ravel(), weights_arr):\n    X, y = create_dataset(n_samples=300, weights=weights)\n    clf.fit(X, y)\n    plot_decision_function(X, y, clf, ax, title=f\"weight={weights}\")\n    fig.suptitle(f\"Decision function of {clf.__class__.__name__}\")\nfig.tight_layout()\n\n\n\n\nGreater is the difference between the number of samples in each class, poorer are the classification results."
  },
  {
    "objectID": "posts/Study/imbalaced data/plot_comparison_over_sampling.html#random-over-sampling-to-balance-the-data-set",
    "href": "posts/Study/imbalaced data/plot_comparison_over_sampling.html#random-over-sampling-to-balance-the-data-set",
    "title": "plot_comparison_over_sampling",
    "section": "Random over-sampling to balance the data set",
    "text": "Random over-sampling to balance the data set\nRandom over-sampling can be used to repeat some samples and balance the number of samples between the dataset. It can be seen that with this trivial approach the boundary decision is already less biased toward the majority class. The class :class:~imblearn.over_sampling.RandomOverSampler implements such of a strategy.\n\nfrom imblearn.over_sampling import RandomOverSampler\n\n\nfrom imblearn.pipeline import make_pipeline\n\nX, y = create_dataset(n_samples=100, weights=(0.05, 0.25, 0.7))\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 7))\n\nclf.fit(X, y)\nplot_decision_function(X, y, clf, axs[0], title=\"Without resampling\")\n\nsampler = RandomOverSampler(random_state=0)\nmodel = make_pipeline(sampler, clf).fit(X, y)\nplot_decision_function(X, y, model, axs[1], f\"Using {model[0].__class__.__name__}\")\n\nfig.suptitle(f\"Decision function of {clf.__class__.__name__}\")\nfig.tight_layout()\n\n\n\n\nBy default, random over-sampling generates a bootstrap. The parameter shrinkage allows adding a small perturbation to the generated data to generate a smoothed bootstrap instead. The plot below shows the difference between the two data generation strategies.\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 7))\n\nsampler.set_params(shrinkage=None)\nplot_resampling(X, y, sampler, ax=axs[0], title=\"Normal bootstrap\")\n\nsampler.set_params(shrinkage=0.3)\nplot_resampling(X, y, sampler, ax=axs[1], title=\"Smoothed bootstrap\")\n\nfig.suptitle(f\"Resampling with {sampler.__class__.__name__}\")\nfig.tight_layout()\n\n\n\n\nIt looks like more samples are generated with smoothed bootstrap. This is due to the fact that the samples generated are not superimposing with the original samples."
  },
  {
    "objectID": "posts/Study/imbalaced data/plot_comparison_over_sampling.html#more-advanced-over-sampling-using-adasyn-and-smote",
    "href": "posts/Study/imbalaced data/plot_comparison_over_sampling.html#more-advanced-over-sampling-using-adasyn-and-smote",
    "title": "plot_comparison_over_sampling",
    "section": "More advanced over-sampling using ADASYN and SMOTE",
    "text": "More advanced over-sampling using ADASYN and SMOTE\nInstead of repeating the same samples when over-sampling or perturbating the generated bootstrap samples, one can use some specific heuristic instead. :class:~imblearn.over_sampling.ADASYN and :class:~imblearn.over_sampling.SMOTE can be used in this case.\n\nfrom imblearn import FunctionSampler  # to use a idendity sampler\nfrom imblearn.over_sampling import ADASYN, SMOTE\n\nX, y = create_dataset(n_samples=150, weights=(0.1, 0.2, 0.7))\n\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\n\nsamplers = [\n    FunctionSampler(),\n    RandomOverSampler(random_state=0),\n    SMOTE(random_state=0),\n    ADASYN(random_state=0),\n]\n\nfor ax, sampler in zip(axs.ravel(), samplers):\n    title = \"Original dataset\" if isinstance(sampler, FunctionSampler) else None\n    plot_resampling(X, y, sampler, ax, title=title)\nfig.tight_layout()\n\n\n\n\nThe following plot illustrates the difference between :class:~imblearn.over_sampling.ADASYN and :class:~imblearn.over_sampling.SMOTE. :class:~imblearn.over_sampling.ADASYN will focus on the samples which are difficult to classify with a nearest-neighbors rule while regular :class:~imblearn.over_sampling.SMOTE will not make any distinction. Therefore, the decision function depending of the algorithm.\n\nX, y = create_dataset(n_samples=150, weights=(0.05, 0.25, 0.7))\n\nfig, axs = plt.subplots(nrows=1, ncols=3, figsize=(20, 6))\n\nmodels = {\n    \"Without sampler\": clf,\n    \"ADASYN sampler\": make_pipeline(ADASYN(random_state=0), clf),\n    \"SMOTE sampler\": make_pipeline(SMOTE(random_state=0), clf),\n}\n\nfor ax, (title, model) in zip(axs, models.items()):\n    model.fit(X, y)\n    plot_decision_function(X, y, model, ax=ax, title=title)\n\nfig.suptitle(f\"Decision function using a {clf.__class__.__name__}\")\nfig.tight_layout()\n\n\n\n\nDue to those sampling particularities, it can give rise to some specific issues as illustrated below.\n\nX, y = create_dataset(n_samples=5000, weights=(0.01, 0.05, 0.94), class_sep=0.8)\n\nsamplers = [SMOTE(random_state=0), ADASYN(random_state=0)]\n\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X, y, clf, ax[0], title=f\"Decision function with {sampler.__class__.__name__}\"\n    )\n    plot_resampling(X, y, sampler, ax[1])\n\nfig.suptitle(\"Particularities of over-sampling with SMOTE and ADASYN\")\nfig.tight_layout()\n\n\n\n\nSMOTE proposes several variants by identifying specific samples to consider during the resampling. The borderline version (:class:~imblearn.over_sampling.BorderlineSMOTE) will detect which point to select which are in the border between two classes. The SVM version (:class:~imblearn.over_sampling.SVMSMOTE) will use the support vectors found using an SVM algorithm to create new sample while the KMeans version (:class:~imblearn.over_sampling.KMeansSMOTE) will make a clustering before to generate samples in each cluster independently depending each cluster density.\n\nfrom sklearn.cluster import MiniBatchKMeans\n\nfrom imblearn.over_sampling import SVMSMOTE, BorderlineSMOTE, KMeansSMOTE\n\nX, y = create_dataset(n_samples=5000, weights=(0.01, 0.05, 0.94), class_sep=0.8)\n\nfig, axs = plt.subplots(5, 2, figsize=(15, 30))\n\nsamplers = [\n    SMOTE(random_state=0),\n    BorderlineSMOTE(random_state=0, kind=\"borderline-1\"),\n    BorderlineSMOTE(random_state=0, kind=\"borderline-2\"),\n    KMeansSMOTE(\n        kmeans_estimator=MiniBatchKMeans(n_init=1, random_state=0), random_state=0\n    ),\n    SVMSMOTE(random_state=0),\n]\n\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X, y, clf, ax[0], title=f\"Decision function for {sampler.__class__.__name__}\"\n    )\n    plot_resampling(X, y, sampler, ax[1])\n\nfig.suptitle(\"Decision function and resampling using SMOTE variants\")\nfig.tight_layout()\n\n\n\n\nWhen dealing with a mixed of continuous and categorical features, :class:~imblearn.over_sampling.SMOTENC is the only method which can handle this case.\n\nfrom collections import Counter\n\nfrom imblearn.over_sampling import SMOTENC\n\nrng = np.random.RandomState(42)\nn_samples = 50\n# Create a dataset of a mix of numerical and categorical data\nX = np.empty((n_samples, 3), dtype=object)\nX[:, 0] = rng.choice([\"A\", \"B\", \"C\"], size=n_samples).astype(object)\nX[:, 1] = rng.randn(n_samples)\nX[:, 2] = rng.randint(3, size=n_samples)\ny = np.array([0] * 20 + [1] * 30)\n\nprint(\"The original imbalanced dataset\")\nprint(sorted(Counter(y).items()))\nprint()\nprint(\"The first and last columns are containing categorical features:\")\nprint(X[:5])\nprint()\n\nsmote_nc = SMOTENC(categorical_features=[0, 2], random_state=0)\nX_resampled, y_resampled = smote_nc.fit_resample(X, y)\nprint(\"Dataset after resampling:\")\nprint(sorted(Counter(y_resampled).items()))\nprint()\nprint(\"SMOTE-NC will generate categories for the categorical features:\")\nprint(X_resampled[-5:])\nprint()\n\nThe original imbalanced dataset\n[(0, 20), (1, 30)]\n\nThe first and last columns are containing categorical features:\n[['C' -0.14021849735700803 2]\n ['A' -0.033193400066544886 2]\n ['C' -0.7490765234433554 1]\n ['C' -0.7783820070908942 2]\n ['A' 0.948842857719016 2]]\n\nDataset after resampling:\n[(0, 30), (1, 30)]\n\nSMOTE-NC will generate categories for the categorical features:\n[['A' 0.5246469549655818 2]\n ['B' -0.3657680728116921 2]\n ['B' 0.9344237230779993 2]\n ['B' 0.3710891618824609 2]\n ['B' 0.3327240726719727 2]]\n\n\n\nHowever, if the dataset is composed of only categorical features then one should use :class:~imblearn.over_sampling.SMOTEN.\n\nfrom imblearn.over_sampling import SMOTEN\n\n# Generate only categorical data\nX = np.array([\"A\"] * 10 + [\"B\"] * 20 + [\"C\"] * 30, dtype=object).reshape(-1, 1)\ny = np.array([0] * 20 + [1] * 40, dtype=np.int32)\n\nprint(f\"Original class counts: {Counter(y)}\")\nprint()\nprint(X[:5])\nprint()\n\nsampler = SMOTEN(random_state=0)\nX_res, y_res = sampler.fit_resample(X, y)\nprint(f\"Class counts after resampling {Counter(y_res)}\")\nprint()\nprint(X_res[-5:])\nprint()\n\nOriginal class counts: Counter({1: 40, 0: 20})\n\n[['A']\n ['A']\n ['A']\n ['A']\n ['A']]\n\nClass counts after resampling Counter({0: 40, 1: 40})\n\n[['B']\n ['B']\n ['A']\n ['B']\n ['A']]"
  },
  {
    "objectID": "posts/Study/boostcourse/0. jupyter basic.html",
    "href": "posts/Study/boostcourse/0. jupyter basic.html",
    "title": "1: 주피터 노트북 사용법",
    "section": "",
    "text": "주피터 노트북 사용법\n\nShift+Enter 키를 누르면 셀이 실행되고 커서가 다음셀로 이동\nEnter 키 누르면 편집상태로 돌아온다.\nESC 키를 누르고\n\na 키를 누르면 위에 셀이 추가 된다.\nb 키를 누르면 아래에 셀이 추가 된다.\ndd 키를 누르면 셀이 삭제 된다.\nm 키를 누르면 문서 셀로 변경 된다.\ny 키를 누르면 코드 셀로 변경 된다.\n\n단축키는 h 버튼 눌러서 확인이 가능하다\n\n\n마크다운(Markdown)이란?\n\n코드와 함께 문서화를 할 수 있다.\n문서화를 할 수 있는 문법\n\n여러 줄의 설명을\n줄바꿈으로 쓰고자 할 때\n\nprint(\"Hello World\")\n\nHello World\n\n\n\na=1\nb=2\na+b\n\n3\n\n\n\nfor i in range(10000000):\n    print(i)"
  },
  {
    "objectID": "posts/Study/boostcourse/1. file-path-setting.html",
    "href": "posts/Study/boostcourse/1. file-path-setting.html",
    "title": "2: file-path-setting",
    "section": "",
    "text": "!move \"C:\\Users\\user\\Downloads\\도로교통공단_사망 교통사고 정보_20211231.csv\" .\n# mv: 다운로드 받은 파일을 같은 경로로 옮긴다. 근데 이건 맥에서만..\n# !move: 이게 윈도우즈 환경. 윈도우즈 환경에서는 맥과 다르게 작성하는듯 \n\n        1개 파일을 이동했습니다.\n\n\n\n# 주피터 노트북이 있는 폴더의 경로를 출력한다.\n%pwd\n\n'C:\\\\Users\\\\user\\\\Untitled Folder'\n\n\n\n%ls\n\n C 드라이브의 볼륨: system\n 볼륨 일련 번호: 0819-B8FC\n\n C:\\Users\\user\\Untitled Folder 디렉터리\n\n2022-10-27  오전 10:43    &lt;DIR&gt;          .\n2022-10-27  오전 10:43    &lt;DIR&gt;          ..\n2022-10-27  오전 10:35    &lt;DIR&gt;          .ipynb_checkpoints\n2022-10-26  오후 02:24            45,288 2. 데이터 분석 준비하기.ipynb\n2022-10-27  오전 10:43             1,498 file-path-setting.ipynb\n2022-10-25  오후 01:57             2,883 jupyter basic.ipynb\n2022-10-27  오전 10:37           459,343 도로교통공단_사망 교통사고 정보_20211231.csv\n               4개 파일             509,012 바이트\n               3개 디렉터리  113,083,060,224 바이트 남음\n\n\n\nimport pandas as pd\n\n\npd.read_csv(\"data/도로교통공단_사망 교통사고 정보_20211231.csv\", encoding=\"cp949\")\n# UnicodeDecodeError 이파일의 인코딩이 UTF8이 아니라는 뜻이다.\n# ()안에서 shift+tab키를 누르면 도움말 본다.\n# 옵션에 encording=None 으로 설정되어 있음. 인코딩 지정을 해줘야 한다. 엑셀은 cp949 \n\n# 파일 옮겨주면 FileNotFoundError 가 난다. -&gt; 파일이 속해있는 곳을  적어주기 \n\n\n\n\n\n\n\n\n발생년\n발생년월일시\n주야\n요일\n사망자수\n부상자수\n중상자수\n경상자수\n부상신고자수\n발생지시도\n...\n사고유형\n가해자법규위반\n도로형태_대분류\n도로형태\n가해자_당사자종별\n피해자_당사자종별\n발생위치X(UTMK)\n발생위치Y(UTMK)\n경도\n위도\n\n\n\n\n0\n2021\n2021-01-01 03:00\n야\n금\n1\n3\n0\n3\n0\n경북\n...\n추돌\n안전운전 의무 불이행\n교차로\n교차로부근\n승용차\n승용차\n1097010.0\n1793385.0\n128.578152\n36.132653\n\n\n1\n2021\n2021-01-01 09:00\n주\n금\n1\n0\n0\n0\n0\n충남\n...\n공작물충돌\n안전운전 의무 불이행\n단일로\n기타단일로\n승용차\n없음\n902369.0\n1847109.0\n126.408201\n36.616845\n\n\n2\n2021\n2021-01-01 15:00\n주\n금\n1\n0\n0\n0\n0\n강원\n...\n측면충돌\n안전운전 의무 불이행\n교차로\n교차로내\n원동기장치자전거\n승용차\n1123975.0\n1974509.0\n128.907484\n37.761842\n\n\n3\n2021\n2021-01-01 19:00\n야\n금\n1\n0\n0\n0\n0\n전남\n...\n횡단중\n안전운전 의무 불이행\n단일로\n기타단일로\n화물차\n보행자\n886507.0\n1613961.0\n126.263573\n34.513391\n\n\n4\n2021\n2021-01-01 21:00\n야\n금\n1\n0\n0\n0\n0\n경기\n...\n기타\n기타\n단일로\n기타단일로\n승용차\n보행자\n953522.0\n1915403.0\n126.976011\n37.236327\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2811\n2021\n2021-12-31 16:00\n주\n금\n1\n0\n0\n0\n0\n경북\n...\n정면충돌\n안전운전 의무 불이행\n교차로\n교차로내\n승용차\n이륜차\n1119020.0\n1766895.0\n128.818730\n35.891434\n\n\n2812\n2021\n2021-12-31 17:00\n주\n금\n1\n0\n0\n0\n0\n제주\n...\n추돌\n안전운전 의무 불이행\n단일로\n기타단일로\n화물차\n화물차\n940588.0\n1503049.6\n126.860248\n33.517699\n\n\n2813\n2021\n2021-12-31 18:00\n야\n금\n1\n0\n0\n0\n0\n강원\n...\n횡단중\n보행자 보호의무 위반\n단일로\n기타단일로\n승용차\n보행자\n1023127.0\n1982332.0\n127.762845\n37.840465\n\n\n2814\n2021\n2021-12-31 19:00\n야\n금\n1\n0\n0\n0\n0\n경북\n...\n횡단중\n보행자 보호의무 위반\n교차로\n교차로횡단보도내\n승용차\n보행자\n1058805.0\n1824755.0\n128.155943\n36.418521\n\n\n2815\n2021\n2021-12-31 21:00\n야\n금\n1\n0\n0\n0\n0\n강원\n...\n전복\n중앙선 침범\n단일로\n기타단일로\n승용차\n없음\n1042559.0\n2010975.0\n127.985386\n38.097913\n\n\n\n\n2816 rows × 23 columns"
  },
  {
    "objectID": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#데이터-미리보기",
    "href": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#데이터-미리보기",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "데이터 미리보기",
    "text": "데이터 미리보기\n\n# sample, head, tail 통해 데이터 미리보기\ndf.head()\n\n\n\n\n\n\n\n\n기준년도\n가입자일련번호\n성별코드\n연령대코드(5세단위)\n시도코드\n신장(5Cm단위)\n체중(5Kg 단위)\n허리둘레\n시력(좌)\n시력(우)\n...\n감마지티피\n흡연상태\n음주여부\n구강검진 수검여부\n치아우식증유무\n결손치유무\n치아마모증유무\n제3대구치(사랑니)이상\n치석\n데이터공개일자\n\n\n\n\n0\n2017\n1\n1\n13\n46\n170.0\n65.0\n91.0\n1.0\n1.2\n...\n25.0\n3.0\n0.0\n1\nNaN\nNaN\nNaN\nNaN\n1.0\n20181126\n\n\n1\n2017\n2\n2\n8\n41\n150.0\n45.0\n73.4\n1.2\n1.0\n...\n10.0\n1.0\n0.0\n1\nNaN\nNaN\nNaN\nNaN\n1.0\n20181126\n\n\n2\n2017\n3\n1\n8\n45\n175.0\n75.0\n94.0\n1.0\n0.8\n...\n136.0\n1.0\n0.0\n1\nNaN\nNaN\nNaN\nNaN\n0.0\n20181126\n\n\n3\n2017\n4\n2\n12\n11\n155.0\n55.0\n67.5\n0.9\n1.0\n...\n30.0\n1.0\n1.0\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n20181126\n\n\n4\n2017\n5\n1\n8\n41\n175.0\n75.0\n93.0\n1.5\n1.5\n...\n68.0\n3.0\n0.0\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n20181126\n\n\n\n\n5 rows × 34 columns\n\n\n\n\ndf.tail()\n\n\n\n\n\n\n\n\n기준년도\n가입자일련번호\n성별코드\n연령대코드(5세단위)\n시도코드\n신장(5Cm단위)\n체중(5Kg 단위)\n허리둘레\n시력(좌)\n시력(우)\n...\n감마지티피\n흡연상태\n음주여부\n구강검진 수검여부\n치아우식증유무\n결손치유무\n치아마모증유무\n제3대구치(사랑니)이상\n치석\n데이터공개일자\n\n\n\n\n999995\n2017\n999996\n2\n9\n41\n165.0\n55.0\n70.0\n1.5\n1.5\n...\n11.0\n1.0\n1.0\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n20181126\n\n\n999996\n2017\n999997\n2\n9\n11\n165.0\n50.0\n68.0\n1.2\n1.5\n...\n11.0\n1.0\n0.0\n1\nNaN\nNaN\nNaN\nNaN\n0.0\n20181126\n\n\n999997\n2017\n999998\n2\n12\n27\n155.0\n50.0\n83.8\n0.2\n1.0\n...\n12.0\n1.0\n0.0\n1\nNaN\nNaN\nNaN\nNaN\n0.0\n20181126\n\n\n999998\n2017\n999999\n1\n11\n47\n160.0\n70.0\n99.0\n0.8\n0.9\n...\n35.0\n2.0\n1.0\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n20181126\n\n\n999999\n2017\n1000000\n2\n9\n27\n165.0\n60.0\n74.0\n1.2\n1.2\n...\n15.0\n1.0\n0.0\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n20181126\n\n\n\n\n5 rows × 34 columns\n\n\n\n\ndf.sample()\n\n\n\n\n\n\n\n\n기준년도\n가입자일련번호\n성별코드\n연령대코드(5세단위)\n시도코드\n신장(5Cm단위)\n체중(5Kg 단위)\n허리둘레\n시력(좌)\n시력(우)\n...\n감마지티피\n흡연상태\n음주여부\n구강검진 수검여부\n치아우식증유무\n결손치유무\n치아마모증유무\n제3대구치(사랑니)이상\n치석\n데이터공개일자\n\n\n\n\n1931\n2017\n1932\n1\n10\n43\n165.0\n80.0\n92.0\n1.2\n1.0\n...\n104.0\n1.0\n1.0\n1\nNaN\nNaN\nNaN\nNaN\n0.0\n20181126\n\n\n\n\n1 rows × 34 columns"
  },
  {
    "objectID": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#groupby",
    "href": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#groupby",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "groupby",
    "text": "groupby\n\n# 성별코드로 그룹화 한 데이터 세어보기\ndf.groupby?\n\n\ndf.groupby([\"성별코드\"])[\"가입자일련번호\"].count()\n\n성별코드\n1    531172\n2    468828\nName: 가입자일련번호, dtype: int64\n\n\n\n# 성별코드와 음주여부로 그룹화 하고 갯수 세기\ndf.groupby([\"성별코드\", \"음주여부\"])[\"가입자일련번호\"].count()\n\n성별코드  음주여부\n1     0.0     175150\n      1.0     355826\n2     0.0     327579\n      1.0     140920\nName: 가입자일련번호, dtype: int64\n\n\n\n# 성별코드와 음주여부로 그룹화 하고 감마지티피의 평균 구하기\n\ndf.groupby([\"성별코드\", \"음주여부\"])[\"감마지티피\"].mean()\n\n성별코드  음주여부\n1     0.0     34.710544\n      1.0     56.707919\n2     0.0     22.660238\n      1.0     25.115149\nName: 감마지티피, dtype: float64\n\n\n\n# 성별코드와 음주여부로 그룹화를 하고 감마지티피의 요약수치를 구한다.\n\ndf.groupby([\"성별코드\", \"음주여부\"])[\"감마지티피\"].describe()\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n성별코드\n음주여부\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0.0\n175139.0\n34.710544\n37.715218\n1.0\n18.0\n25.0\n38.0\n999.0\n\n\n1.0\n355819.0\n56.707919\n69.039084\n1.0\n24.0\n37.0\n63.0\n999.0\n\n\n2\n0.0\n327559.0\n22.660238\n25.181300\n1.0\n13.0\n17.0\n24.0\n999.0\n\n\n1.0\n140913.0\n25.115149\n35.870812\n1.0\n13.0\n17.0\n25.0\n999.0\n\n\n\n\n\n\n\n\n# agg를 사용하면 여러 수치를 함께 구할 수 있다.\ndf.groupby([\"성별코드\", \"음주여부\"])[\"감마지티피\"].agg([\"count\",\"mean\",\"median\"])\n\n\n\n\n\n\n\n\n\ncount\nmean\nmedian\n\n\n성별코드\n음주여부\n\n\n\n\n\n\n\n1\n0.0\n175139\n34.710544\n25.0\n\n\n1.0\n355819\n56.707919\n37.0\n\n\n2\n0.0\n327559\n22.660238\n17.0\n\n\n1.0\n140913\n25.115149\n17.0"
  },
  {
    "objectID": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#pivot_table",
    "href": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#pivot_table",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "pivot_table",
    "text": "pivot_table\n\n\n# 음주여부에 따른 그룹화된 수 피봇테이블 구하기\ndf.pivot_table(index=\"음주여부\")\n\n\n\n\n\n\n\n\n(혈청지오티)ALT\n(혈청지오티)AST\nHDL콜레스테롤\nLDL콜레스테롤\n가입자일련번호\n감마지티피\n구강검진 수검여부\n기준년도\n데이터공개일자\n성별코드\n...\n청력(우)\n청력(좌)\n체중(5Kg 단위)\n총콜레스테롤\n치석\n트리글리세라이드\n허리둘레\n혈색소\n혈청크레아티닌\n흡연상태\n\n\n음주여부\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.0\n24.107862\n25.094792\n56.161852\n114.467632\n499800.113284\n26.858541\n0.358768\n2017\n20181126\n1.651602\n...\n1.041086\n1.042881\n60.082827\n194.699007\n0.573111\n122.063887\n80.269019\n13.748950\n0.837132\n1.320330\n\n\n1.0\n27.634991\n27.069879\n57.606351\n111.444394\n500196.825986\n47.745678\n0.439257\n2017\n20181126\n1.283686\n...\n1.020745\n1.021758\n66.778226\n196.346568\n0.626153\n144.077696\n82.484576\n14.704997\n0.892460\n1.896158\n\n\n\n\n2 rows × 29 columns\n\n\n\n\ndf.pivot_table(index=\"음주여부\", values=\"가입자일련번호\", aggfunc=\"count\")\n# mean값이 기본 세팅값이므로 aggfunc로 바꿔주기. \n\n\n\n\n\n\n\n\n가입자일련번호\n\n\n음주여부\n\n\n\n\n\n0.0\n502729\n\n\n1.0\n496746\n\n\n\n\n\n\n\n\ndf.pivot_table(index=\"성별코드\", values=\"가입자일련번호\", aggfunc=\"count\")\n# data frame으로 출력된다! \n\n\n\n\n\n\n\n\n가입자일련번호\n\n\n성별코드\n\n\n\n\n\n1\n531172\n\n\n2\n468828\n\n\n\n\n\n\n\n\n# 음주여부에 따른 감마지티피의 평균 구하기\n\npd.pivot_table(df, index=\"음주여부\", values=\"감마지티피\")\n\n\n\n\n\n\n\n\n감마지티피\n\n\n음주여부\n\n\n\n\n\n0.0\n26.858541\n\n\n1.0\n47.745678\n\n\n\n\n\n\n\n\n# 기본값은 평균을 구하지만 aggfunc를 통해 지정이 가능하다.\n\npd.pivot_table(df, index=\"음주여부\", values=\"감마지티피\", aggfunc=[\"mean\", \"median\"])\n\n\n\n\n\n\n\n\nmean\nmedian\n\n\n\n감마지티피\n감마지티피\n\n\n음주여부\n\n\n\n\n\n\n0.0\n26.858541\n19.0\n\n\n1.0\n47.745678\n30.0\n\n\n\n\n\n\n\n\n# aggfunc에 describe를 사용해 통계요약값을 볼수있다.\n\npd.pivot_table(df, index=[\"성별코드\", \"음주여부\"], values=\"감마지티피\", aggfunc=\"describe\")\n\n\n\n\n\n\n\n\n\n25%\n50%\n75%\ncount\nmax\nmean\nmin\nstd\n\n\n성별코드\n음주여부\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0.0\n18.0\n25.0\n38.0\n175139.0\n999.0\n34.710544\n1.0\n37.715218\n\n\n1.0\n24.0\n37.0\n63.0\n355819.0\n999.0\n56.707919\n1.0\n69.039084\n\n\n2\n0.0\n13.0\n17.0\n24.0\n327559.0\n999.0\n22.660238\n1.0\n25.181300\n\n\n1.0\n13.0\n17.0\n25.0\n140913.0\n999.0\n25.115149\n1.0\n35.870812"
  },
  {
    "objectID": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#히스토그램",
    "href": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#히스토그램",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "히스토그램",
    "text": "히스토그램\n\ndf.hist(figsize=(12,12))\n\narray([[&lt;AxesSubplot:title={'center':'기준년도'}&gt;,\n        &lt;AxesSubplot:title={'center':'가입자일련번호'}&gt;,\n        &lt;AxesSubplot:title={'center':'성별코드'}&gt;,\n        &lt;AxesSubplot:title={'center':'연령대코드(5세단위)'}&gt;,\n        &lt;AxesSubplot:title={'center':'시도코드'}&gt;,\n        &lt;AxesSubplot:title={'center':'신장(5Cm단위)'}&gt;],\n       [&lt;AxesSubplot:title={'center':'체중(5Kg 단위)'}&gt;,\n        &lt;AxesSubplot:title={'center':'허리둘레'}&gt;,\n        &lt;AxesSubplot:title={'center':'시력(좌)'}&gt;,\n        &lt;AxesSubplot:title={'center':'시력(우)'}&gt;,\n        &lt;AxesSubplot:title={'center':'청력(좌)'}&gt;,\n        &lt;AxesSubplot:title={'center':'청력(우)'}&gt;],\n       [&lt;AxesSubplot:title={'center':'수축기혈압'}&gt;,\n        &lt;AxesSubplot:title={'center':'이완기혈압'}&gt;,\n        &lt;AxesSubplot:title={'center':'식전혈당(공복혈당)'}&gt;,\n        &lt;AxesSubplot:title={'center':'총콜레스테롤'}&gt;,\n        &lt;AxesSubplot:title={'center':'트리글리세라이드'}&gt;,\n        &lt;AxesSubplot:title={'center':'HDL콜레스테롤'}&gt;],\n       [&lt;AxesSubplot:title={'center':'LDL콜레스테롤'}&gt;,\n        &lt;AxesSubplot:title={'center':'혈색소'}&gt;,\n        &lt;AxesSubplot:title={'center':'요단백'}&gt;,\n        &lt;AxesSubplot:title={'center':'혈청크레아티닌'}&gt;,\n        &lt;AxesSubplot:title={'center':'(혈청지오티)AST'}&gt;,\n        &lt;AxesSubplot:title={'center':'(혈청지오티)ALT'}&gt;],\n       [&lt;AxesSubplot:title={'center':'감마지티피'}&gt;,\n        &lt;AxesSubplot:title={'center':'흡연상태'}&gt;,\n        &lt;AxesSubplot:title={'center':'음주여부'}&gt;,\n        &lt;AxesSubplot:title={'center':'구강검진 수검여부'}&gt;,\n        &lt;AxesSubplot:title={'center':'치아우식증유무'}&gt;,\n        &lt;AxesSubplot:title={'center':'결손치유무'}&gt;],\n       [&lt;AxesSubplot:title={'center':'치아마모증유무'}&gt;,\n        &lt;AxesSubplot:title={'center':'제3대구치(사랑니)이상'}&gt;,\n        &lt;AxesSubplot:title={'center':'치석'}&gt;,\n        &lt;AxesSubplot:title={'center':'데이터공개일자'}&gt;, &lt;AxesSubplot:&gt;,\n        &lt;AxesSubplot:&gt;]], dtype=object)\n\n\n\n\n\n\nh = df.hist(figsize=(12,12))"
  },
  {
    "objectID": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#슬라이싱을-사용해-히스토그램-그래기",
    "href": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#슬라이싱을-사용해-히스토그램-그래기",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "슬라이싱을 사용해 히스토그램 그래기",
    "text": "슬라이싱을 사용해 히스토그램 그래기\n\n# [행, 열]\ndf.iloc[:,:] # 몇번째에 있는 행인지, 컬럼인지 지정 가능\n# [:,:] 전체데이터\n\n\n\n\n\n\n\n\n기준년도\n가입자일련번호\n성별코드\n연령대코드(5세단위)\n시도코드\n신장(5Cm단위)\n체중(5Kg 단위)\n허리둘레\n시력(좌)\n시력(우)\n...\n감마지티피\n흡연상태\n음주여부\n구강검진 수검여부\n치아우식증유무\n결손치유무\n치아마모증유무\n제3대구치(사랑니)이상\n치석\n데이터공개일자\n\n\n\n\n0\n2017\n1\n1\n13\n46\n170.0\n65.0\n91.0\n1.0\n1.2\n...\n25.0\n3.0\n0.0\n1\nNaN\nNaN\nNaN\nNaN\n1.0\n20181126\n\n\n1\n2017\n2\n2\n8\n41\n150.0\n45.0\n73.4\n1.2\n1.0\n...\n10.0\n1.0\n0.0\n1\nNaN\nNaN\nNaN\nNaN\n1.0\n20181126\n\n\n2\n2017\n3\n1\n8\n45\n175.0\n75.0\n94.0\n1.0\n0.8\n...\n136.0\n1.0\n0.0\n1\nNaN\nNaN\nNaN\nNaN\n0.0\n20181126\n\n\n3\n2017\n4\n2\n12\n11\n155.0\n55.0\n67.5\n0.9\n1.0\n...\n30.0\n1.0\n1.0\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n20181126\n\n\n4\n2017\n5\n1\n8\n41\n175.0\n75.0\n93.0\n1.5\n1.5\n...\n68.0\n3.0\n0.0\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n20181126\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n999995\n2017\n999996\n2\n9\n41\n165.0\n55.0\n70.0\n1.5\n1.5\n...\n11.0\n1.0\n1.0\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n20181126\n\n\n999996\n2017\n999997\n2\n9\n11\n165.0\n50.0\n68.0\n1.2\n1.5\n...\n11.0\n1.0\n0.0\n1\nNaN\nNaN\nNaN\nNaN\n0.0\n20181126\n\n\n999997\n2017\n999998\n2\n12\n27\n155.0\n50.0\n83.8\n0.2\n1.0\n...\n12.0\n1.0\n0.0\n1\nNaN\nNaN\nNaN\nNaN\n0.0\n20181126\n\n\n999998\n2017\n999999\n1\n11\n47\n160.0\n70.0\n99.0\n0.8\n0.9\n...\n35.0\n2.0\n1.0\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n20181126\n\n\n999999\n2017\n1000000\n2\n9\n27\n165.0\n60.0\n74.0\n1.2\n1.2\n...\n15.0\n1.0\n0.0\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n20181126\n\n\n\n\n1000000 rows × 34 columns\n\n\n\n\ndf.iloc[:,:12].hist(figsize=(12,12))\n\narray([[&lt;AxesSubplot:title={'center':'기준년도'}&gt;,\n        &lt;AxesSubplot:title={'center':'가입자일련번호'}&gt;,\n        &lt;AxesSubplot:title={'center':'성별코드'}&gt;],\n       [&lt;AxesSubplot:title={'center':'연령대코드(5세단위)'}&gt;,\n        &lt;AxesSubplot:title={'center':'시도코드'}&gt;,\n        &lt;AxesSubplot:title={'center':'신장(5Cm단위)'}&gt;],\n       [&lt;AxesSubplot:title={'center':'체중(5Kg 단위)'}&gt;,\n        &lt;AxesSubplot:title={'center':'허리둘레'}&gt;,\n        &lt;AxesSubplot:title={'center':'시력(좌)'}&gt;],\n       [&lt;AxesSubplot:title={'center':'시력(우)'}&gt;,\n        &lt;AxesSubplot:title={'center':'청력(좌)'}&gt;,\n        &lt;AxesSubplot:title={'center':'청력(우)'}&gt;]], dtype=object)\n\n\n\n\n\n\n# 슬라이싱을 사용해 앞에서 12번째부터 23번쨰까지 (12:24) \nh = df.iloc[:,12:24].hist(figsize=(12,12), bins=100)   # bins : 막대 개수를 더 ...  연속된 수치데이터를 카테고리 형태로!!"
  },
  {
    "objectID": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#countplot---음주여부",
    "href": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#countplot---음주여부",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "countplot - 음주여부",
    "text": "countplot - 음주여부\n\n# 음주여부에 따른 countplot을 그린다\ndf[\"음주여부\"].value_counts().plot.bar()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nsns.countplot(x=\"음주여부\", data=df)\n\n&lt;AxesSubplot:xlabel='음주여부', ylabel='count'&gt;\n\n\n\n\n\n\nsns.countplot(x=\"흡연상태\", data=df)\n\n&lt;AxesSubplot:xlabel='흡연상태', ylabel='count'&gt;"
  },
  {
    "objectID": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#hue-옵션-사용하기",
    "href": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#hue-옵션-사용하기",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "hue 옵션 사용하기",
    "text": "hue 옵션 사용하기\n\n#window\n# sns.set(font_scale=1.5, font=\"Malgun Gothic\") 이렇게도 사용 가능\n\nsns.countplot(data=df, x=\"음주여부\", hue=\"성별코드\")\n\n&lt;AxesSubplot:xlabel='음주여부', ylabel='count'&gt;\n\n\n\n\n\n\n\nsns.countplot(data=df, x=\"연령대코드(5세단위)\", hue=\"음주여부\") \n\n&lt;AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='count'&gt;"
  },
  {
    "objectID": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#countplot---키와-몸무게",
    "href": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#countplot---키와-몸무게",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "countplot - 키와 몸무게",
    "text": "countplot - 키와 몸무게\n\n키와 몸무게는 연속형 데이터이다.\n하지만 데이터는 키는 5cm, 체중은 5kg 단위로 되어 있다.\n이렇게 특정 범위로 묶게 되면 연속형 데이터라기 보다는 범주형 데이터라고 본다.\n\n\nplt.figure(figsize=(15,4))\nsns.countplot(data=df, x=\"신장(5Cm단위)\")\n\n&lt;AxesSubplot:xlabel='신장(5Cm단위)', ylabel='count'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.countplot(data=df, x=\"체중(5Kg 단위)\")\n\n&lt;AxesSubplot:xlabel='체중(5Kg 단위)', ylabel='count'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.countplot(data=df, x=\"신장(5Cm단위)\", hue=\"성별코드\")\n\n&lt;AxesSubplot:xlabel='신장(5Cm단위)', ylabel='count'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.countplot(data=df, x=\"체중(5Kg 단위)\", hue=\"성별코드\")\n\n&lt;AxesSubplot:xlabel='체중(5Kg 단위)', ylabel='count'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.countplot(data=df, x=\"체중(5Kg 단위)\", hue=\"음주여부\")\n\n&lt;AxesSubplot:xlabel='체중(5Kg 단위)', ylabel='count'&gt;"
  },
  {
    "objectID": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#braplot---수치형-vs-범주형",
    "href": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#braplot---수치형-vs-범주형",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "14. 4 braplot - 수치형 vs 범주형",
    "text": "14. 4 braplot - 수치형 vs 범주형\n\n# 연령대코드와 총 콜레스테롤 보기\n# hue로 색상 다르게 표현. 음주여부 같이 보기\n\nsns.barplot(data=df, x=\"연령대코드(5세단위)\", y=\"총콜레스테롤\", hue=\"음주여부\")\n\n# 느리다! countplot와 비교했을때 ! 백만개의 데이터가 있는데.. 연령대 코드별로 총 콜레스테롤을 그리긴 했지만. .\n\n&lt;AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='총콜레스테롤'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.barplot(data=df, x=\"연령대코드(5세단위)\", y=\"총콜레스테롤\", hue=\"흡연상태\")\n\n&lt;AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='총콜레스테롤'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.barplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"총콜레스테롤\", hue=\"흡연상태\")\n\n&lt;AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='총콜레스테롤'&gt;\n\n\n\n\n\n\n#트리글리세라이드(중성지방)에 따른 연령대코드(5세단위)를 음주여부에 따라 barplot로 그리기\nsns.barplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"트리글리세라이드\", hue=\"음주여부\", ci=95)\n\n# 검은색 막대: 신뢰구간을 의미 (ci: 95)-&gt; 95%의 신뢰구간을 표시한다.\n\n&lt;AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='트리글리세라이드'&gt;\n\n\n\n\n\n\nsns.barplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"트리글리세라이드\", hue=\"음주여부\", ci=\"sd\") #sd:표준편차\n# sample로 그려서 편차가 커보인다. \n\n&lt;AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='트리글리세라이드'&gt;\n\n\n\n\n\n\nsns.barplot(data=df, x=\"연령대코드(5세단위)\", y=\"트리글리세라이드\", hue=\"음주여부\", ci=\"sd\") #sd:표준편차\n\n&lt;AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='트리글리세라이드'&gt;\n\n\n\n\n\n\nsns.barplot(data=df, x=\"연령대코드(5세단위)\", y=\"트리글리세라이드\", hue=\"음주여부\", ci=None) #sd:표준편차\n\n&lt;AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='트리글리세라이드'&gt;\n\n\n\n\n\n\n# 연령대코드와 체중(5kg 단위)을 성별에 따라서.\nsns.barplot(data=df, x=\"연령대코드(5세단위)\", y=\"체중(5Kg 단위)\", hue=\"성별코드\", ci=None) \n\n&lt;AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='체중(5Kg 단위)'&gt;\n\n\n\n\n\n\nsns.barplot(data=df, x=\"연령대코드(5세단위)\", y=\"체중(5Kg 단위)\", hue=\"음주여부\", ci=None)\n\n&lt;AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='체중(5Kg 단위)'&gt;"
  },
  {
    "objectID": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#lineplot-and-pointplot",
    "href": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#lineplot-and-pointplot",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "lineplot and pointplot",
    "text": "lineplot and pointplot\n\nsns.lineplot(data=df, x=\"연령대코드(5세단위)\", y=\"체중(5Kg 단위)\", hue=\"성별코드\", ci=None)\n\n&lt;AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='체중(5Kg 단위)'&gt;\n\n\n\n\n\n\nsns.lineplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"체중(5Kg 단위)\", hue=\"성별코드\")\n# 그림자로 표시!! \n\n&lt;AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='체중(5Kg 단위)'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.lineplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"체중(5Kg 단위)\", hue=\"성별코드\", ci=\"sd\")\n# 그림자로 표시!! \n\n&lt;AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='체중(5Kg 단위)'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.lineplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"신장(5Cm단위)\", hue=\"성별코드\", ci=\"sd\")\n\n&lt;AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='신장(5Cm단위)'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.lineplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"신장(5Cm단위)\", hue=\"음주여부\", ci=\"sd\")\n\n&lt;AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='신장(5Cm단위)'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.pointplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"신장(5Cm단위)\", hue=\"음주여부\", ci=\"sd\")\n\n&lt;AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='신장(5Cm단위)'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.barplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"신장(5Cm단위)\", hue=\"음주여부\", ci=\"sd\")\nsns.pointplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"신장(5Cm단위)\", hue=\"음주여부\", ci=\"sd\")\n# 두개를 겹쳐서 그릴 수도 있다.\n\n&lt;AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='신장(5Cm단위)'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.lineplot(data=df, x=\"연령대코드(5세단위)\", y=\"혈색소\", hue=\"음주여부\", ci=None)\n\n&lt;AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='혈색소'&gt;"
  },
  {
    "objectID": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#boxplot",
    "href": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#boxplot",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "boxplot",
    "text": "boxplot\n\n# boxplot으로 신장에 따른 체중을 그리며, 성별코드에 따른 색상으로 표현하기\nplt.figure(figsize=(15,4))\nsns.boxplot(data=df, x=\"신장(5Cm단위)\", y=\"체중(5Kg 단위)\", hue=\"성별코드\")\n\n&lt;AxesSubplot:xlabel='신장(5Cm단위)', ylabel='체중(5Kg 단위)'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.boxplot(data=df, x=\"신장(5Cm단위)\", y=\"체중(5Kg 단위)\", hue=\"음주여부\")\n\n&lt;AxesSubplot:xlabel='신장(5Cm단위)', ylabel='체중(5Kg 단위)'&gt;"
  },
  {
    "objectID": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#violinplot",
    "href": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#violinplot",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "violinplot",
    "text": "violinplot\n\nplt.figure(figsize=(15,4))\nsns.violinplot(data=df, x=\"신장(5Cm단위)\", y=\"체중(5Kg 단위)\")\n\n&lt;AxesSubplot:xlabel='신장(5Cm단위)', ylabel='체중(5Kg 단위)'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.violinplot(data=df_sample, x=\"신장(5Cm단위)\", y=\"체중(5Kg 단위)\", hue=\"음주여부\")\n\n&lt;AxesSubplot:xlabel='신장(5Cm단위)', ylabel='체중(5Kg 단위)'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.violinplot(data=df_sample, x=\"신장(5Cm단위)\", y=\"체중(5Kg 단위)\", hue=\"음주여부\", split=True)\n# split : 두개의 값을 합쳐서 그림\n\n&lt;AxesSubplot:xlabel='신장(5Cm단위)', ylabel='체중(5Kg 단위)'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.violinplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"혈색소\", hue=\"음주여부\", split=True)\n\n&lt;AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='혈색소'&gt;"
  },
  {
    "objectID": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#swarm-plot",
    "href": "posts/Study/boostcourse/4. 건강검진 데이터로 가설검정.html#swarm-plot",
    "title": "5: 건강검진 데이터로 가설검정",
    "section": "swarm plot",
    "text": "swarm plot\n\n범주형 데이터를 산점도로 시각화하고자 할 대 사용한다.\n\n\nplt.figure(figsize=(15,4))\nsns.swarmplot(data=df_sample, x=\"신장(5Cm단위)\", y=\"체중(5Kg 단위)\", hue=\"음주여부\")\n\nC:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 37.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\nC:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 52.7% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\nC:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 51.9% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\nC:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 55.4% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\nC:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 48.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\nC:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 26.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n&lt;AxesSubplot:xlabel='신장(5Cm단위)', ylabel='체중(5Kg 단위)'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(15,4))\nsns.swarmplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"혈색소\", hue=\"음주여부\")\n\nC:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 7.1% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\nC:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 12.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\nC:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 9.3% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\nC:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 11.2% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n&lt;AxesSubplot:xlabel='연령대코드(5세단위)', ylabel='혈색소'&gt;\n\n\n\n\n\n\n# lmplot 으로 그리기\nsns.lmplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"혈색소\", hue=\"음주여부\")\n# 회귀선을 그려서 상관관계를 보여준다.\n\n\n\n\n\n# lmplot 으로 그리기\nsns.lmplot(data=df_sample, x=\"연령대코드(5세단위)\", y=\"혈색소\", hue=\"음주여부\", col=\"성별코드\")\n# 회귀선을 그려서 상관관계를 보여준다.\n# col통해서 여러게 나오게 한다."
  },
  {
    "objectID": "posts/Study/Deep learning with pytorch/tensor basic.html",
    "href": "posts/Study/Deep learning with pytorch/tensor basic.html",
    "title": "Tensor Basic",
    "section": "",
    "text": "Book: 딥러닝 파이토치 교과서(서지영 지음, 길벗 출판사)\ngithub: https://github.com/gilbutITbook/080289/tree/main"
  },
  {
    "objectID": "posts/Study/Deep learning with pytorch/tensor basic.html#ref",
    "href": "posts/Study/Deep learning with pytorch/tensor basic.html#ref",
    "title": "Tensor Basic",
    "section": "",
    "text": "Book: 딥러닝 파이토치 교과서(서지영 지음, 길벗 출판사)\ngithub: https://github.com/gilbutITbook/080289/tree/main"
  },
  {
    "objectID": "posts/Study/Deep learning with pytorch/tensor basic.html#basic",
    "href": "posts/Study/Deep learning with pytorch/tensor basic.html#basic",
    "title": "Tensor Basic",
    "section": "Basic",
    "text": "Basic\n\nimport torch\n\n\nprint(torch.tensor([[1,2],[3,4]]))\n\ntensor([[1, 2],\n        [3, 4]])\n\n\n\nprint(torch.tensor([[1,2],[3,4]],device=\"cuda:0\"))\n\ntensor([[1, 2],\n        [3, 4]], device='cuda:0')\n\n\n\nprint(torch.tensor([[1,2],[3,4]],dtype=torch.float64))\n\ntensor([[1., 2.],\n        [3., 4.]], dtype=torch.float64)\n\n\n- ndarray 변환\n\ntemp = torch.tensor([[1,2],[3,4]])\nprint(temp.numpy())\n\n[[1 2]\n [3 4]]\n\n\n\ntemp = torch.tensor([[1,2],[3,4]],device=\"cuda:0\")\nprint(temp.to(\"cpu\").numpy())\n\n[[1 2]\n [3 4]]\n\n\n- 텐서 차원 조작\n\ntemp = torch.tensor([[1,2],[3,4]])\n\n\ntemp.shape\n\ntorch.Size([2, 2])\n\n\n\ntemp.view(4,1)\n\ntensor([[1],\n        [2],\n        [3],\n        [4]])\n\n\n\ntemp.view(-1)\n\ntensor([1, 2, 3, 4])\n\n\n\ntemp.view(1,-1)\n\ntensor([[1, 2, 3, 4]])\n\n\n\ntemp.view(-1,1)\n\ntensor([[1],\n        [2],\n        [3],\n        [4]])"
  },
  {
    "objectID": "posts/Study/Deep learning with pytorch/tensor basic.html#모델-정의",
    "href": "posts/Study/Deep learning with pytorch/tensor basic.html#모델-정의",
    "title": "Tensor Basic",
    "section": "모델 정의",
    "text": "모델 정의\n- nn.Module() 상속하여 정의\nclass MLP(Module):\n    def __init__(self, inputs):\n        super(MLP, self).__init__()\n        self.layer = Linear(inputs,1) # 계층 정의\n        self.activation = Sigmoid() # 활성화 함수 정의\n        \n    def forward(self, X):\n        X = self.layer(X)\n        X = self.activation(X)\n        return X\n- Sequential 신경망 정의하는 방법\nimport torch.nn as nn\nclass MLP(nn.Module):\n    def __init__(self):\n        super(MLP, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2))\n        \n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=30, kernel_size=5),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2))\n        \n        self.layer3 = nn.Sequential(\n            nn.Linear(in_features=30*5*5, out_features=10, bias=True),\n            nn.ReLU(inplace=True))\n        \n        def forward(self,x):\n            x = self.layer1(x)\n            x = self.layer2(x)\n            x = x.view(x.shape[0],-1)\n            x = slelf.layer3(x)\n            return x\n        \n    model = MLP()   # 모델에 대한 객체 생성"
  },
  {
    "objectID": "posts/Study/Deep learning with pytorch/tensor basic.html#모델의-파라미터-정의",
    "href": "posts/Study/Deep learning with pytorch/tensor basic.html#모델의-파라미터-정의",
    "title": "Tensor Basic",
    "section": "모델의 파라미터 정의",
    "text": "모델의 파라미터 정의\n- 손실 함수 (loss function)\n\n\\(y\\)와 \\(\\hat y\\)의 오차를 구해서 모델의 정확성을 측정\nBCELoss: 이진 분류\nCrossEntropyLoss: 다중 클래스 분류\nMSELoss: 회귀 모델\n\n- 옵티마이저 (optimizer)\n\n데이터의 손실 함수를 바탕으로 모델의 업데이트 방법 결정\ntorch.optim.Optimizer(params, defaults)는 옵티마이저 기본 클래스\nzero_grad() 옵티마이저 사용된 파라미터의 기울기 0으로\ntorch.optim.lr_scheduler 에폭에 따라 학습률 조절\n종류\n\n&lt;optim Adadelta, optim.Adagrad, optim.Adam, optim.SparseAdam, optim.Adamax&gt;\n&lt;optim.ASGD, optim.LBFGS&gt;\n&lt;optim.RMSProp, optim.Rprop, optim.SGD&gt;"
  },
  {
    "objectID": "posts/Study/Deep learning with pytorch/tensor basic.html#모델-훈련",
    "href": "posts/Study/Deep learning with pytorch/tensor basic.html#모델-훈련",
    "title": "Tensor Basic",
    "section": "모델 훈련",
    "text": "모델 훈련\n- 예시 코드\nfor epoch in range(100):\n    yhat = model(x_train)\n    loss = criterion(yhat, y_train)\n    optimizer.zero_gard()\n    loss.backward()\n    optimizer.step()"
  },
  {
    "objectID": "posts/Study/Deep learning with pytorch/tensor basic.html#예시",
    "href": "posts/Study/Deep learning with pytorch/tensor basic.html#예시",
    "title": "Tensor Basic",
    "section": "예시",
    "text": "예시\n\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\ndataset = pd.read_csv('car_evaluation.csv')\n\n\ndataset.head()\n\n\n\n\n\n\n\n\nprice\nmaint\ndoors\npersons\nlug_capacity\nsafety\noutput\n\n\n\n\n0\nvhigh\nvhigh\n2\n2\nsmall\nlow\nunacc\n\n\n1\nvhigh\nvhigh\n2\n2\nsmall\nmed\nunacc\n\n\n2\nvhigh\nvhigh\n2\n2\nsmall\nhigh\nunacc\n\n\n3\nvhigh\nvhigh\n2\n2\nmed\nlow\nunacc\n\n\n4\nvhigh\nvhigh\n2\n2\nmed\nmed\nunacc\n\n\n\n\n\n\n\n\noutput(차상태): unacc(허용 불가능한 수준), 양호(good), 매우 좋은(very good, vgood)\n\n\ndataset.head()\n\n\n\n\n\n\n\n\nprice\nmaint\ndoors\npersons\nlug_capacity\nsafety\noutput\n\n\n\n\n0\nvhigh\nvhigh\n2\n2\nsmall\nlow\nunacc\n\n\n1\nvhigh\nvhigh\n2\n2\nsmall\nmed\nunacc\n\n\n2\nvhigh\nvhigh\n2\n2\nsmall\nhigh\nunacc\n\n\n3\nvhigh\nvhigh\n2\n2\nmed\nlow\nunacc\n\n\n4\nvhigh\nvhigh\n2\n2\nmed\nmed\nunacc\n\n\n\n\n\n\n\n\nfig_size = plt.rcParams[\"figure.figsize\"]\nfig_size[0] = 8\nfig_size[1] = 6\nplt.rcParams[\"figure.figsize\"] = fig_size\ndataset.output.value_counts().plot(kind='pie', autopct='%0.05f%%', colors=['lightblue', 'lightgreen', 'orange', 'pink'], explode=(0.05, 0.05, 0.05, 0.05))\n\n&lt;Axes: ylabel='output'&gt;\n\n\n\n\n\n\ncategorical_columns = ['price', 'maint', 'doors', 'persons', 'lug_capacity', 'safety']\n\n\nfor category in categorical_columns:\n    dataset[category] = dataset[category].astype('category')\n\n\nastype() 메서드를 이용하여 데이터를 범주형으로 변환\n\n- 범주형 데이터를 텐서로 변환하기 위해서는\n범주형 데이터 -&gt; dataser[category] -&gt; 넘파이 배열(NumPy array) -&gt; 텐서(Tensor)\n\nprice = dataset['price'].cat.codes.values\nmaint = dataset['maint'].cat.codes.values\ndoors = dataset['doors'].cat.codes.values\npersons = dataset['persons'].cat.codes.values\nlug_capacity = dataset['lug_capacity'].cat.codes.values\nsafety = dataset['safety'].cat.codes.values\n\ncategorical_data = np.stack([price, maint, doors, persons, lug_capacity, safety], 1)\ncategorical_data[:10]\n\narray([[3, 3, 0, 0, 2, 1],\n       [3, 3, 0, 0, 2, 2],\n       [3, 3, 0, 0, 2, 0],\n       [3, 3, 0, 0, 1, 1],\n       [3, 3, 0, 0, 1, 2],\n       [3, 3, 0, 0, 1, 0],\n       [3, 3, 0, 0, 0, 1],\n       [3, 3, 0, 0, 0, 2],\n       [3, 3, 0, 0, 0, 0],\n       [3, 3, 0, 1, 2, 1]], dtype=int8)\n\n\n- 배열 텐서로 변환\n\ncategorical_data = torch.tensor(categorical_data, dtype=torch.int64)\ncategorical_data[:10]\n\ntensor([[3, 3, 0, 0, 2, 1],\n        [3, 3, 0, 0, 2, 2],\n        [3, 3, 0, 0, 2, 0],\n        [3, 3, 0, 0, 1, 1],\n        [3, 3, 0, 0, 1, 2],\n        [3, 3, 0, 0, 1, 0],\n        [3, 3, 0, 0, 0, 1],\n        [3, 3, 0, 0, 0, 2],\n        [3, 3, 0, 0, 0, 0],\n        [3, 3, 0, 1, 2, 1]])\n\n\n- 레이블(outputs) 사용할 칼럼을 텐서로 변환\n\noutputs = pd.get_dummies(dataset.output)\noutputs = outputs.values\noutputs = torch.tensor(outputs).flatten()\n\nprint(categorical_data.shape)\nprint(outputs.shape)\n\ntorch.Size([1728, 6])\ntorch.Size([6912])\n\n\n\nnote: revel(), reshape(), flatten()\n텐서 차원 변경\n\n\ncategorical_column_sizes = [len(dataset[column].cat.categories) for column in categorical_columns]\ncategorical_embedding_sizes = [(col_size, min(50, (col_size+1)//2)) for col_size in categorical_column_sizes]\nprint(categorical_embedding_sizes)\n\n[(4, 2), (4, 2), (4, 2), (3, 2), (3, 2), (3, 2)]\n\n\n- test/train\n\ntotal_records = 1728\ntest_records = int(total_records * .2)   # 20%만 테스트 용도로 사용\n\ncategorical_train_data = categorical_data[:total_records-test_records]\ncategorical_test_data = categorical_data[total_records-test_records:total_records]\ntrain_outputs = outputs[:total_records-test_records]\ntest_outputs = outputs[total_records-test_records:total_records]\n\n\nprint(len(categorical_train_data))\nprint(len(train_outputs))\nprint(len(categorical_test_data))\nprint(len(test_outputs))\n\n1383\n1383\n345\n345\n\n\n- 네트워크 생성\n\nclass Model(nn.Module):\n    def __init__(self, embedding_size, output_size, layers, p=0.4):\n        super().__init__()\n        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])\n        self.embedding_dropout = nn.Dropout(p)\n        \n        all_layers = []\n        num_categorical_cols = sum((nf for ni, nf in embedding_size))\n        input_size = num_categorical_cols \n\n        for i in layers:\n            all_layers.append(nn.Linear(input_size, i))\n            all_layers.append(nn.ReLU(inplace=True))\n            all_layers.append(nn.BatchNorm1d(i)) # 배치정규화.. 평균0 분산 1\n            all_layers.append(nn.Dropout(p))\n            input_size = i\n\n        all_layers.append(nn.Linear(layers[-1], output_size))\n        self.layers = nn.Sequential(*all_layers)\n\n    def forward(self, x_categorical):\n        embeddings = []\n        for i,e in enumerate(self.all_embeddings):\n            embeddings.append(e(x_categorical[:,i]))\n        x = torch.cat(embeddings, 1)\n        x = self.embedding_dropout(x)\n        x = self.layers(x)\n        return x\n\n\nmodel = Model(categorical_embedding_sizes, 4, [200,100,50], p=0.4)\nprint(model)\n\nModel(\n  (all_embeddings): ModuleList(\n    (0-2): 3 x Embedding(4, 2)\n    (3-5): 3 x Embedding(3, 2)\n  )\n  (embedding_dropout): Dropout(p=0.4, inplace=False)\n  (layers): Sequential(\n    (0): Linear(in_features=12, out_features=200, bias=True)\n    (1): ReLU(inplace=True)\n    (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.4, inplace=False)\n    (4): Linear(in_features=200, out_features=100, bias=True)\n    (5): ReLU(inplace=True)\n    (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.4, inplace=False)\n    (8): Linear(in_features=100, out_features=50, bias=True)\n    (9): ReLU(inplace=True)\n    (10): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (11): Dropout(p=0.4, inplace=False)\n    (12): Linear(in_features=50, out_features=4, bias=True)\n  )\n)\n\n\n\n\nloss_function = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n\n\nepochs = 500\naggregated_losses = []\ntrain_outputs = train_outputs.to(device=device, dtype=torch.int64)\nfor i in range(epochs):\n    i += 1\n    y_pred = model(categorical_train_data)\n    single_loss = loss_function(y_pred, train_outputs)\n    aggregated_losses.append(single_loss)   # 반복할 때마다 오차 aggregated_losses에 추가\n\n    if i%25 == 1:\n        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n\n    optimizer.zero_grad()\n    single_loss.backward()   # 가중치 업데이트\n    optimizer.step()         # 기울기 업데이트\n\nprint(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n\n\ntest_outputs = test_outputs.to(device=device, dtype=torch.int64)\nwith torch.no_grad():\n    y_val = model(categorical_test_data)\n    loss = loss_function(y_val, test_outputs)\nprint(f'Loss: {loss:.8f}')\n\n\nprint(y_val[:5])\n\n\ny_val = np.argmax(y_val, axis=1)\nprint(y_val[:5])\n\n\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(confusion_matrix(test_outputs,y_val))\nprint(classification_report(test_outputs,y_val))\nprint(accuracy_score(test_outputs, y_val))"
  },
  {
    "objectID": "posts/Study/autogluon fraud.html",
    "href": "posts/Study/autogluon fraud.html",
    "title": "Autogluon fraud",
    "section": "",
    "text": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\nimport pandas as pd\nimport numpy as np\nfrom autogluon.tabular import TabularPredictor\n\n# directory = '~/IEEEfraud/'  # directory where you have downloaded the data CSV files from the competition\nlabel = 'isFraud'  # name of target variable to predict in this competition\neval_metric = 'roc_auc'  # Optional: specify that competition evaluation metric is AUC\n# save_path = directory + 'AutoGluonModels/'  # where to store trained models\n\n\n!kaggle competitions download -c ieee-fraud-detection\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/coco/.kaggle/kaggle.json'\nDownloading ieee-fraud-detection.zip to /home/coco/Dropbox/coco/posts/study\n 99%|███████████████████████████████████████▌| 117M/118M [00:09&lt;00:00, 18.2MB/s]\n100%|████████████████████████████████████████| 118M/118M [00:09&lt;00:00, 13.5MB/s]\n\n\n\n!unzip ieee-fraud-detection.zip -d ./ieee-fraud-detection\n\nArchive:  ieee-fraud-detection.zip\n  inflating: ./ieee-fraud-detection/sample_submission.csv  \n  inflating: ./ieee-fraud-detection/test_identity.csv  \n  inflating: ./ieee-fraud-detection/test_transaction.csv  \n  inflating: ./ieee-fraud-detection/train_identity.csv  \n  inflating: ./ieee-fraud-detection/train_transaction.csv  \n\n\n\ntrain_identity = pd.read_csv('ieee-fraud-detection/train_identity.csv')\ntrain_transaction = pd.read_csv('ieee-fraud-detection/train_transaction.csv')\n\n\n# !kaggle competitions download -c ieee-fraud-detection\n# !unzip titanic.zip -d ./ieee-fraud-detection\n# df_train = pd.read_csv('titanic/train.csv')\n# df_test = pd.read_csv('titanic/test.csv')\n# !rm titanic.zip\n# !rm -rf titanic/\n\n\ntrain_data = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n\n\ntrain_data\n\n\n\n\n\n\n\n\nTransactionID\nisFraud\nTransactionDT\nTransactionAmt\nProductCD\ncard1\ncard2\ncard3\ncard4\ncard5\n...\nid_31\nid_32\nid_33\nid_34\nid_35\nid_36\nid_37\nid_38\nDeviceType\nDeviceInfo\n\n\n\n\n0\n2987000\n0\n86400\n68.50\nW\n13926\nNaN\n150.0\ndiscover\n142.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n2987001\n0\n86401\n29.00\nW\n2755\n404.0\n150.0\nmastercard\n102.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n2987002\n0\n86469\n59.00\nW\n4663\n490.0\n150.0\nvisa\n166.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n2987003\n0\n86499\n50.00\nW\n18132\n567.0\n150.0\nmastercard\n117.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n2987004\n0\n86506\n50.00\nH\n4497\n514.0\n150.0\nmastercard\n102.0\n...\nsamsung browser 6.2\n32.0\n2220x1080\nmatch_status:2\nT\nF\nT\nT\nmobile\nSAMSUNG SM-G892A Build/NRD90M\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n590535\n3577535\n0\n15811047\n49.00\nW\n6550\nNaN\n150.0\nvisa\n226.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n590536\n3577536\n0\n15811049\n39.50\nW\n10444\n225.0\n150.0\nmastercard\n224.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n590537\n3577537\n0\n15811079\n30.95\nW\n12037\n595.0\n150.0\nmastercard\n224.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n590538\n3577538\n0\n15811088\n117.00\nW\n7826\n481.0\n150.0\nmastercard\n224.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n590539\n3577539\n0\n15811131\n279.95\nW\n15066\n170.0\n150.0\nmastercard\n102.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n590540 rows × 434 columns\n\n\n\n\npredictor = TabularPredictor(label=label, eval_metric=eval_metric, verbosity=3).fit(\n    train_data, presets='best_quality', time_limit=1200\n)\n\nresults = predictor.fit_summary()\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231012_071002\"\nPresets specified: ['best_quality']\n============ fit kwarg info ============\nUser Specified kwargs:\n{'auto_stack': True}\nFull kwargs:\n{'_feature_generator_kwargs': None,\n '_save_bag_folds': None,\n 'ag_args': None,\n 'ag_args_ensemble': None,\n 'ag_args_fit': None,\n 'auto_stack': True,\n 'calibrate': 'auto',\n 'excluded_model_types': None,\n 'feature_generator': 'auto',\n 'feature_prune_kwargs': None,\n 'holdout_frac': None,\n 'hyperparameter_tune_kwargs': None,\n 'included_model_types': None,\n 'keep_only_best': False,\n 'name_suffix': None,\n 'num_bag_folds': None,\n 'num_bag_sets': None,\n 'num_stack_levels': None,\n 'pseudo_data': None,\n 'refit_full': False,\n 'save_space': False,\n 'set_best_to_refit_full': False,\n 'unlabeled_data': None,\n 'use_bag_holdout': False,\n 'verbosity': 3}\n========================================\n/home/coco/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nStack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=20\nSaving AutogluonModels/ag-20231012_071002/learner.pkl\nSaving AutogluonModels/ag-20231012_071002/predictor.pkl\nBeginning AutoGluon training ... Time limit = 1200s\nAutoGluon will save models to \"AutogluonModels/ag-20231012_071002\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.13\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   715.54 GB / 982.82 GB (72.8%)\nTrain Data Rows:    590540\nTrain Data Columns: 433\nLabel Column: isFraud\nPreprocessing data ...\n/home/coco/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n/home/coco/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/learner/default_learner.py:215: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    35500.76 MB\n    Train Data (Original)  Memory Usage: 2715.97 MB (7.7% of available memory)\n    Warning: Data size prior to feature transformation consumes 7.7% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Original Features (exact raw dtype, raw dtype):\n                ('float64', 'float') : 399 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n                ('int64', 'int')     :   3 | ['TransactionID', 'TransactionDT', 'card1']\n                ('object', 'object') :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n            Types of features in original data (raw dtype, special dtypes):\n                ('float', [])  : 399 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n                ('int', [])    :   3 | ['TransactionID', 'TransactionDT', 'card1']\n                ('object', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n            Types of features in processed data (raw dtype, special dtypes):\n                ('float', [])  : 399 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n                ('int', [])    :   3 | ['TransactionID', 'TransactionDT', 'card1']\n                ('object', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n            1.2s = Fit runtime\n            433 features in original data used to generate 433 features in processed data.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n/home/coco/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/features/generators/fillna.py:58: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  X.fillna(self._fillna_feature_map, inplace=True, downcast=False)\n            Types of features in original data (raw dtype, special dtypes):\n                ('float', [])  : 399 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n                ('int', [])    :   3 | ['TransactionID', 'TransactionDT', 'card1']\n                ('object', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n            Types of features in processed data (raw dtype, special dtypes):\n                ('float', [])  : 399 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n                ('int', [])    :   3 | ['TransactionID', 'TransactionDT', 'card1']\n                ('object', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n            0.7s = Fit runtime\n            433 features in original data used to generate 433 features in processed data.\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n            Types of features in original data (raw dtype, special dtypes):\n                ('float', []) : 399 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n                ('int', [])   :   3 | ['TransactionID', 'TransactionDT', 'card1']\n            Types of features in processed data (raw dtype, special dtypes):\n                ('float', []) : 399 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n                ('int', [])   :   3 | ['TransactionID', 'TransactionDT', 'card1']\n            0.2s = Fit runtime\n            402 features in original data used to generate 402 features in processed data.\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n                Types of features in original data (raw dtype, special dtypes):\n                    ('category', []) : 31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n                Types of features in processed data (raw dtype, special dtypes):\n                    ('category', []) : 31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n                0.0s = Fit runtime\n                31 features in original data used to generate 31 features in processed data.\n            Types of features in original data (raw dtype, special dtypes):\n                ('object', []) : 31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n            Types of features in processed data (raw dtype, special dtypes):\n                ('category', []) : 31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n            0.6s = Fit runtime\n            31 features in original data used to generate 31 features in processed data.\n        Skipping DatetimeFeatureGenerator: No input feature with required dtypes.\n        Skipping TextSpecialFeatureGenerator: No input feature with required dtypes.\n        Skipping TextNgramFeatureGenerator: No input feature with required dtypes.\n        Skipping IdentityFeatureGenerator: No input feature with required dtypes.\n        Skipping IsNanFeatureGenerator: No input feature with required dtypes.\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n            Types of features in original data (raw dtype, special dtypes):\n                ('category', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n                ('float', [])    : 399 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n                ('int', [])      :   3 | ['TransactionID', 'TransactionDT', 'card1']\n            Types of features in processed data (raw dtype, special dtypes):\n                ('category', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n                ('float', [])    : 399 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n                ('int', [])      :   3 | ['TransactionID', 'TransactionDT', 'card1']\n            1.6s = Fit runtime\n            433 features in original data used to generate 433 features in processed data.\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n            5 duplicate columns removed: ['V28', 'V113', 'V119', 'V122', 'V154']\n            Types of features in original data (raw dtype, special dtypes):\n                ('category', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n                ('float', [])    : 394 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n                ('int', [])      :   3 | ['TransactionID', 'TransactionDT', 'card1']\n            Types of features in processed data (raw dtype, special dtypes):\n                ('category', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n                ('float', [])    : 394 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n                ('int', [])      :   3 | ['TransactionID', 'TransactionDT', 'card1']\n            2.7s = Fit runtime\n            428 features in original data used to generate 428 features in processed data.\n    Unused Original Features (Count: 5): ['V28', 'V113', 'V119', 'V122', 'V154']\n        These features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n        Features can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n        These features do not need to be present at inference time.\n        ('float', []) : 5 | ['V28', 'V113', 'V119', 'V122', 'V154']\n    Types of features in original data (exact raw dtype, raw dtype):\n        ('float64', 'float') : 394 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n        ('int64', 'int')     :   3 | ['TransactionID', 'TransactionDT', 'card1']\n        ('object', 'object') :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])  : 394 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n        ('int', [])    :   3 | ['TransactionID', 'TransactionDT', 'card1']\n        ('object', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n    Types of features in processed data (exact raw dtype, raw dtype):\n        ('category', 'category') :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n        ('float64', 'float')     : 394 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n        ('int64', 'int')         :   3 | ['TransactionID', 'TransactionDT', 'card1']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n        ('float', [])    : 394 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n        ('int', [])      :   3 | ['TransactionID', 'TransactionDT', 'card1']\n    10.4s = Fit runtime\n    428 features in original data used to generate 428 features in processed data.\n/home/coco/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '591140.6153846154' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    Train Data (Processed) Memory Usage: 1895.06 MB (5.3% of available memory)\nData preprocessing and feature engineering runtime = 11.64s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n    This metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n    To change this, specify the eval_metric parameter of Predictor()\nSaving AutogluonModels/ag-20231012_071002/learner.pkl\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nSaving AutogluonModels/ag-20231012_071002/utils/data/X.pkl\nSaving AutogluonModels/ag-20231012_071002/utils/data/y.pkl\nModel configs that will be trained (in order):\n    KNeighborsUnif_BAG_L1:  {'weights': 'uniform', 'ag_args': {'valid_stacker': False, 'name_suffix': 'Unif', 'model_type': &lt;class 'autogluon.tabular.models.knn.knn_model.KNNModel'&gt;, 'priority': 100}, 'ag_args_ensemble': {'use_child_oof': True}}\n    KNeighborsDist_BAG_L1:  {'weights': 'distance', 'ag_args': {'valid_stacker': False, 'name_suffix': 'Dist', 'model_type': &lt;class 'autogluon.tabular.models.knn.knn_model.KNNModel'&gt;, 'priority': 100}, 'ag_args_ensemble': {'use_child_oof': True}}\n    LightGBMXT_BAG_L1:  {'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': &lt;class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'&gt;, 'priority': 90}}\n    LightGBM_BAG_L1:    {'ag_args': {'model_type': &lt;class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'&gt;, 'priority': 90}}\n    RandomForestGini_BAG_L1:    {'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass'], 'model_type': &lt;class 'autogluon.tabular.models.rf.rf_model.RFModel'&gt;, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n    RandomForestEntr_BAG_L1:    {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass'], 'model_type': &lt;class 'autogluon.tabular.models.rf.rf_model.RFModel'&gt;, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n    CatBoost_BAG_L1:    {'ag_args': {'model_type': &lt;class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'&gt;, 'priority': 70}}\n    ExtraTreesGini_BAG_L1:  {'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass'], 'model_type': &lt;class 'autogluon.tabular.models.xt.xt_model.XTModel'&gt;, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n    ExtraTreesEntr_BAG_L1:  {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass'], 'model_type': &lt;class 'autogluon.tabular.models.xt.xt_model.XTModel'&gt;, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n    NeuralNetFastAI_BAG_L1:     {'ag_args': {'model_type': &lt;class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'&gt;, 'priority': 50}}\n    XGBoost_BAG_L1:     {'ag_args': {'model_type': &lt;class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'&gt;, 'priority': 40}}\n    NeuralNetTorch_BAG_L1:  {'ag_args': {'model_type': &lt;class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'&gt;, 'priority': 25}}\n    LightGBMLarge_BAG_L1:   {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'ag_args': {'model_type': &lt;class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'&gt;, 'name_suffix': 'Large', 'hyperparameter_tune_kwargs': None, 'priority': 0}}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 1188.36s of the 1188.35s of remaining time.\n    Fitting KNeighborsUnif_BAG_L1 with 'num_gpus': 0, 'num_cpus': 16\nSaving AutogluonModels/ag-20231012_071002/models/KNeighborsUnif_BAG_L1/utils/model_template.pkl\nLoading: AutogluonModels/ag-20231012_071002/models/KNeighborsUnif_BAG_L1/utils/model_template.pkl\n    Warning: Not enough memory to safely train model. Estimated to require 3.640 GB out of 32.686 GB available memory (55.676%)... (20.000% of avail memory is the max safe size)\n    To force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to &gt;=0.61 to avoid the error)\n        To set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n    Not enough memory to train KNeighborsUnif_BAG_L1... Skipping this model.\nSaving AutogluonModels/ag-20231012_071002/models/trainer.pkl\nFitting model: KNeighborsDist_BAG_L1 ... Training model for up to 1185.58s of the 1185.58s of remaining time.\n    Fitting KNeighborsDist_BAG_L1 with 'num_gpus': 0, 'num_cpus': 16\nSaving AutogluonModels/ag-20231012_071002/models/KNeighborsDist_BAG_L1/utils/model_template.pkl\nLoading: AutogluonModels/ag-20231012_071002/models/KNeighborsDist_BAG_L1/utils/model_template.pkl\n    Warning: Not enough memory to safely train model. Estimated to require 3.640 GB out of 32.688 GB available memory (55.673%)... (20.000% of avail memory is the max safe size)\n    To force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to &gt;=0.61 to avoid the error)\n        To set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n    Not enough memory to train KNeighborsDist_BAG_L1... Skipping this model.\nSaving AutogluonModels/ag-20231012_071002/models/trainer.pkl\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 1182.87s of the 1182.86s of remaining time.\n    Fitting LightGBMXT_BAG_L1 with 'num_gpus': 0, 'num_cpus': 16\nSaving AutogluonModels/ag-20231012_071002/models/LightGBMXT_BAG_L1/utils/model_template.pkl\nLoading: AutogluonModels/ag-20231012_071002/models/LightGBMXT_BAG_L1/utils/model_template.pkl\nWill use sequential fold fitting strategy because import of ray failed. Reason: ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install ray==2.6.3`\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n/home/coco/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '517312.76923076925' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    Fitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n    Ran out of time, early stopping on iteration 5877. Best iteration is:\n    [5876]  valid_set's binary_logloss: 0.044724\n/home/coco/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '517312.76923076925' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    Fitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n    Ran out of time, early stopping on iteration 6210. Best iteration is:\n    [6184]  valid_set's binary_logloss: 0.044593\n/home/coco/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '517312.76923076925' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    Fitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n    Ran out of time, early stopping on iteration 6268. Best iteration is:\n    [6262]  valid_set's binary_logloss: 0.0450604\n/home/coco/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '517312.76923076925' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    Fitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n    Ran out of time, early stopping on iteration 6519. Best iteration is:\n    [6510]  valid_set's binary_logloss: 0.0438093\n/home/coco/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '517313.76923076925' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    Fitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n    Ran out of time, early stopping on iteration 6926. Best iteration is:\n    [6898]  valid_set's binary_logloss: 0.0450263\n/home/coco/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '517313.76923076925' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    Fitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n    Ran out of time, early stopping on iteration 7246. Best iteration is:\n    [7246]  valid_set's binary_logloss: 0.04315\n/home/coco/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '517313.76923076925' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    Fitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n    Ran out of time, early stopping on iteration 7562. Best iteration is:\n    [7545]  valid_set's binary_logloss: 0.0449857\n/home/coco/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '517313.76923076925' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    Fitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n    Ran out of time, early stopping on iteration 9106. Best iteration is:\n    [7836]  valid_set's binary_logloss: 0.0452498\nSaving AutogluonModels/ag-20231012_071002/models/LightGBMXT_BAG_L1/utils/oof.pkl\nSaving AutogluonModels/ag-20231012_071002/models/LightGBMXT_BAG_L1/model.pkl\n    0.9713   = Validation score   (roc_auc)\n    1113.06s     = Training   runtime\n    28.14s   = Validation runtime\nSaving AutogluonModels/ag-20231012_071002/models/trainer.pkl\nFitting model: LightGBM_BAG_L1 ... Training model for up to 37.45s of the 37.44s of remaining time.\n    Fitting LightGBM_BAG_L1 with 'num_gpus': 0, 'num_cpus': 16\nSaving AutogluonModels/ag-20231012_071002/models/LightGBM_BAG_L1/utils/model_template.pkl\nLoading: AutogluonModels/ag-20231012_071002/models/LightGBM_BAG_L1/utils/model_template.pkl\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n/home/coco/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '517312.76923076925' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    Fitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n    Ran out of time, early stopping on iteration 9. Best iteration is:\n    [9] valid_set's binary_logloss: 0.11505\n    Time limit exceeded... Skipping LightGBM_BAG_L1.\nSaving AutogluonModels/ag-20231012_071002/models/trainer.pkl\nFitting model: RandomForestGini_BAG_L1 ... Training model for up to 29.33s of the 29.32s of remaining time.\n    Fitting RandomForestGini_BAG_L1 with 'num_gpus': 0, 'num_cpus': 16\nSaving AutogluonModels/ag-20231012_071002/models/RandomForestGini_BAG_L1/utils/model_template.pkl\nLoading: AutogluonModels/ag-20231012_071002/models/RandomForestGini_BAG_L1/utils/model_template.pkl\n    Warning: Model is expected to require 262.4s to train, which exceeds the maximum time limit of 29.3s, skipping model...\n    Time limit exceeded... Skipping RandomForestGini_BAG_L1.\nSaving AutogluonModels/ag-20231012_071002/models/trainer.pkl\nFitting model: RandomForestEntr_BAG_L1 ... Training model for up to 22.0s of the 22.0s of remaining time.\n    Fitting RandomForestEntr_BAG_L1 with 'num_gpus': 0, 'num_cpus': 16\nSaving AutogluonModels/ag-20231012_071002/models/RandomForestEntr_BAG_L1/utils/model_template.pkl\nLoading: AutogluonModels/ag-20231012_071002/models/RandomForestEntr_BAG_L1/utils/model_template.pkl\n    Warning: Model is expected to require 192.7s to train, which exceeds the maximum time limit of 22.0s, skipping model...\n    Time limit exceeded... Skipping RandomForestEntr_BAG_L1.\nSaving AutogluonModels/ag-20231012_071002/models/trainer.pkl\nFitting model: CatBoost_BAG_L1 ... Training model for up to 15.66s of the 15.65s of remaining time.\n    Fitting CatBoost_BAG_L1 with 'num_gpus': 0, 'num_cpus': 16\nSaving AutogluonModels/ag-20231012_071002/models/CatBoost_BAG_L1/utils/model_template.pkl\nLoading: AutogluonModels/ag-20231012_071002/models/CatBoost_BAG_L1/utils/model_template.pkl\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n/home/coco/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '517312.76923076925' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    Catboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 8}\n    Time limit exceeded... Skipping CatBoost_BAG_L1.\nSaving AutogluonModels/ag-20231012_071002/models/trainer.pkl\nFitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 10.19s of the 10.19s of remaining time.\n    Fitting ExtraTreesGini_BAG_L1 with 'num_gpus': 0, 'num_cpus': 16\nSaving AutogluonModels/ag-20231012_071002/models/ExtraTreesGini_BAG_L1/utils/model_template.pkl\nLoading: AutogluonModels/ag-20231012_071002/models/ExtraTreesGini_BAG_L1/utils/model_template.pkl\n    Warning: Model is expected to require 155.7s to train, which exceeds the maximum time limit of 10.2s, skipping model...\n    Time limit exceeded... Skipping ExtraTreesGini_BAG_L1.\nSaving AutogluonModels/ag-20231012_071002/models/trainer.pkl\nFitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 4.32s of the 4.31s of remaining time.\n    Fitting ExtraTreesEntr_BAG_L1 with 'num_gpus': 0, 'num_cpus': 16\nSaving AutogluonModels/ag-20231012_071002/models/ExtraTreesEntr_BAG_L1/utils/model_template.pkl\nLoading: AutogluonModels/ag-20231012_071002/models/ExtraTreesEntr_BAG_L1/utils/model_template.pkl\n    Warning: Model is expected to require 154.8s to train, which exceeds the maximum time limit of 4.3s, skipping model...\n    Time limit exceeded... Skipping ExtraTreesEntr_BAG_L1.\nSaving AutogluonModels/ag-20231012_071002/models/trainer.pkl\nSkipping NeuralNetFastAI_BAG_L1 due to lack of time remaining.\nSaving AutogluonModels/ag-20231012_071002/models/trainer.pkl\nSkipping XGBoost_BAG_L1 due to lack of time remaining.\nSaving AutogluonModels/ag-20231012_071002/models/trainer.pkl\nSkipping NeuralNetTorch_BAG_L1 due to lack of time remaining.\nSaving AutogluonModels/ag-20231012_071002/models/trainer.pkl\nSkipping LightGBMLarge_BAG_L1 due to lack of time remaining.\nSaving AutogluonModels/ag-20231012_071002/models/trainer.pkl\nNot enough time left to finish repeated k-fold bagging, stopping early ...\nCompleted 1/20 k-fold bagging repeats ...\nLoading: AutogluonModels/ag-20231012_071002/models/LightGBMXT_BAG_L1/utils/oof.pkl\nModel configs that will be trained (in order):\n    WeightedEnsemble_L2:    {'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': &lt;class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'&gt;, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -1.54s of remaining time.\n    Fitting WeightedEnsemble_L2 with 'num_gpus': 0, 'num_cpus': 16\nSaving AutogluonModels/ag-20231012_071002/models/WeightedEnsemble_L2/utils/model_template.pkl\nLoading: AutogluonModels/ag-20231012_071002/models/WeightedEnsemble_L2/utils/model_template.pkl\nEnsemble size: 1\nEnsemble weights: \n[1.]\n    0.37s   = Estimated out-of-fold prediction time...\nSaving AutogluonModels/ag-20231012_071002/models/WeightedEnsemble_L2/utils/oof.pkl\nSaving AutogluonModels/ag-20231012_071002/models/WeightedEnsemble_L2/model.pkl\n    0.9713   = Validation score   (roc_auc)\n    0.07s    = Training   runtime\n    0.07s    = Validation runtime\nSaving AutogluonModels/ag-20231012_071002/models/trainer.pkl\nSaving AutogluonModels/ag-20231012_071002/models/trainer.pkl\nSaving AutogluonModels/ag-20231012_071002/models/trainer.pkl\nAutoGluon training complete, total runtime = 1203.47s ... Best model: \"WeightedEnsemble_L2\"\nLoading: AutogluonModels/ag-20231012_071002/models/trainer.pkl\nSaving AutogluonModels/ag-20231012_071002/models/trainer.pkl\nSaving AutogluonModels/ag-20231012_071002/learner.pkl\nSaving AutogluonModels/ag-20231012_071002/predictor.pkl\nSaving AutogluonModels/ag-20231012_071002/__version__ with contents \"0.8.2\"\nSaving AutogluonModels/ag-20231012_071002/metadata.json\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231012_071002\")\nLoading: AutogluonModels/ag-20231012_071002/models/LightGBMXT_BAG_L1/model.pkl\nLoading: AutogluonModels/ag-20231012_071002/models/WeightedEnsemble_L2/model.pkl\n/home/coco/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/utils/plots.py:169: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n\n\n[50]    valid_set's binary_logloss: 0.0936737\n[100]   valid_set's binary_logloss: 0.0852533\n[150]   valid_set's binary_logloss: 0.0807703\n[200]   valid_set's binary_logloss: 0.0774437\n[250]   valid_set's binary_logloss: 0.0748597\n[300]   valid_set's binary_logloss: 0.072922\n[350]   valid_set's binary_logloss: 0.0713176\n[400]   valid_set's binary_logloss: 0.0698932\n[450]   valid_set's binary_logloss: 0.068628\n[500]   valid_set's binary_logloss: 0.0675102\n[550]   valid_set's binary_logloss: 0.0664848\n[600]   valid_set's binary_logloss: 0.0656376\n[650]   valid_set's binary_logloss: 0.064794\n[700]   valid_set's binary_logloss: 0.063985\n[750]   valid_set's binary_logloss: 0.0632608\n[800]   valid_set's binary_logloss: 0.0625744\n[850]   valid_set's binary_logloss: 0.0619177\n[900]   valid_set's binary_logloss: 0.0613089\n[950]   valid_set's binary_logloss: 0.0607002\n[1000]  valid_set's binary_logloss: 0.0601336\n[1050]  valid_set's binary_logloss: 0.0596524\n[1100]  valid_set's binary_logloss: 0.0591299\n[1150]  valid_set's binary_logloss: 0.05867\n[1200]  valid_set's binary_logloss: 0.0581907\n[1250]  valid_set's binary_logloss: 0.0577601\n[1300]  valid_set's binary_logloss: 0.0573534\n[1350]  valid_set's binary_logloss: 0.0569495\n[1400]  valid_set's binary_logloss: 0.0565643\n[1450]  valid_set's binary_logloss: 0.0561722\n[1500]  valid_set's binary_logloss: 0.0557824\n[1550]  valid_set's binary_logloss: 0.0554409\n[1600]  valid_set's binary_logloss: 0.0550898\n[1650]  valid_set's binary_logloss: 0.0548449\n[1700]  valid_set's binary_logloss: 0.054555\n[1750]  valid_set's binary_logloss: 0.0542798\n[1800]  valid_set's binary_logloss: 0.0540026\n[1850]  valid_set's binary_logloss: 0.0537238\n[1900]  valid_set's binary_logloss: 0.0534957\n[1950]  valid_set's binary_logloss: 0.0532611\n[2000]  valid_set's binary_logloss: 0.0529911\n[2050]  valid_set's binary_logloss: 0.0527246\n[2100]  valid_set's binary_logloss: 0.0525105\n[2150]  valid_set's binary_logloss: 0.0522785\n[2200]  valid_set's binary_logloss: 0.052111\n[2250]  valid_set's binary_logloss: 0.051931\n[2300]  valid_set's binary_logloss: 0.0517019\n[2350]  valid_set's binary_logloss: 0.0514896\n[2400]  valid_set's binary_logloss: 0.0512912\n[2450]  valid_set's binary_logloss: 0.0510876\n[2500]  valid_set's binary_logloss: 0.0509128\n[2550]  valid_set's binary_logloss: 0.0507453\n[2600]  valid_set's binary_logloss: 0.0505722\n[2650]  valid_set's binary_logloss: 0.0504064\n[2700]  valid_set's binary_logloss: 0.0502477\n[2750]  valid_set's binary_logloss: 0.0500313\n[2800]  valid_set's binary_logloss: 0.0498675\n[2850]  valid_set's binary_logloss: 0.0497107\n[2900]  valid_set's binary_logloss: 0.0495319\n[2950]  valid_set's binary_logloss: 0.0493426\n[3000]  valid_set's binary_logloss: 0.0491939\n[3050]  valid_set's binary_logloss: 0.0490613\n[3100]  valid_set's binary_logloss: 0.048961\n[3150]  valid_set's binary_logloss: 0.0488235\n[3200]  valid_set's binary_logloss: 0.0487102\n[3250]  valid_set's binary_logloss: 0.0485645\n[3300]  valid_set's binary_logloss: 0.0484437\n[3350]  valid_set's binary_logloss: 0.0483204\n[3400]  valid_set's binary_logloss: 0.0481833\n[3450]  valid_set's binary_logloss: 0.0480394\n[3500]  valid_set's binary_logloss: 0.0479323\n[3550]  valid_set's binary_logloss: 0.0478227\n[3600]  valid_set's binary_logloss: 0.0477384\n[3650]  valid_set's binary_logloss: 0.0476293\n[3700]  valid_set's binary_logloss: 0.0475201\n[3750]  valid_set's binary_logloss: 0.0474286\n[3800]  valid_set's binary_logloss: 0.0473531\n[3850]  valid_set's binary_logloss: 0.0472366\n[3900]  valid_set's binary_logloss: 0.0471641\n[3950]  valid_set's binary_logloss: 0.0470557\n[4000]  valid_set's binary_logloss: 0.0469405\n[4050]  valid_set's binary_logloss: 0.0468344\n[4100]  valid_set's binary_logloss: 0.0467228\n[4150]  valid_set's binary_logloss: 0.0466373\n[4200]  valid_set's binary_logloss: 0.0465481\n[4250]  valid_set's binary_logloss: 0.0464336\n[4300]  valid_set's binary_logloss: 0.0463165\n[4350]  valid_set's binary_logloss: 0.0462141\n[4400]  valid_set's binary_logloss: 0.0461617\n[4450]  valid_set's binary_logloss: 0.0460656\n[4500]  valid_set's binary_logloss: 0.046003\n[4550]  valid_set's binary_logloss: 0.0459441\n[4600]  valid_set's binary_logloss: 0.0458618\n[4650]  valid_set's binary_logloss: 0.0457985\n[4700]  valid_set's binary_logloss: 0.0457269\n[4750]  valid_set's binary_logloss: 0.0456808\n[4800]  valid_set's binary_logloss: 0.0456087\n[4850]  valid_set's binary_logloss: 0.0455417\n[4900]  valid_set's binary_logloss: 0.0454853\n[4950]  valid_set's binary_logloss: 0.0454527\n[5000]  valid_set's binary_logloss: 0.0453931\n[5050]  valid_set's binary_logloss: 0.0453198\n[5100]  valid_set's binary_logloss: 0.0452693\n[5150]  valid_set's binary_logloss: 0.045246\n[5200]  valid_set's binary_logloss: 0.0451859\n[5250]  valid_set's binary_logloss: 0.0451422\n[5300]  valid_set's binary_logloss: 0.0451284\n[5350]  valid_set's binary_logloss: 0.0450918\n[5400]  valid_set's binary_logloss: 0.0450484\n[5450]  valid_set's binary_logloss: 0.0450262\n[5500]  valid_set's binary_logloss: 0.0450119\n[5550]  valid_set's binary_logloss: 0.0449669\n[5600]  valid_set's binary_logloss: 0.044913\n[5650]  valid_set's binary_logloss: 0.0448789\n[5700]  valid_set's binary_logloss: 0.0448426\n[5750]  valid_set's binary_logloss: 0.044802\n[5800]  valid_set's binary_logloss: 0.0447707\n[5850]  valid_set's binary_logloss: 0.044755\n[50]    valid_set's binary_logloss: 0.0936941\n[100]   valid_set's binary_logloss: 0.0853461\n[150]   valid_set's binary_logloss: 0.0805932\n[200]   valid_set's binary_logloss: 0.0774739\n[250]   valid_set's binary_logloss: 0.0749666\n[300]   valid_set's binary_logloss: 0.0729618\n[350]   valid_set's binary_logloss: 0.0712934\n[400]   valid_set's binary_logloss: 0.0698307\n[450]   valid_set's binary_logloss: 0.0686244\n[500]   valid_set's binary_logloss: 0.0674185\n[550]   valid_set's binary_logloss: 0.0663523\n[600]   valid_set's binary_logloss: 0.0654039\n[650]   valid_set's binary_logloss: 0.0646239\n[700]   valid_set's binary_logloss: 0.0639209\n[750]   valid_set's binary_logloss: 0.0632\n[800]   valid_set's binary_logloss: 0.0625109\n[850]   valid_set's binary_logloss: 0.0619293\n[900]   valid_set's binary_logloss: 0.0613322\n[950]   valid_set's binary_logloss: 0.0608261\n[1000]  valid_set's binary_logloss: 0.0603095\n[1050]  valid_set's binary_logloss: 0.0597793\n[1100]  valid_set's binary_logloss: 0.0592263\n[1150]  valid_set's binary_logloss: 0.0588145\n[1200]  valid_set's binary_logloss: 0.0583591\n[1250]  valid_set's binary_logloss: 0.0578524\n[1300]  valid_set's binary_logloss: 0.0574527\n[1350]  valid_set's binary_logloss: 0.05699\n[1400]  valid_set's binary_logloss: 0.0566144\n[1450]  valid_set's binary_logloss: 0.0562529\n[1500]  valid_set's binary_logloss: 0.0559153\n[1550]  valid_set's binary_logloss: 0.0555792\n[1600]  valid_set's binary_logloss: 0.0552439\n[1650]  valid_set's binary_logloss: 0.0548792\n[1700]  valid_set's binary_logloss: 0.0545672\n[1750]  valid_set's binary_logloss: 0.0542206\n[1800]  valid_set's binary_logloss: 0.0539491\n[1850]  valid_set's binary_logloss: 0.0536195\n[1900]  valid_set's binary_logloss: 0.0533443\n[1950]  valid_set's binary_logloss: 0.0530605\n[2000]  valid_set's binary_logloss: 0.0528141\n[2050]  valid_set's binary_logloss: 0.0525792\n[2100]  valid_set's binary_logloss: 0.0523766\n[2150]  valid_set's binary_logloss: 0.0521728\n[2200]  valid_set's binary_logloss: 0.0519096\n[2250]  valid_set's binary_logloss: 0.0517099\n[2300]  valid_set's binary_logloss: 0.0515009\n[2350]  valid_set's binary_logloss: 0.0512851\n[2400]  valid_set's binary_logloss: 0.0511188\n[2450]  valid_set's binary_logloss: 0.0509557\n[2500]  valid_set's binary_logloss: 0.0507828\n[2550]  valid_set's binary_logloss: 0.0505682\n[2600]  valid_set's binary_logloss: 0.050393\n[2650]  valid_set's binary_logloss: 0.0502105\n[2700]  valid_set's binary_logloss: 0.0500548\n[2750]  valid_set's binary_logloss: 0.0499204\n[2800]  valid_set's binary_logloss: 0.0497787\n[2850]  valid_set's binary_logloss: 0.0496777\n[2900]  valid_set's binary_logloss: 0.0495056\n[2950]  valid_set's binary_logloss: 0.0493791\n[3000]  valid_set's binary_logloss: 0.0492379\n[3050]  valid_set's binary_logloss: 0.0491287\n[3100]  valid_set's binary_logloss: 0.0489873\n[3150]  valid_set's binary_logloss: 0.0488448\n[3200]  valid_set's binary_logloss: 0.048699\n[3250]  valid_set's binary_logloss: 0.0485927\n[3300]  valid_set's binary_logloss: 0.0484636\n[3350]  valid_set's binary_logloss: 0.0483569\n[3400]  valid_set's binary_logloss: 0.0482166\n[3450]  valid_set's binary_logloss: 0.0481552\n[3500]  valid_set's binary_logloss: 0.0480385\n[3550]  valid_set's binary_logloss: 0.0479487\n[3600]  valid_set's binary_logloss: 0.0478576\n[3650]  valid_set's binary_logloss: 0.0477319\n[3700]  valid_set's binary_logloss: 0.0476267\n[3750]  valid_set's binary_logloss: 0.0475399\n[3800]  valid_set's binary_logloss: 0.0474433\n[3850]  valid_set's binary_logloss: 0.0473576\n[3900]  valid_set's binary_logloss: 0.047269\n[3950]  valid_set's binary_logloss: 0.0471792\n[4000]  valid_set's binary_logloss: 0.0470721\n[4050]  valid_set's binary_logloss: 0.0469912\n[4100]  valid_set's binary_logloss: 0.0469019\n[4150]  valid_set's binary_logloss: 0.0467872\n[4200]  valid_set's binary_logloss: 0.0466825\n[4250]  valid_set's binary_logloss: 0.0465872\n[4300]  valid_set's binary_logloss: 0.0465201\n[4350]  valid_set's binary_logloss: 0.0464279\n[4400]  valid_set's binary_logloss: 0.0463656\n[4450]  valid_set's binary_logloss: 0.0463047\n[4500]  valid_set's binary_logloss: 0.0462652\n[4550]  valid_set's binary_logloss: 0.0461945\n[4600]  valid_set's binary_logloss: 0.0461257\n[4650]  valid_set's binary_logloss: 0.0460408\n[4700]  valid_set's binary_logloss: 0.0459528\n[4750]  valid_set's binary_logloss: 0.0458927\n[4800]  valid_set's binary_logloss: 0.0458173\n[4850]  valid_set's binary_logloss: 0.0457738\n[4900]  valid_set's binary_logloss: 0.0457117\n[4950]  valid_set's binary_logloss: 0.0456326\n[5000]  valid_set's binary_logloss: 0.0455434\n[5050]  valid_set's binary_logloss: 0.0455037\n[5100]  valid_set's binary_logloss: 0.0454415\n[5150]  valid_set's binary_logloss: 0.0453727\n[5200]  valid_set's binary_logloss: 0.0453197\n[5250]  valid_set's binary_logloss: 0.0452807\n[5300]  valid_set's binary_logloss: 0.0452186\n[5350]  valid_set's binary_logloss: 0.0451712\n[5400]  valid_set's binary_logloss: 0.0451253\n[5450]  valid_set's binary_logloss: 0.0450528\n[5500]  valid_set's binary_logloss: 0.0449985\n[5550]  valid_set's binary_logloss: 0.0449491\n[5600]  valid_set's binary_logloss: 0.0449064\n[5650]  valid_set's binary_logloss: 0.0448602\n[5700]  valid_set's binary_logloss: 0.0448243\n[5750]  valid_set's binary_logloss: 0.0448183\n[5800]  valid_set's binary_logloss: 0.0447804\n[5850]  valid_set's binary_logloss: 0.0447788\n[5900]  valid_set's binary_logloss: 0.0447217\n[5950]  valid_set's binary_logloss: 0.0446996\n[6000]  valid_set's binary_logloss: 0.0446905\n[6050]  valid_set's binary_logloss: 0.0446752\n[6100]  valid_set's binary_logloss: 0.0446451\n[6150]  valid_set's binary_logloss: 0.0446198\n[6200]  valid_set's binary_logloss: 0.0446077\n[50]    valid_set's binary_logloss: 0.0953617\n[100]   valid_set's binary_logloss: 0.0866117\n[150]   valid_set's binary_logloss: 0.0820183\n[200]   valid_set's binary_logloss: 0.0789598\n[250]   valid_set's binary_logloss: 0.07639\n[300]   valid_set's binary_logloss: 0.0746054\n[350]   valid_set's binary_logloss: 0.0729009\n[400]   valid_set's binary_logloss: 0.0714208\n[450]   valid_set's binary_logloss: 0.0699856\n[500]   valid_set's binary_logloss: 0.068841\n[550]   valid_set's binary_logloss: 0.0678098\n[600]   valid_set's binary_logloss: 0.066849\n[650]   valid_set's binary_logloss: 0.0659862\n[700]   valid_set's binary_logloss: 0.06517\n[750]   valid_set's binary_logloss: 0.0644492\n[800]   valid_set's binary_logloss: 0.063698\n[850]   valid_set's binary_logloss: 0.0631679\n[900]   valid_set's binary_logloss: 0.0625741\n[950]   valid_set's binary_logloss: 0.0619526\n[1000]  valid_set's binary_logloss: 0.0613726\n[1050]  valid_set's binary_logloss: 0.0608881\n[1100]  valid_set's binary_logloss: 0.060333\n[1150]  valid_set's binary_logloss: 0.0598478\n[1200]  valid_set's binary_logloss: 0.0593026\n[1250]  valid_set's binary_logloss: 0.0588779\n[1300]  valid_set's binary_logloss: 0.0585185\n[1350]  valid_set's binary_logloss: 0.0581324\n[1400]  valid_set's binary_logloss: 0.0577518\n[1450]  valid_set's binary_logloss: 0.0573949\n[1500]  valid_set's binary_logloss: 0.057027\n[1550]  valid_set's binary_logloss: 0.0566636\n[1600]  valid_set's binary_logloss: 0.0563203\n[1650]  valid_set's binary_logloss: 0.0559076\n[1700]  valid_set's binary_logloss: 0.0556244\n[1750]  valid_set's binary_logloss: 0.05531\n[1800]  valid_set's binary_logloss: 0.0550484\n[1850]  valid_set's binary_logloss: 0.0547708\n[1900]  valid_set's binary_logloss: 0.0544223\n[1950]  valid_set's binary_logloss: 0.054124\n[2000]  valid_set's binary_logloss: 0.0538415\n[2050]  valid_set's binary_logloss: 0.0536154\n[2100]  valid_set's binary_logloss: 0.0533372\n[2150]  valid_set's binary_logloss: 0.0531066\n[2200]  valid_set's binary_logloss: 0.0528901\n[2250]  valid_set's binary_logloss: 0.0526725\n[2300]  valid_set's binary_logloss: 0.0524141\n[2350]  valid_set's binary_logloss: 0.0521988\n[2400]  valid_set's binary_logloss: 0.0519733\n[2450]  valid_set's binary_logloss: 0.0517348\n[2500]  valid_set's binary_logloss: 0.0515702\n[2550]  valid_set's binary_logloss: 0.0513385\n[2600]  valid_set's binary_logloss: 0.0511175\n[2650]  valid_set's binary_logloss: 0.0509345\n[2700]  valid_set's binary_logloss: 0.0507911\n[2750]  valid_set's binary_logloss: 0.0506237\n[2800]  valid_set's binary_logloss: 0.0504895\n[2850]  valid_set's binary_logloss: 0.0503309\n[2900]  valid_set's binary_logloss: 0.0501717\n[2950]  valid_set's binary_logloss: 0.0500072\n[3000]  valid_set's binary_logloss: 0.0498751\n[3050]  valid_set's binary_logloss: 0.0497354\n[3100]  valid_set's binary_logloss: 0.0495826\n[3150]  valid_set's binary_logloss: 0.0494818\n[3200]  valid_set's binary_logloss: 0.0493455\n[3250]  valid_set's binary_logloss: 0.0492197\n[3300]  valid_set's binary_logloss: 0.0490787\n[3350]  valid_set's binary_logloss: 0.0490053\n[3400]  valid_set's binary_logloss: 0.0489099\n[3450]  valid_set's binary_logloss: 0.0487986\n[3500]  valid_set's binary_logloss: 0.0486927\n[3550]  valid_set's binary_logloss: 0.0485646\n[3600]  valid_set's binary_logloss: 0.0484413\n[3650]  valid_set's binary_logloss: 0.048333\n[3700]  valid_set's binary_logloss: 0.0482383\n[3750]  valid_set's binary_logloss: 0.0481231\n[3800]  valid_set's binary_logloss: 0.0479968\n[3850]  valid_set's binary_logloss: 0.0479246\n[3900]  valid_set's binary_logloss: 0.0478247\n[3950]  valid_set's binary_logloss: 0.0477391\n[4000]  valid_set's binary_logloss: 0.0476406\n[4050]  valid_set's binary_logloss: 0.0475602\n[4100]  valid_set's binary_logloss: 0.0474809\n[4150]  valid_set's binary_logloss: 0.047411\n[4200]  valid_set's binary_logloss: 0.0473249\n[4250]  valid_set's binary_logloss: 0.0472436\n[4300]  valid_set's binary_logloss: 0.0471634\n[4350]  valid_set's binary_logloss: 0.0471066\n[4400]  valid_set's binary_logloss: 0.0470278\n[4450]  valid_set's binary_logloss: 0.046957\n[4500]  valid_set's binary_logloss: 0.0468649\n[4550]  valid_set's binary_logloss: 0.0467664\n[4600]  valid_set's binary_logloss: 0.0466469\n[4650]  valid_set's binary_logloss: 0.0465849\n[4700]  valid_set's binary_logloss: 0.046506\n[4750]  valid_set's binary_logloss: 0.0464294\n[4800]  valid_set's binary_logloss: 0.0463723\n[4850]  valid_set's binary_logloss: 0.0463084\n[4900]  valid_set's binary_logloss: 0.046257\n[4950]  valid_set's binary_logloss: 0.0462023\n[5000]  valid_set's binary_logloss: 0.0461294\n[5050]  valid_set's binary_logloss: 0.0460733\n[5100]  valid_set's binary_logloss: 0.0459967\n[5150]  valid_set's binary_logloss: 0.0459475\n[5200]  valid_set's binary_logloss: 0.0458843\n[5250]  valid_set's binary_logloss: 0.0458291\n[5300]  valid_set's binary_logloss: 0.0457864\n[5350]  valid_set's binary_logloss: 0.0457388\n[5400]  valid_set's binary_logloss: 0.0456673\n[5450]  valid_set's binary_logloss: 0.045639\n[5500]  valid_set's binary_logloss: 0.0455991\n[5550]  valid_set's binary_logloss: 0.0455616\n[5600]  valid_set's binary_logloss: 0.0455333\n[5650]  valid_set's binary_logloss: 0.0454896\n[5700]  valid_set's binary_logloss: 0.0454491\n[5750]  valid_set's binary_logloss: 0.0454119\n[5800]  valid_set's binary_logloss: 0.0453569\n[5850]  valid_set's binary_logloss: 0.0453235\n[5900]  valid_set's binary_logloss: 0.0452943\n[5950]  valid_set's binary_logloss: 0.0452621\n[6000]  valid_set's binary_logloss: 0.0452319\n[6050]  valid_set's binary_logloss: 0.0451858\n[6100]  valid_set's binary_logloss: 0.0451748\n[6150]  valid_set's binary_logloss: 0.0451364\n[6200]  valid_set's binary_logloss: 0.045098\n[6250]  valid_set's binary_logloss: 0.045078\n[50]    valid_set's binary_logloss: 0.0927631\n[100]   valid_set's binary_logloss: 0.0840394\n[150]   valid_set's binary_logloss: 0.0793017\n[200]   valid_set's binary_logloss: 0.0761152\n[250]   valid_set's binary_logloss: 0.073898\n[300]   valid_set's binary_logloss: 0.0720469\n[350]   valid_set's binary_logloss: 0.0703314\n[400]   valid_set's binary_logloss: 0.0687589\n[450]   valid_set's binary_logloss: 0.06746\n[500]   valid_set's binary_logloss: 0.0663718\n[550]   valid_set's binary_logloss: 0.0653125\n[600]   valid_set's binary_logloss: 0.0644994\n[650]   valid_set's binary_logloss: 0.0637179\n[700]   valid_set's binary_logloss: 0.0629826\n[750]   valid_set's binary_logloss: 0.062309\n[800]   valid_set's binary_logloss: 0.0616543\n[850]   valid_set's binary_logloss: 0.0610427\n[900]   valid_set's binary_logloss: 0.0603622\n[950]   valid_set's binary_logloss: 0.0598431\n[1000]  valid_set's binary_logloss: 0.0592598\n[1050]  valid_set's binary_logloss: 0.0587273\n[1100]  valid_set's binary_logloss: 0.0582562\n[1150]  valid_set's binary_logloss: 0.0578348\n[1200]  valid_set's binary_logloss: 0.0573426\n[1250]  valid_set's binary_logloss: 0.0568541\n[1300]  valid_set's binary_logloss: 0.056457\n[1350]  valid_set's binary_logloss: 0.056099\n[1400]  valid_set's binary_logloss: 0.0557775\n[1450]  valid_set's binary_logloss: 0.055421\n[1500]  valid_set's binary_logloss: 0.0550517\n[1550]  valid_set's binary_logloss: 0.0547038\n[1600]  valid_set's binary_logloss: 0.0543526\n[1650]  valid_set's binary_logloss: 0.0539812\n[1700]  valid_set's binary_logloss: 0.0536435\n[1750]  valid_set's binary_logloss: 0.0533643\n[1800]  valid_set's binary_logloss: 0.0531052\n[1850]  valid_set's binary_logloss: 0.052775\n[1900]  valid_set's binary_logloss: 0.0525077\n[1950]  valid_set's binary_logloss: 0.0522301\n[2000]  valid_set's binary_logloss: 0.0520019\n[2050]  valid_set's binary_logloss: 0.0517571\n[2100]  valid_set's binary_logloss: 0.0514827\n[2150]  valid_set's binary_logloss: 0.051247\n[2200]  valid_set's binary_logloss: 0.0510767\n[2250]  valid_set's binary_logloss: 0.0508807\n[2300]  valid_set's binary_logloss: 0.0506708\n[2350]  valid_set's binary_logloss: 0.0504988\n[2400]  valid_set's binary_logloss: 0.0502891\n[2450]  valid_set's binary_logloss: 0.0501304\n[2500]  valid_set's binary_logloss: 0.0499786\n[2550]  valid_set's binary_logloss: 0.0497681\n[2600]  valid_set's binary_logloss: 0.0495752\n[2650]  valid_set's binary_logloss: 0.0494133\n[2700]  valid_set's binary_logloss: 0.0492863\n[2750]  valid_set's binary_logloss: 0.0491365\n[2800]  valid_set's binary_logloss: 0.048967\n[2850]  valid_set's binary_logloss: 0.0488281\n[2900]  valid_set's binary_logloss: 0.0486981\n[2950]  valid_set's binary_logloss: 0.0485372\n[3000]  valid_set's binary_logloss: 0.0484107\n[3050]  valid_set's binary_logloss: 0.0482393\n[3100]  valid_set's binary_logloss: 0.0481092\n[3150]  valid_set's binary_logloss: 0.0479626\n[3200]  valid_set's binary_logloss: 0.0478025\n[3250]  valid_set's binary_logloss: 0.0477056\n[3300]  valid_set's binary_logloss: 0.0475677\n[3350]  valid_set's binary_logloss: 0.0474623\n[3400]  valid_set's binary_logloss: 0.0473243\n[3450]  valid_set's binary_logloss: 0.0471746\n[3500]  valid_set's binary_logloss: 0.0470553\n[3550]  valid_set's binary_logloss: 0.0469179\n[3600]  valid_set's binary_logloss: 0.0468118\n[3650]  valid_set's binary_logloss: 0.0467056\n[3700]  valid_set's binary_logloss: 0.046612\n[3750]  valid_set's binary_logloss: 0.046511\n[3800]  valid_set's binary_logloss: 0.0464114\n[3850]  valid_set's binary_logloss: 0.0462902\n[3900]  valid_set's binary_logloss: 0.0461908\n[3950]  valid_set's binary_logloss: 0.0460768\n[4000]  valid_set's binary_logloss: 0.045977\n[4050]  valid_set's binary_logloss: 0.0459112\n[4100]  valid_set's binary_logloss: 0.0458248\n[4150]  valid_set's binary_logloss: 0.0457574\n[4200]  valid_set's binary_logloss: 0.0457007\n[4250]  valid_set's binary_logloss: 0.0456269\n[4300]  valid_set's binary_logloss: 0.0455602\n[4350]  valid_set's binary_logloss: 0.0455173\n[4400]  valid_set's binary_logloss: 0.0454434\n[4450]  valid_set's binary_logloss: 0.0453812\n[4500]  valid_set's binary_logloss: 0.0453452\n[4550]  valid_set's binary_logloss: 0.0452793\n[4600]  valid_set's binary_logloss: 0.0452373\n[4650]  valid_set's binary_logloss: 0.0451718\n[4700]  valid_set's binary_logloss: 0.0451099\n[4750]  valid_set's binary_logloss: 0.0450686\n[4800]  valid_set's binary_logloss: 0.0450216\n[4850]  valid_set's binary_logloss: 0.0449526\n[4900]  valid_set's binary_logloss: 0.04493\n[4950]  valid_set's binary_logloss: 0.0448861\n[5000]  valid_set's binary_logloss: 0.0448365\n[5050]  valid_set's binary_logloss: 0.0447974\n[5100]  valid_set's binary_logloss: 0.0447464\n[5150]  valid_set's binary_logloss: 0.0447008\n[5200]  valid_set's binary_logloss: 0.0446738\n[5250]  valid_set's binary_logloss: 0.0446196\n[5300]  valid_set's binary_logloss: 0.0445873\n[5350]  valid_set's binary_logloss: 0.0445419\n[5400]  valid_set's binary_logloss: 0.0445112\n[5450]  valid_set's binary_logloss: 0.0444979\n[5500]  valid_set's binary_logloss: 0.0444828\n[5550]  valid_set's binary_logloss: 0.0444646\n[5600]  valid_set's binary_logloss: 0.0444162\n[5650]  valid_set's binary_logloss: 0.0443655\n[5700]  valid_set's binary_logloss: 0.0443122\n[5750]  valid_set's binary_logloss: 0.0442762\n[5800]  valid_set's binary_logloss: 0.0442372\n[5850]  valid_set's binary_logloss: 0.0442111\n[5900]  valid_set's binary_logloss: 0.044202\n[5950]  valid_set's binary_logloss: 0.0441641\n[6000]  valid_set's binary_logloss: 0.0441388\n[6050]  valid_set's binary_logloss: 0.044127\n[6100]  valid_set's binary_logloss: 0.0440757\n[6150]  valid_set's binary_logloss: 0.0440481\n[6200]  valid_set's binary_logloss: 0.0440184\n[6250]  valid_set's binary_logloss: 0.0439897\n[6300]  valid_set's binary_logloss: 0.0439608\n[6350]  valid_set's binary_logloss: 0.0439372\n[6400]  valid_set's binary_logloss: 0.0439038\n[6450]  valid_set's binary_logloss: 0.0438652\n[6500]  valid_set's binary_logloss: 0.043821\n[50]    valid_set's binary_logloss: 0.0958529\n[100]   valid_set's binary_logloss: 0.0871841\n[150]   valid_set's binary_logloss: 0.0825358\n[200]   valid_set's binary_logloss: 0.0793639\n[250]   valid_set's binary_logloss: 0.0767544\n[300]   valid_set's binary_logloss: 0.0748024\n[350]   valid_set's binary_logloss: 0.0729768\n[400]   valid_set's binary_logloss: 0.0715264\n[450]   valid_set's binary_logloss: 0.0702451\n[500]   valid_set's binary_logloss: 0.0691385\n[550]   valid_set's binary_logloss: 0.0681567\n[600]   valid_set's binary_logloss: 0.0671696\n[650]   valid_set's binary_logloss: 0.0662754\n[700]   valid_set's binary_logloss: 0.0655184\n[750]   valid_set's binary_logloss: 0.0647833\n[800]   valid_set's binary_logloss: 0.0640332\n[850]   valid_set's binary_logloss: 0.0634018\n[900]   valid_set's binary_logloss: 0.0628251\n[950]   valid_set's binary_logloss: 0.0622536\n[1000]  valid_set's binary_logloss: 0.0616541\n[1050]  valid_set's binary_logloss: 0.0611593\n[1100]  valid_set's binary_logloss: 0.0606662\n[1150]  valid_set's binary_logloss: 0.0601493\n[1200]  valid_set's binary_logloss: 0.059709\n[1250]  valid_set's binary_logloss: 0.0592883\n[1300]  valid_set's binary_logloss: 0.05882\n[1350]  valid_set's binary_logloss: 0.0583718\n[1400]  valid_set's binary_logloss: 0.0579895\n[1450]  valid_set's binary_logloss: 0.0575995\n[1500]  valid_set's binary_logloss: 0.0572909\n[1550]  valid_set's binary_logloss: 0.0569859\n[1600]  valid_set's binary_logloss: 0.0566061\n[1650]  valid_set's binary_logloss: 0.0562683\n[1700]  valid_set's binary_logloss: 0.05591\n[1750]  valid_set's binary_logloss: 0.0556\n[1800]  valid_set's binary_logloss: 0.0552755\n[1850]  valid_set's binary_logloss: 0.0549707\n[1900]  valid_set's binary_logloss: 0.0546319\n[1950]  valid_set's binary_logloss: 0.0543291\n[2000]  valid_set's binary_logloss: 0.0540671\n[2050]  valid_set's binary_logloss: 0.0538636\n[2100]  valid_set's binary_logloss: 0.0535979\n[2150]  valid_set's binary_logloss: 0.0533004\n[2200]  valid_set's binary_logloss: 0.0530543\n[2250]  valid_set's binary_logloss: 0.0528484\n[2300]  valid_set's binary_logloss: 0.0525922\n[2350]  valid_set's binary_logloss: 0.0523899\n[2400]  valid_set's binary_logloss: 0.0521693\n[2450]  valid_set's binary_logloss: 0.0519451\n[2500]  valid_set's binary_logloss: 0.051739\n[2550]  valid_set's binary_logloss: 0.0515424\n[2600]  valid_set's binary_logloss: 0.0513854\n[2650]  valid_set's binary_logloss: 0.0512003\n[2700]  valid_set's binary_logloss: 0.0510369\n[2750]  valid_set's binary_logloss: 0.0508115\n[2800]  valid_set's binary_logloss: 0.0506688\n[2850]  valid_set's binary_logloss: 0.050544\n[2900]  valid_set's binary_logloss: 0.0503976\n[2950]  valid_set's binary_logloss: 0.050281\n[3000]  valid_set's binary_logloss: 0.050103\n[3050]  valid_set's binary_logloss: 0.0499684\n[3100]  valid_set's binary_logloss: 0.0498283\n[3150]  valid_set's binary_logloss: 0.0496718\n[3200]  valid_set's binary_logloss: 0.0495287\n[3250]  valid_set's binary_logloss: 0.0493909\n[3300]  valid_set's binary_logloss: 0.0492666\n[3350]  valid_set's binary_logloss: 0.0491547\n[3400]  valid_set's binary_logloss: 0.0490356\n[3450]  valid_set's binary_logloss: 0.0489135\n[3500]  valid_set's binary_logloss: 0.0488012\n[3550]  valid_set's binary_logloss: 0.0486779\n[3600]  valid_set's binary_logloss: 0.0485859\n[3650]  valid_set's binary_logloss: 0.0485029\n[3700]  valid_set's binary_logloss: 0.0483896\n[3750]  valid_set's binary_logloss: 0.0482569\n[3800]  valid_set's binary_logloss: 0.0481321\n[3850]  valid_set's binary_logloss: 0.0480145\n[3900]  valid_set's binary_logloss: 0.0479179\n[3950]  valid_set's binary_logloss: 0.0478284\n[4000]  valid_set's binary_logloss: 0.0477612\n[4050]  valid_set's binary_logloss: 0.0476764\n[4100]  valid_set's binary_logloss: 0.0475856\n[4150]  valid_set's binary_logloss: 0.0475034\n[4200]  valid_set's binary_logloss: 0.0474197\n[4250]  valid_set's binary_logloss: 0.0473214\n[4300]  valid_set's binary_logloss: 0.0472319\n[4350]  valid_set's binary_logloss: 0.0471829\n[4400]  valid_set's binary_logloss: 0.0471137\n[4450]  valid_set's binary_logloss: 0.0470401\n[4500]  valid_set's binary_logloss: 0.046964\n[4550]  valid_set's binary_logloss: 0.046908\n[4600]  valid_set's binary_logloss: 0.0468463\n[4650]  valid_set's binary_logloss: 0.0467809\n[4700]  valid_set's binary_logloss: 0.0466973\n[4750]  valid_set's binary_logloss: 0.0466348\n[4800]  valid_set's binary_logloss: 0.0465433\n[4850]  valid_set's binary_logloss: 0.0464919\n[4900]  valid_set's binary_logloss: 0.0464466\n[4950]  valid_set's binary_logloss: 0.0463859\n[5000]  valid_set's binary_logloss: 0.0463414\n[5050]  valid_set's binary_logloss: 0.0462629\n[5100]  valid_set's binary_logloss: 0.0462046\n[5150]  valid_set's binary_logloss: 0.0461347\n[5200]  valid_set's binary_logloss: 0.0461115\n[5250]  valid_set's binary_logloss: 0.0460565\n[5300]  valid_set's binary_logloss: 0.0460264\n[5350]  valid_set's binary_logloss: 0.0459682\n[5400]  valid_set's binary_logloss: 0.045935\n[5450]  valid_set's binary_logloss: 0.0459093\n[5500]  valid_set's binary_logloss: 0.045864\n[5550]  valid_set's binary_logloss: 0.0458426\n[5600]  valid_set's binary_logloss: 0.0458031\n[5650]  valid_set's binary_logloss: 0.0457566\n[5700]  valid_set's binary_logloss: 0.0457197\n[5750]  valid_set's binary_logloss: 0.045645\n[5800]  valid_set's binary_logloss: 0.0456126\n[5850]  valid_set's binary_logloss: 0.0455753\n[5900]  valid_set's binary_logloss: 0.0455112\n[5950]  valid_set's binary_logloss: 0.0454972\n[6000]  valid_set's binary_logloss: 0.0454535\n[6050]  valid_set's binary_logloss: 0.045403\n[6100]  valid_set's binary_logloss: 0.0453709\n[6150]  valid_set's binary_logloss: 0.0453439\n[6200]  valid_set's binary_logloss: 0.0453197\n[6250]  valid_set's binary_logloss: 0.0452929\n[6300]  valid_set's binary_logloss: 0.0452681\n[6350]  valid_set's binary_logloss: 0.0452358\n[6400]  valid_set's binary_logloss: 0.0452135\n[6450]  valid_set's binary_logloss: 0.0451854\n[6500]  valid_set's binary_logloss: 0.045153\n[6550]  valid_set's binary_logloss: 0.045136\n[6600]  valid_set's binary_logloss: 0.0451256\n[6650]  valid_set's binary_logloss: 0.0450953\n[6700]  valid_set's binary_logloss: 0.0450769\n[6750]  valid_set's binary_logloss: 0.0450618\n[6800]  valid_set's binary_logloss: 0.0450364\n[6850]  valid_set's binary_logloss: 0.0450598\n[6900]  valid_set's binary_logloss: 0.0450282\n[50]    valid_set's binary_logloss: 0.096029\n[100]   valid_set's binary_logloss: 0.0875963\n[150]   valid_set's binary_logloss: 0.0828263\n[200]   valid_set's binary_logloss: 0.079459\n[250]   valid_set's binary_logloss: 0.076807\n[300]   valid_set's binary_logloss: 0.0746964\n[350]   valid_set's binary_logloss: 0.0728091\n[400]   valid_set's binary_logloss: 0.0712396\n[450]   valid_set's binary_logloss: 0.0699792\n[500]   valid_set's binary_logloss: 0.0688837\n[550]   valid_set's binary_logloss: 0.0677572\n[600]   valid_set's binary_logloss: 0.0667425\n[650]   valid_set's binary_logloss: 0.0658026\n[700]   valid_set's binary_logloss: 0.0647754\n[750]   valid_set's binary_logloss: 0.0639841\n[800]   valid_set's binary_logloss: 0.0633219\n[850]   valid_set's binary_logloss: 0.062481\n[900]   valid_set's binary_logloss: 0.0618838\n[950]   valid_set's binary_logloss: 0.0612218\n[1000]  valid_set's binary_logloss: 0.0606204\n[1050]  valid_set's binary_logloss: 0.0601329\n[1100]  valid_set's binary_logloss: 0.0596067\n[1150]  valid_set's binary_logloss: 0.059146\n[1200]  valid_set's binary_logloss: 0.0586892\n[1250]  valid_set's binary_logloss: 0.0581933\n[1300]  valid_set's binary_logloss: 0.0577691\n[1350]  valid_set's binary_logloss: 0.0572867\n[1400]  valid_set's binary_logloss: 0.0568609\n[1450]  valid_set's binary_logloss: 0.0563865\n[1500]  valid_set's binary_logloss: 0.0559666\n[1550]  valid_set's binary_logloss: 0.0556471\n[1600]  valid_set's binary_logloss: 0.055327\n[1650]  valid_set's binary_logloss: 0.0549609\n[1700]  valid_set's binary_logloss: 0.0546156\n[1750]  valid_set's binary_logloss: 0.0542652\n[1800]  valid_set's binary_logloss: 0.0539369\n[1850]  valid_set's binary_logloss: 0.0536615\n[1900]  valid_set's binary_logloss: 0.0533676\n[1950]  valid_set's binary_logloss: 0.053119\n[2000]  valid_set's binary_logloss: 0.0528895\n[2050]  valid_set's binary_logloss: 0.0526299\n[2100]  valid_set's binary_logloss: 0.0523551\n[2150]  valid_set's binary_logloss: 0.0521056\n[2200]  valid_set's binary_logloss: 0.0518746\n[2250]  valid_set's binary_logloss: 0.051583\n[2300]  valid_set's binary_logloss: 0.0513986\n[2350]  valid_set's binary_logloss: 0.0511758\n[2400]  valid_set's binary_logloss: 0.0509808\n[2450]  valid_set's binary_logloss: 0.0508065\n[2500]  valid_set's binary_logloss: 0.050617\n[2550]  valid_set's binary_logloss: 0.0504458\n[2600]  valid_set's binary_logloss: 0.0502504\n[2650]  valid_set's binary_logloss: 0.0500439\n[2700]  valid_set's binary_logloss: 0.0498422\n[2750]  valid_set's binary_logloss: 0.0497158\n[2800]  valid_set's binary_logloss: 0.0495342\n[2850]  valid_set's binary_logloss: 0.0493658\n[2900]  valid_set's binary_logloss: 0.0492014\n[2950]  valid_set's binary_logloss: 0.0490622\n[3000]  valid_set's binary_logloss: 0.0489108\n[3050]  valid_set's binary_logloss: 0.0487223\n[3100]  valid_set's binary_logloss: 0.0485742\n[3150]  valid_set's binary_logloss: 0.0484105\n[3200]  valid_set's binary_logloss: 0.0482798\n[3250]  valid_set's binary_logloss: 0.0481619\n[3300]  valid_set's binary_logloss: 0.0480308\n[3350]  valid_set's binary_logloss: 0.0478707\n[3400]  valid_set's binary_logloss: 0.0477059\n[3450]  valid_set's binary_logloss: 0.0475891\n[3500]  valid_set's binary_logloss: 0.0474538\n[3550]  valid_set's binary_logloss: 0.0473513\n[3600]  valid_set's binary_logloss: 0.0472098\n[3650]  valid_set's binary_logloss: 0.0470663\n[3700]  valid_set's binary_logloss: 0.0469297\n[3750]  valid_set's binary_logloss: 0.0468268\n[3800]  valid_set's binary_logloss: 0.0467256\n[3850]  valid_set's binary_logloss: 0.0466264\n[3900]  valid_set's binary_logloss: 0.0465223\n[3950]  valid_set's binary_logloss: 0.046401\n[4000]  valid_set's binary_logloss: 0.046291\n[4050]  valid_set's binary_logloss: 0.0462018\n[4100]  valid_set's binary_logloss: 0.0460895\n[4150]  valid_set's binary_logloss: 0.0460154\n[4200]  valid_set's binary_logloss: 0.0459285\n[4250]  valid_set's binary_logloss: 0.045827\n[4300]  valid_set's binary_logloss: 0.0457041\n[4350]  valid_set's binary_logloss: 0.0456109\n[4400]  valid_set's binary_logloss: 0.0455477\n[4450]  valid_set's binary_logloss: 0.0454325\n[4500]  valid_set's binary_logloss: 0.0453829\n[4550]  valid_set's binary_logloss: 0.0453399\n[4600]  valid_set's binary_logloss: 0.0452509\n[4650]  valid_set's binary_logloss: 0.0451889\n[4700]  valid_set's binary_logloss: 0.0451453\n[4750]  valid_set's binary_logloss: 0.0450557\n[4800]  valid_set's binary_logloss: 0.0449748\n[4850]  valid_set's binary_logloss: 0.044914\n[4900]  valid_set's binary_logloss: 0.0448246\n[4950]  valid_set's binary_logloss: 0.0447387\n[5000]  valid_set's binary_logloss: 0.0446659\n[5050]  valid_set's binary_logloss: 0.0445939\n[5100]  valid_set's binary_logloss: 0.0445724\n[5150]  valid_set's binary_logloss: 0.0445304\n[5200]  valid_set's binary_logloss: 0.0444925\n[5250]  valid_set's binary_logloss: 0.0444439\n[5300]  valid_set's binary_logloss: 0.0443933\n[5350]  valid_set's binary_logloss: 0.0443689\n[5400]  valid_set's binary_logloss: 0.0443306\n[5450]  valid_set's binary_logloss: 0.0443063\n[5500]  valid_set's binary_logloss: 0.0442446\n[5550]  valid_set's binary_logloss: 0.0441769\n[5600]  valid_set's binary_logloss: 0.0441459\n[5650]  valid_set's binary_logloss: 0.0440855\n[5700]  valid_set's binary_logloss: 0.0440464\n[5750]  valid_set's binary_logloss: 0.0440239\n[5800]  valid_set's binary_logloss: 0.0439486\n[5850]  valid_set's binary_logloss: 0.0439129\n[5900]  valid_set's binary_logloss: 0.0438784\n[5950]  valid_set's binary_logloss: 0.0438326\n[6000]  valid_set's binary_logloss: 0.0438114\n[6050]  valid_set's binary_logloss: 0.0437905\n[6100]  valid_set's binary_logloss: 0.0437841\n[6150]  valid_set's binary_logloss: 0.0437536\n[6200]  valid_set's binary_logloss: 0.0437328\n[6250]  valid_set's binary_logloss: 0.0437097\n[6300]  valid_set's binary_logloss: 0.0436838\n[6350]  valid_set's binary_logloss: 0.0436201\n[6400]  valid_set's binary_logloss: 0.0435791\n[6450]  valid_set's binary_logloss: 0.0435448\n[6500]  valid_set's binary_logloss: 0.0435206\n[6550]  valid_set's binary_logloss: 0.0434969\n[6600]  valid_set's binary_logloss: 0.0434361\n[6650]  valid_set's binary_logloss: 0.0434091\n[6700]  valid_set's binary_logloss: 0.0433701\n[6750]  valid_set's binary_logloss: 0.0433598\n[6800]  valid_set's binary_logloss: 0.0433449\n[6850]  valid_set's binary_logloss: 0.043322\n[6900]  valid_set's binary_logloss: 0.0433011\n[6950]  valid_set's binary_logloss: 0.0432873\n[7000]  valid_set's binary_logloss: 0.0432648\n[7050]  valid_set's binary_logloss: 0.0432352\n[7100]  valid_set's binary_logloss: 0.0431976\n[7150]  valid_set's binary_logloss: 0.0431651\n[7200]  valid_set's binary_logloss: 0.0431723\n[50]    valid_set's binary_logloss: 0.0963595\n[100]   valid_set's binary_logloss: 0.0874971\n[150]   valid_set's binary_logloss: 0.0822255\n[200]   valid_set's binary_logloss: 0.0787811\n[250]   valid_set's binary_logloss: 0.0763279\n[300]   valid_set's binary_logloss: 0.0742303\n[350]   valid_set's binary_logloss: 0.0726398\n[400]   valid_set's binary_logloss: 0.0712395\n[450]   valid_set's binary_logloss: 0.0699367\n[500]   valid_set's binary_logloss: 0.0686127\n[550]   valid_set's binary_logloss: 0.0675576\n[600]   valid_set's binary_logloss: 0.0666049\n[650]   valid_set's binary_logloss: 0.0656221\n[700]   valid_set's binary_logloss: 0.0647134\n[750]   valid_set's binary_logloss: 0.0639351\n[800]   valid_set's binary_logloss: 0.06328\n[850]   valid_set's binary_logloss: 0.0625951\n[900]   valid_set's binary_logloss: 0.062073\n[950]   valid_set's binary_logloss: 0.0615084\n[1000]  valid_set's binary_logloss: 0.0609661\n[1050]  valid_set's binary_logloss: 0.0604399\n[1100]  valid_set's binary_logloss: 0.0600036\n[1150]  valid_set's binary_logloss: 0.0595467\n[1200]  valid_set's binary_logloss: 0.0589833\n[1250]  valid_set's binary_logloss: 0.0586464\n[1300]  valid_set's binary_logloss: 0.05828\n[1350]  valid_set's binary_logloss: 0.0578237\n[1400]  valid_set's binary_logloss: 0.0574375\n[1450]  valid_set's binary_logloss: 0.0570635\n[1500]  valid_set's binary_logloss: 0.0566872\n[1550]  valid_set's binary_logloss: 0.0563722\n[1600]  valid_set's binary_logloss: 0.0560513\n[1650]  valid_set's binary_logloss: 0.0557359\n[1700]  valid_set's binary_logloss: 0.0554226\n[1750]  valid_set's binary_logloss: 0.0551295\n[1800]  valid_set's binary_logloss: 0.0548514\n[1850]  valid_set's binary_logloss: 0.054562\n[1900]  valid_set's binary_logloss: 0.0542853\n[1950]  valid_set's binary_logloss: 0.0539922\n[2000]  valid_set's binary_logloss: 0.0537153\n[2050]  valid_set's binary_logloss: 0.0535054\n[2100]  valid_set's binary_logloss: 0.053234\n[2150]  valid_set's binary_logloss: 0.0530146\n[2200]  valid_set's binary_logloss: 0.0528032\n[2250]  valid_set's binary_logloss: 0.0525949\n[2300]  valid_set's binary_logloss: 0.0523296\n[2350]  valid_set's binary_logloss: 0.0520953\n[2400]  valid_set's binary_logloss: 0.0519209\n[2450]  valid_set's binary_logloss: 0.0517044\n[2500]  valid_set's binary_logloss: 0.0515098\n[2550]  valid_set's binary_logloss: 0.051358\n[2600]  valid_set's binary_logloss: 0.051202\n[2650]  valid_set's binary_logloss: 0.0510337\n[2700]  valid_set's binary_logloss: 0.0508504\n[2750]  valid_set's binary_logloss: 0.0506773\n[2800]  valid_set's binary_logloss: 0.0505178\n[2850]  valid_set's binary_logloss: 0.0503554\n[2900]  valid_set's binary_logloss: 0.0501675\n[2950]  valid_set's binary_logloss: 0.0500212\n[3000]  valid_set's binary_logloss: 0.0498818\n[3050]  valid_set's binary_logloss: 0.0497166\n[3100]  valid_set's binary_logloss: 0.0495503\n[3150]  valid_set's binary_logloss: 0.0494104\n[3200]  valid_set's binary_logloss: 0.0492911\n[3250]  valid_set's binary_logloss: 0.049172\n[3300]  valid_set's binary_logloss: 0.0490426\n[3350]  valid_set's binary_logloss: 0.048887\n[3400]  valid_set's binary_logloss: 0.0488056\n[3450]  valid_set's binary_logloss: 0.048684\n[3500]  valid_set's binary_logloss: 0.0485444\n[3550]  valid_set's binary_logloss: 0.0484011\n[3600]  valid_set's binary_logloss: 0.0482951\n[3650]  valid_set's binary_logloss: 0.0481692\n[3700]  valid_set's binary_logloss: 0.0480661\n[3750]  valid_set's binary_logloss: 0.047946\n[3800]  valid_set's binary_logloss: 0.0478613\n[3850]  valid_set's binary_logloss: 0.0477531\n[3900]  valid_set's binary_logloss: 0.0476474\n[3950]  valid_set's binary_logloss: 0.0475589\n[4000]  valid_set's binary_logloss: 0.0474945\n[4050]  valid_set's binary_logloss: 0.0474054\n[4100]  valid_set's binary_logloss: 0.0473383\n[4150]  valid_set's binary_logloss: 0.0472701\n[4200]  valid_set's binary_logloss: 0.0471839\n[4250]  valid_set's binary_logloss: 0.0471112\n[4300]  valid_set's binary_logloss: 0.0470491\n[4350]  valid_set's binary_logloss: 0.0469657\n[4400]  valid_set's binary_logloss: 0.0469111\n[4450]  valid_set's binary_logloss: 0.0468494\n[4500]  valid_set's binary_logloss: 0.0467667\n[4550]  valid_set's binary_logloss: 0.0466633\n[4600]  valid_set's binary_logloss: 0.0466025\n[4650]  valid_set's binary_logloss: 0.0465532\n[4700]  valid_set's binary_logloss: 0.0464863\n[4750]  valid_set's binary_logloss: 0.0464151\n[4800]  valid_set's binary_logloss: 0.046364\n[4850]  valid_set's binary_logloss: 0.0463181\n[4900]  valid_set's binary_logloss: 0.0462468\n[4950]  valid_set's binary_logloss: 0.0461862\n[5000]  valid_set's binary_logloss: 0.0461098\n[5050]  valid_set's binary_logloss: 0.0460828\n[5100]  valid_set's binary_logloss: 0.0460382\n[5150]  valid_set's binary_logloss: 0.0460061\n[5200]  valid_set's binary_logloss: 0.0459409\n[5250]  valid_set's binary_logloss: 0.0459004\n[5300]  valid_set's binary_logloss: 0.0458499\n[5350]  valid_set's binary_logloss: 0.0458205\n[5400]  valid_set's binary_logloss: 0.0457765\n[5450]  valid_set's binary_logloss: 0.0457331\n[5500]  valid_set's binary_logloss: 0.0456862\n[5550]  valid_set's binary_logloss: 0.045656\n[5600]  valid_set's binary_logloss: 0.0456313\n[5650]  valid_set's binary_logloss: 0.0455883\n[5700]  valid_set's binary_logloss: 0.0455565\n[5750]  valid_set's binary_logloss: 0.0455113\n[5800]  valid_set's binary_logloss: 0.0454627\n[5850]  valid_set's binary_logloss: 0.0454342\n[5900]  valid_set's binary_logloss: 0.0453972\n[5950]  valid_set's binary_logloss: 0.0453747\n[6000]  valid_set's binary_logloss: 0.0453566\n[6050]  valid_set's binary_logloss: 0.0453355\n[6100]  valid_set's binary_logloss: 0.0453014\n[6150]  valid_set's binary_logloss: 0.0452635\n[6200]  valid_set's binary_logloss: 0.0452424\n[6250]  valid_set's binary_logloss: 0.0452184\n[6300]  valid_set's binary_logloss: 0.0451961\n[6350]  valid_set's binary_logloss: 0.0451502\n[6400]  valid_set's binary_logloss: 0.0451273\n[6450]  valid_set's binary_logloss: 0.0451117\n[6500]  valid_set's binary_logloss: 0.0450916\n[6550]  valid_set's binary_logloss: 0.0450568\n[6600]  valid_set's binary_logloss: 0.0450442\n[6650]  valid_set's binary_logloss: 0.0450293\n[6700]  valid_set's binary_logloss: 0.0450295\n[6750]  valid_set's binary_logloss: 0.0450123\n[6800]  valid_set's binary_logloss: 0.0450061\n[6850]  valid_set's binary_logloss: 0.045008\n[6900]  valid_set's binary_logloss: 0.0450171\n[6950]  valid_set's binary_logloss: 0.0450367\n[7000]  valid_set's binary_logloss: 0.0450351\n[7050]  valid_set's binary_logloss: 0.0450223\n[7100]  valid_set's binary_logloss: 0.045006\n[7150]  valid_set's binary_logloss: 0.0450014\n[7200]  valid_set's binary_logloss: 0.045005\n[7250]  valid_set's binary_logloss: 0.0450355\n[7300]  valid_set's binary_logloss: 0.0450434\n[7350]  valid_set's binary_logloss: 0.045031\n[7400]  valid_set's binary_logloss: 0.0450103\n[7450]  valid_set's binary_logloss: 0.0450016\n[7500]  valid_set's binary_logloss: 0.0449951\n[7550]  valid_set's binary_logloss: 0.0449908\n[50]    valid_set's binary_logloss: 0.0962948\n[100]   valid_set's binary_logloss: 0.0875366\n[150]   valid_set's binary_logloss: 0.0826311\n[200]   valid_set's binary_logloss: 0.0791537\n[250]   valid_set's binary_logloss: 0.0766791\n[300]   valid_set's binary_logloss: 0.0746304\n[350]   valid_set's binary_logloss: 0.0730532\n[400]   valid_set's binary_logloss: 0.0715409\n[450]   valid_set's binary_logloss: 0.0703641\n[500]   valid_set's binary_logloss: 0.0693024\n[550]   valid_set's binary_logloss: 0.0682694\n[600]   valid_set's binary_logloss: 0.0672797\n[650]   valid_set's binary_logloss: 0.0664449\n[700]   valid_set's binary_logloss: 0.0656632\n[750]   valid_set's binary_logloss: 0.0648896\n[800]   valid_set's binary_logloss: 0.0642512\n[850]   valid_set's binary_logloss: 0.0636429\n[900]   valid_set's binary_logloss: 0.0629948\n[950]   valid_set's binary_logloss: 0.0623619\n[1000]  valid_set's binary_logloss: 0.0618802\n[1050]  valid_set's binary_logloss: 0.0614347\n[1100]  valid_set's binary_logloss: 0.0609686\n[1150]  valid_set's binary_logloss: 0.0604567\n[1200]  valid_set's binary_logloss: 0.0599484\n[1250]  valid_set's binary_logloss: 0.0595137\n[1300]  valid_set's binary_logloss: 0.0590859\n[1350]  valid_set's binary_logloss: 0.0586598\n[1400]  valid_set's binary_logloss: 0.0582442\n[1450]  valid_set's binary_logloss: 0.057846\n[1500]  valid_set's binary_logloss: 0.0575185\n[1550]  valid_set's binary_logloss: 0.0571447\n[1600]  valid_set's binary_logloss: 0.0568225\n[1650]  valid_set's binary_logloss: 0.0564835\n[1700]  valid_set's binary_logloss: 0.0562151\n[1750]  valid_set's binary_logloss: 0.0559457\n[1800]  valid_set's binary_logloss: 0.055691\n[1850]  valid_set's binary_logloss: 0.0553846\n[1900]  valid_set's binary_logloss: 0.0551046\n[1950]  valid_set's binary_logloss: 0.0548127\n[2000]  valid_set's binary_logloss: 0.0545277\n[2050]  valid_set's binary_logloss: 0.0542339\n[2100]  valid_set's binary_logloss: 0.0539807\n[2150]  valid_set's binary_logloss: 0.0537343\n[2200]  valid_set's binary_logloss: 0.0534642\n[2250]  valid_set's binary_logloss: 0.0532622\n[2300]  valid_set's binary_logloss: 0.0530425\n[2350]  valid_set's binary_logloss: 0.0528564\n[2400]  valid_set's binary_logloss: 0.0526538\n[2450]  valid_set's binary_logloss: 0.0524329\n[2500]  valid_set's binary_logloss: 0.0521854\n[2550]  valid_set's binary_logloss: 0.0519891\n[2600]  valid_set's binary_logloss: 0.0518008\n[2650]  valid_set's binary_logloss: 0.0516694\n[2700]  valid_set's binary_logloss: 0.0514709\n[2750]  valid_set's binary_logloss: 0.0512856\n[2800]  valid_set's binary_logloss: 0.0511404\n[2850]  valid_set's binary_logloss: 0.0509686\n[2900]  valid_set's binary_logloss: 0.0508043\n[2950]  valid_set's binary_logloss: 0.0506575\n[3000]  valid_set's binary_logloss: 0.0504941\n[3050]  valid_set's binary_logloss: 0.0503218\n[3100]  valid_set's binary_logloss: 0.0501923\n[3150]  valid_set's binary_logloss: 0.050039\n[3200]  valid_set's binary_logloss: 0.0498999\n[3250]  valid_set's binary_logloss: 0.0497412\n[3300]  valid_set's binary_logloss: 0.0496217\n[3350]  valid_set's binary_logloss: 0.0495241\n[3400]  valid_set's binary_logloss: 0.0494252\n[3450]  valid_set's binary_logloss: 0.0493008\n[3500]  valid_set's binary_logloss: 0.0491831\n[3550]  valid_set's binary_logloss: 0.0490707\n[3600]  valid_set's binary_logloss: 0.0489746\n[3650]  valid_set's binary_logloss: 0.0488395\n[3700]  valid_set's binary_logloss: 0.0487442\n[3750]  valid_set's binary_logloss: 0.0486169\n[3800]  valid_set's binary_logloss: 0.0485238\n[3850]  valid_set's binary_logloss: 0.048429\n[3900]  valid_set's binary_logloss: 0.0483117\n[3950]  valid_set's binary_logloss: 0.0482465\n[4000]  valid_set's binary_logloss: 0.0481866\n[4050]  valid_set's binary_logloss: 0.0480781\n[4100]  valid_set's binary_logloss: 0.0479657\n[4150]  valid_set's binary_logloss: 0.0479174\n[4200]  valid_set's binary_logloss: 0.0478491\n[4250]  valid_set's binary_logloss: 0.0477598\n[4300]  valid_set's binary_logloss: 0.0476699\n[4350]  valid_set's binary_logloss: 0.0475929\n[4400]  valid_set's binary_logloss: 0.0475102\n[4450]  valid_set's binary_logloss: 0.0474583\n[4500]  valid_set's binary_logloss: 0.0473897\n[4550]  valid_set's binary_logloss: 0.0473208\n[4600]  valid_set's binary_logloss: 0.0472478\n[4650]  valid_set's binary_logloss: 0.0471707\n[4700]  valid_set's binary_logloss: 0.0470975\n[4750]  valid_set's binary_logloss: 0.0470223\n[4800]  valid_set's binary_logloss: 0.046933\n[4850]  valid_set's binary_logloss: 0.0468955\n[4900]  valid_set's binary_logloss: 0.0467916\n[4950]  valid_set's binary_logloss: 0.0467316\n[5000]  valid_set's binary_logloss: 0.0466726\n[5050]  valid_set's binary_logloss: 0.0466299\n[5100]  valid_set's binary_logloss: 0.0465787\n[5150]  valid_set's binary_logloss: 0.0465216\n[5200]  valid_set's binary_logloss: 0.0464555\n[5250]  valid_set's binary_logloss: 0.0464032\n[5300]  valid_set's binary_logloss: 0.0463644\n[5350]  valid_set's binary_logloss: 0.046345\n[5400]  valid_set's binary_logloss: 0.0463021\n[5450]  valid_set's binary_logloss: 0.0462572\n[5500]  valid_set's binary_logloss: 0.0462455\n[5550]  valid_set's binary_logloss: 0.046189\n[5600]  valid_set's binary_logloss: 0.0461821\n[5650]  valid_set's binary_logloss: 0.0461512\n[5700]  valid_set's binary_logloss: 0.0461375\n[5750]  valid_set's binary_logloss: 0.0460782\n[5800]  valid_set's binary_logloss: 0.046044\n[5850]  valid_set's binary_logloss: 0.0460181\n[5900]  valid_set's binary_logloss: 0.0459886\n[5950]  valid_set's binary_logloss: 0.0459739\n[6000]  valid_set's binary_logloss: 0.0459499\n[6050]  valid_set's binary_logloss: 0.0459214\n[6100]  valid_set's binary_logloss: 0.0458806\n[6150]  valid_set's binary_logloss: 0.0458286\n[6200]  valid_set's binary_logloss: 0.0458169\n[6250]  valid_set's binary_logloss: 0.0458025\n[6300]  valid_set's binary_logloss: 0.045789\n[6350]  valid_set's binary_logloss: 0.045785\n[6400]  valid_set's binary_logloss: 0.0457392\n[6450]  valid_set's binary_logloss: 0.0457052\n[6500]  valid_set's binary_logloss: 0.0456856\n[6550]  valid_set's binary_logloss: 0.0456551\n[6600]  valid_set's binary_logloss: 0.0456049\n[6650]  valid_set's binary_logloss: 0.0455808\n[6700]  valid_set's binary_logloss: 0.0455627\n[6750]  valid_set's binary_logloss: 0.0455433\n[6800]  valid_set's binary_logloss: 0.0455255\n[6850]  valid_set's binary_logloss: 0.0454953\n[6900]  valid_set's binary_logloss: 0.0454847\n[6950]  valid_set's binary_logloss: 0.0454637\n[7000]  valid_set's binary_logloss: 0.0454715\n[7050]  valid_set's binary_logloss: 0.0454629\n[7100]  valid_set's binary_logloss: 0.0454673\n[7150]  valid_set's binary_logloss: 0.0454424\n[7200]  valid_set's binary_logloss: 0.0454276\n[7250]  valid_set's binary_logloss: 0.0454074\n[7300]  valid_set's binary_logloss: 0.045406\n[7350]  valid_set's binary_logloss: 0.0454003\n[7400]  valid_set's binary_logloss: 0.045375\n[7450]  valid_set's binary_logloss: 0.0453647\n[7500]  valid_set's binary_logloss: 0.0453676\n[7550]  valid_set's binary_logloss: 0.0453669\n[7600]  valid_set's binary_logloss: 0.0453402\n[7650]  valid_set's binary_logloss: 0.045347\n[7700]  valid_set's binary_logloss: 0.0453404\n[7750]  valid_set's binary_logloss: 0.0452905\n[7800]  valid_set's binary_logloss: 0.0452678\n[7850]  valid_set's binary_logloss: 0.0452621\n[7900]  valid_set's binary_logloss: 0.0452721\n[7950]  valid_set's binary_logloss: 0.0453034\n[8000]  valid_set's binary_logloss: 0.0453192\n[8050]  valid_set's binary_logloss: 0.0453174\n[8100]  valid_set's binary_logloss: 0.04531\n[8150]  valid_set's binary_logloss: 0.0453097\n[8200]  valid_set's binary_logloss: 0.0453064\n[8250]  valid_set's binary_logloss: 0.0453017\n[8300]  valid_set's binary_logloss: 0.0453148\n[8350]  valid_set's binary_logloss: 0.0453153\n[8400]  valid_set's binary_logloss: 0.0453138\n[8450]  valid_set's binary_logloss: 0.0453403\n[8500]  valid_set's binary_logloss: 0.0453684\n[8550]  valid_set's binary_logloss: 0.0453877\n[8600]  valid_set's binary_logloss: 0.0454095\n[8650]  valid_set's binary_logloss: 0.0453896\n[8700]  valid_set's binary_logloss: 0.0453852\n[8750]  valid_set's binary_logloss: 0.045368\n[8800]  valid_set's binary_logloss: 0.0453844\n[8850]  valid_set's binary_logloss: 0.0453988\n[8900]  valid_set's binary_logloss: 0.0454074\n[8950]  valid_set's binary_logloss: 0.0453968\n[9000]  valid_set's binary_logloss: 0.0454079\n[9050]  valid_set's binary_logloss: 0.0454114\n[9100]  valid_set's binary_logloss: 0.045438\n*** Summary of fit() ***\nEstimated performance of each model:\n                 model  score_val  pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0    LightGBMXT_BAG_L1   0.971274      28.143910  1113.060551               28.143910        1113.060551            1       True          1\n1  WeightedEnsemble_L2   0.971274      28.211386  1113.131086                0.067476           0.070535            2       True          2\nNumber of models trained: 2\nTypes of models trained:\n{'StackerEnsembleModel_LGB', 'WeightedEnsembleModel'}\nBagging used: True  (with 8 folds)\nMulti-layer stack-ensembling used: False \nFeature Metadata (Processed):\n(raw dtype, special dtypes):\n('category', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n('float', [])    : 394 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n('int', [])      :   3 | ['TransactionID', 'TransactionDT', 'card1']\n*** End of fit() summary ***\n\n\n\ntest_identity = pd.read_csv('ieee-fraud-detection/test_identity.csv')\ntest_transaction = pd.read_csv('ieee-fraud-detection/test_transaction.csv')\ntest_data = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')  # same join applied to training files\n\ny_predproba = predictor.predict_proba(test_data)\ny_predproba.head(5)  # some example predicted fraud-probabilities\n\nKeyError: \"38 required columns are missing from the provided dataset to transform using AutoMLPipelineFeatureGenerator. 38 missing columns: ['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38'] | 433 available columns: ['TransactionID', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339', 'id-01', 'id-02', 'id-03', 'id-04', 'id-05', 'id-06', 'id-07', 'id-08', 'id-09', 'id-10', 'id-11', 'id-12', 'id-13', 'id-14', 'id-15', 'id-16', 'id-17', 'id-18', 'id-19', 'id-20', 'id-21', 'id-22', 'id-23', 'id-24', 'id-25', 'id-26', 'id-27', 'id-28', 'id-29', 'id-30', 'id-31', 'id-32', 'id-33', 'id-34', 'id-35', 'id-36', 'id-37', 'id-38', 'DeviceType', 'DeviceInfo']\""
  },
  {
    "objectID": "Note.html",
    "href": "Note.html",
    "title": "Note",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nApr 12, 2023\n\n\nRef\n\n\n김보람 \n\n\n\n\nMar 13, 2023\n\n\n콰트로 블로그\n\n\n김보람 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "GNN.html",
    "href": "GNN.html",
    "title": "GNN",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 18, 2023\n\n\n[GNN] 논문 정리 (~ing)\n\n\n김보람 \n\n\n\n\nJul 18, 2023\n\n\n[GNN] Graph Basic\n\n\n김보람 \n\n\n\n\nJul 11, 2023\n\n\n[GNN] Laplacian matrix\n\n\n김보람 \n\n\n\n\nJun 2, 2023\n\n\n[GNN] An Introduction to Graph Neural Network(GNN) For Analysing Structured Data\n\n\n김보람 \n\n\n\n\nJun 2, 2023\n\n\n[GNN] Neural Network\n\n\n김보람 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "coco",
    "section": "",
    "text": "1+1\n\n2"
  }
]