{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73950eca-5fed-4ea4-83cb-2c9e620b2d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "37bbb557-7233-4b51-9c0b-80736e2878d1",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"[한국통계학회] 통계계산연구회 튜토리얼\"\n",
    "author: \"김보람\"\n",
    "date: \"06/29/2023\"\n",
    "categories:\n",
    "  - 한국통계학회\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52617a69-5d9c-4807-9e0e-13491dbb1c15",
   "metadata": {},
   "source": [
    "## ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78916bad-bea4-49b7-b843-6c70090e0413",
   "metadata": {},
   "source": [
    "해당 자료는 2023 한국통계학회 통계계산연구회 여인권 교수님 자료임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57bbdb8-bee7-4e70-bde2-50424af1e57a",
   "metadata": {},
   "source": [
    "https://github.com/statfunny/Bigdata-statistical-Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb664f64-6bc7-4f0d-b7eb-88013c984763",
   "metadata": {},
   "source": [
    "아래 내용 실행하기 전에 깔아야 할 게 엄청 많음...... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "060e33cc-28f3-42a5-ae65-3d8182390b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyarrow pyspark==3.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4219c1-5153-4588-a34c-3de9ab1b091f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ec5fe90",
   "metadata": {},
   "source": [
    "## 효율적인 메모리 관리와 프로그램 작성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103f885f",
   "metadata": {},
   "source": [
    "### 빅데이터분석에서의 메모리 관리\n",
    "- 문자열보다는 범주형 \n",
    "- 범위가 제한적인 정수형\n",
    "- 최소한의 실수형\n",
    "- 이진(binary)인 경우 Boolean(True/False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33d61e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d414f6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def 데이터프레임생성(size):\n",
    "    df = pd.DataFrame()\n",
    "    df[\"나이\"] = np.random.choice(100,size)\n",
    "    df[\"수행평가1\"] = np.random.choice([\"A\",\"B\",\"C\",\"D\",\"F\"], size)\n",
    "    df[\"수행평가2\"] = np.random.choice([\"상\",\"중\",\"하\"], size)    \n",
    "    df[\"학점\"] = np.random.choice([\"[0,3)\",\"[3,3.5)\",\"[3.5,4)\",\"[4,4.3]\"], size)\n",
    "    df[\"합격확률\"] = np.random.uniform(0,1,size)\n",
    "    df[\"결과\"] = np.random.choice([\"합격\",\"불합격\"],size)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a367c7e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype  \n",
      "---  ------  --------------    -----  \n",
      " 0   나이      1000000 non-null  int64  \n",
      " 1   수행평가1   1000000 non-null  object \n",
      " 2   수행평가2   1000000 non-null  object \n",
      " 3   학점      1000000 non-null  object \n",
      " 4   합격확률    1000000 non-null  float64\n",
      " 5   결과      1000000 non-null  object \n",
      "dtypes: float64(1), int64(1), object(4)\n",
      "memory usage: 45.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df = 데이터프레임생성(1000000)\n",
    "# df = 데이터프레임생성(1_000_000) # 컴마 대신에 언더바를 작성해서 나눠주기\n",
    "df1 = df.copy()\n",
    "df2 = df.copy()\n",
    "df1.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d79f824",
   "metadata": {},
   "source": [
    "### 수행작업\n",
    "- 수행평가1, 학점에 따라 데이터를 나누고 그 안에서 나이의 순위\n",
    "- 수행평가1, 학점에 따라 데이터를 나누고 그 안에서 합격확률의 순위\n",
    "- 수행평가1, 학점, 결과에 따라 데이터를 나누고 그 안에서 합격확률의 순위\n",
    "- 수행시간계산 \n",
    "    - %timeit : 반복 작업을 하며 해당 프로그램을 수행하는데 걸린 시간의 평균과 표준편차 제고 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46fac89b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157 ms ± 533 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "219 ms ± 2.95 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "256 ms ± 384 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df1[\"순위1\"] = df1.groupby([\"수행평가1\",\"학점\"])[\"나이\"].rank()\n",
    "%timeit df1[\"순위2\"] = df1.groupby([\"수행평가1\",\"학점\"])[\"합격확률\"].rank()\n",
    "%timeit df1[\"순위3\"] = df1.groupby([\"수행평가1\",\"학점\",\"결과\"])[\"합격확률\"].rank()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37945d4",
   "metadata": {},
   "source": [
    "### string $\\rightarrow$ 범주형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecef0612",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype   \n",
      "---  ------  --------------    -----   \n",
      " 0   나이      1000000 non-null  int64   \n",
      " 1   수행평가1   1000000 non-null  category\n",
      " 2   수행평가2   1000000 non-null  category\n",
      " 3   학점      1000000 non-null  category\n",
      " 4   합격확률    1000000 non-null  float64 \n",
      " 5   결과      1000000 non-null  object  \n",
      "dtypes: category(3), float64(1), int64(1), object(1)\n",
      "memory usage: 25.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df2[\"수행평가1\"] = df2[\"수행평가1\"].astype('category')\n",
    "df2[\"수행평가2\"] = df2[\"수행평가2\"].astype('category')\n",
    "df2[\"학점\"] = df2[\"학점\"].astype('category')\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859b265",
   "metadata": {},
   "source": [
    "### Downcastrng\n",
    "- int8: -128~127\n",
    "    - uint8: 0~255\n",
    "- int16: -32,768 ~ 32,767\n",
    "    - uint16: 0~65,535\n",
    "- int32: -2,147,483,648~2,147,483,647\n",
    "    - uint32: 0~ 4,294,967,295\n",
    "- int64: -9,223,372,036,854,775,808 ~ -9,223,372,036,854,775,807\n",
    "    - uint64: 0~18,446,744,073,709,551,615"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8080f6b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype   \n",
      "---  ------  --------------    -----   \n",
      " 0   나이      1000000 non-null  int8    \n",
      " 1   수행평가1   1000000 non-null  category\n",
      " 2   수행평가2   1000000 non-null  category\n",
      " 3   학점      1000000 non-null  category\n",
      " 4   합격확률    1000000 non-null  float64 \n",
      " 5   결과      1000000 non-null  object  \n",
      "dtypes: category(3), float64(1), int8(1), object(1)\n",
      "memory usage: 19.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df2[\"나이\"] = df2[\"나이\"].astype('int8')\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a59028aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype   \n",
      "---  ------  --------------    -----   \n",
      " 0   나이      1000000 non-null  int8    \n",
      " 1   수행평가1   1000000 non-null  category\n",
      " 2   수행평가2   1000000 non-null  category\n",
      " 3   학점      1000000 non-null  category\n",
      " 4   합격확률    1000000 non-null  float32 \n",
      " 5   결과      1000000 non-null  object  \n",
      "dtypes: category(3), float32(1), int8(1), object(1)\n",
      "memory usage: 15.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df2[\"합격확률\"] = df2[\"합격확률\"].astype('float32')\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "340a653d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype   \n",
      "---  ------  --------------    -----   \n",
      " 0   나이      1000000 non-null  int8    \n",
      " 1   수행평가1   1000000 non-null  category\n",
      " 2   수행평가2   1000000 non-null  category\n",
      " 3   학점      1000000 non-null  category\n",
      " 4   합격확률    1000000 non-null  float32 \n",
      " 5   결과      1000000 non-null  bool    \n",
      "dtypes: bool(1), category(3), float32(1), int8(1)\n",
      "memory usage: 8.6 MB\n"
     ]
    }
   ],
   "source": [
    "df2[\"결과\"] = df2[\"결과\"].map({\"합격\":True,\"불합격\":False})\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37084661",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 ms ± 377 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "164 ms ± 2.18 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "169 ms ± 1.55 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df2[\"순위1\"] = df2.groupby([\"수행평가1\",\"학점\"])[\"나이\"].rank()\n",
    "%timeit df2[\"순위2\"] = df2.groupby([\"수행평가1\",\"학점\"])[\"합격확률\"].rank()\n",
    "%timeit df2[\"순위3\"] = df2.groupby([\"수행평가1\",\"학점\",\"결과\"])[\"합격확률\"].rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cde4ae81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 파일 저장 및 읽기\n",
    "변수 = [\"나이\",\"수행평가1\",\"수행평가2\",\"학점\",\"합격확률\",\"결과\"] \n",
    "df1 = df1[변수]\n",
    "df2 = df2[변수]\n",
    "df1.to_csv(\"BSA03_df1.csv\",index=False)\n",
    "df2.to_csv('BSA03_df2.csv',index=False)\n",
    "df1csv = pd.read_csv('BSA03_df1.csv')\n",
    "df2csv = pd.read_csv('BSA03_df2.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe843a-08c5-4ab4-afcf-fd09d70d28bf",
   "metadata": {},
   "source": [
    "`df1.to_csv` : csv파일로 저장해라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dd5e94e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype  \n",
      "---  ------  --------------    -----  \n",
      " 0   나이      1000000 non-null  int64  \n",
      " 1   수행평가1   1000000 non-null  object \n",
      " 2   수행평가2   1000000 non-null  object \n",
      " 3   학점      1000000 non-null  object \n",
      " 4   합격확률    1000000 non-null  float64\n",
      " 5   결과      1000000 non-null  object \n",
      "dtypes: float64(1), int64(1), object(4)\n",
      "memory usage: 45.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df1csv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3be1740c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype  \n",
      "---  ------  --------------    -----  \n",
      " 0   나이      1000000 non-null  int64  \n",
      " 1   수행평가1   1000000 non-null  object \n",
      " 2   수행평가2   1000000 non-null  object \n",
      " 3   학점      1000000 non-null  object \n",
      " 4   합격확률    1000000 non-null  float64\n",
      " 5   결과      1000000 non-null  bool   \n",
      "dtypes: bool(1), float64(1), int64(1), object(3)\n",
      "memory usage: 39.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df2csv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acf99203",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype   \n",
      "---  ------  --------------    -----   \n",
      " 0   나이      1000000 non-null  int8    \n",
      " 1   수행평가1   1000000 non-null  category\n",
      " 2   수행평가2   1000000 non-null  category\n",
      " 3   학점      1000000 non-null  category\n",
      " 4   합격확률    1000000 non-null  float32 \n",
      " 5   결과      1000000 non-null  bool    \n",
      "dtypes: bool(1), category(3), float32(1), int8(1)\n",
      "memory usage: 8.6 MB\n"
     ]
    }
   ],
   "source": [
    "# pip install pyarrow\n",
    "df2.to_parquet('BSA03_df2.parquet')\n",
    "df2pqt = pd.read_parquet('BSA03_df2.parquet')\n",
    "df2pqt.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e456fc6f-cef1-467b-b781-9abf8a3ba000",
   "metadata": {},
   "source": [
    "- pandas에서 제공함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc745d8",
   "metadata": {},
   "source": [
    "## 효율적인 프로그램"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8de459-0e70-423c-b453-8f36cf74f673",
   "metadata": {},
   "source": [
    "> 반복적인 작업 진행시 for문 말고 아래와 같은 문법 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdffc0e",
   "metadata": {},
   "source": [
    "### 수행작업\n",
    "\"평가\"라는 새로운 변수에\n",
    "- \"나이\"가 65세 미만이거나 \"합격확률\"이 0.6 이상이고 \"학점\"이 [4,4.3]이면 \"수행평가1\"를\n",
    "- 위 조건이 아니면 \"수행평가2\"를\n",
    "대입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9be68e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def 변수추가(행자료):\n",
    "    if 행자료[\"나이\"] < 65:\n",
    "        return 행자료[\"수행평가1\"]\n",
    "    if (행자료[\"합격확률\"] >= 0.6) & (행자료[\"학점\"] == \"[4,4.3]\"):\n",
    "        return 행자료[\"수행평가1\"]\n",
    "    return(행자료[\"수행평가2\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105a0704",
   "metadata": {},
   "source": [
    "### Loop를 이용한 프로그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2839d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df =데이터프레임생성(100_000)\n",
    "df1 = df.copy()\n",
    "df2 = df.copy()\n",
    "df3 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3ebc199",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.3 s ± 1.05 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for index, row in df1.iterrows():\n",
    "    df1.loc[index,\"평가\"] = 변수추가(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3b143d",
   "metadata": {},
   "source": [
    "### Apply를 이용한 프로그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5ab6861",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.54 s ± 104 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df2[\"평가\"] = df2.apply(변수추가,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34c88c",
   "metadata": {},
   "source": [
    "### Vectorized를 이용한 프로그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3852add4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         True\n",
       "1         True\n",
       "2        False\n",
       "3        False\n",
       "4         True\n",
       "         ...  \n",
       "99995     True\n",
       "99996     True\n",
       "99997     True\n",
       "99998     True\n",
       "99999    False\n",
       "Length: 100000, dtype: bool"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df3[\"나이\"] < 65) | ((df3[\"합격확률\"] >= 0.6) & (df3[\"학점\"] == \"[4,4.3]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c41c2f47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.2 ms ± 1.66 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df3[\"평가\"] = df3[\"수행평가2\"]\n",
    "조건 = (df3[\"나이\"] < 65) | ((df3[\"합격확률\"] >= 0.6) & (df3[\"학점\"] == \"[4,4.3]\"))\n",
    "df3.loc[조건,\"평가\"] = df[\"수행평가1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75ffeb5-620d-4562-a5c2-a1a9214b49bf",
   "metadata": {},
   "source": [
    "- Vectorized로 바꾸는게 가장 빠르게 할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17cf1b5-1cd7-4b9e-a6e0-8c055acdbb5d",
   "metadata": {},
   "source": [
    "## Set up the environment variables for Pyspark, Java, Spark, and python\n",
    "- 오류 발생 시 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa7115f-aaa1-4dbb-bb32-0a22f268b138",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "import sys\n",
    "os.environ['JAVA_HOME'] = \"C:\\Java\"\n",
    "os.environ['SPARK_HOME'] = \"C:\\spark-3.4.0\"\n",
    "os.environ['PYLIB'] = \"C:\\spark-3.4.0\\python\\lib\"\n",
    "sys.path.insert(0,os.environ['PYLIB']+\"\\py4j-0.10.9.7-src.zip\")\n",
    "sys.path.insert(0,os.environ['PYLIB']+\"\\pyspark.zip\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66e03cff-32f0-46f9-a07e-e61894dc6709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c51f6cd2-f92d-47cb-9890-f261ec29df0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a09fea9430>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "스파크 = SparkSession.builder.appName('Test').getOrCreate()\n",
    "스파크"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f523e7dc-80cf-488e-aec3-5893fe8db0f5",
   "metadata": {},
   "source": [
    "### 웹브라우저에서 localhost:4040 연결\n",
    "### Pyspark에서 hdfs 데이터 불러오기\n",
    "- CMD에서 start-dfs.cmd와 start-yarn.cmd 실행 후\n",
    "- Spark 경로를 찾지 못하는 경우\n",
    "```Python\n",
    "!pip install findspark\n",
    "import findspark\n",
    "import os\n",
    "findspark.find()\n",
    "findspark.init(os.environ.get(\"SPARK_HOME\"))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee7d29e6-cd0d-459d-ab48-8e85242f0306",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---------+-------+-------------------+------+\n",
      "| _c0|      _c1|      _c2|    _c3|                _c4|   _c5|\n",
      "+----+---------+---------+-------+-------------------+------+\n",
      "|나이|수행평가1|수행평가2|   학점|           합격확률|  결과|\n",
      "|  19|        C|       하|  [0,3)| 0.1704850998911155|불합격|\n",
      "|  78|        F|       중|[3,3.5)| 0.7007295241834984|불합격|\n",
      "|  78|        F|       중|[4,4.3]|0.06793823954810418|불합격|\n",
      "|  23|        A|       중|[3.5,4)| 0.8262506446089442|  합격|\n",
      "|  97|        C|       하|[4,4.3]| 0.5911258463622218|불합격|\n",
      "|  45|        B|       하|  [0,3)| 0.3677844602679712|  합격|\n",
      "|  66|        A|       하|[3,3.5)| 0.9721303956886912|  합격|\n",
      "|  66|        F|       중|[4,4.3]|0.33333421672000396|  합격|\n",
      "|  61|        A|       상|[3,3.5)| 0.7048925310189916|불합격|\n",
      "+----+---------+---------+-------+-------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkDF1 = 스파크.read.csv(\"hdfs://localhost:9000/Spark/BSA03_df1.csv\")\n",
    "sparkDF1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53ae871d-9418-4d02-ae30-cbd6145dd047",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkDF1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1c65417-97f3-40cd-80f6-ca687d478084",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 나이: integer (nullable = true)\n",
      " |-- 수행평가1: string (nullable = true)\n",
      " |-- 수행평가2: string (nullable = true)\n",
      " |-- 학점: string (nullable = true)\n",
      " |-- 합격확률: double (nullable = true)\n",
      " |-- 결과: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkDF2 = 스파크.read.csv(\"hdfs://localhost:9000/Spark/BSA03_df1.csv\", header=True, encoding=\"utf-8\", inferSchema=\"true\")\n",
    "#sparkDF2 = 스파크.read.option('encoding','utf-8').option('header',True).option(inferSchema='True') \\\n",
    "#    .csv(\"hdfs://localhost:9000/Spark/BSA03_df1.csv\")\n",
    "sparkDF2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7308b152-0964-410c-a1de-459fae2a5535",
   "metadata": {},
   "source": [
    "`-` 하둡에 설치된 데이터를 가지고 올 때\n",
    "\n",
    "- `inferSchema=\"true\"` 설정을 해주면 형태에 맞게 가지고 옴 (위에서는 string으로 다 가져옴)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d098916c",
   "metadata": {},
   "source": [
    "### 여러개의 CSV 파일 읽기\n",
    "- can also read multiple csv files, just pass all file names by separating comma as a path\n",
    "```Python\n",
    "sparkDF = 스파크.read.csv(\"path1,path2,path3\")\n",
    "```\n",
    "### 직접 읽을 데이터의 type 지정\n",
    "```Python\n",
    "import pyspark\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType \n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType\n",
    "\n",
    "스키마 = StructType() \\\n",
    "    .add(\"나이\",IntegerType(),True) \\\n",
    "    .add(\"수행평가1\",StringType(),True) \\\n",
    "    .add(\"수행평가2\",StringType(),True) \\\n",
    "    .add(\"학점\",StringType(),True) \\\n",
    "    .add(\"합격확률\",DoubleType(),True) \\\n",
    "    .add(\"결과\",StringType(),True)\n",
    "\n",
    "sparkDF = 스파크.read.format('csv')\\\n",
    "    .option('header',True).schema(스키마)\\\n",
    "    load(\"hdfs://localhost:9000/Spark/BSA03_df1.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0db8e7b-3499-45ee-ac6d-909bd08a8642",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---------+-------+----------+-----+\n",
      "|나이|수행평가1|수행평가2|   학점|  합격확률| 결과|\n",
      "+----+---------+---------+-------+----------+-----+\n",
      "|  19|        C|       하|  [0,3)| 0.1704851|false|\n",
      "|  78|        F|       중|[3,3.5)|0.70072955|false|\n",
      "|  78|        F|       중|[4,4.3]|0.06793824|false|\n",
      "|  23|        A|       중|[3.5,4)| 0.8262507| true|\n",
      "|  97|        C|       하|[4,4.3]|0.59112585|false|\n",
      "|  45|        B|       하|  [0,3)|0.36778447| true|\n",
      "|  66|        A|       하|[3,3.5)| 0.9721304| true|\n",
      "|  66|        F|       중|[4,4.3]| 0.3333342| true|\n",
      "|  61|        A|       상|[3,3.5)| 0.7048925|false|\n",
      "|  46|        C|       상|[3,3.5)| 0.8856867|false|\n",
      "+----+---------+---------+-------+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkDF3 = 스파크.read.parquet(\"hdfs://localhost:9000/Spark/BSA03_df2.parquet\")\n",
    "sparkDF3.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c15db56-b820-4e72-a7fc-19c8be9ee7ff",
   "metadata": {},
   "source": [
    "### DataFrame을 다른 형식으로 변환하고 저장/불러오기\n",
    "- pandas DataFrame $\\Longleftrightarrow$ spark DataFrame\n",
    "- csv $\\Longleftrightarrow$ parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ef8b49f-f7f9-44e8-a78e-0a959a209fae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>나이</th>\n",
       "      <th>수행평가1</th>\n",
       "      <th>수행평가2</th>\n",
       "      <th>학점</th>\n",
       "      <th>합격확률</th>\n",
       "      <th>결과</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>C</td>\n",
       "      <td>하</td>\n",
       "      <td>[0,3)</td>\n",
       "      <td>0.170485</td>\n",
       "      <td>불합격</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78</td>\n",
       "      <td>F</td>\n",
       "      <td>중</td>\n",
       "      <td>[3,3.5)</td>\n",
       "      <td>0.700730</td>\n",
       "      <td>불합격</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78</td>\n",
       "      <td>F</td>\n",
       "      <td>중</td>\n",
       "      <td>[4,4.3]</td>\n",
       "      <td>0.067938</td>\n",
       "      <td>불합격</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>A</td>\n",
       "      <td>중</td>\n",
       "      <td>[3.5,4)</td>\n",
       "      <td>0.826251</td>\n",
       "      <td>합격</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97</td>\n",
       "      <td>C</td>\n",
       "      <td>하</td>\n",
       "      <td>[4,4.3]</td>\n",
       "      <td>0.591126</td>\n",
       "      <td>불합격</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   나이 수행평가1 수행평가2       학점      합격확률   결과\n",
       "0  19     C     하    [0,3)  0.170485  불합격\n",
       "1  78     F     중  [3,3.5)  0.700730  불합격\n",
       "2  78     F     중  [4,4.3]  0.067938  불합격\n",
       "3  23     A     중  [3.5,4)  0.826251   합격\n",
       "4  97     C     하  [4,4.3]  0.591126  불합격"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandasDF_spark = sparkDF2.toPandas()\n",
    "pandasDF_spark.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a699f7aa-4eb9-4a1e-9288-acf47db64fcc",
   "metadata": {},
   "source": [
    "#### pandas DataFrame을 spark DataFrame으로\n",
    "Apache Spark uses Apache Arrow which is an in-memory columnar format to transfer the data between Python and JVM. \n",
    "You need to enable to use Arrow as this is disabled by default and have Apache Arrow (PyArrow) install on all Spark cluster nodes using pip install pyspark[sql] or by directly downloading from Apache Arrow for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "642adf00-cf4e-4730-954e-88aa88fe4c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### !pip install pyarrow\n",
    "스파크.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\")\n",
    "## pandas 2.0 : AttributeError: 'DataFrame' object has no attribute 'iteritems'\n",
    "## -->  iteritems was removed in pandas 2.0 ==> pandas downgrade \n",
    "#sparkDF_pandas = 스파크.createDataFrame(df1csv)\n",
    "#sparkDF_pandas.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d51ad6-4d19-4620-9024-c6a5961cccc3",
   "metadata": {},
   "source": [
    "#### When an error occurs, \n",
    "Spark automatically fallback to non-Arrow optimization implementation, this can be controlled by spark.sql.execution.arrow.pyspark.fallback.enabled.\n",
    "```pythpn\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\",\"true\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aee244",
   "metadata": {},
   "source": [
    "### Pyspark vs Python\n",
    "- pyspark는 scala로 만든 spark의 python 버전\n",
    "- scala는 JVM object $\\rightarrow$ pyspark 또한 JVM object\n",
    "- python은 python object\n",
    "- python(pandas) $\\rightarrow$ py4J $\\rightarrow$ scala(pyspark)  : 오류가 자주 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12f4002-9269-4a2a-bd2e-017fa755b15e",
   "metadata": {},
   "source": [
    "#### Spark에서 \n",
    "- sparkDF.write.csv(\"경로\")\n",
    "- sparkDF.format('csv').save(\"경로\")\n",
    "- 기존 파일이 있는 경우 \n",
    "    - 덮어쓰기: sparkDF.write.mode('overwrite').csv(\"경로\")\n",
    "    - 추가하기: sparkDF.write.mode('append').csv(\"경로\")\n",
    "    - 무시하기: sparkDF.write.mode('ignore').csv(\"경로\")\n",
    "    - 오류발생: sparkDF.write.mode('error').csv(\"경로\")  $\\Leftarrow$ default    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eb46b4-005c-413d-8382-ad6b2fb66c52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sparkDF3.write.csv(\"hdfs://localhost:9000/Test/csv\")\n",
    "sparkDF3.write.parquet(\"hdfs://localhost:9000/Test/parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a394cbf-c74e-4204-96ce-4dc8fabc798a",
   "metadata": {},
   "source": [
    "### Pyspark 기본 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de261607-54bf-4b05-a667-b918db6e6188",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+------+-------+--------+-------+-------+--------+\n",
      "| id|gender|educ|jobcat| salary|salbegin|jobtime|prevexp|minority|\n",
      "+---+------+----+------+-------+--------+-------+-------+--------+\n",
      "|  1|  남성|  15|경영자|57000.0|   27000|     98|    144|      No|\n",
      "|  2|  남성|  16|사무직|40200.0|   18750|     98|     36|      No|\n",
      "|  3|  여성|  12|사무직|21450.0|   12000|     98|    381|      No|\n",
      "|  4|  여성|   8|사무직|21900.0|   13200|     98|    190|      No|\n",
      "|  5|  남성|  15|사무직|45000.0|   21000|     98|    138|      No|\n",
      "|  6|  남성|  15|사무직|32100.0|   13500|     98|     67|      No|\n",
      "|  7|  남성|  15|사무직|36000.0|   18750|     98|    114|      No|\n",
      "|  8|  여성|  12|사무직|21900.0|    9750|     98|      0|      No|\n",
      "|  9|  여성|  15|사무직|27900.0|   12750|     98|    115|      No|\n",
      "| 10|  여성|  12|사무직|24000.0|   13500|     98|    244|      No|\n",
      "+---+------+----+------+-------+--------+-------+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hdfs = 스파크.read.csv(\"hdfs://localhost:9000/Spark/Employee.csv\", header=True, encoding='cp949', inferSchema='true')\n",
    "df_hdfs.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "031fe264-27d1-45c4-8d33-8c9cf19c054d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+------+--------+--------+-------+-------+--------+\n",
      "| id|gender|educ|jobcat|  salary|salbegin|jobtime|prevexp|minority|\n",
      "+---+------+----+------+--------+--------+-------+-------+--------+\n",
      "|  1|  남성|  15|경영자| 57000.0|   27000|     98|    144|      No|\n",
      "| 18|  남성|  16|경영자|103750.0|   27510|     97|     70|      No|\n",
      "| 27|  남성|  19|경영자| 60375.0|   27480|     96|     96|      No|\n",
      "| 29|  남성|  19|경영자|135000.0|   79980|     96|    199|      No|\n",
      "| 32|  남성|  19|경영자|110625.0|   45000|     96|    120|      No|\n",
      "| 34|  남성|  19|경영자| 92000.0|   39990|     96|    175|      No|\n",
      "| 35|  남성|  17|경영자| 81250.0|   30000|     96|     18|      No|\n",
      "| 50|  남성|  16|경영자| 60000.0|   23730|     94|     59|      No|\n",
      "| 53|  남성|  18|경영자| 73750.0|   26250|     94|     56|      No|\n",
      "| 62|  남성|  16|경영자| 48000.0|   21750|     93|     22|      No|\n",
      "+---+------+----+------+--------+--------+-------+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hdfs.where('jobcat==\"경영자\"').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f3087e7-8216-47ec-9922-7e12339dfa78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+------+-------+--------+-------+-------+--------+------------------+------------------+\n",
      "| id|gender|educ|jobcat| salary|salbegin|jobtime|prevexp|minority|           Lsalary|          LBsalary|\n",
      "+---+------+----+------+-------+--------+-------+-------+--------+------------------+------------------+\n",
      "|  1|  남성|  15|경영자|57000.0|   27000|     98|    144|      No|10.950806546816688|10.203592144986466|\n",
      "|  2|  남성|  16|사무직|40200.0|   18750|     98|     36|      No|10.601622274607113| 9.838949031398556|\n",
      "|  3|  여성|  12|사무직|21450.0|   12000|     98|    381|      No| 9.973479924356162| 9.392661928770137|\n",
      "|  4|  여성|   8|사무직|21900.0|   13200|     98|    190|      No| 9.994241915804592| 9.487972108574462|\n",
      "|  5|  남성|  15|사무직|45000.0|   21000|     98|    138|      No|10.714417768752456|  9.95227771670556|\n",
      "+---+------+----+------+-------+--------+-------+-------+--------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, log, exp, when\n",
    "df_hdfs.withColumn(\"Lsalary\",log(\"salary\")).withColumn(\"LBsalary\",log(\"salbegin\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28333262-5fce-4714-9032-3be06489124e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|gender|jobcat|\n",
      "+------+------+\n",
      "|  남성|사무직|\n",
      "|  여성|사무직|\n",
      "|  여성|경영자|\n",
      "|  남성|경영자|\n",
      "|  남성|관리직|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hdfs.select([\"gender\",\"jobcat\"]).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ffa4ec78-c262-46c0-9b5f-3b03fb175af2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+------+-------+--------+-------+-------+--------+----+\n",
      "| id|gender|educ|jobcat| salary|salbegin|jobtime|prevexp|minority| Job|\n",
      "+---+------+----+------+-------+--------+-------+-------+--------+----+\n",
      "|  1|  남성|  15|경영자|57000.0|   27000|     98|    144|      No|임원|\n",
      "|  2|  남성|  16|사무직|40200.0|   18750|     98|     36|      No|사원|\n",
      "|  3|  여성|  12|사무직|21450.0|   12000|     98|    381|      No|사원|\n",
      "|  4|  여성|   8|사무직|21900.0|   13200|     98|    190|      No|사원|\n",
      "|  5|  남성|  15|사무직|45000.0|   21000|     98|    138|      No|사원|\n",
      "+---+------+----+------+-------+--------+-------+-------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hdfs.withColumn(\"Job\",when(col(\"jobcat\")==\"경영자\",\"임원\").otherwise(\"사원\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b77402-c7a8-4943-939d-45739cdfab09",
   "metadata": {},
   "source": [
    "### How to use SQL in Pyspark\n",
    "- DF: DSL(domain specific language)\n",
    "- Tables: pure SQL(Structured Query Language)\n",
    "- DF(DataFrame)으로부터 (temporary, permanant) table를 create할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccc9a3af-1c49-4359-a47f-50d23540b06f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\smu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pyspark\\sql\\dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_hdfs.registerTempTable(\"table1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1bd28b-3e6e-45a1-9a48-14390aaad8de",
   "metadata": {},
   "source": [
    "- Database 형태로 만드는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f26168d-888d-4916-af34-4c43ab5af073",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      27|\n",
      "|      84|\n",
      "|     363|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "스파크.sql(\"select count(*) from table1 group by jobcat\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05860fe6-3e51-4f3a-a872-92d00baa5d60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "스파크.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93605d59-7201-4b6e-b170-b58ba547e98f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|      db1|\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "스파크.sql(\"create database db1\")\n",
    "스파크.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "840450ff-1ca5-48aa-ac76-aa1a37d8606a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|         |   table1|       true|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hdfs.registerTempTable(\"table1\")\t\n",
    "스파크.sql(\"show tables in default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ed3106a-b29a-42a9-b792-b145fa811dd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_hdfs.registerTempTable(\"table1\")\n",
    "df_hdfs.write.saveAsTable(\"db1.permtable\")  \n",
    "## 현재 jupyter 실행 폴더의 spark-warehouse/db1.db/permtable에 저장\n",
    "## 기존 table이 존재하는 경우 오류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f17b95-90b6-40e6-8169-01d5257f2729",
   "metadata": {},
   "source": [
    "#### mode 변경: \n",
    "- 관리자권한\n",
    "- append, overwrite, error, errorifexists, ignore (default: error) 중 선택\n",
    "```Python\n",
    "df_hdfs.write.mode('overwrite').saveAsTable(\"db1.permtable\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0da0ac7-adfb-4aee-b9be-2c0db2daebb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|      db1|permtable|      false|\n",
      "|         |   table1|       true|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "스파크.sql(\"show tables in db1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5c0bd5-139e-4c91-9ae1-0b3a17e11eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
