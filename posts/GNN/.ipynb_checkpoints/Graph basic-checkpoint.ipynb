{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9386d3-8dd6-4f01-b738-407e305e1d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "fde9cb5b-c656-4e9f-9e72-12fa22fe1930",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"[GNN] Graph Basic\"\n",
    "author: \"김보람\"\n",
    "date: \"07/18/2023\"\n",
    "categories:\n",
    "  - GNN\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ee1ee1-b2ec-47dd-824e-aa430a539324",
   "metadata": {},
   "source": [
    "# ref\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67412480-e1d8-4a04-a681-1aef6e184c9f",
   "metadata": {},
   "source": [
    "- [Boostcourse](https://www.boostcourse.org/ai211/joinLectures/324261?isDesc=false)\n",
    "\n",
    "- https://kijungs.github.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725315cb-504e-420b-91eb-3482e3bbf625",
   "metadata": {},
   "source": [
    "# 1. 정점 표현 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59597d8-7178-4e86-8554-0e10fcc3e3a5",
   "metadata": {},
   "source": [
    "`-` 그래프의 정점(Node)를 벡터의 형태로 표현으로 정점 임베딩(Node Embedding)이라고도 부른다.\n",
    "\n",
    "- 입력: 그래프\n",
    "\n",
    "- 출력: 정점 u에 대한 임베딩, 즉 벡터 표현 $z_u$가 출력\n",
    "\n",
    "- 그래프에서 정점간 유사도를 임베딩 공간에서도 보존하는 것을 목표로 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d77e0-d31d-4bcd-acba-29d0ab22529e",
   "metadata": {},
   "source": [
    "`-` 유사도\n",
    "\n",
    "- 인접성/거리/경로/중첩/임의보행 기반 접근법\n",
    "\n",
    "$$similarity(u,v) \\approx z_v^Tz_u$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192bea50-4d4c-4fdc-9563-50e63c202fbf",
   "metadata": {},
   "source": [
    "`-` 변환식(Transductive) 방법\n",
    "\n",
    "- 학습의 결과로 정점의 임베딩 자체를 얻는다.\n",
    "\n",
    "- 한계1: 학습이 진행된 이후에 추가된 정점에 대해서 임베딩을 얻을 수 없다.\n",
    "\n",
    "- 한계2: 모든 정점에 대한 임베딩을 미리 계산해 저장해 두어야 한다.\n",
    "\n",
    "- 한계3: 정점이 속성(Attribute) 정보를 가진 경우에 활용이 어렵다.\n",
    "\n",
    "`-` 귀납식(Inductive) 방법\n",
    "\n",
    "- 정점을 임베딩으로 변환시키는 함수 $\\to$ 인코더를 얻는다.\n",
    "\n",
    "$$ENC(v) = z_v$$\n",
    "\n",
    "- 장점1: 학습이 진행된 이후에 추가된 정점에 대해서 임베딩을 얻을 수 있다.\n",
    "\n",
    "- 장점2: 모든 정점에 대한 임베딩을 미리 계산해 저장해 둘 필요가 없다\n",
    "\n",
    "- 장점3: 정점이 속성(Attribute) 정보를 가진 경우에 활용할 수 있다.\n",
    "\n",
    "- GNN: 대표적인 귀납식 임베딩 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7423af40-7e62-452f-8b96-041cea7843c4",
   "metadata": {},
   "source": [
    "# 2. 그래프 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353265cd-8c9c-4281-b43b-5315e11b7cae",
   "metadata": {},
   "source": [
    "## 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737fa167-45f9-4262-b602-b0e4bd696b42",
   "metadata": {},
   "source": [
    "이웃 정점들의 정보를 집계하는 과정을 반복하여 임베딩을 얻는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6bc97f-cd27-4c75-9a4f-684a0f966a0d",
   "metadata": {},
   "source": [
    "층(Layer)마다 임베딩을 얻는다.\n",
    "\n",
    "각 층에서는 이웃들의 이전 층 임베딩을 집계하여 새로운 임베딩을 얻는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb9473e-9542-455f-921f-36164c6e8d40",
   "metadata": {},
   "source": [
    "`-` 집계 함수\n",
    "\n",
    "- 이웃들 정보의 평균 계산\n",
    "\n",
    "- 신경망에 적용\n",
    "\n",
    "$$h_v^0 = x_v$$  \n",
    "\n",
    "$$h_v^k = \\sigma \\left( W_k \\sum_{u \\in N(v)} \\frac{h_u^{k-1}}{|N(v)|} + B_kh_v^{k-1} \\right), \\forall k >0$$\n",
    "\n",
    "$h_v^0 = x_v$: 0번 층에서 정점 v의 임베딩으로 정점 v의 속성 벡터로 초기화\n",
    "\n",
    "$h_v^k$: 현재 층, 즉 k번 층에서 정점 v의 임베딩\n",
    "\n",
    "$\\sigma$: 비선형 함수(ReLU, tanh 등)\n",
    "\n",
    "$\\sum_{u \\in N(v)} \\frac{h_u^{k-1}}{|N(v)|}$: 이전 층에서 이웃들의 임베딩에 대한 평균 계산\n",
    "\n",
    "$h_v^{k-1}$: 이전 층에서 정점 v의 임베딩\n",
    "\n",
    "- 마지막 층에서 정점 별 임베딩이 해당 정점의 출력 임베딩\n",
    "\n",
    "`-` 한계: 이웃들의 정보를 동일한 가중치로 평균을 낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036b8264-4af3-442e-a3e1-7edb8e8e631a",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31688fc7-f93d-49b5-b5aa-d0fce8bb9ccf",
   "metadata": {},
   "source": [
    "`-` 학습 변수(Trainable Parameter)는 층 별 신경망의 가중치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a56b731-1dd0-4c5b-8c4a-a993812a3186",
   "metadata": {},
   "source": [
    "- $W_k, B_k$: 학습 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db966c6-adae-4210-9785-9414973cf2d1",
   "metadata": {},
   "source": [
    "`-` 손실함수 결정: 정점간 거리 보존하는 것 목표\n",
    "\n",
    "인접성 기반 유사도 정의 $\\to$ 손실 함수\n",
    "\n",
    "$${\\cal L} = \\sum_{(u,v) \\in V \\times V} ||z_u^Tz_v - A_{u,v}||^2$$\n",
    "\n",
    "$\\cal L$: 비용함수\n",
    "\n",
    "$\\sum_{(u,v) \\in V \\times V}$:모든 정점 쌍에 대하여 합산\n",
    "\n",
    "$z_u^Tz_v$: 임베딩 공간에서의 유사도\n",
    "\n",
    "$A$: 그래프에서의 유사도 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2798f5-2ef3-4cba-ab7a-0b8424964c31",
   "metadata": {},
   "source": [
    "- 위의 내용은 분류를 하기 위한 전 단계이며, 분류(Classfier)를 하기 위한 손실함수는 따로 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee49f7ee-ab71-4e3f-ad39-6a87ee7bdc4c",
   "metadata": {},
   "source": [
    "$\\to$ 후속 과제(Downstream Task)의 손실함수를 이용한 **종단종(End-to-End) 학습**도 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c6f05b-efd4-448b-b446-afcdbc7c256f",
   "metadata": {},
   "source": [
    "분류기의 손실함수, 예를 들어 교차 엔트로피(Cross-Entropy)를, 전체 프로세스의 손실함수로 사용하여 종단종 학습을 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b6fcf9-37c5-4a6e-a663-1db73d271332",
   "metadata": {},
   "source": [
    "$${\\cal L} = \\sum_{v \\in V} y_v log(\\sigma(z_v^T \\theta)) + (1-y_v)log(1-\\sigma(z_v^T \\theta))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2d596e-05da-4d51-9b28-63946ad72948",
   "metadata": {},
   "source": [
    "$y_v$: 정점의 실제 유형(0 혹은 1)\n",
    "\n",
    "$z_v^T$: 정점의 임베딩\n",
    "\n",
    "$\\theta$: 분류기의 학습 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ffd1ae-edae-4df5-843a-7eafdf662020",
   "metadata": {},
   "source": [
    "![](image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d59da7f-36a6-404a-8eaf-ce530509da82",
   "metadata": {},
   "source": [
    "- 변환적 정점 임베딩 이후에 별도의 분류기를 학습하는 것보다 정확도가 대체로 높다.\n",
    "\n",
    "- 학습에 사용할 대상 정점을 결정하여 학습 데이터를 구성한다.\n",
    "\n",
    "- 오차역전파(Backpropagation)을 통해 손실함수를 최소화한다. 신경망의 학습 변수를 학습한다.\n",
    "\n",
    "- 학습된 신경망을 적용해, 학습에 사용되지 않은 정점의 임베딩을 얻는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43846b4-82b7-4870-9418-55c0ddfaa5a2",
   "metadata": {},
   "source": [
    "## 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1fdb05-e5d3-409b-8c77-2449cffff59f",
   "metadata": {},
   "source": [
    "- 학습 이후에 추가된 정점의 임베딩을 얻는다.\n",
    "\n",
    "EX) 온라인 소셜네트워크 등 실제 그래프들은 시간에 따라서 변화한다.\n",
    "\n",
    "- 학습된 그래프 신경망을, 새로운 그래프에 적용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7736b152-7f84-4a5b-9d3e-27dc1171d26a",
   "metadata": {},
   "source": [
    "# 3. 그래프 신경망 변형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222d0457-f401-43ec-898c-a4748be132de",
   "metadata": {},
   "source": [
    "## 그래프 합성곱 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e8c6a2-25eb-4b5f-b63c-ff9d83245c10",
   "metadata": {},
   "source": [
    "`-` 집계함수\n",
    "\n",
    "$$h_v^0 = x_v$$\n",
    "\n",
    "$$h_b^k = \\sigma \\left( W_k \\sum_{u \\in N(v) \\cup v} \\frac{h_u^{k-1}}{\\sqrt{|N(u)||N(v)|}} \\right), \\forall k \\in \\{1, \\dots, K \\}$$\n",
    "\n",
    "$$z_v = h_v^K$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5722a6-73e0-42a7-825c-1a94ba46ddfb",
   "metadata": {},
   "source": [
    "- 기존 집계 함수와 비교 했을 때 정규화 방법이 변화되었고, 동일 신경망 사용으로 학습 변수를 공유한다.\n",
    "\n",
    "`-` 한계: 단순히 연결성을 고려한 가중치로 평균을 낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9598ed4-eafe-4aea-90fa-80d48eca2bc3",
   "metadata": {},
   "source": [
    "## GraphSAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e06aa6-cfe6-4243-b50c-2e41b318af85",
   "metadata": {},
   "source": [
    "`-` 집계함수\n",
    "\n",
    "- 이웃들의 임베딩 AGG 함수를 이용해 합치고 자신의 임베딩과 연결(Concatenation)\n",
    "\n",
    "$$h_v^k = \\sigma([W_k \\cdot \\text{AGG} (\\{h_u^{k-1}, \\forall u \\in N(v)\\}), B_kh_v^{k-1}])$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdf4947-6681-4949-83b4-9158e5cc850b",
   "metadata": {},
   "source": [
    "`-` AGG 함수\n",
    "\n",
    "- MEAN: AGG = $\\sum_{u \\in N(v)} \\frac{h_u^{k-1}}{|N(v)|}$\n",
    "\n",
    "- Pool: AGG = $\\gamma(\\{\\text{Q}h_u^{k-1}, \\forall u \\in N(v)\\})$\n",
    "\n",
    "$\\gamma$: 원소별 최대\n",
    "\n",
    "- LSTM: AGG = $\\text{LSTM}([h_u^{k-1}, \\forall u \\in \\pi(N(v))])$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21e7c09-8e1c-4c37-b360-862e26702796",
   "metadata": {},
   "source": [
    "# 4. 합성곱 신경망과의 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3ae4b4-2f08-4843-8527-3c752f9273b0",
   "metadata": {},
   "source": [
    "`-` 합성곱 신경망과 그래프 신경망의 유사성\n",
    "\n",
    "- 모두 이웃의 정보를 집계하는 과정 반복\n",
    "\n",
    "- GCN은 이웃 픽셀의 정보를 집계하는 과정 반복\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564159db-29ec-4301-9081-74d533ffbb54",
   "metadata": {},
   "source": [
    "`-` 합성곱 신경망과 그래프 신경망의 차이\n",
    "\n",
    "- GCN은 이웃의 수가 균일하지만 그래프 신경망은 정점 별로 집계하는 이웃의 수가 다르다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ca7d75-7ae7-4e9f-b814-0b7f79133207",
   "metadata": {},
   "source": [
    "`-` 그래프에는 합성곱 신경망이 아닌 그래프 신경망을 적용해야 한다.\n",
    "\n",
    "- 합성곱 신경망이 주로 쓰이는 이미지는 인접 픽셀이 유용한 정보를 담고 있을 가능성이 높다.\n",
    "\n",
    "- 그래프의 인접 행렬에서의 인접 원소는 제한된 정보를 가지며 인접 행렬의 행, 열의 순서는 임의로 결정되는 경우가 많다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe51ec63-05ee-4696-98b1-380946e8718b",
   "metadata": {},
   "source": [
    "# 5. 그래프 어텐션 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa31d92-ff7c-4911-95fe-fdf40bab43ff",
   "metadata": {},
   "source": [
    "Graph Attention Networ, GAT: 가중치 자체도 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a835ad6d-7552-4439-abd0-47403ba9e046",
   "metadata": {},
   "source": [
    "실제 그래프에서 이웃 별로 미치는 영향이 다를 수 있기 때문에 가중치를 학습하기 위해서 셀프-어텐션(Self-Attention) 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171cd339-88f7-4919-970e-b9aee2273593",
   "metadata": {},
   "source": [
    "![](image2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229cbcf8-4dd0-447f-9a10-efb78afec7c6",
   "metadata": {},
   "source": [
    "`-` 각 층에서 정점 $i$로부터 이웃 $j$로의 가중치 $a_{ij}$ 계산법\n",
    "\n",
    "`1` 해당 층의 정점 $i$의 임베딩 $h_k$에 신경망 $W$를 곱해 새로운 임베딩을 얻는다.\n",
    "\n",
    "$\\tilde h_i = h_i W$\n",
    "\n",
    "`2` 정점 $i$와 $j$의 새로운 임베딩을 연결한 후, 어텐션 계수 $a$를 내적한다. $a$는 모든 정점이 공유하는 학습 변수이다.\n",
    "\n",
    "$e_{ij} = a^T [\\text{CONCAT}(\\tilde h_i, \\tilde h_j)]$\n",
    "\n",
    "`3` 2의 결과에 소프트맥스를 적용한다.\n",
    "\n",
    "$a_{ij} = \\text{softmax}_j(e_{ij}) = \\frac{\\text{exp}(e_{ij})}{\\sum_{K \\in N_i} \\text{exp}(e_{iK})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa060a5-9a54-4918-8ee8-ab22bb1bf525",
   "metadata": {},
   "source": [
    "여러 개의 어텐션을 동시에 학습한 뒤, 결과를 연결하여 사용 $\\to$ 멀티헤드 어텐션(Multi-head Attention)\n",
    "\n",
    "$$h_i' = \\text{CONCAT}_{1 \\leq k \\leq K} \\sigma \\left( \\sum_{j \\in N_i} a_{ij}^k h_j W_k \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a66e7d6-1143-4db4-b086-b51128f1016e",
   "metadata": {},
   "source": [
    "- 어텐션의 결과 정점 분류의 정확도가 향상"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46726ee4-ffb5-4470-983f-3e95a15cdad2",
   "metadata": {},
   "source": [
    "# 6. 그래프 표현 학습, 그래프 임베딩, 획일화 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bddf5f-3467-4798-a9fc-1f39d8554480",
   "metadata": {},
   "source": [
    "`-` 표현 학습, 임베딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec0190-60e6-4716-b824-4f2188a6ac9d",
   "metadata": {},
   "source": [
    "- 그래프 전체를 벡터의 형태로 표현\n",
    "\n",
    "- 그래프 임베딩은 벡터의 형태로 표현된 그래프 자체를 의미하기도 하며 그래프 분류 등에 활용\n",
    "\n",
    "- 그래프 풀링(Graph Pooling): 정점 임베딩들로부터 그래프 임베딩을 얻는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e70c8-3740-4f53-ab5f-2d93cf8c47ae",
   "metadata": {},
   "source": [
    "`-` 지나친 획일화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c710ea52-3a24-465f-9fcf-4b279c5ef000",
   "metadata": {},
   "source": [
    "- 지나친 획일화(Over-smoothing) 문제: 그래프 신경망의 층의 수가 증가하면서 정점으 ㅣ임베딩이 서로 유사해지는 현상으로 적은 수의 층으로도 다수의 정점에 의해 영향을 받는다.\n",
    "\n",
    "- 그래프 신경망의 층의 수를 늘렸을 때 후속 과제에서 정확도가 감소하는 현상 발견 \n",
    "\n",
    "- 대응: JK네트워크(Jumping Knowledge Network)는 마지막 층의 임메딩+모든 층의 임베딩 함께 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec752b-f7fe-4fde-87a2-b4b2b8d3b04c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
